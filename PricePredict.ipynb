{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DYrruwflcB2z",
        "FK3d79jdcGkE",
        "zOtsxwcNeDIh",
        "DXaxsVJ4T6v5",
        "28RCPiHK-VMS",
        "QxMPmlBOJZ0q"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "DYrruwflcB2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First , we load our data frame using pandas library."
      ],
      "metadata": {
        "id": "jPAwyn3tituY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hv9WhcI5a5xA",
        "outputId": "ff9bb24e-48bd-4498-9e2c-e157c47a66be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title            Author  \\\n",
              "0                   The Prisoner's Gold (The Hunters 3)    Chris Kuzneski   \n",
              "1                    Guru Dutt: A Tragedy in Three Acts      Arun Khopkar   \n",
              "2                          Leviathan (Penguin Classics)     Thomas Hobbes   \n",
              "3                    A Pocket Full of Rye (Miss Marple)   Agatha Christie   \n",
              "4            LIFE 70 Years of Extraordinary Photography   Editors of Life   \n",
              "...                                                 ...               ...   \n",
              "5694  Who Ordered This Truckload of Dung?: Inspiring...       Ajahn Brahm   \n",
              "5695              PostCapitalism: A Guide to Our Future        Paul Mason   \n",
              "5696                             The Great Zoo Of China    Matthew Reilly   \n",
              "5697                                            Engleby  Sebastian Faulks   \n",
              "5698  Only Dull People Are Brilliant at Breakfast (P...       Oscar Wilde   \n",
              "\n",
              "                      Edition             Reviews              Ratings  \\\n",
              "0     Paperback,– 10 Mar 2016  4.0 out of 5 stars   8 customer reviews   \n",
              "1      Paperback,– 7 Nov 2012  3.9 out of 5 stars  14 customer reviews   \n",
              "2     Paperback,– 25 Feb 1982  4.8 out of 5 stars   6 customer reviews   \n",
              "3      Paperback,– 5 Oct 2017  4.1 out of 5 stars  13 customer reviews   \n",
              "4     Hardcover,– 10 Oct 2006  5.0 out of 5 stars    1 customer review   \n",
              "...                       ...                 ...                  ...   \n",
              "5694  Paperback,– 30 Aug 2005  4.9 out of 5 stars   9 customer reviews   \n",
              "5695   Paperback,– 2 Jun 2016  4.1 out of 5 stars   2 customer reviews   \n",
              "5696  Paperback,– 14 Jan 2016  4.1 out of 5 stars  28 customer reviews   \n",
              "5697  Paperback,– 27 Mar 2008  1.0 out of 5 stars    1 customer review   \n",
              "5698   Paperback,– 3 Mar 2016  4.5 out of 5 stars   7 customer reviews   \n",
              "\n",
              "                                               Synopsis  \\\n",
              "0     THE HUNTERS return in their third brilliant no...   \n",
              "1     A layered portrait of a troubled genius for wh...   \n",
              "2     \"During the time men live without a common Pow...   \n",
              "3     A handful of grain is found in the pocket of a...   \n",
              "4     For seven decades, \"Life\" has been thrilling t...   \n",
              "...                                                 ...   \n",
              "5694  “Laugh your way to enlightenment” with this in...   \n",
              "5695  'The most important book about our economy and...   \n",
              "5696  The Chinese government has been keeping a secr...   \n",
              "5697  Mike Engleby has a secret...\\n\\nThis is the st...   \n",
              "5698  'It would be unfair to expect other people to ...   \n",
              "\n",
              "                             Genre                          BookCategory  \\\n",
              "0       Action & Adventure (Books)                    Action & Adventure   \n",
              "1       Cinema & Broadcast (Books)  Biographies, Diaries & True Accounts   \n",
              "2          International Relations                                Humour   \n",
              "3     Contemporary Fiction (Books)             Crime, Thriller & Mystery   \n",
              "4            Photography Textbooks              Arts, Film & Photography   \n",
              "...                            ...                                   ...   \n",
              "5694              Buddhism (Books)                                Humour   \n",
              "5695      Macroeconomics Textbooks                              Politics   \n",
              "5696    Action & Adventure (Books)             Crime, Thriller & Mystery   \n",
              "5697  Contemporary Fiction (Books)             Crime, Thriller & Mystery   \n",
              "5698                Essays (Books)                                Humour   \n",
              "\n",
              "        Price  \n",
              "0      220.00  \n",
              "1      202.93  \n",
              "2      299.00  \n",
              "3      180.00  \n",
              "4      965.62  \n",
              "...       ...  \n",
              "5694  1009.00  \n",
              "5695   781.00  \n",
              "5696   449.00  \n",
              "5697   108.00  \n",
              "5698    99.00  \n",
              "\n",
              "[5699 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7610f03d-5d22-47ef-a917-f1ea4f54d16a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Ratings</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Genre</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>4.0 out of 5 stars</td>\n",
              "      <td>8 customer reviews</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>3.9 out of 5 stars</td>\n",
              "      <td>14 customer reviews</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Cinema &amp; Broadcast (Books)</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>Paperback,– 25 Feb 1982</td>\n",
              "      <td>4.8 out of 5 stars</td>\n",
              "      <td>6 customer reviews</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>International Relations</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>Paperback,– 5 Oct 2017</td>\n",
              "      <td>4.1 out of 5 stars</td>\n",
              "      <td>13 customer reviews</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Contemporary Fiction (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>Hardcover,– 10 Oct 2006</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>1 customer review</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>Photography Textbooks</td>\n",
              "      <td>Arts, Film &amp; Photography</td>\n",
              "      <td>965.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>Who Ordered This Truckload of Dung?: Inspiring...</td>\n",
              "      <td>Ajahn Brahm</td>\n",
              "      <td>Paperback,– 30 Aug 2005</td>\n",
              "      <td>4.9 out of 5 stars</td>\n",
              "      <td>9 customer reviews</td>\n",
              "      <td>“Laugh your way to enlightenment” with this in...</td>\n",
              "      <td>Buddhism (Books)</td>\n",
              "      <td>Humour</td>\n",
              "      <td>1009.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>PostCapitalism: A Guide to Our Future</td>\n",
              "      <td>Paul Mason</td>\n",
              "      <td>Paperback,– 2 Jun 2016</td>\n",
              "      <td>4.1 out of 5 stars</td>\n",
              "      <td>2 customer reviews</td>\n",
              "      <td>'The most important book about our economy and...</td>\n",
              "      <td>Macroeconomics Textbooks</td>\n",
              "      <td>Politics</td>\n",
              "      <td>781.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>The Great Zoo Of China</td>\n",
              "      <td>Matthew Reilly</td>\n",
              "      <td>Paperback,– 14 Jan 2016</td>\n",
              "      <td>4.1 out of 5 stars</td>\n",
              "      <td>28 customer reviews</td>\n",
              "      <td>The Chinese government has been keeping a secr...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>449.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>Engleby</td>\n",
              "      <td>Sebastian Faulks</td>\n",
              "      <td>Paperback,– 27 Mar 2008</td>\n",
              "      <td>1.0 out of 5 stars</td>\n",
              "      <td>1 customer review</td>\n",
              "      <td>Mike Engleby has a secret...\\n\\nThis is the st...</td>\n",
              "      <td>Contemporary Fiction (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>108.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>Only Dull People Are Brilliant at Breakfast (P...</td>\n",
              "      <td>Oscar Wilde</td>\n",
              "      <td>Paperback,– 3 Mar 2016</td>\n",
              "      <td>4.5 out of 5 stars</td>\n",
              "      <td>7 customer reviews</td>\n",
              "      <td>'It would be unfair to expect other people to ...</td>\n",
              "      <td>Essays (Books)</td>\n",
              "      <td>Humour</td>\n",
              "      <td>99.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5699 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7610f03d-5d22-47ef-a917-f1ea4f54d16a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7610f03d-5d22-47ef-a917-f1ea4f54d16a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7610f03d-5d22-47ef-a917-f1ea4f54d16a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-74663acc-d4fe-47e6-a96e-4b2a1edcbb92\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-74663acc-d4fe-47e6-a96e-4b2a1edcbb92')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-74663acc-d4fe-47e6-a96e-4b2a1edcbb92 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('/content/drive/MyDrive/Data_Train.xlsx')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see our dataset isn't one of those large ones and it contains only 9 columns."
      ],
      "metadata": {
        "id": "hGzewheIi07M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ohSNzBVMb19-",
        "outputId": "a7643d9f-3f1f-4e0f-a711-fce73941717d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title          Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)  Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts    Arun Khopkar   \n",
              "2         Leviathan (Penguin Classics)   Thomas Hobbes   \n",
              "\n",
              "                   Edition             Reviews              Ratings  \\\n",
              "0  Paperback,– 10 Mar 2016  4.0 out of 5 stars   8 customer reviews   \n",
              "1   Paperback,– 7 Nov 2012  3.9 out of 5 stars  14 customer reviews   \n",
              "2  Paperback,– 25 Feb 1982  4.8 out of 5 stars   6 customer reviews   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "\n",
              "                        Genre                          BookCategory   Price  \n",
              "0  Action & Adventure (Books)                    Action & Adventure  220.00  \n",
              "1  Cinema & Broadcast (Books)  Biographies, Diaries & True Accounts  202.93  \n",
              "2     International Relations                                Humour  299.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fdb6845-2e47-4b14-80fe-0af91f732deb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Ratings</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Genre</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>4.0 out of 5 stars</td>\n",
              "      <td>8 customer reviews</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>3.9 out of 5 stars</td>\n",
              "      <td>14 customer reviews</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Cinema &amp; Broadcast (Books)</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>Paperback,– 25 Feb 1982</td>\n",
              "      <td>4.8 out of 5 stars</td>\n",
              "      <td>6 customer reviews</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>International Relations</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fdb6845-2e47-4b14-80fe-0af91f732deb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6fdb6845-2e47-4b14-80fe-0af91f732deb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6fdb6845-2e47-4b14-80fe-0af91f732deb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23a9eebd-c1ef-46c0-930d-b925123aa7e0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23a9eebd-c1ef-46c0-930d-b925123aa7e0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23a9eebd-c1ef-46c0-930d-b925123aa7e0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4oSaCqPRb45m",
        "outputId": "b56d4b61-94ea-4f12-8535-54d194a0def5"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title            Author  \\\n",
              "5696                             The Great Zoo Of China    Matthew Reilly   \n",
              "5697                                            Engleby  Sebastian Faulks   \n",
              "5698  Only Dull People Are Brilliant at Breakfast (P...       Oscar Wilde   \n",
              "\n",
              "                      Edition             Reviews              Ratings  \\\n",
              "5696  Paperback,– 14 Jan 2016  4.1 out of 5 stars  28 customer reviews   \n",
              "5697  Paperback,– 27 Mar 2008  1.0 out of 5 stars    1 customer review   \n",
              "5698   Paperback,– 3 Mar 2016  4.5 out of 5 stars   7 customer reviews   \n",
              "\n",
              "                                               Synopsis  \\\n",
              "5696  The Chinese government has been keeping a secr...   \n",
              "5697  Mike Engleby has a secret...\\n\\nThis is the st...   \n",
              "5698  'It would be unfair to expect other people to ...   \n",
              "\n",
              "                             Genre               BookCategory  Price  \n",
              "5696    Action & Adventure (Books)  Crime, Thriller & Mystery  449.0  \n",
              "5697  Contemporary Fiction (Books)  Crime, Thriller & Mystery  108.0  \n",
              "5698                Essays (Books)                     Humour   99.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-defe0ae5-cbee-4344-8963-fc20b1a2758a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Ratings</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Genre</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>The Great Zoo Of China</td>\n",
              "      <td>Matthew Reilly</td>\n",
              "      <td>Paperback,– 14 Jan 2016</td>\n",
              "      <td>4.1 out of 5 stars</td>\n",
              "      <td>28 customer reviews</td>\n",
              "      <td>The Chinese government has been keeping a secr...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>449.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>Engleby</td>\n",
              "      <td>Sebastian Faulks</td>\n",
              "      <td>Paperback,– 27 Mar 2008</td>\n",
              "      <td>1.0 out of 5 stars</td>\n",
              "      <td>1 customer review</td>\n",
              "      <td>Mike Engleby has a secret...\\n\\nThis is the st...</td>\n",
              "      <td>Contemporary Fiction (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>108.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>Only Dull People Are Brilliant at Breakfast (P...</td>\n",
              "      <td>Oscar Wilde</td>\n",
              "      <td>Paperback,– 3 Mar 2016</td>\n",
              "      <td>4.5 out of 5 stars</td>\n",
              "      <td>7 customer reviews</td>\n",
              "      <td>'It would be unfair to expect other people to ...</td>\n",
              "      <td>Essays (Books)</td>\n",
              "      <td>Humour</td>\n",
              "      <td>99.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-defe0ae5-cbee-4344-8963-fc20b1a2758a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-defe0ae5-cbee-4344-8963-fc20b1a2758a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-defe0ae5-cbee-4344-8963-fc20b1a2758a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b50c3e4c-3fe3-4a3f-9791-5c680b34040e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b50c3e4c-3fe3-4a3f-9791-5c680b34040e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b50c3e4c-3fe3-4a3f-9791-5c680b34040e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7cb6f0rbpT7",
        "outputId": "6d98ba18-8fc9-4ff7-c080-d6f391e61939"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5699, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get a list of our columns' names and later the type of each column to have a better mindset for  processing our data."
      ],
      "metadata": {
        "id": "CLcyZDZsjB1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H1S7IZLblNU",
        "outputId": "e49d2a3e-f9a1-4f3a-acbe-b1be071783b7"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Title', 'Author', 'Edition', 'Reviews', 'Ratings', 'Synopsis', 'Genre',\n",
              "       'BookCategory', 'Price'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXeqjKpabnpO",
        "outputId": "61e71141-b17e-45af-edbb-69c18497790b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Title            object\n",
              "Author           object\n",
              "Edition          object\n",
              "Reviews          object\n",
              "Ratings          object\n",
              "Synopsis         object\n",
              "Genre            object\n",
              "BookCategory     object\n",
              "Price           float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are many columns with the \"object\" type in our dataset , we need to figure out the number of unique values of each feature."
      ],
      "metadata": {
        "id": "xR2m2cDcjRQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mncBcgxebr8m",
        "outputId": "b9ffbe79-3b49-4d9d-e7b1-730dbd691a62"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Title           5130\n",
              "Author          3438\n",
              "Edition         3183\n",
              "Reviews           36\n",
              "Ratings          333\n",
              "Synopsis        5114\n",
              "Genre            335\n",
              "BookCategory      11\n",
              "Price           1538\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mXwa1LNtbub2",
        "outputId": "f54167ec-e186-464e-dcd5-0a8d968dd4c7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Price\n",
              "count   5699.000000\n",
              "mean     554.857428\n",
              "std      674.363427\n",
              "min       25.000000\n",
              "25%      249.000000\n",
              "50%      373.000000\n",
              "75%      599.000000\n",
              "max    14100.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8c2afdc-beac-4867-8ed7-13ec0773eac7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5699.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>554.857428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>674.363427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>25.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>249.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>373.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>599.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14100.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8c2afdc-beac-4867-8ed7-13ec0773eac7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8c2afdc-beac-4867-8ed7-13ec0773eac7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8c2afdc-beac-4867-8ed7-13ec0773eac7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b26a695-9728-432a-86bc-49a2694d5ec1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b26a695-9728-432a-86bc-49a2694d5ec1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b26a695-9728-432a-86bc-49a2694d5ec1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Null Values"
      ],
      "metadata": {
        "id": "FK3d79jdcGkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we get a copy of our dataframe to work with that. This will leave the original dataframe unchanged and therefore if we do anything wrong in the following steps , we can just come back and get another copy of the data frame."
      ],
      "metadata": {
        "id": "dOifbqyHjd2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_df = df.copy(deep=True)"
      ],
      "metadata": {
        "id": "1QQeAGmubzK2"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "na_cols=df.columns[null_df.isna().any()].tolist()\n",
        "na_cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfvnSaM-cs_W",
        "outputId": "a7076049-b267-4d21-e14d-b30cc8aada82"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_values=pd.DataFrame(null_df[na_cols].isna().sum(), columns=['Number of Null values'])\n",
        "null_values['Percentage of Null Values']=np.round(100*null_values['Number of Null values']/len(null_df),2)\n",
        "print(null_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRH8a04Hcu0F",
        "outputId": "b4d55c8f-a4cf-4225-ebd3-e49626aac050"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Number of Null values, Percentage of Null Values]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that there no null values in our dataset, due to the fact that none of the cells are empty. But we should keep that in my that there may be some missing values in our dataset that are filled with irrelevant data or only some parts of it are missing.\n",
        "like how in date the year could be missing but the whole value exists:\n",
        "\"-12-6\"."
      ],
      "metadata": {
        "id": "kQhMZoqTjyXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Categorical columns"
      ],
      "metadata": {
        "id": "zOtsxwcNeDIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the number of unique values of our categorical columns are quite large the simple methods like one-hot or binary encoding wouldn't work for encoding the categorical features.\n",
        "And we have to decide to use a new method for each column based on the feature's unique characteristics."
      ],
      "metadata": {
        "id": "CjjTgq13ke8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = null_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "UQ66gbjwc0hZ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ratings**"
      ],
      "metadata": {
        "id": "oejZTRJn4gXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = cat_df['Ratings'].values\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJBjoj8seP8l",
        "outputId": "e8af72aa-6511-4e06-e76a-7c37797b7a11"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['8 customer reviews', '14 customer reviews', '6 customer reviews',\n",
              "       ..., '28 customer reviews', '1 customer review',\n",
              "       '7 customer reviews'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings.tolist()"
      ],
      "metadata": {
        "id": "0hyAQeO94rCm"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD3P9Hxr41GB",
        "outputId": "5a123ad0-1aa8-4298-f74d-b729ab254c76"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8 customer reviews',\n",
              " '14 customer reviews',\n",
              " '6 customer reviews',\n",
              " '13 customer reviews',\n",
              " '1 customer review',\n",
              " '8 customer reviews',\n",
              " '72 customer reviews',\n",
              " '16 customer reviews',\n",
              " '111 customer reviews',\n",
              " '1 customer review',\n",
              " '132 customer reviews',\n",
              " '17 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '23 customer reviews',\n",
              " '76 customer reviews',\n",
              " '5 customer reviews',\n",
              " '10 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '10 customer reviews',\n",
              " '9 customer reviews',\n",
              " '1 customer review',\n",
              " '15 customer reviews',\n",
              " '34 customer reviews',\n",
              " '17 customer reviews',\n",
              " '9 customer reviews',\n",
              " '32 customer reviews',\n",
              " '2 customer reviews',\n",
              " '49 customer reviews',\n",
              " '49 customer reviews',\n",
              " '10 customer reviews',\n",
              " '8 customer reviews',\n",
              " '62 customer reviews',\n",
              " '61 customer reviews',\n",
              " '1 customer review',\n",
              " '8 customer reviews',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '18 customer reviews',\n",
              " '16 customer reviews',\n",
              " '6 customer reviews',\n",
              " '3 customer reviews',\n",
              " '2 customer reviews',\n",
              " '98 customer reviews',\n",
              " '12 customer reviews',\n",
              " '14 customer reviews',\n",
              " '3 customer reviews',\n",
              " '97 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '285 customer reviews',\n",
              " '29 customer reviews',\n",
              " '1 customer review',\n",
              " '27 customer reviews',\n",
              " '4 customer reviews',\n",
              " '267 customer reviews',\n",
              " '24 customer reviews',\n",
              " '7 customer reviews',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '146 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '95 customer reviews',\n",
              " '1 customer review',\n",
              " '234 customer reviews',\n",
              " '35 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '7 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '66 customer reviews',\n",
              " '1 customer review',\n",
              " '15 customer reviews',\n",
              " '8 customer reviews',\n",
              " '20 customer reviews',\n",
              " '39 customer reviews',\n",
              " '9 customer reviews',\n",
              " '6 customer reviews',\n",
              " '3 customer reviews',\n",
              " '7 customer reviews',\n",
              " '7 customer reviews',\n",
              " '12 customer reviews',\n",
              " '171 customer reviews',\n",
              " '13 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '9 customer reviews',\n",
              " '2 customer reviews',\n",
              " '7 customer reviews',\n",
              " '399 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '42 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '142 customer reviews',\n",
              " '4 customer reviews',\n",
              " '10 customer reviews',\n",
              " '62 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '15 customer reviews',\n",
              " '5 customer reviews',\n",
              " '6 customer reviews',\n",
              " '11 customer reviews',\n",
              " '20 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '839 customer reviews',\n",
              " '47 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '18 customer reviews',\n",
              " '1 customer review',\n",
              " '165 customer reviews',\n",
              " '30 customer reviews',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '53 customer reviews',\n",
              " '3 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '6 customer reviews',\n",
              " '32 customer reviews',\n",
              " '10 customer reviews',\n",
              " '14 customer reviews',\n",
              " '77 customer reviews',\n",
              " '16 customer reviews',\n",
              " '33 customer reviews',\n",
              " '35 customer reviews',\n",
              " '37 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '6 customer reviews',\n",
              " '8 customer reviews',\n",
              " '10 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '9 customer reviews',\n",
              " '6 customer reviews',\n",
              " '54 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '6 customer reviews',\n",
              " '1 customer review',\n",
              " '35 customer reviews',\n",
              " '13 customer reviews',\n",
              " '13 customer reviews',\n",
              " '1 customer review',\n",
              " '28 customer reviews',\n",
              " '7 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '23 customer reviews',\n",
              " '50 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '197 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '30 customer reviews',\n",
              " '1 customer review',\n",
              " '26 customer reviews',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '114 customer reviews',\n",
              " '32 customer reviews',\n",
              " '12 customer reviews',\n",
              " '1 customer review',\n",
              " '46 customer reviews',\n",
              " '15 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '72 customer reviews',\n",
              " '51 customer reviews',\n",
              " '1 customer review',\n",
              " '161 customer reviews',\n",
              " '3 customer reviews',\n",
              " '135 customer reviews',\n",
              " '26 customer reviews',\n",
              " '41 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '156 customer reviews',\n",
              " '15 customer reviews',\n",
              " '77 customer reviews',\n",
              " '1,416 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '49 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '93 customer reviews',\n",
              " '7 customer reviews',\n",
              " '3 customer reviews',\n",
              " '27 customer reviews',\n",
              " '1 customer review',\n",
              " '23 customer reviews',\n",
              " '221 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '36 customer reviews',\n",
              " '3 customer reviews',\n",
              " '6 customer reviews',\n",
              " '154 customer reviews',\n",
              " '3 customer reviews',\n",
              " '8 customer reviews',\n",
              " '74 customer reviews',\n",
              " '20 customer reviews',\n",
              " '10 customer reviews',\n",
              " '49 customer reviews',\n",
              " '14 customer reviews',\n",
              " '2 customer reviews',\n",
              " '24 customer reviews',\n",
              " '2 customer reviews',\n",
              " '7 customer reviews',\n",
              " '35 customer reviews',\n",
              " '1 customer review',\n",
              " '45 customer reviews',\n",
              " '6 customer reviews',\n",
              " '3 customer reviews',\n",
              " '11 customer reviews',\n",
              " '7 customer reviews',\n",
              " '13 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '41 customer reviews',\n",
              " '2 customer reviews',\n",
              " '20 customer reviews',\n",
              " '8 customer reviews',\n",
              " '2 customer reviews',\n",
              " '10 customer reviews',\n",
              " '4 customer reviews',\n",
              " '39 customer reviews',\n",
              " '1 customer review',\n",
              " '27 customer reviews',\n",
              " '1 customer review',\n",
              " '11 customer reviews',\n",
              " '3 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '240 customer reviews',\n",
              " '50 customer reviews',\n",
              " '62 customer reviews',\n",
              " '1 customer review',\n",
              " '55 customer reviews',\n",
              " '4 customer reviews',\n",
              " '9 customer reviews',\n",
              " '2 customer reviews',\n",
              " '55 customer reviews',\n",
              " '1 customer review',\n",
              " '19 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '17 customer reviews',\n",
              " '7 customer reviews',\n",
              " '26 customer reviews',\n",
              " '4 customer reviews',\n",
              " '7 customer reviews',\n",
              " '37 customer reviews',\n",
              " '48 customer reviews',\n",
              " '15 customer reviews',\n",
              " '404 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '8 customer reviews',\n",
              " '8 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '29 customer reviews',\n",
              " '245 customer reviews',\n",
              " '11 customer reviews',\n",
              " '35 customer reviews',\n",
              " '8 customer reviews',\n",
              " '37 customer reviews',\n",
              " '5 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '45 customer reviews',\n",
              " '5 customer reviews',\n",
              " '14 customer reviews',\n",
              " '6 customer reviews',\n",
              " '62 customer reviews',\n",
              " '35 customer reviews',\n",
              " '2 customer reviews',\n",
              " '9 customer reviews',\n",
              " '3 customer reviews',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '22 customer reviews',\n",
              " '2 customer reviews',\n",
              " '26 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '11 customer reviews',\n",
              " '26 customer reviews',\n",
              " '18 customer reviews',\n",
              " '36 customer reviews',\n",
              " '5 customer reviews',\n",
              " '62 customer reviews',\n",
              " '7 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '10 customer reviews',\n",
              " '292 customer reviews',\n",
              " '5 customer reviews',\n",
              " '14 customer reviews',\n",
              " '1 customer review',\n",
              " '25 customer reviews',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '6 customer reviews',\n",
              " '9 customer reviews',\n",
              " '75 customer reviews',\n",
              " '12 customer reviews',\n",
              " '381 customer reviews',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '18 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '44 customer reviews',\n",
              " '32 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '17 customer reviews',\n",
              " '3 customer reviews',\n",
              " '8 customer reviews',\n",
              " '11 customer reviews',\n",
              " '1 customer review',\n",
              " '27 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '443 customer reviews',\n",
              " '425 customer reviews',\n",
              " '16 customer reviews',\n",
              " '10 customer reviews',\n",
              " '8 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '2 customer reviews',\n",
              " '21 customer reviews',\n",
              " '238 customer reviews',\n",
              " '165 customer reviews',\n",
              " '15 customer reviews',\n",
              " '8 customer reviews',\n",
              " '84 customer reviews',\n",
              " '68 customer reviews',\n",
              " '1 customer review',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '20 customer reviews',\n",
              " '1 customer review',\n",
              " '9 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '9 customer reviews',\n",
              " '33 customer reviews',\n",
              " '2 customer reviews',\n",
              " '9 customer reviews',\n",
              " '11 customer reviews',\n",
              " '1 customer review',\n",
              " '108 customer reviews',\n",
              " '7 customer reviews',\n",
              " '25 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '16 customer reviews',\n",
              " '17 customer reviews',\n",
              " '8 customer reviews',\n",
              " '23 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '74 customer reviews',\n",
              " '7 customer reviews',\n",
              " '12 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '4 customer reviews',\n",
              " '32 customer reviews',\n",
              " '52 customer reviews',\n",
              " '28 customer reviews',\n",
              " '15 customer reviews',\n",
              " '57 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '109 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '6 customer reviews',\n",
              " '9 customer reviews',\n",
              " '17 customer reviews',\n",
              " '12 customer reviews',\n",
              " '22 customer reviews',\n",
              " '32 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '89 customer reviews',\n",
              " '1 customer review',\n",
              " '16 customer reviews',\n",
              " '8 customer reviews',\n",
              " '37 customer reviews',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '4 customer reviews',\n",
              " '42 customer reviews',\n",
              " '3 customer reviews',\n",
              " '45 customer reviews',\n",
              " '304 customer reviews',\n",
              " '18 customer reviews',\n",
              " '19 customer reviews',\n",
              " '96 customer reviews',\n",
              " '5 customer reviews',\n",
              " '4 customer reviews',\n",
              " '22 customer reviews',\n",
              " '419 customer reviews',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '3 customer reviews',\n",
              " '12 customer reviews',\n",
              " '205 customer reviews',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '8 customer reviews',\n",
              " '15 customer reviews',\n",
              " '4 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '85 customer reviews',\n",
              " '6 customer reviews',\n",
              " '4 customer reviews',\n",
              " '68 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '9 customer reviews',\n",
              " '960 customer reviews',\n",
              " '6 customer reviews',\n",
              " '71 customer reviews',\n",
              " '5 customer reviews',\n",
              " '37 customer reviews',\n",
              " '167 customer reviews',\n",
              " '14 customer reviews',\n",
              " '8 customer reviews',\n",
              " '11 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '58 customer reviews',\n",
              " '32 customer reviews',\n",
              " '11 customer reviews',\n",
              " '9 customer reviews',\n",
              " '27 customer reviews',\n",
              " '5 customer reviews',\n",
              " '19 customer reviews',\n",
              " '1 customer review',\n",
              " '12 customer reviews',\n",
              " '20 customer reviews',\n",
              " '50 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '33 customer reviews',\n",
              " '1 customer review',\n",
              " '37 customer reviews',\n",
              " '33 customer reviews',\n",
              " '40 customer reviews',\n",
              " '15 customer reviews',\n",
              " '4 customer reviews',\n",
              " '102 customer reviews',\n",
              " '2 customer reviews',\n",
              " '9 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '42 customer reviews',\n",
              " '7 customer reviews',\n",
              " '20 customer reviews',\n",
              " '76 customer reviews',\n",
              " '16 customer reviews',\n",
              " '42 customer reviews',\n",
              " '17 customer reviews',\n",
              " '101 customer reviews',\n",
              " '1 customer review',\n",
              " '20 customer reviews',\n",
              " '27 customer reviews',\n",
              " '48 customer reviews',\n",
              " '3 customer reviews',\n",
              " '8 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '74 customer reviews',\n",
              " '85 customer reviews',\n",
              " '65 customer reviews',\n",
              " '23 customer reviews',\n",
              " '1 customer review',\n",
              " '9 customer reviews',\n",
              " '3 customer reviews',\n",
              " '13 customer reviews',\n",
              " '6 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '70 customer reviews',\n",
              " '74 customer reviews',\n",
              " '7 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '6 customer reviews',\n",
              " '19 customer reviews',\n",
              " '9 customer reviews',\n",
              " '2 customer reviews',\n",
              " '16 customer reviews',\n",
              " '2 customer reviews',\n",
              " '13 customer reviews',\n",
              " '6 customer reviews',\n",
              " '8 customer reviews',\n",
              " '12 customer reviews',\n",
              " '106 customer reviews',\n",
              " '6 customer reviews',\n",
              " '1 customer review',\n",
              " '74 customer reviews',\n",
              " '1 customer review',\n",
              " '6 customer reviews',\n",
              " '21 customer reviews',\n",
              " '29 customer reviews',\n",
              " '9 customer reviews',\n",
              " '1 customer review',\n",
              " '11 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '76 customer reviews',\n",
              " '3 customer reviews',\n",
              " '28 customer reviews',\n",
              " '2 customer reviews',\n",
              " '146 customer reviews',\n",
              " '7 customer reviews',\n",
              " '62 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '6 customer reviews',\n",
              " '1 customer review',\n",
              " '922 customer reviews',\n",
              " '31 customer reviews',\n",
              " '11 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '30 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '8 customer reviews',\n",
              " '38 customer reviews',\n",
              " '1 customer review',\n",
              " '13 customer reviews',\n",
              " '4 customer reviews',\n",
              " '5 customer reviews',\n",
              " '22 customer reviews',\n",
              " '1 customer review',\n",
              " '63 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '23 customer reviews',\n",
              " '12 customer reviews',\n",
              " '2 customer reviews',\n",
              " '49 customer reviews',\n",
              " '9 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '30 customer reviews',\n",
              " '11 customer reviews',\n",
              " '3 customer reviews',\n",
              " '6 customer reviews',\n",
              " '8 customer reviews',\n",
              " '1 customer review',\n",
              " '42 customer reviews',\n",
              " '1 customer review',\n",
              " '13 customer reviews',\n",
              " '6 customer reviews',\n",
              " '19 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '8 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '75 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '13 customer reviews',\n",
              " '12 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '177 customer reviews',\n",
              " '32 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '138 customer reviews',\n",
              " '7 customer reviews',\n",
              " '55 customer reviews',\n",
              " '2 customer reviews',\n",
              " '6 customer reviews',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '52 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '164 customer reviews',\n",
              " '28 customer reviews',\n",
              " '23 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '97 customer reviews',\n",
              " '9 customer reviews',\n",
              " '19 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '257 customer reviews',\n",
              " '207 customer reviews',\n",
              " '48 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '16 customer reviews',\n",
              " '7 customer reviews',\n",
              " '11 customer reviews',\n",
              " '3 customer reviews',\n",
              " '2 customer reviews',\n",
              " '16 customer reviews',\n",
              " '74 customer reviews',\n",
              " '2 customer reviews',\n",
              " '95 customer reviews',\n",
              " '1 customer review',\n",
              " '55 customer reviews',\n",
              " '11 customer reviews',\n",
              " '17 customer reviews',\n",
              " '64 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '13 customer reviews',\n",
              " '4 customer reviews',\n",
              " '55 customer reviews',\n",
              " '1 customer review',\n",
              " '234 customer reviews',\n",
              " '3 customer reviews',\n",
              " '25 customer reviews',\n",
              " '6 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '7 customer reviews',\n",
              " '3 customer reviews',\n",
              " '28 customer reviews',\n",
              " '27 customer reviews',\n",
              " '17 customer reviews',\n",
              " '1 customer review',\n",
              " '13 customer reviews',\n",
              " '4 customer reviews',\n",
              " '9 customer reviews',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '9 customer reviews',\n",
              " '8 customer reviews',\n",
              " '14 customer reviews',\n",
              " '3 customer reviews',\n",
              " '10 customer reviews',\n",
              " '56 customer reviews',\n",
              " '196 customer reviews',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '82 customer reviews',\n",
              " '19 customer reviews',\n",
              " '8 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '4 customer reviews',\n",
              " '1,558 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '8 customer reviews',\n",
              " '1 customer review',\n",
              " '51 customer reviews',\n",
              " '138 customer reviews',\n",
              " '15 customer reviews',\n",
              " '567 customer reviews',\n",
              " '18 customer reviews',\n",
              " '22 customer reviews',\n",
              " '114 customer reviews',\n",
              " '6 customer reviews',\n",
              " '22 customer reviews',\n",
              " '19 customer reviews',\n",
              " '1 customer review',\n",
              " '11 customer reviews',\n",
              " '31 customer reviews',\n",
              " '55 customer reviews',\n",
              " '70 customer reviews',\n",
              " '31 customer reviews',\n",
              " '8 customer reviews',\n",
              " '10 customer reviews',\n",
              " '60 customer reviews',\n",
              " '74 customer reviews',\n",
              " '37 customer reviews',\n",
              " '1 customer review',\n",
              " '16 customer reviews',\n",
              " '2 customer reviews',\n",
              " '30 customer reviews',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '53 customer reviews',\n",
              " '8 customer reviews',\n",
              " '80 customer reviews',\n",
              " '6 customer reviews',\n",
              " '5 customer reviews',\n",
              " '15 customer reviews',\n",
              " '7 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '7 customer reviews',\n",
              " '303 customer reviews',\n",
              " '73 customer reviews',\n",
              " '1 customer review',\n",
              " '11 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '22 customer reviews',\n",
              " '66 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '9 customer reviews',\n",
              " '10 customer reviews',\n",
              " '54 customer reviews',\n",
              " '193 customer reviews',\n",
              " '2 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '16 customer reviews',\n",
              " '155 customer reviews',\n",
              " '4 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '798 customer reviews',\n",
              " '128 customer reviews',\n",
              " '1 customer review',\n",
              " '36 customer reviews',\n",
              " '2 customer reviews',\n",
              " '5 customer reviews',\n",
              " '4 customer reviews',\n",
              " '87 customer reviews',\n",
              " '2 customer reviews',\n",
              " '2 customer reviews',\n",
              " '49 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '7 customer reviews',\n",
              " '3 customer reviews',\n",
              " '6 customer reviews',\n",
              " '1 customer review',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '90 customer reviews',\n",
              " '33 customer reviews',\n",
              " '9 customer reviews',\n",
              " '15 customer reviews',\n",
              " '1 customer review',\n",
              " '286 customer reviews',\n",
              " '18 customer reviews',\n",
              " '8 customer reviews',\n",
              " '5 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '97 customer reviews',\n",
              " '2 customer reviews',\n",
              " '161 customer reviews',\n",
              " '1 customer review',\n",
              " '16 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '84 customer reviews',\n",
              " '20 customer reviews',\n",
              " '9 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '10 customer reviews',\n",
              " '4 customer reviews',\n",
              " '55 customer reviews',\n",
              " '2 customer reviews',\n",
              " '7 customer reviews',\n",
              " '3 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '63 customer reviews',\n",
              " '2 customer reviews',\n",
              " '8 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '108 customer reviews',\n",
              " '13 customer reviews',\n",
              " '2 customer reviews',\n",
              " '240 customer reviews',\n",
              " '2 customer reviews',\n",
              " '565 customer reviews',\n",
              " '4 customer reviews',\n",
              " '3 customer reviews',\n",
              " '10 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '137 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '21 customer reviews',\n",
              " '5 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '9 customer reviews',\n",
              " '12 customer reviews',\n",
              " '2 customer reviews',\n",
              " '99 customer reviews',\n",
              " '1 customer review',\n",
              " '10 customer reviews',\n",
              " '1 customer review',\n",
              " '20 customer reviews',\n",
              " '26 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '17 customer reviews',\n",
              " '2 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '37 customer reviews',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '403 customer reviews',\n",
              " '1 customer review',\n",
              " '38 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '277 customer reviews',\n",
              " '15 customer reviews',\n",
              " '6 customer reviews',\n",
              " '2 customer reviews',\n",
              " '8 customer reviews',\n",
              " '2 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '16 customer reviews',\n",
              " '2 customer reviews',\n",
              " '95 customer reviews',\n",
              " '2 customer reviews',\n",
              " '5 customer reviews',\n",
              " '3 customer reviews',\n",
              " '3 customer reviews',\n",
              " '28 customer reviews',\n",
              " '2 customer reviews',\n",
              " '20 customer reviews',\n",
              " '9 customer reviews',\n",
              " '2 customer reviews',\n",
              " '902 customer reviews',\n",
              " '52 customer reviews',\n",
              " '5 customer reviews',\n",
              " '9 customer reviews',\n",
              " '1 customer review',\n",
              " '1 customer review',\n",
              " '3 customer reviews',\n",
              " '7 customer reviews',\n",
              " '19 customer reviews',\n",
              " '6 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '59 customer reviews',\n",
              " '8 customer reviews',\n",
              " '5 customer reviews',\n",
              " '13 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '7 customer reviews',\n",
              " '5 customer reviews',\n",
              " '11 customer reviews',\n",
              " '47 customer reviews',\n",
              " '101 customer reviews',\n",
              " '1 customer review',\n",
              " '25 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '861 customer reviews',\n",
              " '22 customer reviews',\n",
              " '7 customer reviews',\n",
              " '19 customer reviews',\n",
              " '67 customer reviews',\n",
              " '25 customer reviews',\n",
              " '815 customer reviews',\n",
              " '4 customer reviews',\n",
              " '2 customer reviews',\n",
              " '17 customer reviews',\n",
              " '1 customer review',\n",
              " '19 customer reviews',\n",
              " '3 customer reviews',\n",
              " '12 customer reviews',\n",
              " '13 customer reviews',\n",
              " '14 customer reviews',\n",
              " '39 customer reviews',\n",
              " '1 customer review',\n",
              " '13 customer reviews',\n",
              " '10 customer reviews',\n",
              " '13 customer reviews',\n",
              " '20 customer reviews',\n",
              " '3 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '1 customer review',\n",
              " '5 customer reviews',\n",
              " '1 customer review',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '13 customer reviews',\n",
              " '11 customer reviews',\n",
              " '5 customer reviews',\n",
              " '87 customer reviews',\n",
              " '2 customer reviews',\n",
              " '1 customer review',\n",
              " '4 customer reviews',\n",
              " '4 customer reviews',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_ratings  =[]\n",
        "for i in ratings:\n",
        "   r = i.split(\" \")[0]\n",
        "   num_ratings.append(r)\n",
        "num_ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5VjYl43424Q",
        "outputId": "de75688e-f905-4e5f-d1b5-ca0b02049c8f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8',\n",
              " '14',\n",
              " '6',\n",
              " '13',\n",
              " '1',\n",
              " '8',\n",
              " '72',\n",
              " '16',\n",
              " '111',\n",
              " '1',\n",
              " '132',\n",
              " '17',\n",
              " '4',\n",
              " '3',\n",
              " '5',\n",
              " '2',\n",
              " '1',\n",
              " '23',\n",
              " '76',\n",
              " '5',\n",
              " '10',\n",
              " '2',\n",
              " '2',\n",
              " '10',\n",
              " '9',\n",
              " '1',\n",
              " '15',\n",
              " '34',\n",
              " '17',\n",
              " '9',\n",
              " '32',\n",
              " '2',\n",
              " '49',\n",
              " '49',\n",
              " '10',\n",
              " '8',\n",
              " '62',\n",
              " '61',\n",
              " '1',\n",
              " '8',\n",
              " '7',\n",
              " '5',\n",
              " '18',\n",
              " '16',\n",
              " '6',\n",
              " '3',\n",
              " '2',\n",
              " '98',\n",
              " '12',\n",
              " '14',\n",
              " '3',\n",
              " '97',\n",
              " '1',\n",
              " '7',\n",
              " '2',\n",
              " '2',\n",
              " '7',\n",
              " '5',\n",
              " '285',\n",
              " '29',\n",
              " '1',\n",
              " '27',\n",
              " '4',\n",
              " '267',\n",
              " '24',\n",
              " '7',\n",
              " '6',\n",
              " '2',\n",
              " '146',\n",
              " '1',\n",
              " '4',\n",
              " '1',\n",
              " '95',\n",
              " '1',\n",
              " '234',\n",
              " '35',\n",
              " '3',\n",
              " '5',\n",
              " '7',\n",
              " '2',\n",
              " '4',\n",
              " '66',\n",
              " '1',\n",
              " '15',\n",
              " '8',\n",
              " '20',\n",
              " '39',\n",
              " '9',\n",
              " '6',\n",
              " '3',\n",
              " '7',\n",
              " '7',\n",
              " '12',\n",
              " '171',\n",
              " '13',\n",
              " '2',\n",
              " '2',\n",
              " '9',\n",
              " '2',\n",
              " '7',\n",
              " '399',\n",
              " '1',\n",
              " '7',\n",
              " '42',\n",
              " '2',\n",
              " '2',\n",
              " '2',\n",
              " '142',\n",
              " '4',\n",
              " '10',\n",
              " '62',\n",
              " '3',\n",
              " '1',\n",
              " '1',\n",
              " '15',\n",
              " '5',\n",
              " '6',\n",
              " '11',\n",
              " '20',\n",
              " '4',\n",
              " '1',\n",
              " '839',\n",
              " '47',\n",
              " '5',\n",
              " '1',\n",
              " '3',\n",
              " '18',\n",
              " '1',\n",
              " '165',\n",
              " '30',\n",
              " '7',\n",
              " '5',\n",
              " '53',\n",
              " '3',\n",
              " '2',\n",
              " '1',\n",
              " '6',\n",
              " '32',\n",
              " '10',\n",
              " '14',\n",
              " '77',\n",
              " '16',\n",
              " '33',\n",
              " '35',\n",
              " '37',\n",
              " '3',\n",
              " '4',\n",
              " '6',\n",
              " '8',\n",
              " '10',\n",
              " '2',\n",
              " '1',\n",
              " '9',\n",
              " '6',\n",
              " '54',\n",
              " '1',\n",
              " '2',\n",
              " '6',\n",
              " '1',\n",
              " '35',\n",
              " '13',\n",
              " '13',\n",
              " '1',\n",
              " '28',\n",
              " '7',\n",
              " '2',\n",
              " '2',\n",
              " '23',\n",
              " '50',\n",
              " '2',\n",
              " '4',\n",
              " '197',\n",
              " '2',\n",
              " '4',\n",
              " '30',\n",
              " '1',\n",
              " '26',\n",
              " '5',\n",
              " '3',\n",
              " '114',\n",
              " '32',\n",
              " '12',\n",
              " '1',\n",
              " '46',\n",
              " '15',\n",
              " '1',\n",
              " '1',\n",
              " '72',\n",
              " '51',\n",
              " '1',\n",
              " '161',\n",
              " '3',\n",
              " '135',\n",
              " '26',\n",
              " '41',\n",
              " '1',\n",
              " '1',\n",
              " '156',\n",
              " '15',\n",
              " '77',\n",
              " '1,416',\n",
              " '4',\n",
              " '1',\n",
              " '3',\n",
              " '49',\n",
              " '3',\n",
              " '4',\n",
              " '2',\n",
              " '93',\n",
              " '7',\n",
              " '3',\n",
              " '27',\n",
              " '1',\n",
              " '23',\n",
              " '221',\n",
              " '5',\n",
              " '2',\n",
              " '36',\n",
              " '3',\n",
              " '6',\n",
              " '154',\n",
              " '3',\n",
              " '8',\n",
              " '74',\n",
              " '20',\n",
              " '10',\n",
              " '49',\n",
              " '14',\n",
              " '2',\n",
              " '24',\n",
              " '2',\n",
              " '7',\n",
              " '35',\n",
              " '1',\n",
              " '45',\n",
              " '6',\n",
              " '3',\n",
              " '11',\n",
              " '7',\n",
              " '13',\n",
              " '5',\n",
              " '1',\n",
              " '41',\n",
              " '2',\n",
              " '20',\n",
              " '8',\n",
              " '2',\n",
              " '10',\n",
              " '4',\n",
              " '39',\n",
              " '1',\n",
              " '27',\n",
              " '1',\n",
              " '11',\n",
              " '3',\n",
              " '3',\n",
              " '1',\n",
              " '240',\n",
              " '50',\n",
              " '62',\n",
              " '1',\n",
              " '55',\n",
              " '4',\n",
              " '9',\n",
              " '2',\n",
              " '55',\n",
              " '1',\n",
              " '19',\n",
              " '3',\n",
              " '5',\n",
              " '2',\n",
              " '2',\n",
              " '4',\n",
              " '17',\n",
              " '7',\n",
              " '26',\n",
              " '4',\n",
              " '7',\n",
              " '37',\n",
              " '48',\n",
              " '15',\n",
              " '404',\n",
              " '2',\n",
              " '1',\n",
              " '8',\n",
              " '8',\n",
              " '1',\n",
              " '3',\n",
              " '3',\n",
              " '1',\n",
              " '7',\n",
              " '29',\n",
              " '245',\n",
              " '11',\n",
              " '35',\n",
              " '8',\n",
              " '37',\n",
              " '5',\n",
              " '5',\n",
              " '1',\n",
              " '45',\n",
              " '5',\n",
              " '14',\n",
              " '6',\n",
              " '62',\n",
              " '35',\n",
              " '2',\n",
              " '9',\n",
              " '3',\n",
              " '10',\n",
              " '1',\n",
              " '22',\n",
              " '2',\n",
              " '26',\n",
              " '1',\n",
              " '3',\n",
              " '11',\n",
              " '26',\n",
              " '18',\n",
              " '36',\n",
              " '5',\n",
              " '62',\n",
              " '7',\n",
              " '2',\n",
              " '3',\n",
              " '10',\n",
              " '292',\n",
              " '5',\n",
              " '14',\n",
              " '1',\n",
              " '25',\n",
              " '7',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '6',\n",
              " '9',\n",
              " '75',\n",
              " '12',\n",
              " '381',\n",
              " '10',\n",
              " '1',\n",
              " '18',\n",
              " '1',\n",
              " '2',\n",
              " '44',\n",
              " '32',\n",
              " '3',\n",
              " '1',\n",
              " '17',\n",
              " '3',\n",
              " '8',\n",
              " '11',\n",
              " '1',\n",
              " '27',\n",
              " '3',\n",
              " '5',\n",
              " '443',\n",
              " '425',\n",
              " '16',\n",
              " '10',\n",
              " '8',\n",
              " '1',\n",
              " '3',\n",
              " '2',\n",
              " '21',\n",
              " '238',\n",
              " '165',\n",
              " '15',\n",
              " '8',\n",
              " '84',\n",
              " '68',\n",
              " '1',\n",
              " '6',\n",
              " '2',\n",
              " '4',\n",
              " '3',\n",
              " '20',\n",
              " '1',\n",
              " '9',\n",
              " '2',\n",
              " '4',\n",
              " '3',\n",
              " '9',\n",
              " '33',\n",
              " '2',\n",
              " '9',\n",
              " '11',\n",
              " '1',\n",
              " '108',\n",
              " '7',\n",
              " '25',\n",
              " '1',\n",
              " '5',\n",
              " '16',\n",
              " '17',\n",
              " '8',\n",
              " '23',\n",
              " '2',\n",
              " '1',\n",
              " '7',\n",
              " '10',\n",
              " '1',\n",
              " '74',\n",
              " '7',\n",
              " '12',\n",
              " '1',\n",
              " '5',\n",
              " '4',\n",
              " '1',\n",
              " '7',\n",
              " '5',\n",
              " '4',\n",
              " '32',\n",
              " '52',\n",
              " '28',\n",
              " '15',\n",
              " '57',\n",
              " '1',\n",
              " '3',\n",
              " '109',\n",
              " '1',\n",
              " '2',\n",
              " '6',\n",
              " '9',\n",
              " '17',\n",
              " '12',\n",
              " '22',\n",
              " '32',\n",
              " '2',\n",
              " '1',\n",
              " '7',\n",
              " '1',\n",
              " '89',\n",
              " '1',\n",
              " '16',\n",
              " '8',\n",
              " '37',\n",
              " '5',\n",
              " '3',\n",
              " '5',\n",
              " '4',\n",
              " '42',\n",
              " '3',\n",
              " '45',\n",
              " '304',\n",
              " '18',\n",
              " '19',\n",
              " '96',\n",
              " '5',\n",
              " '4',\n",
              " '22',\n",
              " '419',\n",
              " '10',\n",
              " '1',\n",
              " '3',\n",
              " '3',\n",
              " '12',\n",
              " '205',\n",
              " '6',\n",
              " '2',\n",
              " '10',\n",
              " '1',\n",
              " '8',\n",
              " '15',\n",
              " '4',\n",
              " '4',\n",
              " '1',\n",
              " '3',\n",
              " '1',\n",
              " '85',\n",
              " '6',\n",
              " '4',\n",
              " '68',\n",
              " '1',\n",
              " '1',\n",
              " '10',\n",
              " '1',\n",
              " '7',\n",
              " '9',\n",
              " '960',\n",
              " '6',\n",
              " '71',\n",
              " '5',\n",
              " '37',\n",
              " '167',\n",
              " '14',\n",
              " '8',\n",
              " '11',\n",
              " '2',\n",
              " '2',\n",
              " '3',\n",
              " '58',\n",
              " '32',\n",
              " '11',\n",
              " '9',\n",
              " '27',\n",
              " '5',\n",
              " '19',\n",
              " '1',\n",
              " '12',\n",
              " '20',\n",
              " '50',\n",
              " '1',\n",
              " '7',\n",
              " '1',\n",
              " '3',\n",
              " '33',\n",
              " '1',\n",
              " '37',\n",
              " '33',\n",
              " '40',\n",
              " '15',\n",
              " '4',\n",
              " '102',\n",
              " '2',\n",
              " '9',\n",
              " '4',\n",
              " '2',\n",
              " '42',\n",
              " '7',\n",
              " '20',\n",
              " '76',\n",
              " '16',\n",
              " '42',\n",
              " '17',\n",
              " '101',\n",
              " '1',\n",
              " '20',\n",
              " '27',\n",
              " '48',\n",
              " '3',\n",
              " '8',\n",
              " '4',\n",
              " '3',\n",
              " '74',\n",
              " '85',\n",
              " '65',\n",
              " '23',\n",
              " '1',\n",
              " '9',\n",
              " '3',\n",
              " '13',\n",
              " '6',\n",
              " '4',\n",
              " '1',\n",
              " '70',\n",
              " '74',\n",
              " '7',\n",
              " '2',\n",
              " '2',\n",
              " '3',\n",
              " '1',\n",
              " '4',\n",
              " '6',\n",
              " '19',\n",
              " '9',\n",
              " '2',\n",
              " '16',\n",
              " '2',\n",
              " '13',\n",
              " '6',\n",
              " '8',\n",
              " '12',\n",
              " '106',\n",
              " '6',\n",
              " '1',\n",
              " '74',\n",
              " '1',\n",
              " '6',\n",
              " '21',\n",
              " '29',\n",
              " '9',\n",
              " '1',\n",
              " '11',\n",
              " '3',\n",
              " '1',\n",
              " '76',\n",
              " '3',\n",
              " '28',\n",
              " '2',\n",
              " '146',\n",
              " '7',\n",
              " '62',\n",
              " '1',\n",
              " '3',\n",
              " '5',\n",
              " '6',\n",
              " '1',\n",
              " '922',\n",
              " '31',\n",
              " '11',\n",
              " '1',\n",
              " '1',\n",
              " '30',\n",
              " '1',\n",
              " '4',\n",
              " '3',\n",
              " '8',\n",
              " '38',\n",
              " '1',\n",
              " '13',\n",
              " '4',\n",
              " '5',\n",
              " '22',\n",
              " '1',\n",
              " '63',\n",
              " '4',\n",
              " '1',\n",
              " '23',\n",
              " '12',\n",
              " '2',\n",
              " '49',\n",
              " '9',\n",
              " '1',\n",
              " '1',\n",
              " '30',\n",
              " '11',\n",
              " '3',\n",
              " '6',\n",
              " '8',\n",
              " '1',\n",
              " '42',\n",
              " '1',\n",
              " '13',\n",
              " '6',\n",
              " '19',\n",
              " '4',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '2',\n",
              " '8',\n",
              " '2',\n",
              " '4',\n",
              " '75',\n",
              " '1',\n",
              " '2',\n",
              " '13',\n",
              " '12',\n",
              " '3',\n",
              " '4',\n",
              " '1',\n",
              " '177',\n",
              " '32',\n",
              " '1',\n",
              " '4',\n",
              " '138',\n",
              " '7',\n",
              " '55',\n",
              " '2',\n",
              " '6',\n",
              " '7',\n",
              " '5',\n",
              " '2',\n",
              " '2',\n",
              " '2',\n",
              " '1',\n",
              " '52',\n",
              " '2',\n",
              " '1',\n",
              " '164',\n",
              " '28',\n",
              " '23',\n",
              " '1',\n",
              " '5',\n",
              " '5',\n",
              " '1',\n",
              " '97',\n",
              " '9',\n",
              " '19',\n",
              " '1',\n",
              " '1',\n",
              " '257',\n",
              " '207',\n",
              " '48',\n",
              " '3',\n",
              " '1',\n",
              " '1',\n",
              " '5',\n",
              " '3',\n",
              " '16',\n",
              " '7',\n",
              " '11',\n",
              " '3',\n",
              " '2',\n",
              " '16',\n",
              " '74',\n",
              " '2',\n",
              " '95',\n",
              " '1',\n",
              " '55',\n",
              " '11',\n",
              " '17',\n",
              " '64',\n",
              " '1',\n",
              " '4',\n",
              " '13',\n",
              " '4',\n",
              " '55',\n",
              " '1',\n",
              " '234',\n",
              " '3',\n",
              " '25',\n",
              " '6',\n",
              " '3',\n",
              " '1',\n",
              " '7',\n",
              " '3',\n",
              " '28',\n",
              " '27',\n",
              " '17',\n",
              " '1',\n",
              " '13',\n",
              " '4',\n",
              " '9',\n",
              " '7',\n",
              " '1',\n",
              " '5',\n",
              " '3',\n",
              " '9',\n",
              " '8',\n",
              " '14',\n",
              " '3',\n",
              " '10',\n",
              " '56',\n",
              " '196',\n",
              " '6',\n",
              " '2',\n",
              " '82',\n",
              " '19',\n",
              " '8',\n",
              " '3',\n",
              " '4',\n",
              " '4',\n",
              " '1,558',\n",
              " '2',\n",
              " '4',\n",
              " '8',\n",
              " '1',\n",
              " '51',\n",
              " '138',\n",
              " '15',\n",
              " '567',\n",
              " '18',\n",
              " '22',\n",
              " '114',\n",
              " '6',\n",
              " '22',\n",
              " '19',\n",
              " '1',\n",
              " '11',\n",
              " '31',\n",
              " '55',\n",
              " '70',\n",
              " '31',\n",
              " '8',\n",
              " '10',\n",
              " '60',\n",
              " '74',\n",
              " '37',\n",
              " '1',\n",
              " '16',\n",
              " '2',\n",
              " '30',\n",
              " '7',\n",
              " '1',\n",
              " '1',\n",
              " '53',\n",
              " '8',\n",
              " '80',\n",
              " '6',\n",
              " '5',\n",
              " '15',\n",
              " '7',\n",
              " '1',\n",
              " '5',\n",
              " '3',\n",
              " '7',\n",
              " '303',\n",
              " '73',\n",
              " '1',\n",
              " '11',\n",
              " '3',\n",
              " '4',\n",
              " '22',\n",
              " '66',\n",
              " '1',\n",
              " '1',\n",
              " '9',\n",
              " '10',\n",
              " '54',\n",
              " '193',\n",
              " '2',\n",
              " '5',\n",
              " '2',\n",
              " '16',\n",
              " '155',\n",
              " '4',\n",
              " '4',\n",
              " '2',\n",
              " '3',\n",
              " '798',\n",
              " '128',\n",
              " '1',\n",
              " '36',\n",
              " '2',\n",
              " '5',\n",
              " '4',\n",
              " '87',\n",
              " '2',\n",
              " '2',\n",
              " '49',\n",
              " '1',\n",
              " '5',\n",
              " '7',\n",
              " '3',\n",
              " '6',\n",
              " '1',\n",
              " '6',\n",
              " '2',\n",
              " '90',\n",
              " '33',\n",
              " '9',\n",
              " '15',\n",
              " '1',\n",
              " '286',\n",
              " '18',\n",
              " '8',\n",
              " '5',\n",
              " '5',\n",
              " '2',\n",
              " '97',\n",
              " '2',\n",
              " '161',\n",
              " '1',\n",
              " '16',\n",
              " '1',\n",
              " '3',\n",
              " '2',\n",
              " '1',\n",
              " '3',\n",
              " '84',\n",
              " '20',\n",
              " '9',\n",
              " '5',\n",
              " '1',\n",
              " '4',\n",
              " '10',\n",
              " '4',\n",
              " '55',\n",
              " '2',\n",
              " '7',\n",
              " '3',\n",
              " '4',\n",
              " '3',\n",
              " '63',\n",
              " '2',\n",
              " '8',\n",
              " '2',\n",
              " '1',\n",
              " '4',\n",
              " '108',\n",
              " '13',\n",
              " '2',\n",
              " '240',\n",
              " '2',\n",
              " '565',\n",
              " '4',\n",
              " '3',\n",
              " '10',\n",
              " '5',\n",
              " '2',\n",
              " '1',\n",
              " '3',\n",
              " '137',\n",
              " '5',\n",
              " '1',\n",
              " '2',\n",
              " '5',\n",
              " '1',\n",
              " '21',\n",
              " '5',\n",
              " '2',\n",
              " '3',\n",
              " '5',\n",
              " '1',\n",
              " '9',\n",
              " '12',\n",
              " '2',\n",
              " '99',\n",
              " '1',\n",
              " '10',\n",
              " '1',\n",
              " '20',\n",
              " '26',\n",
              " '1',\n",
              " '3',\n",
              " '17',\n",
              " '2',\n",
              " '4',\n",
              " '2',\n",
              " '37',\n",
              " '1',\n",
              " '3',\n",
              " '1',\n",
              " '403',\n",
              " '1',\n",
              " '38',\n",
              " '1',\n",
              " '1',\n",
              " '4',\n",
              " '1',\n",
              " '277',\n",
              " '15',\n",
              " '6',\n",
              " '2',\n",
              " '8',\n",
              " '2',\n",
              " '3',\n",
              " '1',\n",
              " '16',\n",
              " '2',\n",
              " '95',\n",
              " '2',\n",
              " '5',\n",
              " '3',\n",
              " '3',\n",
              " '28',\n",
              " '2',\n",
              " '20',\n",
              " '9',\n",
              " '2',\n",
              " '902',\n",
              " '52',\n",
              " '5',\n",
              " '9',\n",
              " '1',\n",
              " '1',\n",
              " '3',\n",
              " '7',\n",
              " '19',\n",
              " '6',\n",
              " '1',\n",
              " '2',\n",
              " '59',\n",
              " '8',\n",
              " '5',\n",
              " '13',\n",
              " '1',\n",
              " '2',\n",
              " '1',\n",
              " '2',\n",
              " '7',\n",
              " '5',\n",
              " '11',\n",
              " '47',\n",
              " '101',\n",
              " '1',\n",
              " '25',\n",
              " '4',\n",
              " '2',\n",
              " '861',\n",
              " '22',\n",
              " '7',\n",
              " '19',\n",
              " '67',\n",
              " '25',\n",
              " '815',\n",
              " '4',\n",
              " '2',\n",
              " '17',\n",
              " '1',\n",
              " '19',\n",
              " '3',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '39',\n",
              " '1',\n",
              " '13',\n",
              " '10',\n",
              " '13',\n",
              " '20',\n",
              " '3',\n",
              " '1',\n",
              " '4',\n",
              " '1',\n",
              " '5',\n",
              " '1',\n",
              " '2',\n",
              " '1',\n",
              " '13',\n",
              " '11',\n",
              " '5',\n",
              " '87',\n",
              " '2',\n",
              " '1',\n",
              " '4',\n",
              " '4',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['New_Ratings'] = num_ratings"
      ],
      "metadata": {
        "id": "5SG-NWwQI8dW"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['New_Ratings'] = pd.to_numeric(cat_df['New_Ratings'].str.replace(',', ''), errors='coerce')"
      ],
      "metadata": {
        "id": "PrEOCTuXJOEI"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['New_Ratings'] = pd.to_numeric(cat_df['New_Ratings'], errors='coerce')"
      ],
      "metadata": {
        "id": "sI0Ii4x2Hm-x"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reviews**"
      ],
      "metadata": {
        "id": "-VE5yKUu5vKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = cat_df['Reviews'].values.tolist()\n",
        "reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV5mD1oQ5tQY",
        "outputId": "40c0a058-2349-4360-9da3-9aad50827d58"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '2.6 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '2.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '2.9 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '2.9 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '2.5 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '2.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '2.7 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '2.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '2.8 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '2.8 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '1.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '2.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '2.7 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.9 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '2.9 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '2.7 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.6 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '2.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '3.3 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '1.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '3.7 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.4 out of 5 stars',\n",
              " '4.8 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.4 out of 5 stars',\n",
              " '1.5 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.0 out of 5 stars',\n",
              " '4.3 out of 5 stars',\n",
              " '4.1 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '3.9 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '4.7 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '2.0 out of 5 stars',\n",
              " '4.2 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '4.5 out of 5 stars',\n",
              " '4.6 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " '5.0 out of 5 stars',\n",
              " '3.5 out of 5 stars',\n",
              " '3.1 out of 5 stars',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_reviews = []\n",
        "for i in reviews:\n",
        "  r = i.split(\" \")[0]\n",
        "  new_reviews.append(r)"
      ],
      "metadata": {
        "id": "bnw3NcUu55y6"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaPSAswk6Gsc",
        "outputId": "d85c3d4c-3a64-45a9-ad8c-8a5527dad338"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4.0',\n",
              " '3.9',\n",
              " '4.8',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '4.7',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '4.9',\n",
              " '3.5',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.9',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.9',\n",
              " '5.0',\n",
              " '3.1',\n",
              " '3.1',\n",
              " '4.8',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.9',\n",
              " '4.4',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '4.4',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '3.8',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.8',\n",
              " '4.8',\n",
              " '3.8',\n",
              " '4.4',\n",
              " '2.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '4.7',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.0',\n",
              " '3.2',\n",
              " '4.0',\n",
              " '4.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.5',\n",
              " '3.2',\n",
              " '4.6',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.8',\n",
              " '2.6',\n",
              " '4.9',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.5',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '3.2',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '2.8',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '3.4',\n",
              " '2.9',\n",
              " '3.9',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.7',\n",
              " '4.7',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '3.9',\n",
              " '3.5',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '3.7',\n",
              " '3.3',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '3.7',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '4.8',\n",
              " '4.3',\n",
              " '4.0',\n",
              " '4.5',\n",
              " '4.2',\n",
              " '4.6',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '4.9',\n",
              " '3.8',\n",
              " '3.0',\n",
              " '4.3',\n",
              " '3.7',\n",
              " '4.5',\n",
              " '4.5',\n",
              " '3.7',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.7',\n",
              " '4.4',\n",
              " '4.2',\n",
              " '3.7',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '3.0',\n",
              " '4.5',\n",
              " '4.8',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.3',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '3.0',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.7',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.7',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.5',\n",
              " '4.1',\n",
              " '3.3',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '3.0',\n",
              " '3.8',\n",
              " '4.5',\n",
              " '4.8',\n",
              " '5.0',\n",
              " '3.3',\n",
              " '4.6',\n",
              " '4.8',\n",
              " '3.9',\n",
              " '4.0',\n",
              " '3.4',\n",
              " '4.4',\n",
              " '4.6',\n",
              " '3.3',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '3.9',\n",
              " '4.4',\n",
              " '4.6',\n",
              " '4.4',\n",
              " '4.3',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.7',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.0',\n",
              " '2.9',\n",
              " '3.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '3.4',\n",
              " '3.5',\n",
              " '2.0',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '4.5',\n",
              " '3.3',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.7',\n",
              " '4.6',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '4.7',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '3.9',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '2.5',\n",
              " '3.8',\n",
              " '4.0',\n",
              " '4.8',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '4.5',\n",
              " '4.1',\n",
              " '4.0',\n",
              " '4.4',\n",
              " '1.0',\n",
              " '4.5',\n",
              " '4.5',\n",
              " '3.5',\n",
              " '4.2',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '3.0',\n",
              " '3.0',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '4.4',\n",
              " '3.7',\n",
              " '4.7',\n",
              " '3.5',\n",
              " '3.8',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '3.6',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '3.4',\n",
              " '5.0',\n",
              " '3.0',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '3.6',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '3.6',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.0',\n",
              " '3.9',\n",
              " '2.9',\n",
              " '4.0',\n",
              " '3.6',\n",
              " '5.0',\n",
              " '3.1',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '3.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.6',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '4.5',\n",
              " '4.1',\n",
              " '2.7',\n",
              " '4.3',\n",
              " '3.0',\n",
              " '4.2',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.6',\n",
              " '2.0',\n",
              " '4.3',\n",
              " '4.1',\n",
              " '3.8',\n",
              " '4.0',\n",
              " '4.8',\n",
              " '4.1',\n",
              " '3.9',\n",
              " '4.4',\n",
              " '4.1',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.7',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.2',\n",
              " '3.6',\n",
              " '3.8',\n",
              " '3.6',\n",
              " '4.5',\n",
              " '4.8',\n",
              " '4.3',\n",
              " '2.0',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.8',\n",
              " '4.5',\n",
              " '4.9',\n",
              " '4.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '2.0',\n",
              " '4.5',\n",
              " '3.0',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '4.1',\n",
              " '4.3',\n",
              " '4.7',\n",
              " '4.8',\n",
              " '3.8',\n",
              " '4.8',\n",
              " '4.7',\n",
              " '4.3',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '4.4',\n",
              " '3.7',\n",
              " '4.2',\n",
              " '1.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.9',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '3.9',\n",
              " '4.4',\n",
              " '4.3',\n",
              " '4.1',\n",
              " '4.8',\n",
              " '4.1',\n",
              " '4.7',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '3.6',\n",
              " '2.5',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '3.5',\n",
              " '4.7',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '4.1',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.6',\n",
              " '3.0',\n",
              " '3.8',\n",
              " '4.6',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '4.5',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '3.2',\n",
              " '5.0',\n",
              " '3.8',\n",
              " '3.8',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.9',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.7',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.6',\n",
              " '4.3',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '4.9',\n",
              " '4.6',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '3.8',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.3',\n",
              " '5.0',\n",
              " '3.7',\n",
              " '4.6',\n",
              " '3.9',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.1',\n",
              " '4.3',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '3.5',\n",
              " '4.8',\n",
              " '5.0',\n",
              " '3.2',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.8',\n",
              " '4.1',\n",
              " '4.3',\n",
              " '1.0',\n",
              " '4.3',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '4.1',\n",
              " '4.2',\n",
              " '4.8',\n",
              " '2.8',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.7',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '4.1',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '3.8',\n",
              " '1.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.8',\n",
              " '3.0',\n",
              " '4.9',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.3',\n",
              " '3.1',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '3.8',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '3.8',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '2.8',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '3.1',\n",
              " '4.3',\n",
              " '3.7',\n",
              " '4.0',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '3.0',\n",
              " '3.5',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '4.0',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '4.2',\n",
              " '4.5',\n",
              " '3.3',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.4',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '4.7',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '3.2',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.8',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '1.5',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '3.0',\n",
              " '4.7',\n",
              " '4.9',\n",
              " '4.4',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '3.6',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '3.9',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '3.6',\n",
              " '4.3',\n",
              " '3.2',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.8',\n",
              " '4.1',\n",
              " '4.0',\n",
              " '3.7',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '4.7',\n",
              " '4.1',\n",
              " '4.4',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.1',\n",
              " '4.5',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.8',\n",
              " '4.4',\n",
              " '4.3',\n",
              " '3.9',\n",
              " '4.0',\n",
              " '4.0',\n",
              " '4.3',\n",
              " '4.1',\n",
              " '4.1',\n",
              " '4.9',\n",
              " '5.0',\n",
              " '4.3',\n",
              " '4.3',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '4.8',\n",
              " '4.5',\n",
              " '4.7',\n",
              " '4.4',\n",
              " '3.7',\n",
              " '4.2',\n",
              " '1.0',\n",
              " '4.1',\n",
              " '4.5',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '3.0',\n",
              " '1.0',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '4.8',\n",
              " '4.2',\n",
              " '4.6',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '3.7',\n",
              " '3.3',\n",
              " '4.3',\n",
              " '3.6',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '3.6',\n",
              " '4.9',\n",
              " '4.0',\n",
              " '4.2',\n",
              " '4.5',\n",
              " '3.7',\n",
              " '4.6',\n",
              " '3.8',\n",
              " '4.2',\n",
              " '3.1',\n",
              " '3.3',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.1',\n",
              " '2.5',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '4.3',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '3.9',\n",
              " '4.3',\n",
              " '3.0',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.7',\n",
              " '4.2',\n",
              " '4.2',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '4.5',\n",
              " '4.7',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '3.1',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '2.7',\n",
              " '4.6',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.9',\n",
              " '4.1',\n",
              " '4.1',\n",
              " '2.9',\n",
              " '4.8',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.2',\n",
              " '2.7',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.6',\n",
              " '4.1',\n",
              " '4.0',\n",
              " '4.5',\n",
              " '3.9',\n",
              " '4.5',\n",
              " '4.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '4.8',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '3.6',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '4.0',\n",
              " '3.8',\n",
              " '3.9',\n",
              " '4.6',\n",
              " '4.2',\n",
              " '4.0',\n",
              " '2.9',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '3.0',\n",
              " '3.4',\n",
              " '4.5',\n",
              " '3.5',\n",
              " '3.3',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '3.0',\n",
              " '4.4',\n",
              " '5.0',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '4.7',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '3.5',\n",
              " '4.1',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.0',\n",
              " '3.8',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '4.4',\n",
              " '3.8',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '5.0',\n",
              " '1.0',\n",
              " '4.4',\n",
              " '4.1',\n",
              " '3.4',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '4.5',\n",
              " '4.7',\n",
              " '3.1',\n",
              " '4.0',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.5',\n",
              " '5.0',\n",
              " '3.4',\n",
              " '4.7',\n",
              " '3.9',\n",
              " '4.6',\n",
              " '4.0',\n",
              " '4.2',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.4',\n",
              " '3.7',\n",
              " '4.7',\n",
              " '4.3',\n",
              " '4.4',\n",
              " '4.8',\n",
              " '4.5',\n",
              " '3.4',\n",
              " '1.5',\n",
              " '4.2',\n",
              " '3.0',\n",
              " '4.1',\n",
              " '4.2',\n",
              " '4.3',\n",
              " '4.6',\n",
              " '4.1',\n",
              " '4.5',\n",
              " '3.0',\n",
              " '4.3',\n",
              " '4.1',\n",
              " '4.5',\n",
              " '3.9',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '4.7',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '5.0',\n",
              " '5.0',\n",
              " '2.0',\n",
              " '4.2',\n",
              " '4.6',\n",
              " '4.5',\n",
              " '4.6',\n",
              " '3.1',\n",
              " '5.0',\n",
              " '3.5',\n",
              " '3.1',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df[\"New_Reviews\"] = new_reviews\n",
        "cat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ewrp-LnS6IYz",
        "outputId": "79e2d798-d8d2-45c8-8e98-44c0dae15803"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life   \n",
              "\n",
              "                   Edition             Reviews              Ratings  \\\n",
              "0  Paperback,– 10 Mar 2016  4.0 out of 5 stars   8 customer reviews   \n",
              "1   Paperback,– 7 Nov 2012  3.9 out of 5 stars  14 customer reviews   \n",
              "2  Paperback,– 25 Feb 1982  4.8 out of 5 stars   6 customer reviews   \n",
              "3   Paperback,– 5 Oct 2017  4.1 out of 5 stars  13 customer reviews   \n",
              "4  Hardcover,– 10 Oct 2006  5.0 out of 5 stars    1 customer review   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "3  A handful of grain is found in the pocket of a...   \n",
              "4  For seven decades, \"Life\" has been thrilling t...   \n",
              "\n",
              "                          Genre                          BookCategory   Price  \\\n",
              "0    Action & Adventure (Books)                    Action & Adventure  220.00   \n",
              "1    Cinema & Broadcast (Books)  Biographies, Diaries & True Accounts  202.93   \n",
              "2       International Relations                                Humour  299.00   \n",
              "3  Contemporary Fiction (Books)             Crime, Thriller & Mystery  180.00   \n",
              "4         Photography Textbooks              Arts, Film & Photography  965.62   \n",
              "\n",
              "   New_Ratings New_Reviews  \n",
              "0            8         4.0  \n",
              "1           14         3.9  \n",
              "2            6         4.8  \n",
              "3           13         4.1  \n",
              "4            1         5.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d59c37ca-1c6d-4701-8725-e26055eaba4e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Reviews</th>\n",
              "      <th>Ratings</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Genre</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>4.0 out of 5 stars</td>\n",
              "      <td>8 customer reviews</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>3.9 out of 5 stars</td>\n",
              "      <td>14 customer reviews</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Cinema &amp; Broadcast (Books)</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>Paperback,– 25 Feb 1982</td>\n",
              "      <td>4.8 out of 5 stars</td>\n",
              "      <td>6 customer reviews</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>International Relations</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>Paperback,– 5 Oct 2017</td>\n",
              "      <td>4.1 out of 5 stars</td>\n",
              "      <td>13 customer reviews</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Contemporary Fiction (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>Hardcover,– 10 Oct 2006</td>\n",
              "      <td>5.0 out of 5 stars</td>\n",
              "      <td>1 customer review</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>Photography Textbooks</td>\n",
              "      <td>Arts, Film &amp; Photography</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d59c37ca-1c6d-4701-8725-e26055eaba4e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d59c37ca-1c6d-4701-8725-e26055eaba4e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d59c37ca-1c6d-4701-8725-e26055eaba4e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-771e2135-2c03-434b-89af-182611281e40\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-771e2135-2c03-434b-89af-182611281e40')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-771e2135-2c03-434b-89af-182611281e40 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['New_Reviews'] = pd.to_numeric(cat_df['New_Reviews'], errors='coerce')"
      ],
      "metadata": {
        "id": "CG5zSWP2HvrW"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop([\"Ratings\",'Reviews'],axis=1)\n",
        "cat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ffHKC9LT6Q0k",
        "outputId": "e844f4cd-b9cb-4b4c-8791-72a785301bf9"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life   \n",
              "\n",
              "                   Edition                                           Synopsis  \\\n",
              "0  Paperback,– 10 Mar 2016  THE HUNTERS return in their third brilliant no...   \n",
              "1   Paperback,– 7 Nov 2012  A layered portrait of a troubled genius for wh...   \n",
              "2  Paperback,– 25 Feb 1982  \"During the time men live without a common Pow...   \n",
              "3   Paperback,– 5 Oct 2017  A handful of grain is found in the pocket of a...   \n",
              "4  Hardcover,– 10 Oct 2006  For seven decades, \"Life\" has been thrilling t...   \n",
              "\n",
              "                          Genre                          BookCategory   Price  \\\n",
              "0    Action & Adventure (Books)                    Action & Adventure  220.00   \n",
              "1    Cinema & Broadcast (Books)  Biographies, Diaries & True Accounts  202.93   \n",
              "2       International Relations                                Humour  299.00   \n",
              "3  Contemporary Fiction (Books)             Crime, Thriller & Mystery  180.00   \n",
              "4         Photography Textbooks              Arts, Film & Photography  965.62   \n",
              "\n",
              "   New_Ratings  New_Reviews  \n",
              "0            8          4.0  \n",
              "1           14          3.9  \n",
              "2            6          4.8  \n",
              "3           13          4.1  \n",
              "4            1          5.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7612bb6e-a6a9-40fb-98a6-3bcc0720cc49\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Genre</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure (Books)</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Cinema &amp; Broadcast (Books)</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>Paperback,– 25 Feb 1982</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>International Relations</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>Paperback,– 5 Oct 2017</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Contemporary Fiction (Books)</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>Hardcover,– 10 Oct 2006</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>Photography Textbooks</td>\n",
              "      <td>Arts, Film &amp; Photography</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7612bb6e-a6a9-40fb-98a6-3bcc0720cc49')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7612bb6e-a6a9-40fb-98a6-3bcc0720cc49 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7612bb6e-a6a9-40fb-98a6-3bcc0720cc49');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a16a7f39-3af7-42d5-b6ed-8aca0fa30042\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a16a7f39-3af7-42d5-b6ed-8aca0fa30042')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a16a7f39-3af7-42d5-b6ed-8aca0fa30042 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Genre"
      ],
      "metadata": {
        "id": "gW7-VTJE6fse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df[\"Genre\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga2P-68Y6asB",
        "outputId": "021d7e07-c0d9-4d86-9e6a-b12bc5ceddac"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Action & Adventure (Books)', 'Cinema & Broadcast (Books)',\n",
              "       'International Relations', 'Contemporary Fiction (Books)',\n",
              "       'Photography Textbooks', 'Healthy Living & Wellness (Books)',\n",
              "       'Crime, Thriller & Mystery (Books)',\n",
              "       'Sports Training & Coaching (Books)',\n",
              "       'Biographies & Autobiographies (Books)', 'Asian History',\n",
              "       'Banks & Banking', 'Comics & Mangas (Books)',\n",
              "       \"Children's Mysteries & Curiosities (Books)\", 'Mangas',\n",
              "       'Artificial Intelligence',\n",
              "       'Software & Business Applications (Books)', 'German',\n",
              "       'International Business', 'Cricket (Books)',\n",
              "       'Comics & Graphic Novels (Books)', 'PC & Video Games (Books)',\n",
              "       'Short Stories (Books)', 'Astrology', 'Romance (Books)', 'Design',\n",
              "       'Introductory & Beginning Programming', 'Travel (Books)',\n",
              "       'Sports (Books)', 'Communications', 'Foreign Languages',\n",
              "       'Linguistics (Books)', 'Music Books',\n",
              "       'Outdoor Survival Skills (Books)', 'True Accounts (Books)',\n",
              "       'Literature', 'Society & Culture (Books)',\n",
              "       'Industries & Business Sectors (Books)',\n",
              "       'Programming Languages (Books)', 'Media Studies',\n",
              "       'Indian Writing (Books)',\n",
              "       'Networks & System Administration (Books)',\n",
              "       'Classic Fiction (Books)', 'Dictionaries',\n",
              "       'Computer Hardware & Handheld Devices (Books)',\n",
              "       'Government (Books)', 'Arts History, Theory & Criticism (Books)',\n",
              "       'Science & Mathematics', 'Hunting (Books)',\n",
              "       'Handicrafts, Decorative Arts & Crafts (Books)', 'Inflation',\n",
              "       'Religious & Spiritual Fiction (Books)', 'Law (Books)',\n",
              "       'Vocabulary Books', 'Encyclopaedias & Reference Works (Books)',\n",
              "       'IT Certification Exams', 'Sign Language Reference',\n",
              "       'Engineering & Technology (Books)', 'Business Communication',\n",
              "       'IELTS', \"Children's Adventure (Books)\", 'Hinduism (Books)',\n",
              "       \"Children's Games, Toys & Activities (Books)\",\n",
              "       'Political Theory (Books)', 'Investments & Securities',\n",
              "       \"Children's Historical Fiction (Books)\", 'Sustainable Development',\n",
              "       'Painting Textbooks', 'Economic Theory', 'Writing Guides (Books)',\n",
              "       'Budget Travel', 'Biotechnology Engineering Textbooks', 'Sales',\n",
              "       'Computing, Internet & Digital Media (Books)',\n",
              "       'Illustrated Travel Books (Books)', 'Comics',\n",
              "       'Agriculture & Farming (Books)',\n",
              "       \"Children's Crafts, Hobbies & Practical Interests (Books)\",\n",
              "       'Historical Fiction (Books)',\n",
              "       'Language, Linguistics & Writing (Books)', 'GRE', 'Reincarnation',\n",
              "       'Public Policy', \"Children's Reference (Books)\",\n",
              "       'Film & Television', 'Anthologies (Books)',\n",
              "       'Personal Development & Self-Help (Books)', 'Martial Arts (Books)',\n",
              "       'Language & Linguistics',\n",
              "       'World African & Middle Eastern Literature', 'Soccer (Books)',\n",
              "       'Grammar (Books)', 'Asian Literature', 'Essays (Books)', 'TOEFL',\n",
              "       'American Literature', 'GATE Exams',\n",
              "       'Language Learning & Teaching (Books)',\n",
              "       'Political Ideologies (Books)', 'Theater',\n",
              "       'Mind, Body & Spirit (Books)', 'Earth Sciences Textbooks',\n",
              "       'Climbing & Mountaineering (Books)', 'Programming Algorithms',\n",
              "       'Horse Racing (Books)', 'Games & Quizzes (Books)',\n",
              "       'Family & Relationships (Books)',\n",
              "       \"Children's Crime & Thriller (Books)\", 'Economics Books',\n",
              "       'Theatre & Spectacles (Books)', 'Architecture (Books)',\n",
              "       'Humour (Books)', \"Children's Humour (Books)\",\n",
              "       'Internet & Web (Books)', 'Horror (Books)',\n",
              "       'Fantasy, Science Fiction & Horror (Books)',\n",
              "       'Programming & Software Development (Books)', 'Algorithms',\n",
              "       'Entrepreneurship', 'Computer Security (Books)',\n",
              "       'Food, Drink & Entertaining (Books)', 'Engineering',\n",
              "       'Anthropology (Books)', 'History (Books)',\n",
              "       'Computer Science Books', 'IGCSE', 'Visual C++ Programming',\n",
              "       'Visual Arts', \"Children's Fantasy (Books)\",\n",
              "       'Software Architecture', 'Teaching & Education',\n",
              "       'Public Speaking Reference', 'Public Health', 'Psychology (Books)',\n",
              "       'United States History', 'Calculus', 'Political Parties (Books)',\n",
              "       'Zoology', 'Neuroscience', 'Gender Studies', 'General Dentistry',\n",
              "       'Governmental', 'Economic History',\n",
              "       'History of Slavery & Emancipation',\n",
              "       'Economic Policy & Development', 'Astronomy (Books)', 'Ethics',\n",
              "       'French', 'International Relations & Globalization (Books)',\n",
              "       'Japanese', 'Software Design & Engineering', 'History & Surveys',\n",
              "       'Geography Textbooks', 'Crafts, Home & Lifestyle (Books)',\n",
              "       'Economics Textbooks',\n",
              "       \"Children's Family, Personal & Social Issues (Books)\",\n",
              "       'Computer Databases (Books)', 'Mathematics (Books)',\n",
              "       'Political Freedom & Security (Books)',\n",
              "       'Engineering Services Exams',\n",
              "       'Sporting Events & Organisations (Books)', 'Spirituality',\n",
              "       \"Children's Painting, Arts & Music (Books)\",\n",
              "       \"Children's & Young Adult (Books)\", 'American Football (Books)',\n",
              "       'Rhetoric & Speech (Books)', 'Motorcycle Racing', 'Study Guides',\n",
              "       'Mental & Spiritual Healing', 'Labor & Industrial Relations',\n",
              "       'Alternative Medicine (Books)', 'FPC', 'Python Programming',\n",
              "       'Literature & Fiction (Books)', 'Holocaust',\n",
              "       'Encyclopedias for Children', 'Politics (Books)',\n",
              "       'Biographies, Diaries & True Accounts (Books)', 'Literary Travel',\n",
              "       'Triathlon (Books)', 'Travel & Holiday Guides (Books)',\n",
              "       \"Children's Picture Books (Books)\", 'Travel with Pets',\n",
              "       'Reading Skills', 'Baseball (Books)', 'Meditation',\n",
              "       'Transportation & Automotive (Books)', 'Occupational Therapy',\n",
              "       'Journalism Books', 'PGMEE Exam', 'History of Religion (Books)',\n",
              "       'Theory', 'Arts, Film & Photography (Books)',\n",
              "       'Interface Design Programming',\n",
              "       'Combat Sports & Self-Defence (Books)',\n",
              "       'Digital Media & Graphic Design (Books)', 'Food & Lodging',\n",
              "       'Exam Preparation (Books)', 'Military Sciences',\n",
              "       'Colonialism & Imperialism History', 'Education & Training',\n",
              "       'Operating Systems Textbooks', 'Software Testing',\n",
              "       'Networking (Books)', 'Diaries, Letters & Journals (Books)',\n",
              "       'Biology Books', 'Government Exams',\n",
              "       'Society & Social Sciences (Books)', 'Physician & Patient',\n",
              "       'International Baccalaureate', 'Rugby (Books)',\n",
              "       'Design & Fashion (Books)', 'Plays (Books)',\n",
              "       'Aeronautical Engineering', 'Business & Economics (Books)',\n",
              "       'Literature Encyclopedias', 'Environmental Economics',\n",
              "       'Intolerance, Persecution & Fundamentalism', 'Islam (Books)',\n",
              "       'Myths, Legends & Sagas (Books)', 'Golf (Books)',\n",
              "       'European History', 'Pets (Books)', 'Italian', 'Geometry',\n",
              "       'Physics (Books)', 'Interview Preparation',\n",
              "       'Gay & Lesbian Studies', 'Swimming, Snorkelling & Diving (Books)',\n",
              "       'History of Ideas', 'Religion (Books)',\n",
              "       'Sciences, Technology & Medicine (Books)', 'Internal Medicine',\n",
              "       'Pool, Billiards & Snooker (Books)', 'Humanities',\n",
              "       'Library & Information Science',\n",
              "       \"Children's Traditional Stories (Books)\",\n",
              "       'Business, Strategy & Management (Books)', 'Ecotourism',\n",
              "       'Anatomy & Physiology', 'Target Shooting (Books)',\n",
              "       'Political Structure & Processes (Books)',\n",
              "       'Environmental Engineering', 'Social Sciences',\n",
              "       'English Literature', 'Microeconomics Textbooks',\n",
              "       'Social Welfare & Social Services (Books)', 'Alphabet Reference',\n",
              "       'Business Ethics', 'History of Civilization & Culture',\n",
              "       'Public Administration (Books)', 'Civil Rights', 'CSS Programming',\n",
              "       'Management', 'Fishing & Angling (Books)',\n",
              "       'Algebra & Trigonometry', 'Immunology',\n",
              "       'Bodybuilding & Weightlifting (Books)', 'Chemistry Books',\n",
              "       'Music Textbooks', 'Gymnastics (Books)', 'Sociology (Books)',\n",
              "       'Communication Reference', 'Neurology',\n",
              "       'XHTML Software Programming', 'Operating Systems Books',\n",
              "       'International Entrance Exams', 'Sikhism (Books)',\n",
              "       'Environmental Studies', 'Americas', 'Developmental Psychology',\n",
              "       'Development & Growth', 'Urban & Regional',\n",
              "       'Structural Engineering', 'Cold War', 'Painting Books',\n",
              "       'Graphics & Visualization', 'Marathon & Running (Books)',\n",
              "       'Walking, Hiking & Trekking (Books)', 'Vascular Surgery',\n",
              "       'Readers', \"Children's Language Learning (Books)\", 'Art History',\n",
              "       \"Young Adults' Money & Jobs (Books)\", 'Dance (Books)',\n",
              "       'Car Racing (Books)', 'Design Pattern Programming',\n",
              "       'Basketball (Books)', 'Buddhism (Books)',\n",
              "       'Electrical & Electronic Engineering', 'Ancient History (Books)',\n",
              "       'Astronomy & Astrophysics',\n",
              "       \"Children's Horror & Ghost Stories (Books)\",\n",
              "       'Translation Reference', 'Biomedical Engineering',\n",
              "       'Sports Medicine', 'SSC Exam', 'Ecology', 'Sailing (Books)',\n",
              "       'Middle Eastern History',\n",
              "       \"Children's Science, Nature & Technology (Books)\",\n",
              "       'Camping & Woodcraft (Books)', 'Macroeconomics Textbooks',\n",
              "       'Nuclear Engineering', 'Home & House Maintenance (Books)',\n",
              "       'Textbooks & Study Guides', 'Public Affairs', 'Archery (Books)',\n",
              "       'Words, Language & Grammar Reference',\n",
              "       'Active Outdoor Pursuits (Books)',\n",
              "       \"Children's Science Fiction (Books)\", 'European History Textbooks',\n",
              "       'Art Encyclopedias', 'Flash Cards',\n",
              "       'Object-Oriented Software Design', 'Waste Management', 'Speech',\n",
              "       'API & Operating Environments',\n",
              "       'Radiological & Ultrasound Technology',\n",
              "       'Software Programming Compilers', 'Biology & Life Sciences',\n",
              "       'Workbooks', 'Hockey', 'Medicine`', 'History of Ancient Greece',\n",
              "       'CBSE', 'SAT', \"Children's History (Books)\", 'Atheism',\n",
              "       'Industrial Engineering', 'Medical Dictionaries', 'GMAT',\n",
              "       'Aesthetics', 'Reference (Books)', 'Literary Theory', 'Rome'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df.groupby('BookCategory')['Genre'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr0EKH2R6pe6",
        "outputId": "958bf1d3-86c6-4408-b7e9-ca8c59409b7a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BookCategory\n",
              "Action & Adventure                       38\n",
              "Arts, Film & Photography                 77\n",
              "Biographies, Diaries & True Accounts    121\n",
              "Comics & Mangas                          66\n",
              "Computing, Internet & Digital Media      87\n",
              "Crime, Thriller & Mystery                47\n",
              "Humour                                   92\n",
              "Language, Linguistics & Writing         110\n",
              "Politics                                 86\n",
              "Romance                                  28\n",
              "Sports                                   93\n",
              "Name: Genre, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking at the \"genre\" and \"bookcategory\" columns an also the numbers above we can easily undrestand that genre is somehow the subset of bookcategory. This means that book categories are divided into different genres ( which are basically the catgeories only more detailed). So by having the data of book category column we can that there's no need for genre and we can just drop this column to make everything easier."
      ],
      "metadata": {
        "id": "RPfI7O3RqI18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop([\"Genre\"],axis=1)\n",
        "cat_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Hnpt5z9ErGCq",
        "outputId": "2d17ebf1-a4c7-46c0-de1d-7ebe840b156b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title          Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)  Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts    Arun Khopkar   \n",
              "\n",
              "                   Edition                                           Synopsis  \\\n",
              "0  Paperback,– 10 Mar 2016  THE HUNTERS return in their third brilliant no...   \n",
              "1   Paperback,– 7 Nov 2012  A layered portrait of a troubled genius for wh...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \n",
              "0                    Action & Adventure  220.00            8          4.0  \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e3a48980-ac1a-44b4-9345-e254b4363469\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3a48980-ac1a-44b4-9345-e254b4363469')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e3a48980-ac1a-44b4-9345-e254b4363469 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e3a48980-ac1a-44b4-9345-e254b4363469');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5ad5b77d-c764-4596-ae46-840931cc1a8a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5ad5b77d-c764-4596-ae46-840931cc1a8a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5ad5b77d-c764-4596-ae46-840931cc1a8a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Edition**"
      ],
      "metadata": {
        "id": "YqhrxP1o8d7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edition = cat_df['Edition'].values.tolist()\n",
        "edition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4PVea3y8hbS",
        "outputId": "0e650bb6-fbfc-4ff4-c207-8b05c2a69c83"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Paperback,– 10 Mar 2016',\n",
              " 'Paperback,– 7 Nov 2012',\n",
              " 'Paperback,– 25 Feb 1982',\n",
              " 'Paperback,– 5 Oct 2017',\n",
              " 'Hardcover,– 10 Oct 2006',\n",
              " 'Paperback,– 5 May 2009',\n",
              " 'Paperback,– 5 Oct 2017',\n",
              " 'Hardcover,– Import, 1 Mar 2018',\n",
              " 'Paperback,– 15 Dec 2015',\n",
              " 'Paperback,– 26 Mar 2013',\n",
              " 'Paperback,– 20 Jan 2017',\n",
              " 'Paperback,– Import, 14 Jun 2018',\n",
              " 'Paperback,– 1 Jul 1999',\n",
              " 'Paperback,– 15 Nov 2002',\n",
              " 'Paperback,– 1 Sep 2011',\n",
              " 'Paperback,– 26 Feb 2015',\n",
              " 'Hardcover,– 8 Mar 2018',\n",
              " 'Paperback,– 1 Nov 2016',\n",
              " 'Mass Market Paperback,– 1 Jan 1991',\n",
              " 'Paperback,– 2016',\n",
              " 'Hardcover,– 24 Nov 2018',\n",
              " 'Paperback,– Import, 4 Oct 2018',\n",
              " 'Paperback,– 5 Jul 2012',\n",
              " 'Paperback,– 1 Nov 2014',\n",
              " 'Paperback,– 31 Aug 2012',\n",
              " 'Hardcover,– Deckle Edge, 18 Oct 2011',\n",
              " 'Paperback,– 1 Mar 2016',\n",
              " 'Paperback,– Box set, 15 Jun 2014',\n",
              " 'Hardcover,– 15 Sep 2014',\n",
              " 'Paperback,– 23 Apr 1989',\n",
              " 'Paperback,– 21 Nov 2013',\n",
              " 'Paperback,– 21 Jul 2015',\n",
              " 'Paperback,– 14 Oct 2000',\n",
              " 'Paperback,– 5 Sep 2005',\n",
              " 'Hardcover,– 10 May 2016',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 11 Jun 2014',\n",
              " 'Paperback,– 17 Apr 2009',\n",
              " 'Paperback,– 30 Nov 2006',\n",
              " 'Paperback,– 1 Dec 2013',\n",
              " 'Paperback,– 3 Jan 2013',\n",
              " 'Hardcover,– 28 Sep 2013',\n",
              " 'Paperback,– 20 Aug 2008',\n",
              " 'Hardcover,– 8 Jul 2015',\n",
              " 'Hardcover,– 2019',\n",
              " 'Paperback,– 3 Jul 2014',\n",
              " 'Paperback,– 29 Dec 2006',\n",
              " 'Paperback,– 22 Feb 2014',\n",
              " 'Paperback,– 1 Nov 2012',\n",
              " 'Paperback,– 30 Mar 2012',\n",
              " 'Paperback,– Import, 12 Jan 2017',\n",
              " 'Paperback,– 16 Jun 2016',\n",
              " 'Sheet music,– Import, 7 Jun 2018',\n",
              " 'Mass Market Paperback,– 5 Oct 2000',\n",
              " 'Paperback,– Import, 24 Aug 2018',\n",
              " 'Paperback,– 22 Mar 2019',\n",
              " 'Paperback,– 21 Apr 2017',\n",
              " 'Hardcover,– 4 Oct 2016',\n",
              " 'Paperback,– 7 May 2015',\n",
              " 'Paperback,– 1 Jan 2013',\n",
              " 'Paperback,– 2016',\n",
              " 'Hardcover,– 25 Dec 2016',\n",
              " 'Paperback,– 2 Sep 2004',\n",
              " 'Paperback,– 10 Jul 2018',\n",
              " 'Paperback,– 27 Apr 2014',\n",
              " 'Hardcover,– 2 Mar 1999',\n",
              " 'Paperback,– 7 Oct 2010',\n",
              " 'Paperback,– International Edition, 7 Mar 2013',\n",
              " 'Paperback,– 12 Oct 2014',\n",
              " 'Paperback,– 5 Jul 2011',\n",
              " 'Hardcover,– 1 Sep 2016',\n",
              " 'Paperback,– 13 Sep 2016',\n",
              " 'Paperback,– Import, 27 Sep 2018',\n",
              " 'Paperback,– 22 Nov 2016',\n",
              " 'Paperback,– 6 Sep 2016',\n",
              " 'Paperback,– 12 Nov 2015',\n",
              " 'Hardcover,– 21 Jan 2019',\n",
              " 'Paperback,– Unabridged, 1 Nov 2007',\n",
              " 'Paperback,– Special Edition, 6 May 2008',\n",
              " 'Hardcover,– 20 Aug 2018',\n",
              " 'Paperback,– 14 Jul 2015',\n",
              " 'Paperback,– International Edition, 7 Jun 2012',\n",
              " 'Paperback,– 21 Dec 2016',\n",
              " 'Paperback,– 14 Jan 2016',\n",
              " 'Paperback,– 18 Jan 2018',\n",
              " 'Paperback,– 20 Jul 2015',\n",
              " 'Paperback,– 12 Mar 2001',\n",
              " 'Hardcover,– Import, 5 Jul 2018',\n",
              " 'Paperback,– Import, 29 May 2018',\n",
              " 'Hardcover,– Import, 25 Mar 2019',\n",
              " 'Hardcover,– 5 Nov 2015',\n",
              " 'Paperback,– 20 Feb 2017',\n",
              " 'Paperback,– 21 Sep 2017',\n",
              " 'Paperback,– 29 Nov 2016',\n",
              " 'Hardcover,– 19 May 2019',\n",
              " 'Paperback,– 1 Jun 1969',\n",
              " 'Paperback,– 2 May 2019',\n",
              " 'Paperback,– 25 Sep 2008',\n",
              " 'Paperback,– 2011',\n",
              " 'Paperback,– Student Edition, 14 Jun 2016',\n",
              " 'Paperback,– 28 Sep 2018',\n",
              " 'Hardcover,– 4 Sep 2014',\n",
              " 'Paperback,– 1 Jan 2009',\n",
              " 'Paperback,– 31 Jul 2019',\n",
              " 'Hardcover,– 1 Sep 2016',\n",
              " 'Paperback,– 10 Apr 2017',\n",
              " 'Hardcover,– 16 Jan 2016',\n",
              " 'Paperback,– 1 Feb 2017',\n",
              " 'Paperback,– 12 Mar 2013',\n",
              " 'Paperback,– 16 Oct 2004',\n",
              " 'Paperback,– 13 Feb 2014',\n",
              " 'Paperback,– 6 Sep 2004',\n",
              " 'Paperback,– 2019',\n",
              " 'Flexibound,– Import, 2018',\n",
              " 'Paperback,– 1 Jan 2010',\n",
              " 'Paperback,– 2015',\n",
              " 'Paperback,– Import, 14 Jan 2016',\n",
              " 'Paperback,– 13 Nov 2001',\n",
              " 'Paperback,– 28 Oct 1993',\n",
              " 'Paperback,– 4 Oct 2016',\n",
              " 'Paperback,– 3 Mar 2009',\n",
              " 'Paperback,– 5 May 2014',\n",
              " 'Paperback,– 17 Jun 2015',\n",
              " 'Paperback,– Import, 2 Oct 2018',\n",
              " 'Paperback,– 19 Apr 2016',\n",
              " 'Paperback,– 5 Feb 2015',\n",
              " 'Paperback,– 27 Feb 2013',\n",
              " 'Hardcover,– 25 Aug 2002',\n",
              " 'Paperback,– 20 Nov 2000',\n",
              " 'Paperback,– 20 Oct 2013',\n",
              " 'Paperback,– Jan 2011',\n",
              " 'Sheet music,– 15 Sep 2017',\n",
              " 'Paperback,– 28 May 2015',\n",
              " 'Paperback,– Illustrated, 3 Jan 2008',\n",
              " 'Paperback,– 29 Jan 2009',\n",
              " 'Hardcover,– 1 Apr 2009',\n",
              " 'Paperback,– 10 Aug 1999',\n",
              " 'Paperback,– 14 Oct 2008',\n",
              " 'Paperback,– 26 Mar 2013',\n",
              " 'Paperback,– 25 Apr 2019',\n",
              " 'Paperback,– 28 Aug 2018',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 10 Aug 2016',\n",
              " 'Paperback,– 2015',\n",
              " 'Paperback,– 3 Dec 1992',\n",
              " 'Hardcover,– 17 Dec 2018',\n",
              " 'Paperback,– 19 Jul 2016',\n",
              " 'Hardcover,– 8 Jul 2016',\n",
              " 'Paperback,– 10 Jun 2019',\n",
              " 'Paperback,– 2012',\n",
              " 'Paperback,– 25 Dec 2018',\n",
              " 'Paperback,– 23 Apr 2009',\n",
              " 'Paperback,– 2016',\n",
              " 'Hardcover,– 7 Sep 2006',\n",
              " 'Hardcover,– Import, 16 Mar 2018',\n",
              " 'Paperback,– 1 May 2019',\n",
              " 'Hardcover,– 26 Apr 2011',\n",
              " 'Paperback,– 31 Mar 2010',\n",
              " 'Paperback,– 7 Jan 2003',\n",
              " 'Paperback,– 27 Feb 2003',\n",
              " 'Paperback,– 30 Oct 2017',\n",
              " 'Paperback,– 23 Apr 1992',\n",
              " 'Hardcover,– 22 Jul 2018',\n",
              " 'Paperback,– 13 Jan 2017',\n",
              " 'Paperback,– 1 Jun 2017',\n",
              " 'Paperback,– Import, 5 Jul 1996',\n",
              " 'Hardcover,– 18 Aug 2009',\n",
              " 'Paperback,– 26 Jul 2018',\n",
              " 'Paperback,– Import, 22 Mar 2018',\n",
              " 'Paperback,– Abridged, Import',\n",
              " 'Paperback,– 10 Jan 2018',\n",
              " 'Paperback,– Apr 2011',\n",
              " 'Paperback,– 17 Feb 2009',\n",
              " 'Paperback,– 22 Aug 2000',\n",
              " 'Paperback,– 5 Jan 2012',\n",
              " 'Paperback,– 20 Dec 2012',\n",
              " 'Mass Market Paperback,– 1 Jan 1982',\n",
              " 'Paperback,– 28 Oct 2014',\n",
              " 'Paperback,– 15 Feb 2019',\n",
              " 'Paperback,– 1 May 2017',\n",
              " 'Paperback,– Import, 16 Feb 2019',\n",
              " 'Paperback,– Import, 1 Mar 2018',\n",
              " 'Hardcover,– 2 Aug 2009',\n",
              " 'Paperback,– Unabridged, 5 Mar 2013',\n",
              " 'Paperback,– 15 Aug 2014',\n",
              " 'Paperback,– 8 May 2015',\n",
              " 'Hardcover,– 14 Jun 2018',\n",
              " 'Paperback,– 27 Feb 1997',\n",
              " 'Paperback,– 16 Dec 2011',\n",
              " 'Paperback,– 2 Jun 2015',\n",
              " 'Hardcover,– 2016',\n",
              " 'Paperback,– 13 Nov 2015',\n",
              " 'Paperback,– 1 Apr 2010',\n",
              " 'Paperback,– 14 Feb 2008',\n",
              " 'Paperback,– 7 Jun 2016',\n",
              " 'Paperback,– 18 Jun 1989',\n",
              " 'Paperback,– 15 Jul 2014',\n",
              " 'Paperback,– 9 Mar 2015',\n",
              " 'Hardcover,– 21 Feb 2019',\n",
              " 'Paperback,– 21 Apr 2015',\n",
              " 'Paperback,– 1 Dec 2017',\n",
              " 'Paperback,– 5 Feb 2015',\n",
              " 'Mass Market Paperback,– Import, 30 Jan 2018',\n",
              " 'Paperback,– Import, 30 Nov 2017',\n",
              " 'Paperback,– Import, 30 Jul 2015',\n",
              " 'Paperback,– 24 Mar 2015',\n",
              " 'Paperback,– Import, 5 Oct 2017',\n",
              " 'Paperback,– 6 Sep 2018',\n",
              " 'Paperback,– Import, 27 Sep 2018',\n",
              " 'Paperback,– 2010',\n",
              " 'Paperback,– 23 Apr 2018',\n",
              " 'Paperback,– 27 Sep 2016',\n",
              " 'Paperback,– Import, 23 Mar 2018',\n",
              " 'Paperback,– 26 May 2011',\n",
              " 'Hardcover,– 26 Sep 2017',\n",
              " 'Mass Market Paperback,– 1 Jun 1982',\n",
              " 'Paperback,– 2 Mar 2010',\n",
              " 'Paperback,– 1 Jul 2011',\n",
              " 'Paperback,– 6 Jul 1995',\n",
              " 'Paperback,– 25 Jul 2018',\n",
              " 'Paperback,– 1 Feb 2007',\n",
              " 'Paperback,– 5 Aug 2004',\n",
              " 'Paperback,– 2013',\n",
              " 'Paperback,– 2014',\n",
              " 'Paperback,– 7 Jul 2011',\n",
              " 'Paperback,– 30 Jun 2011',\n",
              " 'Paperback,– 30 Aug 2012',\n",
              " 'Paperback,– 4 Jan 2017',\n",
              " 'Paperback,– 14 Apr 2009',\n",
              " 'Paperback,– 5 Oct 2017',\n",
              " 'Hardcover,– Illustrated, 24 Sep 2014',\n",
              " 'Paperback,– 18 Mar 2019',\n",
              " 'Paperback,– Special Edition, 29 Sep 2016',\n",
              " 'Paperback,– 19 Aug 2016',\n",
              " 'Paperback,– 1 Jun 2017',\n",
              " 'Plastic Comb,– DVD, NTSC',\n",
              " 'Paperback,– 15 Nov 2012',\n",
              " 'Paperback,– 8 May 2007',\n",
              " 'Paperback,– 12 Mar 2015',\n",
              " 'Paperback,– 7 Nov 2013',\n",
              " 'Paperback,– 1 Jan 1987',\n",
              " 'Paperback,– 2 Dec 1999',\n",
              " 'Paperback,– 1 Jan 2013',\n",
              " 'Hardcover,– 24 Dec 2018',\n",
              " 'Paperback,– 2011',\n",
              " 'Paperback,– 19 Nov 2003',\n",
              " 'Paperback,– 3 Jan 2017',\n",
              " 'Paperback,– 10 Feb 2017',\n",
              " 'Hardcover,– 19 Nov 2013',\n",
              " 'Paperback,– 4 Oct 2017',\n",
              " 'Paperback,– 3 May 2016',\n",
              " 'Paperback,– 24 Nov 2015',\n",
              " 'Paperback,– 15 Sep 2015',\n",
              " 'Hardcover,– Import, 26 Oct 2018',\n",
              " 'Paperback,– 6 Nov 2003',\n",
              " 'Paperback,– 17 Oct 2014',\n",
              " 'Paperback,– Import, 5 Dec 2017',\n",
              " 'Paperback,– 5 Apr 2001',\n",
              " 'Paperback,– 7 May 2013',\n",
              " 'Paperback,– 4 Nov 2011',\n",
              " 'Paperback,– Import, 20 Dec 2016',\n",
              " 'Paperback,– Bargain Price, 29 Dec 2011',\n",
              " 'Paperback,– 24 Feb 2003',\n",
              " 'Paperback,– Import, 12 Apr 2012',\n",
              " 'Paperback,– Import, 21 Aug 2018',\n",
              " 'Paperback,– Import, 25 Oct 2017',\n",
              " 'Paperback,– 10 Oct 2017',\n",
              " 'Paperback,– 15 Jun 2002',\n",
              " 'Paperback,– 2002',\n",
              " 'Paperback,– 15 Jun 2016',\n",
              " 'Hardcover,– 2 Feb 2012',\n",
              " 'Paperback,– Deckle Edge, 4 Mar 2002',\n",
              " 'Paperback,– 29 Apr 2003',\n",
              " 'Paperback,– 20 Jan 2019',\n",
              " 'Paperback,– 3 May 2018',\n",
              " 'Paperback,– 27 Nov 2007',\n",
              " 'Paperback,– 26 May 2014',\n",
              " 'Paperback,– 8 Jul 2010',\n",
              " 'Paperback,– 6 Aug 2017',\n",
              " 'Paperback,– Import, 12 Jul 2018',\n",
              " 'Paperback,– 25 Apr 2019',\n",
              " 'Paperback,– 19 Oct 2014',\n",
              " 'Mass Market Paperback,– 7 Dec 2010',\n",
              " 'Paperback,– 24 Oct 2017',\n",
              " 'Paperback,– 18 Apr 2019',\n",
              " 'Paperback,– 26 Feb 2018',\n",
              " 'Paperback,– 23 Jun 2014',\n",
              " 'Paperback,– 25 May 2010',\n",
              " 'Paperback,– 7 Jan 1986',\n",
              " 'Mass Market Paperback,– 5 Jul 2016',\n",
              " 'Paperback,– 1 Jun 2013',\n",
              " 'Paperback,– 4 Aug 2003',\n",
              " 'Paperback,– 31 Jul 2012',\n",
              " 'Paperback,– 1 May 2017',\n",
              " 'Paperback,– 9 Dec 2014',\n",
              " 'Paperback,– 28 Dec 2014',\n",
              " 'Paperback,– 15 Mar 2012',\n",
              " 'Paperback,– 8 Jan 1990',\n",
              " 'Paperback,– 20 Oct 2016',\n",
              " 'Paperback,– Import, 19 Jun 1991',\n",
              " 'Paperback,– 21 Feb 2009',\n",
              " 'Mass Market Paperback,– 25 Jul 2013',\n",
              " 'Paperback,– 25 Oct 2016',\n",
              " 'Hardcover,– 29 Oct 2018',\n",
              " 'Paperback,– 26 Apr 2016',\n",
              " 'Paperback,– 15 Jun 2018',\n",
              " 'Paperback,– 25 Feb 2019',\n",
              " 'Hardcover,– 2014',\n",
              " 'Paperback,– 1 Jun 2016',\n",
              " 'Hardcover,– 30 May 2018',\n",
              " 'Hardcover,– Import, 7 Sep 2017',\n",
              " 'Paperback,– 25 Jun 2010',\n",
              " 'Paperback,– Import, 4 May 2017',\n",
              " 'Paperback,– Import, 28 Nov 2017',\n",
              " 'Paperback,– 12 Apr 2012',\n",
              " 'Hardcover,– 2 Nov 2015',\n",
              " 'Paperback,– 31 Dec 2013',\n",
              " 'Paperback,– 12 Sep 2016',\n",
              " 'Hardcover,– 1 Jan 2012',\n",
              " 'Hardcover,– 31 Aug 2018',\n",
              " 'Paperback,– 30 Jun 2007',\n",
              " 'Hardcover,– 10 Jan 2012',\n",
              " 'Paperback,– 25 Sep 2014',\n",
              " 'Paperback,– 14 Jul 2016',\n",
              " 'Paperback,– 10 Jul 2018',\n",
              " 'Paperback,– 15 Jul 2010',\n",
              " 'Paperback,– 15 Nov 2016',\n",
              " 'Paperback,– 1 Jul 2014',\n",
              " 'Paperback,– 22 Feb 2018',\n",
              " 'Hardcover,– 2 Feb 2015',\n",
              " 'Paperback,– 27 May 2008',\n",
              " 'Paperback,– 24 Nov 2009',\n",
              " 'Paperback,– Import, 7 Sep 2017',\n",
              " 'Mass Market Paperback,– 30 Jan 2016',\n",
              " 'Paperback,– 26 Apr 2011',\n",
              " 'Paperback,– 2017',\n",
              " 'Hardcover,– Import, 24 Oct 2017',\n",
              " 'Paperback,– 30 Jun 2015',\n",
              " 'Paperback,– 17 Jan 2013',\n",
              " 'Hardcover,– Import, 1 Nov 2018',\n",
              " 'Loose Leaf,– 15 May 2006',\n",
              " 'Paperback,– 29 Jun 2019',\n",
              " 'Paperback,– 23 Aug 2017',\n",
              " 'Paperback,– 5 Feb 2015',\n",
              " 'Paperback,– 1 Sep 1988',\n",
              " 'Paperback,– 25 Feb 2003',\n",
              " 'Paperback,– 26 Jun 2016',\n",
              " 'Paperback,– 6 Sep 2017',\n",
              " 'Paperback,– 3 Jan 2007',\n",
              " 'Paperback,– 18 Oct 2017',\n",
              " 'Paperback,– Import, 9 Oct 2018',\n",
              " 'Paperback,– 5 Feb 2004',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 5 Sep 2006',\n",
              " 'Paperback,– 28 Feb 2003',\n",
              " 'Hardcover,– 25 Oct 2016',\n",
              " 'Paperback,– 12 Dec 2012',\n",
              " 'Paperback,– Import, 27 Nov 2017',\n",
              " 'Paperback,– 23 Aug 2017',\n",
              " 'Paperback,– 1 Jan 2011',\n",
              " 'Paperback,– 2 Nov 2016',\n",
              " 'Mass Market Paperback,– 1 Nov 1991',\n",
              " 'Mass Market Paperback,– 27 Aug 2013',\n",
              " 'Hardcover,– 22 Nov 2013',\n",
              " 'Paperback,– 3 Oct 2017',\n",
              " 'Paperback,– Sep 2016',\n",
              " 'Paperback,– 26 Sep 2017',\n",
              " 'Paperback,– 10 Jan 2019',\n",
              " 'Paperback,– 21 Dec 2010',\n",
              " 'Hardcover,– 23 Sep 2014',\n",
              " 'Paperback,– 15 May 2018',\n",
              " 'Paperback,– 2015',\n",
              " 'Paperback,– 1 Jun 2017',\n",
              " 'Mass Market Paperback,– 1 Jan 1991',\n",
              " 'Mass Market Paperback,– 1 Feb 1993',\n",
              " 'Mass Market Paperback,– Large Print, 1 Jan 1981',\n",
              " 'Paperback,– Import, 25 Feb 2019',\n",
              " 'Hardcover,– 17 Nov 2015',\n",
              " 'Paperback,– Import, 1 Mar 2018',\n",
              " 'Paperback,– 26 May 2009',\n",
              " 'Paperback,– 27 Sep 2012',\n",
              " 'Paperback,– 1 Jun 2000',\n",
              " 'Paperback,– 8 Jul 1988',\n",
              " 'Paperback,– 1 Jun 1989',\n",
              " 'Paperback,– 30 Jun 2018',\n",
              " 'Paperback,– 20 May 1976',\n",
              " 'Paperback,– 1 Dec 2005',\n",
              " 'Hardcover,– 19 Feb 2008',\n",
              " 'Paperback,– 24 Jul 2008',\n",
              " 'Hardcover,– 26 Apr 2019',\n",
              " 'Paperback,– 28 May 2014',\n",
              " 'Paperback,– Oct 2013',\n",
              " 'Paperback,– 1 Nov 1994',\n",
              " 'Paperback,– 13 Sep 2017',\n",
              " 'Paperback,– 30 Oct 2017',\n",
              " 'Hardcover,– 28 Apr 2016',\n",
              " 'Paperback,– 2012',\n",
              " 'Paperback,– 2007',\n",
              " 'Paperback,– 7 Apr 2016',\n",
              " 'Paperback,– Student Edition, 2014',\n",
              " 'Paperback,– 10 Jan 2016',\n",
              " 'Hardcover,– 1 Mar 2016',\n",
              " 'Paperback,– 13 Jul 2004',\n",
              " 'Paperback,– Import, 6 Oct 2015',\n",
              " 'Paperback,– 30 Oct 2017',\n",
              " 'Paperback,– 1 Aug 2009',\n",
              " 'Hardcover,– 21 Jun 2010',\n",
              " 'Paperback,– 2011',\n",
              " 'Hardcover,– 3 Dec 2015',\n",
              " 'Paperback,– 31 Jul 2014',\n",
              " 'Paperback,– 10 Nov 2011',\n",
              " 'Paperback,– 2010',\n",
              " 'Paperback,– Import, 7 Jul 2016',\n",
              " 'Paperback,– 25 Apr 2019',\n",
              " 'Paperback,– 2012',\n",
              " 'Paperback,– 12 Feb 2009',\n",
              " 'Paperback,– 3 Mar 2017',\n",
              " 'Paperback,– 12 Sep 2017',\n",
              " 'Paperback,– 24 Mar 2014',\n",
              " 'Paperback,– 2 Sep 1999',\n",
              " 'Paperback,– 5 Sep 2005',\n",
              " 'Paperback,– 31 Jan 2019',\n",
              " 'Paperback,– 12 Feb 2015',\n",
              " 'Paperback,– 23 Apr 1992',\n",
              " 'Paperback,– 5 Sep 1996',\n",
              " 'Paperback,– 6 Jun 2017',\n",
              " 'Paperback,– 2 Jun 2015',\n",
              " 'Paperback,– 1 Feb 2016',\n",
              " 'Paperback,– 1 Jan 2013',\n",
              " 'Paperback,– 31 Jul 2008',\n",
              " 'Paperback,– 25 Jun 2009',\n",
              " 'Paperback,– 1 Sep 2015',\n",
              " 'Paperback,– 19 Aug 2010',\n",
              " 'Hardcover,– 30 Nov 2018',\n",
              " 'Paperback,– 21 Mar 2017',\n",
              " 'Paperback,– Dec 2018',\n",
              " 'Paperback,– 11 Jul 2016',\n",
              " 'Paperback,– 17 Jul 2018',\n",
              " 'Paperback,– 6 Sep 2016',\n",
              " 'Paperback,– 30 Jul 2017',\n",
              " 'Paperback,– 6 Jul 2000',\n",
              " 'Paperback,– 29 Jul 2014',\n",
              " 'Paperback,– 18 Jan 2012',\n",
              " 'Paperback,– 20 Apr 2018',\n",
              " 'Paperback,– 28 Jan 1998',\n",
              " 'Paperback,– 6 Oct 2013',\n",
              " 'Paperback,– 3 Sep 1992',\n",
              " 'Hardcover,– 14 Dec 2018',\n",
              " 'Mass Market Paperback,– 1 Jul 1982',\n",
              " 'Paperback,– Import, 12 Apr 2019',\n",
              " 'Paperback,– 25 Apr 2014',\n",
              " 'Paperback,– 19 Mar 1996',\n",
              " 'Paperback,– 13 Jul 2016',\n",
              " 'Hardcover,– 30 May 2018',\n",
              " 'Hardcover,– 12 May 2015',\n",
              " 'Paperback,– 4 Jul 2017',\n",
              " 'Paperback,– Import, 25 Jan 2018',\n",
              " 'Paperback,– 7 Nov 1996',\n",
              " 'Paperback,– 29 Nov 2009',\n",
              " 'Paperback,– 21 Apr 2005',\n",
              " 'Paperback,– 13 Sep 2016',\n",
              " 'Paperback,– 15 Mar 2019',\n",
              " 'Paperback,– 27 Jul 2010',\n",
              " 'Hardcover,– 10 Apr 2018',\n",
              " 'Paperback,– 1 Jan 2009',\n",
              " 'Paperback,– 31 Jul 2018',\n",
              " 'Paperback,– 10 Mar 2015',\n",
              " 'Paperback,– 31 Jan 2019',\n",
              " 'Paperback,– 2 Nov 2017',\n",
              " 'Paperback,– 31 Oct 2017',\n",
              " 'Paperback,– 14 Oct 2008',\n",
              " 'Paperback,– 24 Feb 2015',\n",
              " 'Paperback,– 7 Jun 2012',\n",
              " 'Paperback,– 5 Feb 2018',\n",
              " 'Paperback,– 22 Mar 2016',\n",
              " 'Paperback,– 14 Jun 2018',\n",
              " 'Hardcover,– 3 Jan 2017',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 27 Aug 2013',\n",
              " 'Paperback,– 4 Jul 2013',\n",
              " 'Paperback,– 10 Jan 2017',\n",
              " 'Paperback,– 1 Dec 2000',\n",
              " 'Paperback,– 2016',\n",
              " 'Paperback,– 19 Jul 2007',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 19 Aug 2010',\n",
              " 'Paperback,– 15 Oct 2018',\n",
              " 'Hardcover,– 23 Sep 2014',\n",
              " 'Paperback,– 5 Nov 2015',\n",
              " 'Paperback,– 1 Jul 2010',\n",
              " 'Paperback,– 12 May 1998',\n",
              " 'Paperback,– Oct 2008',\n",
              " 'Paperback,– 27 Jan 2016',\n",
              " 'Paperback,– 27 May 2014',\n",
              " 'Paperback,– 27 Dec 2011',\n",
              " 'Hardcover,– 20 Jan 2018',\n",
              " 'Paperback,– 1 Jan 2013',\n",
              " 'Paperback,– 30 Aug 2016',\n",
              " 'Paperback,– 6 Apr 2011',\n",
              " 'Paperback,– 29 Sep 2015',\n",
              " 'Paperback,– 8 Dec 2015',\n",
              " 'Hardcover,– 11 Sep 2018',\n",
              " 'Hardcover,– Import, 19 Jun 2018',\n",
              " 'Paperback,– 23 Feb 2012',\n",
              " 'Mass Market Paperback,– 1 Nov 2004',\n",
              " 'Paperback,– 17 Sep 2013',\n",
              " 'Paperback,– Import, 1 Sep 2018',\n",
              " 'Paperback,– 1 Dec 2009',\n",
              " 'Paperback,– 20 Jan 2019',\n",
              " 'Hardcover,– 26 May 2015',\n",
              " 'Paperback,– 2011',\n",
              " 'Paperback,– 5 Mar 2007',\n",
              " 'Paperback,– 1 Aug 1994',\n",
              " 'Paperback,– Import, 1 Nov 2005',\n",
              " 'Paperback,– 1 Apr 2019',\n",
              " 'Paperback,– 7 Jul 1992',\n",
              " 'Hardcover,– 3 Dec 2015',\n",
              " 'Hardcover,– 29 Mar 2003',\n",
              " 'Paperback,– 20 Jun 2019',\n",
              " 'Paperback,– 22 Aug 2015',\n",
              " 'Paperback,– 27 May 2011',\n",
              " 'Paperback,– 4 Sep 2008',\n",
              " 'Paperback,– 2013',\n",
              " 'Paperback,– Import, 4 Aug 2003',\n",
              " 'Paperback,– 1 Aug 2012',\n",
              " 'Paperback,– 29 Sep 2015',\n",
              " 'Paperback,– 10 Aug 2018',\n",
              " 'Paperback,– 2019',\n",
              " 'Hardcover,– 18 Aug 2009',\n",
              " 'Paperback,– 30 May 2017',\n",
              " 'Paperback,– 15 Oct 2014',\n",
              " 'Hardcover,– Special Edition, 18 Oct 2016',\n",
              " 'Paperback,– 20 Mar 2018',\n",
              " 'Paperback,– 27 Mar 2012',\n",
              " 'Paperback,– 13 Sep 2011',\n",
              " 'Paperback,– 8 May 2014',\n",
              " 'Paperback,– 6 Mar 2003',\n",
              " 'Hardcover,– 1 Jan 2010',\n",
              " 'Paperback,– 16 May 2017',\n",
              " 'Paperback,– Import, 7 Apr 1977',\n",
              " 'Paperback,– 4 May 2017',\n",
              " 'Paperback,– 8 Feb 2015',\n",
              " 'Paperback,– 5 Jan 1988',\n",
              " 'Paperback,– 7 Mar 2017',\n",
              " 'Paperback,– 26 Jan 2018',\n",
              " 'Paperback,– 20 Sep 2018',\n",
              " 'Paperback,– 16 Jun 2017',\n",
              " 'Hardcover,– 28 Jan 2019',\n",
              " 'Hardcover,– 15 Sep 2015',\n",
              " 'Paperback,– Abridged, 28 Sep 2006',\n",
              " 'Hardcover,– 2017',\n",
              " 'Paperback,– 1 Sep 2016',\n",
              " 'Paperback,– 1 Apr 2014',\n",
              " 'Paperback,– 29 Jan 2013',\n",
              " 'Paperback,– 1 Jun 2004',\n",
              " 'Paperback,– 2 Jul 2015',\n",
              " 'Paperback,– Import, 15 May 2018',\n",
              " 'Paperback,– 15 Dec 2015',\n",
              " 'Paperback,– 4 May 2016',\n",
              " 'Paperback,– 23 Nov 2004',\n",
              " 'Paperback,– 25 Dec 2016',\n",
              " 'Paperback,– 28 Aug 2018',\n",
              " 'Paperback,– 15 Dec 2018',\n",
              " 'Paperback,– 1 Jun 1974',\n",
              " 'Paperback,– 25 Apr 2019',\n",
              " 'Paperback,– 13 Jul 2015',\n",
              " 'Paperback,– 7 Jan 2016',\n",
              " 'Paperback,– 12 Feb 2018',\n",
              " 'Paperback,– 15 Jun 2012',\n",
              " 'Paperback,– 3 Dec 2007',\n",
              " 'Paperback,– 1 Jun 1997',\n",
              " 'Paperback,– 4 Oct 2017',\n",
              " 'Paperback,– 6 Aug 2017',\n",
              " 'Paperback,– 2015',\n",
              " 'Hardcover,– 1 Jan 1983',\n",
              " 'Paperback,– Import, 7 Jul 2016',\n",
              " 'Paperback,– 4 Aug 2008',\n",
              " 'Paperback,– 1 Jan 2009',\n",
              " 'Paperback,– 23 Feb 2017',\n",
              " 'Hardcover,– 2015',\n",
              " 'Paperback,– 25 Jul 2016',\n",
              " 'Paperback,– Feb 2015',\n",
              " 'Paperback,– Illustrated, Import',\n",
              " 'Paperback,– 1988',\n",
              " 'Hardcover,– 20 May 2005',\n",
              " 'Hardcover,– 26 May 2015',\n",
              " 'Paperback,– 6 May 2010',\n",
              " 'Hardcover,– 18 Jul 2018',\n",
              " 'Paperback,– 27 Nov 2007',\n",
              " 'Paperback,– 9 Aug 2016',\n",
              " 'Paperback,– 1 Oct 2010',\n",
              " 'Mass Market Paperback,– 1 Jun 1996',\n",
              " 'Paperback,– 26 Oct 2010',\n",
              " 'Paperback,– 1 Oct 2015',\n",
              " 'Paperback,– 1 Apr 2010',\n",
              " 'Paperback,– 12 Apr 2013',\n",
              " 'Tankobon Softcover,– 2016',\n",
              " 'Paperback,– 27 Aug 2013',\n",
              " 'Paperback,– 9 Sep 2015',\n",
              " 'Paperback,– 6 Jul 2014',\n",
              " 'Paperback,– 14 Jun 1976',\n",
              " 'Paperback,– 29 Jan 2018',\n",
              " 'Paperback,– 17 Jan 2012',\n",
              " 'Paperback,– Import, 7 Feb 2017',\n",
              " 'Mass Market Paperback,– 12 Oct 2016',\n",
              " 'Paperback,– 23 May 2013',\n",
              " 'Paperback,– 26 Apr 2015',\n",
              " 'Paperback,– 1 May 2008',\n",
              " 'Hardcover,– Import, 4 Sep 2018',\n",
              " 'Hardcover,– 15 Feb 2013',\n",
              " 'Paperback,– 1 Jul 2016',\n",
              " 'Paperback,– 1 Dec 1995',\n",
              " 'Paperback,– Import, 1 Nov 2018',\n",
              " 'Paperback,– 5 Oct 2017',\n",
              " 'Paperback,– 2018',\n",
              " 'Paperback,– 27 May 1982',\n",
              " 'Perfect Paperback,– 6 Oct 2003',\n",
              " 'Paperback,– 3 Nov 1994',\n",
              " 'Paperback,– 3 Feb 2011',\n",
              " 'Paperback,– 7 Dec 2006',\n",
              " 'Paperback,– 4 Feb 2010',\n",
              " 'Paperback,– Import, 2 Feb 2017',\n",
              " 'Paperback,– 20 Feb 2014',\n",
              " 'Mass Market Paperback,– Import, 21 Aug 2018',\n",
              " 'Paperback,– 3 Feb 2000',\n",
              " 'Mass Market Paperback,– 26 Mar 2010',\n",
              " 'Paperback,– 8 Dec 2015',\n",
              " 'Paperback,– 6 Nov 2015',\n",
              " 'Paperback,– 29 Aug 2006',\n",
              " 'Paperback,– 10 Jul 2014',\n",
              " 'Paperback,– 23 Jan 2018',\n",
              " 'Paperback,– 24 Dec 2008',\n",
              " 'Paperback,– 21 Jul 2005',\n",
              " 'Paperback,– 12 Jul 2018',\n",
              " 'Paperback,– 1 Sep 2008',\n",
              " 'Paperback,– 31 Oct 2013',\n",
              " 'Paperback,– 23 Oct 2014',\n",
              " 'Hardcover,– 2011',\n",
              " 'Paperback,– 2 Aug 2013',\n",
              " 'Paperback,– Mar 2016',\n",
              " 'Paperback,– 24 Oct 2017',\n",
              " 'Paperback,– 18 Oct 2016',\n",
              " 'Hardcover,– 27 Jan 2017',\n",
              " 'Paperback,– 24 Mar 2005',\n",
              " 'Paperback,– 13 Feb 2018',\n",
              " 'Paperback,– 4 Aug 2015',\n",
              " 'Paperback,– 14 Oct 2010',\n",
              " 'Hardcover,– 15 Nov 2016',\n",
              " 'Paperback,– 4 Oct 2016',\n",
              " 'Paperback,– 1 Jun 2016',\n",
              " 'Paperback,– 1995',\n",
              " 'Paperback,– 3 Jul 2014',\n",
              " 'Paperback,– 2014',\n",
              " 'Hardcover,– Illustrated, 28 Aug 2018',\n",
              " 'Paperback,– 27 May 2014',\n",
              " 'Paperback,– 30 Jun 2017',\n",
              " 'Paperback,– Illustrated, 7 May 2015',\n",
              " 'Paperback,– 19 Mar 2013',\n",
              " 'Paperback,– 27 Aug 2015',\n",
              " 'Paperback,– 11 Sep 2014',\n",
              " 'Paperback,– 8 Mar 2017',\n",
              " 'Paperback,– 10 Jan 2019',\n",
              " 'Paperback,– 4 Oct 2015',\n",
              " 'Paperback,– Import, 4 Jun 2018',\n",
              " 'Paperback,– 5 Dec 2002',\n",
              " 'Paperback,– 1 Jan 2008',\n",
              " 'Paperback,– 20 Jun 2016',\n",
              " 'Paperback,– 30 Oct 1998',\n",
              " 'Paperback,– 16 Jun 2016',\n",
              " 'Paperback,– 20 Sep 2017',\n",
              " 'Paperback,– Import, 13 Aug 2015',\n",
              " 'Paperback,– Import, 14 Jun 2016',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 1 Jul 2014',\n",
              " 'Paperback,– 5 Apr 2016',\n",
              " 'Paperback,– 25 Feb 2010',\n",
              " 'Paperback,– 16 Apr 2019',\n",
              " 'Paperback,– Import, 19 Apr 2018',\n",
              " 'Paperback,– 30 Jun 2016',\n",
              " 'Paperback,– 20 Feb 2003',\n",
              " 'Paperback,– 5 Jul 2016',\n",
              " 'Paperback,– 1 Sep 2016',\n",
              " 'Paperback,– 18 Mar 2019',\n",
              " 'Paperback,– 20 Aug 2018',\n",
              " 'Paperback,– 16 Aug 2011',\n",
              " 'Paperback,– 9 Dec 2010',\n",
              " 'Paperback,– 8 Aug 2008',\n",
              " 'Paperback,– Import, 6 Oct 2015',\n",
              " 'Hardcover,– 1 Oct 1997',\n",
              " 'Paperback,– Import, 22 Mar 2018',\n",
              " 'Paperback,– 25 Feb 2019',\n",
              " 'Hardcover,– 7 Dec 2018',\n",
              " 'Paperback,– 10 May 2017',\n",
              " 'Paperback,– 20 Feb 2019',\n",
              " 'Paperback,– 1 Oct 2000',\n",
              " 'Paperback,– 14 Jun 2018',\n",
              " 'Paperback,– 23 Mar 2017',\n",
              " 'Paperback,– 5 Jan 1988',\n",
              " 'Paperback,– 1 Sep 2008',\n",
              " 'Paperback,– 25 Mar 1982',\n",
              " 'Paperback,– 16 Feb 2016',\n",
              " 'Paperback,– 24 Feb 2015',\n",
              " 'Paperback,– 8 May 2017',\n",
              " 'Paperback,– 4 Oct 2018',\n",
              " 'Paperback,– 2016',\n",
              " 'Paperback,– 16 Jun 2010',\n",
              " 'Paperback,– 1 Dec 1998',\n",
              " 'Paperback,– 6 Feb 2012',\n",
              " 'Hardcover,– 1 Sep 2008',\n",
              " 'Paperback,– 30 Jun 2016',\n",
              " 'Paperback,– 1 Apr 2019',\n",
              " 'Paperback,– 29 Nov 2011',\n",
              " 'Paperback,– 13 Sep 2012',\n",
              " 'Paperback,– 10 Jun 2017',\n",
              " 'Paperback,– 10 Jul 2017',\n",
              " 'Paperback,– 5 Oct 2015',\n",
              " 'Paperback,– 1 Mar 2019',\n",
              " 'Paperback,– 1 Sep 2016',\n",
              " 'Loose Leaf,– 13 Oct 2015',\n",
              " 'Paperback,– 27 Oct 2017',\n",
              " 'Mass Market Paperback,– 7 Aug 2008',\n",
              " 'Paperback,– 5 Oct 2016',\n",
              " 'Paperback,– 25 Apr 2019',\n",
              " 'Paperback,– 5 Sep 2006',\n",
              " 'Paperback,– 22 Jun 2017',\n",
              " 'Paperback,– 15 Jan 2013',\n",
              " 'Paperback,– 2016',\n",
              " 'Paperback,– 8 Mar 2007',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 15 Nov 2011',\n",
              " 'Paperback,– 3 Jul 1997',\n",
              " 'Paperback,– 11 Oct 2017',\n",
              " 'Hardcover,– 9 Feb 2016',\n",
              " 'Paperback,– 2 Jan 2015',\n",
              " 'Paperback,– Import, 19 Jul 2016',\n",
              " 'Paperback,– 18 Aug 2015',\n",
              " 'Paperback,– 1 Sep 1990',\n",
              " 'Paperback,– 16 Sep 2014',\n",
              " 'Paperback,– 2 Feb 1998',\n",
              " 'Paperback,– 1 Jul 2011',\n",
              " 'Paperback,– 16 Dec 2011',\n",
              " 'Paperback,– 2016',\n",
              " 'Paperback,– 3 Nov 2015',\n",
              " 'Paperback,– 17 Jul 2013',\n",
              " 'Paperback,– 20 Apr 2016',\n",
              " 'Paperback,– 9 Oct 2018',\n",
              " 'Paperback,– Import, 9 Jan 2019',\n",
              " 'Paperback,– 2 May 2017',\n",
              " 'Paperback,– 8 Jan 2015',\n",
              " 'Hardcover,– 6 Jun 2017',\n",
              " 'Paperback,– 5 Feb 2019',\n",
              " 'Paperback,– 30 Oct 2017',\n",
              " 'Paperback,– 1971',\n",
              " 'Paperback,– 7 Jun 2017',\n",
              " 'Paperback,– 25 Apr 2018',\n",
              " 'Hardcover,– 28 Jun 2018',\n",
              " 'Paperback,– 18 Jan 2018',\n",
              " 'Paperback,– 31 Oct 2006',\n",
              " 'Paperback,– 3 Mar 2003',\n",
              " 'Mass Market Paperback,– 6 Nov 2006',\n",
              " 'Paperback,– 3 Dec 1992',\n",
              " 'Hardcover,– 21 Feb 2017',\n",
              " 'Hardcover,– 1 Feb 2015',\n",
              " 'Paperback,– 20 Sep 2018',\n",
              " 'Hardcover,– 16 Sep 2008',\n",
              " 'Paperback,– 12 Feb 2013',\n",
              " 'Hardcover,– 2008',\n",
              " 'Paperback,– 31 May 2018',\n",
              " 'Hardcover,– 3 Nov 2015',\n",
              " 'Paperback,– 22 Dec 2015',\n",
              " 'Hardcover,– Import, 18 Sep 2018',\n",
              " 'Sheet music,– 6 Jun 2017',\n",
              " 'Mass Market Paperback,– 29 Jul 2014',\n",
              " 'Paperback,– 26 Apr 2012',\n",
              " 'Paperback,– 2013',\n",
              " 'Paperback,– 27 Jul 2010',\n",
              " 'Paperback,– 22 Jul 2018',\n",
              " 'Paperback,– 26 Aug 2004',\n",
              " 'Mass Market Paperback,– 5 Oct 2000',\n",
              " 'Paperback,– 30 Sep 2017',\n",
              " 'Paperback,– 20 Oct 2018',\n",
              " 'Paperback,– Import, 24 Nov 2017',\n",
              " 'Paperback,– 11 Jul 2016',\n",
              " 'Paperback,– Import, 28 Nov 2017',\n",
              " 'Paperback,– 2004',\n",
              " 'Paperback,– 25 Jan 2018',\n",
              " 'Paperback,– 28 Nov 2011',\n",
              " 'Paperback,– 4 Jun 1997',\n",
              " 'Paperback,– 8 Nov 2011',\n",
              " 'Paperback,– 31 Dec 2015',\n",
              " 'Paperback,– 27 Sep 2016',\n",
              " 'Paperback,– 18 Apr 2016',\n",
              " 'Paperback,– 31 Dec 2012',\n",
              " 'Sheet music,– Import, 7 Jun 2018',\n",
              " 'Paperback,– 11 Sep 2014',\n",
              " 'Paperback,– 1 Feb 1996',\n",
              " 'Paperback,– Student Edition, 24 Apr 2008',\n",
              " 'Paperback,– 1 Apr 2011',\n",
              " 'Paperback,– 11 Nov 2010',\n",
              " 'Hardcover,– 15 Dec 2018',\n",
              " 'Paperback,– 24 Sep 2009',\n",
              " 'Hardcover,– 17 Sep 2018',\n",
              " 'Paperback,– 10 Mar 2016',\n",
              " 'Paperback,– 16 May 2016',\n",
              " 'Hardcover,– 27 Nov 2018',\n",
              " 'Paperback,– 9 Dec 2014',\n",
              " 'Mass Market Paperback,– 2015',\n",
              " 'Hardcover,– 2 Sep 2004',\n",
              " 'Hardcover,– 1 Dec 2012',\n",
              " 'Paperback,– 1 Apr 2010',\n",
              " 'Paperback,– 7 Jun 2016',\n",
              " 'Paperback,– 10 Jan 2018',\n",
              " 'Paperback,– 3 Jul 2017',\n",
              " 'Paperback,– 26 Feb 2008',\n",
              " 'Paperback,– 20 Mar 2003',\n",
              " 'Paperback,– 30 Sep 2016',\n",
              " 'Paperback,– Unabridged, 1 Sep 2013',\n",
              " 'Paperback,– 3 Nov 2015',\n",
              " 'Paperback,– 25 Jan 2008',\n",
              " 'Paperback,– 26 May 2005',\n",
              " 'Hardcover,– 18 Oct 2018',\n",
              " 'Paperback,– 26 Sep 2018',\n",
              " 'Paperback,– 5 May 2003',\n",
              " 'Mass Market Paperback,– 1 Nov 1983',\n",
              " 'Paperback,– 3 Feb 2014',\n",
              " 'Paperback,– 2012',\n",
              " 'Paperback,– 14 Apr 2017',\n",
              " 'Paperback,– 24 Nov 2016',\n",
              " 'Flexibound,– 1 Dec 2016',\n",
              " 'Hardcover,– 1 Jan 2019',\n",
              " 'Paperback,– 21 Jan 2009',\n",
              " 'Paperback,– Nov 2013',\n",
              " 'Paperback,– 12 Oct 2015',\n",
              " 'Mass Market Paperback,– 27 Dec 2016',\n",
              " 'Paperback,– 13 Dec 2016',\n",
              " 'Paperback,– Import, 24 Aug 2017',\n",
              " 'Paperback,– 1 Sep 2016',\n",
              " 'Mass Market Paperback,– 28 Apr 2015',\n",
              " 'Paperback,– 1 Dec 2012',\n",
              " 'Paperback,– 3 Oct 2002',\n",
              " 'Paperback,– 30 Nov 2006',\n",
              " 'Paperback,– 18 Jan 2019',\n",
              " 'Paperback,– 23 Mar 2017',\n",
              " 'Paperback,– 21 Jul 2018',\n",
              " 'Paperback,– 9 Feb 1993',\n",
              " 'Paperback,– 13 Dec 2016',\n",
              " 'Paperback,– 1 Feb 2005',\n",
              " 'Paperback,– 5 Dec 2013',\n",
              " 'Paperback,– 9 Nov 1998',\n",
              " 'Paperback,– 5 Dec 2007',\n",
              " 'Paperback,– 27 Mar 2014',\n",
              " 'Paperback,– 22 Jan 2016',\n",
              " 'Paperback,– 16 Oct 2014',\n",
              " 'Paperback,– 23 Dec 2010',\n",
              " 'Paperback,– 2 Apr 1994',\n",
              " 'Paperback,– 15 Jan 2013',\n",
              " 'Paperback,– Special Edition, 11 Sep 2014',\n",
              " 'Hardcover,– 25 Jul 2014',\n",
              " 'Paperback,– 11 Oct 2018',\n",
              " 'Mass Market Paperback,– 29 Nov 2016',\n",
              " 'Paperback,– 13 Jul 2004',\n",
              " 'Paperback,– 9 May 2013',\n",
              " 'Paperback,– 15 May 2008',\n",
              " 'Hardcover,– 6 Nov 2012',\n",
              " 'Paperback,– 6 Apr 1998',\n",
              " 'Paperback,– 6 Oct 2017',\n",
              " 'Paperback,– 7 Nov 2002',\n",
              " 'Paperback,– 2012',\n",
              " 'Paperback,– 18 Dec 2015',\n",
              " 'Hardcover,– 3 Dec 2009',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 14 Aug 2014',\n",
              " 'Paperback,– 4 Nov 2014',\n",
              " 'Paperback,– 2 Jan 2004',\n",
              " 'Paperback,– 15 Sep 2016',\n",
              " 'Paperback,– 11 Jun 1997',\n",
              " 'Paperback,– 1 Jan 1987',\n",
              " 'Paperback,– 28 Apr 2010',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– Import, 13 Mar 2018',\n",
              " 'Paperback,– 30 Sep 2017',\n",
              " 'Paperback,– 8 Oct 2017',\n",
              " 'Mass Market Paperback,– 28 Feb 2006',\n",
              " 'Hardcover,– Import, 26 Mar 2019',\n",
              " 'Paperback,– 5 May 2016',\n",
              " 'Mass Market Paperback,– 1 Sep 1994',\n",
              " 'Hardcover,– Import, 27 Oct 1999',\n",
              " 'Hardcover,– Import, 27 Sep 2005',\n",
              " 'Paperback,– 28 Dec 2010',\n",
              " 'Paperback,– 25 Oct 2006',\n",
              " 'Paperback,– 17 May 2016',\n",
              " 'Paperback,– Import, 6 Sep 2018',\n",
              " 'Paperback,– 1 Jul 2012',\n",
              " 'Paperback,– 24 Aug 2007',\n",
              " 'Paperback,– 2 Feb 2012',\n",
              " 'Paperback,– Import, 31 May 2017',\n",
              " 'Hardcover,– 9 Oct 2012',\n",
              " 'Hardcover,– 10 Aug 2016',\n",
              " 'Paperback,– Jul 2018',\n",
              " 'Paperback,– 26 May 2006',\n",
              " 'Paperback,– 23 Aug 2017',\n",
              " 'Paperback,– 12 Oct 2001',\n",
              " 'Paperback,– 11 Jan 2017',\n",
              " 'Paperback,– 5 Apr 1993',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 22 Jul 2010',\n",
              " 'Paperback,– Import, 2018',\n",
              " 'Paperback,– 16 Jul 2015',\n",
              " 'Hardcover,– 20 Jun 2003',\n",
              " 'Paperback,– 19 May 2015',\n",
              " 'Paperback,– 29 Apr 2003',\n",
              " 'Paperback,– 22 Mar 2011',\n",
              " 'Paperback,– 3 Sep 2015',\n",
              " 'Paperback,– 1 Feb 2017',\n",
              " 'Paperback,– 9 Aug 2017',\n",
              " 'Paperback,– Import, 22 Feb 2018',\n",
              " 'Paperback,– 9 Apr 2015',\n",
              " 'Paperback,– 1 Jan 2002',\n",
              " 'Paperback,– 19 Aug 2005',\n",
              " 'Paperback,– 15 Jul 2013',\n",
              " 'Hardcover,– 28 Aug 2018',\n",
              " 'Paperback,– 14 Apr 2016',\n",
              " 'Paperback,– Import, 9 Oct 2014',\n",
              " 'Paperback,– 3 Apr 2018',\n",
              " 'Paperback,– 16 Feb 2012',\n",
              " 'Paperback,– 10 Aug 2011',\n",
              " 'Paperback,– Illustrated, 9 Jun 2006',\n",
              " 'Paperback,– 12 Mar 1985',\n",
              " 'Hardcover,– 30 Aug 2016',\n",
              " 'Paperback,– 1 May 2008',\n",
              " 'Paperback,– 1 Jan 2007',\n",
              " 'Paperback,– 25 Oct 2016',\n",
              " 'Paperback,– 2019',\n",
              " 'Paperback,– 4 Jun 2015',\n",
              " 'Hardcover,– 2 Jun 2016',\n",
              " 'Paperback,– 31 Jul 2014',\n",
              " 'Paperback,– Import, 18 Dec 2018',\n",
              " 'Paperback,– 30 Jun 2015',\n",
              " 'Paperback,– 29 Apr 2003',\n",
              " 'Hardcover,– 1 Apr 2014',\n",
              " 'Paperback,– 18 Sep 2015',\n",
              " 'Paperback,– 23 Mar 2018',\n",
              " 'Paperback,– May 2013',\n",
              " 'Paperback,– 2 Jun 2014',\n",
              " 'Paperback,– 2013',\n",
              " 'Paperback,– 8 Apr 2014',\n",
              " 'Paperback,– Import, 9 Jan 2019',\n",
              " 'Hardcover,– 10 Apr 2019',\n",
              " 'Paperback,– 1 Jun 2019',\n",
              " 'Paperback,– 7 Mar 2019',\n",
              " 'Paperback,– 1 Sep 2011',\n",
              " 'Paperback,– 7 Oct 2016',\n",
              " 'Paperback,– 18 Mar 2019',\n",
              " 'Paperback,– 12 Jun 2012',\n",
              " 'Paperback,– 13 Oct 2015',\n",
              " 'Paperback,– 7 Nov 2012',\n",
              " 'Paperback,– 10 Aug 2018',\n",
              " 'Hardcover,– 11 Oct 2012',\n",
              " 'Paperback,– 2017',\n",
              " 'Paperback,– 15 Apr 2003',\n",
              " 'Paperback,– 30 Apr 2019',\n",
              " 'Paperback,– May 2019',\n",
              " 'Hardcover,– 29 Apr 2019',\n",
              " 'Paperback,– 30 Jul 2015',\n",
              " 'Paperback,– 6 Jan 2011',\n",
              " 'Paperback,– 5 Sep 2005',\n",
              " 'Paperback,– 27 Sep 2017',\n",
              " 'Hardcover,– 3 Feb 2018',\n",
              " 'Paperback,– 2 Feb 2006',\n",
              " 'Paperback,– 19 Oct 1998',\n",
              " 'Paperback,– 30 May 2006',\n",
              " 'Paperback,– Illustrated, Import',\n",
              " 'Paperback,– 8 Dec 2015',\n",
              " 'Paperback,– Import, 26 Feb 2019',\n",
              " 'Paperback,– 3 Jul 2017',\n",
              " 'Paperback,– 1 Nov 2007',\n",
              " 'Paperback,– 2 Aug 2011',\n",
              " 'Paperback,– 2001',\n",
              " 'Paperback,– Import, 28 Feb 2018',\n",
              " 'Paperback,– 5 Oct 2017',\n",
              " 'Paperback,– 26 May 2011',\n",
              " 'Paperback,– 11 Apr 2017',\n",
              " 'Paperback,– Import, 1 Jan 2005',\n",
              " 'Hardcover,– 25 Oct 2016',\n",
              " 'Paperback,– 8 Mar 2012',\n",
              " 'Paperback,– 22 Jun 2015',\n",
              " 'Paperback,– 24 Oct 2013',\n",
              " 'Paperback,– 20 Jun 2017',\n",
              " 'Paperback,– Import, 31 Mar 2019',\n",
              " 'Paperback,– 29 Mar 2011',\n",
              " 'Mass Market Paperback,– 25 Feb 2014',\n",
              " 'Hardcover,– 2 Aug 2009',\n",
              " 'Paperback,– 4 May 2004',\n",
              " 'Paperback,– 1 Jan 2002',\n",
              " 'Hardcover,– Import, Jan 2016',\n",
              " 'Paperback,– 26 Aug 2014',\n",
              " 'Paperback,– 2 Aug 2012',\n",
              " 'Paperback,– 30 Oct 2017',\n",
              " 'Paperback,– 13 May 2013',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the characteristics of the \"edition\" I assume that the certain month that a book was published wouldn't affect its price that much, but the year and the format of the book are absolutely effective.\n",
        "For instance there must be a huge gap between the price of a hardcover book that had been published in 2000 and a paperback book in the currenct year.\n",
        "So we change the \"edition\" values in the following way:"
      ],
      "metadata": {
        "id": "qn1l_40blFa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_edition=[]\n",
        "for i in edition:\n",
        "  values = i.split(\",– \")\n",
        "  format = values[0]\n",
        "  year = values[1].split(\" \")[-1]\n",
        "  ED = format+year\n",
        "  new_edition.append(ED)\n",
        "new_edition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_AI3WWO8mOT",
        "outputId": "70a7ed2e-d022-40f9-e70c-0c02e9319b8e"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Paperback2016',\n",
              " 'Paperback2012',\n",
              " 'Paperback1982',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2006',\n",
              " 'Paperback2009',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback1999',\n",
              " 'Paperback2002',\n",
              " 'Paperback2011',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2016',\n",
              " 'Mass Market Paperback1991',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Hardcover2011',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Hardcover2014',\n",
              " 'Paperback1989',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Paperback2000',\n",
              " 'Paperback2005',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2014',\n",
              " 'Paperback2009',\n",
              " 'Paperback2006',\n",
              " 'Paperback2013',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2013',\n",
              " 'Paperback2008',\n",
              " 'Hardcover2015',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2014',\n",
              " 'Paperback2006',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Sheet music2018',\n",
              " 'Mass Market Paperback2000',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2004',\n",
              " 'Paperback2018',\n",
              " 'Paperback2014',\n",
              " 'Hardcover1999',\n",
              " 'Paperback2010',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Paperback2011',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2007',\n",
              " 'Paperback2008',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2012',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2001',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2019',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2019',\n",
              " 'Paperback1969',\n",
              " 'Paperback2019',\n",
              " 'Paperback2008',\n",
              " 'Paperback2011',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2009',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2013',\n",
              " 'Paperback2004',\n",
              " 'Paperback2014',\n",
              " 'Paperback2004',\n",
              " 'Paperback2019',\n",
              " 'Flexibound2018',\n",
              " 'Paperback2010',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2001',\n",
              " 'Paperback1993',\n",
              " 'Paperback2016',\n",
              " 'Paperback2009',\n",
              " 'Paperback2014',\n",
              " 'Paperback2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2002',\n",
              " 'Paperback2000',\n",
              " 'Paperback2013',\n",
              " 'Paperback2011',\n",
              " 'Sheet music2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2008',\n",
              " 'Paperback2009',\n",
              " 'Hardcover2009',\n",
              " 'Paperback1999',\n",
              " 'Paperback2008',\n",
              " 'Paperback2013',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback1992',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2012',\n",
              " 'Paperback2018',\n",
              " 'Paperback2009',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2006',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2011',\n",
              " 'Paperback2010',\n",
              " 'Paperback2003',\n",
              " 'Paperback2003',\n",
              " 'Paperback2017',\n",
              " 'Paperback1992',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback1996',\n",
              " 'Hardcover2009',\n",
              " 'Paperback2018',\n",
              " 'Paperback2018',\n",
              " 'PaperbackImport',\n",
              " 'Paperback2018',\n",
              " 'Paperback2011',\n",
              " 'Paperback2009',\n",
              " 'Paperback2000',\n",
              " 'Paperback2012',\n",
              " 'Paperback2012',\n",
              " 'Mass Market Paperback1982',\n",
              " 'Paperback2014',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2009',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2018',\n",
              " 'Paperback1997',\n",
              " 'Paperback2011',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2010',\n",
              " 'Paperback2008',\n",
              " 'Paperback2016',\n",
              " 'Paperback1989',\n",
              " 'Paperback2014',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Mass Market Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback2010',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2011',\n",
              " 'Hardcover2017',\n",
              " 'Mass Market Paperback1982',\n",
              " 'Paperback2010',\n",
              " 'Paperback2011',\n",
              " 'Paperback1995',\n",
              " 'Paperback2018',\n",
              " 'Paperback2007',\n",
              " 'Paperback2004',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Paperback2011',\n",
              " 'Paperback2011',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2009',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2019',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Plastic CombNTSC',\n",
              " 'Paperback2012',\n",
              " 'Paperback2007',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback1987',\n",
              " 'Paperback1999',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2011',\n",
              " 'Paperback2003',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2013',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2003',\n",
              " 'Paperback2014',\n",
              " 'Paperback2017',\n",
              " 'Paperback2001',\n",
              " 'Paperback2013',\n",
              " 'Paperback2011',\n",
              " 'Paperback2016',\n",
              " 'Paperback2011',\n",
              " 'Paperback2003',\n",
              " 'Paperback2012',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2002',\n",
              " 'Paperback2002',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2012',\n",
              " 'Paperback2002',\n",
              " 'Paperback2003',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2007',\n",
              " 'Paperback2014',\n",
              " 'Paperback2010',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Paperback2014',\n",
              " 'Mass Market Paperback2010',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2014',\n",
              " 'Paperback2010',\n",
              " 'Paperback1986',\n",
              " 'Mass Market Paperback2016',\n",
              " 'Paperback2013',\n",
              " 'Paperback2003',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2014',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback1990',\n",
              " 'Paperback2016',\n",
              " 'Paperback1991',\n",
              " 'Paperback2009',\n",
              " 'Mass Market Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2018',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2010',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2012',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2012',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2007',\n",
              " 'Hardcover2012',\n",
              " 'Paperback2014',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2010',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2008',\n",
              " 'Paperback2009',\n",
              " 'Paperback2017',\n",
              " 'Mass Market Paperback2016',\n",
              " 'Paperback2011',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2018',\n",
              " 'Loose Leaf2006',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback1988',\n",
              " 'Paperback2003',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2007',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2004',\n",
              " 'Paperback2017',\n",
              " 'Paperback2006',\n",
              " 'Paperback2003',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2011',\n",
              " 'Paperback2016',\n",
              " 'Mass Market Paperback1991',\n",
              " 'Mass Market Paperback2013',\n",
              " 'Hardcover2013',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Mass Market Paperback1991',\n",
              " 'Mass Market Paperback1993',\n",
              " 'Mass Market Paperback1981',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2009',\n",
              " 'Paperback2012',\n",
              " 'Paperback2000',\n",
              " 'Paperback1988',\n",
              " 'Paperback1989',\n",
              " 'Paperback2018',\n",
              " 'Paperback1976',\n",
              " 'Paperback2005',\n",
              " 'Hardcover2008',\n",
              " 'Paperback2008',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2014',\n",
              " 'Paperback2013',\n",
              " 'Paperback1994',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2012',\n",
              " 'Paperback2007',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2004',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2009',\n",
              " 'Hardcover2010',\n",
              " 'Paperback2011',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2014',\n",
              " 'Paperback2011',\n",
              " 'Paperback2010',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2012',\n",
              " 'Paperback2009',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2014',\n",
              " 'Paperback1999',\n",
              " 'Paperback2005',\n",
              " 'Paperback2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback1992',\n",
              " 'Paperback1996',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2013',\n",
              " 'Paperback2008',\n",
              " 'Paperback2009',\n",
              " 'Paperback2015',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2000',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback2018',\n",
              " 'Paperback1998',\n",
              " 'Paperback2013',\n",
              " 'Paperback1992',\n",
              " 'Hardcover2018',\n",
              " 'Mass Market Paperback1982',\n",
              " 'Paperback2019',\n",
              " 'Paperback2014',\n",
              " 'Paperback1996',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2018',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback1996',\n",
              " 'Paperback2009',\n",
              " 'Paperback2005',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2009',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2008',\n",
              " 'Paperback2015',\n",
              " 'Paperback2012',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2013',\n",
              " 'Paperback2013',\n",
              " 'Paperback2017',\n",
              " 'Paperback2000',\n",
              " 'Paperback2016',\n",
              " 'Paperback2007',\n",
              " 'Paperback2019',\n",
              " 'Paperback2010',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2015',\n",
              " 'Paperback2010',\n",
              " 'Paperback1998',\n",
              " 'Paperback2008',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2011',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Paperback2011',\n",
              " 'Paperback2015',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2018',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2012',\n",
              " 'Mass Market Paperback2004',\n",
              " 'Paperback2013',\n",
              " 'Paperback2018',\n",
              " 'Paperback2009',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2011',\n",
              " 'Paperback2007',\n",
              " 'Paperback1994',\n",
              " 'Paperback2005',\n",
              " 'Paperback2019',\n",
              " 'Paperback1992',\n",
              " 'Hardcover2015',\n",
              " 'Hardcover2003',\n",
              " 'Paperback2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback2011',\n",
              " 'Paperback2008',\n",
              " 'Paperback2013',\n",
              " 'Paperback2003',\n",
              " 'Paperback2012',\n",
              " 'Paperback2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2009',\n",
              " 'Paperback2017',\n",
              " 'Paperback2014',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2011',\n",
              " 'Paperback2014',\n",
              " 'Paperback2003',\n",
              " 'Hardcover2010',\n",
              " 'Paperback2017',\n",
              " 'Paperback1977',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback1988',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2019',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2006',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2013',\n",
              " 'Paperback2004',\n",
              " 'Paperback2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2004',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback1974',\n",
              " 'Paperback2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2007',\n",
              " 'Paperback1997',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Hardcover1983',\n",
              " 'Paperback2016',\n",
              " 'Paperback2008',\n",
              " 'Paperback2009',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'PaperbackImport',\n",
              " 'Paperback1988',\n",
              " 'Hardcover2005',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2007',\n",
              " 'Paperback2016',\n",
              " 'Paperback2010',\n",
              " 'Mass Market Paperback1996',\n",
              " 'Paperback2010',\n",
              " 'Paperback2015',\n",
              " 'Paperback2010',\n",
              " 'Paperback2013',\n",
              " 'Tankobon Softcover2016',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Paperback2014',\n",
              " 'Paperback1976',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Mass Market Paperback2016',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Paperback2008',\n",
              " 'Hardcover2018',\n",
              " 'Hardcover2013',\n",
              " 'Paperback2016',\n",
              " 'Paperback1995',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback1982',\n",
              " 'Perfect Paperback2003',\n",
              " 'Paperback1994',\n",
              " 'Paperback2011',\n",
              " 'Paperback2006',\n",
              " 'Paperback2010',\n",
              " 'Paperback2017',\n",
              " 'Paperback2014',\n",
              " 'Mass Market Paperback2018',\n",
              " 'Paperback2000',\n",
              " 'Mass Market Paperback2010',\n",
              " 'Paperback2015',\n",
              " 'Paperback2015',\n",
              " 'Paperback2006',\n",
              " 'Paperback2014',\n",
              " 'Paperback2018',\n",
              " 'Paperback2008',\n",
              " 'Paperback2005',\n",
              " 'Paperback2018',\n",
              " 'Paperback2008',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Hardcover2011',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2005',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback1995',\n",
              " 'Paperback2014',\n",
              " 'Paperback2014',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2014',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Paperback2014',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2002',\n",
              " 'Paperback2008',\n",
              " 'Paperback2016',\n",
              " 'Paperback1998',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2014',\n",
              " 'Paperback2016',\n",
              " 'Paperback2010',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2003',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2011',\n",
              " 'Paperback2010',\n",
              " 'Paperback2008',\n",
              " 'Paperback2015',\n",
              " 'Hardcover1997',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2000',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback1988',\n",
              " 'Paperback2008',\n",
              " 'Paperback1982',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2010',\n",
              " 'Paperback1998',\n",
              " 'Paperback2012',\n",
              " 'Hardcover2008',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2011',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Paperback2019',\n",
              " 'Paperback2016',\n",
              " 'Loose Leaf2015',\n",
              " 'Paperback2017',\n",
              " 'Mass Market Paperback2008',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2006',\n",
              " 'Paperback2017',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Paperback2007',\n",
              " 'Paperback2019',\n",
              " 'Paperback2011',\n",
              " 'Paperback1997',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback1990',\n",
              " 'Paperback2014',\n",
              " 'Paperback1998',\n",
              " 'Paperback2011',\n",
              " 'Paperback2011',\n",
              " 'Paperback2016',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback1971',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback2006',\n",
              " 'Paperback2003',\n",
              " 'Mass Market Paperback2006',\n",
              " 'Paperback1992',\n",
              " 'Hardcover2017',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2008',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2008',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2015',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2018',\n",
              " 'Sheet music2017',\n",
              " 'Mass Market Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback2013',\n",
              " 'Paperback2010',\n",
              " 'Paperback2018',\n",
              " 'Paperback2004',\n",
              " 'Mass Market Paperback2000',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2004',\n",
              " 'Paperback2018',\n",
              " 'Paperback2011',\n",
              " 'Paperback1997',\n",
              " 'Paperback2011',\n",
              " 'Paperback2015',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2012',\n",
              " 'Sheet music2018',\n",
              " 'Paperback2014',\n",
              " 'Paperback1996',\n",
              " 'Paperback2008',\n",
              " 'Paperback2011',\n",
              " 'Paperback2010',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2009',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2014',\n",
              " 'Mass Market Paperback2015',\n",
              " 'Hardcover2004',\n",
              " 'Hardcover2012',\n",
              " 'Paperback2010',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2008',\n",
              " 'Paperback2003',\n",
              " 'Paperback2016',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Paperback2008',\n",
              " 'Paperback2005',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2018',\n",
              " 'Paperback2003',\n",
              " 'Mass Market Paperback1983',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Flexibound2016',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2009',\n",
              " 'Paperback2013',\n",
              " 'Paperback2015',\n",
              " 'Mass Market Paperback2016',\n",
              " 'Paperback2016',\n",
              " 'Paperback2017',\n",
              " 'Paperback2016',\n",
              " 'Mass Market Paperback2015',\n",
              " 'Paperback2012',\n",
              " 'Paperback2002',\n",
              " 'Paperback2006',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback1993',\n",
              " 'Paperback2016',\n",
              " 'Paperback2005',\n",
              " 'Paperback2013',\n",
              " 'Paperback1998',\n",
              " 'Paperback2007',\n",
              " 'Paperback2014',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2010',\n",
              " 'Paperback1994',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2018',\n",
              " 'Mass Market Paperback2016',\n",
              " 'Paperback2004',\n",
              " 'Paperback2013',\n",
              " 'Paperback2008',\n",
              " 'Hardcover2012',\n",
              " 'Paperback1998',\n",
              " 'Paperback2017',\n",
              " 'Paperback2002',\n",
              " 'Paperback2012',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2009',\n",
              " 'Paperback2019',\n",
              " 'Paperback2014',\n",
              " 'Paperback2014',\n",
              " 'Paperback2004',\n",
              " 'Paperback2016',\n",
              " 'Paperback1997',\n",
              " 'Paperback1987',\n",
              " 'Paperback2010',\n",
              " 'Paperback2019',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Mass Market Paperback2006',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2016',\n",
              " 'Mass Market Paperback1994',\n",
              " 'Hardcover1999',\n",
              " 'Hardcover2005',\n",
              " 'Paperback2010',\n",
              " 'Paperback2006',\n",
              " 'Paperback2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2007',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2012',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2018',\n",
              " 'Paperback2006',\n",
              " 'Paperback2017',\n",
              " 'Paperback2001',\n",
              " 'Paperback2017',\n",
              " 'Paperback1993',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2010',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2003',\n",
              " 'Paperback2015',\n",
              " 'Paperback2003',\n",
              " 'Paperback2011',\n",
              " 'Paperback2015',\n",
              " 'Paperback2017',\n",
              " 'Paperback2017',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2002',\n",
              " 'Paperback2005',\n",
              " 'Paperback2013',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2018',\n",
              " 'Paperback2012',\n",
              " 'Paperback2011',\n",
              " 'Paperback2006',\n",
              " 'Paperback1985',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2008',\n",
              " 'Paperback2007',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2015',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2018',\n",
              " 'Paperback2015',\n",
              " 'Paperback2003',\n",
              " 'Hardcover2014',\n",
              " 'Paperback2015',\n",
              " 'Paperback2018',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Paperback2013',\n",
              " 'Paperback2014',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2019',\n",
              " 'Paperback2019',\n",
              " 'Paperback2011',\n",
              " 'Paperback2016',\n",
              " 'Paperback2019',\n",
              " 'Paperback2012',\n",
              " 'Paperback2015',\n",
              " 'Paperback2012',\n",
              " 'Paperback2018',\n",
              " 'Hardcover2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2003',\n",
              " 'Paperback2019',\n",
              " 'Paperback2019',\n",
              " 'Hardcover2019',\n",
              " 'Paperback2015',\n",
              " 'Paperback2011',\n",
              " 'Paperback2005',\n",
              " 'Paperback2017',\n",
              " 'Hardcover2018',\n",
              " 'Paperback2006',\n",
              " 'Paperback1998',\n",
              " 'Paperback2006',\n",
              " 'PaperbackImport',\n",
              " 'Paperback2015',\n",
              " 'Paperback2019',\n",
              " 'Paperback2017',\n",
              " 'Paperback2007',\n",
              " 'Paperback2011',\n",
              " 'Paperback2001',\n",
              " 'Paperback2018',\n",
              " 'Paperback2017',\n",
              " 'Paperback2011',\n",
              " 'Paperback2017',\n",
              " 'Paperback2005',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2012',\n",
              " 'Paperback2015',\n",
              " 'Paperback2013',\n",
              " 'Paperback2017',\n",
              " 'Paperback2019',\n",
              " 'Paperback2011',\n",
              " 'Mass Market Paperback2014',\n",
              " 'Hardcover2009',\n",
              " 'Paperback2004',\n",
              " 'Paperback2002',\n",
              " 'Hardcover2016',\n",
              " 'Paperback2014',\n",
              " 'Paperback2012',\n",
              " 'Paperback2017',\n",
              " 'Paperback2013',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = set(new_edition)\n",
        "num_unique_values = len(unique_values)\n",
        "print(num_unique_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVFoNRK19CmQ",
        "outputId": "e2a6ad8d-cb44-4477-f725-580a9399c3e2"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see just by omitting the month from the edition the number of unqiue values for this feature decreased highly."
      ],
      "metadata": {
        "id": "Q2z9n1kil2A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cat_df['New_Edition'] = new_edition\n",
        "cat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0HHBxUvn9fUw",
        "outputId": "b3d4a2b6-3583-4905-ad30-54df52c8ae4d"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life   \n",
              "\n",
              "                   Edition                                           Synopsis  \\\n",
              "0  Paperback,– 10 Mar 2016  THE HUNTERS return in their third brilliant no...   \n",
              "1   Paperback,– 7 Nov 2012  A layered portrait of a troubled genius for wh...   \n",
              "2  Paperback,– 25 Feb 1982  \"During the time men live without a common Pow...   \n",
              "3   Paperback,– 5 Oct 2017  A handful of grain is found in the pocket of a...   \n",
              "4  Hardcover,– 10 Oct 2006  For seven decades, \"Life\" has been thrilling t...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "2                                Humour  299.00            6          4.8   \n",
              "3             Crime, Thriller & Mystery  180.00           13          4.1   \n",
              "4              Arts, Film & Photography  965.62            1          5.0   \n",
              "\n",
              "     New_Edition  \n",
              "0  Paperback2016  \n",
              "1  Paperback2012  \n",
              "2  Paperback1982  \n",
              "3  Paperback2017  \n",
              "4  Hardcover2006  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9662f04a-d2c8-4350-86be-15a743ad932d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Edition</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>New_Edition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>Paperback,– 10 Mar 2016</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Paperback2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>Paperback,– 7 Nov 2012</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Paperback2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>Paperback,– 25 Feb 1982</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>Paperback1982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>Paperback,– 5 Oct 2017</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Paperback2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>Hardcover,– 10 Oct 2006</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>Arts, Film &amp; Photography</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Hardcover2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9662f04a-d2c8-4350-86be-15a743ad932d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9662f04a-d2c8-4350-86be-15a743ad932d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9662f04a-d2c8-4350-86be-15a743ad932d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e22c478e-9add-4448-9edf-9dfa717df4c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e22c478e-9add-4448-9edf-9dfa717df4c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e22c478e-9add-4448-9edf-9dfa717df4c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop(['Edition'],axis=1)"
      ],
      "metadata": {
        "id": "BdtG3RXb9xVG"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CFmkqdP694EG",
        "outputId": "4cfa8b49-f291-452a-ede5-d9395439c1dd"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "3  A handful of grain is found in the pocket of a...   \n",
              "4  For seven decades, \"Life\" has been thrilling t...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "2                                Humour  299.00            6          4.8   \n",
              "3             Crime, Thriller & Mystery  180.00           13          4.1   \n",
              "4              Arts, Film & Photography  965.62            1          5.0   \n",
              "\n",
              "     New_Edition  \n",
              "0  Paperback2016  \n",
              "1  Paperback2012  \n",
              "2  Paperback1982  \n",
              "3  Paperback2017  \n",
              "4  Hardcover2006  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f94b645d-67f4-4f32-9f86-2e5d24d4d1b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>New_Edition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Paperback2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Paperback2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>Paperback1982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Paperback2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>Arts, Film &amp; Photography</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Hardcover2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f94b645d-67f4-4f32-9f86-2e5d24d4d1b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f94b645d-67f4-4f32-9f86-2e5d24d4d1b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f94b645d-67f4-4f32-9f86-2e5d24d4d1b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fec38178-2981-448f-bcd1-e41a92f75480\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fec38178-2981-448f-bcd1-e41a92f75480')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fec38178-2981-448f-bcd1-e41a92f75480 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now there's something we can do for better results.\n",
        "We can divide the value into two different sections. one will be the format of the book and the other will be for the year of publishment.\n",
        "This will help us very much because year is a numerical value and therefore there will be no encoding needed for that."
      ],
      "metadata": {
        "id": "vkYGVzVzmFTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def divide(string):\n",
        "    match = re.match(r\"([a-zA-Z ]+)([0-9]+)\", string)\n",
        "    if match:\n",
        "        format = match.group(1)\n",
        "        year = match.group(2)\n",
        "        return format, year\n"
      ],
      "metadata": {
        "id": "VYpc2eIhWfJn"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f , m = divide(\"paperback2002\")\n",
        "print(f,m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzG1qWGhYXcC",
        "outputId": "d6949282-5ca1-42a0-fd63-f51d442f2d30"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paperback 2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Format = []\n",
        "production_year=[]\n",
        "for ed in cat_df[\"New_Edition\"].values.tolist():\n",
        "      print(ed)\n",
        "      format , year = divide(ed)\n",
        "      Format.append(format)\n",
        "      production_year.append(year)\n",
        "print(Format)\n",
        "print(production_year)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OxLlqi7WWkHR",
        "outputId": "c304320a-ae2f-424c-9939-979c01c730f2"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Format = []\\nproduction_year=[]\\nfor ed in cat_df[\"New_Edition\"].values.tolist():\\n      print(ed)\\n      format , year = divide(ed)\\n      Format.append(format)\\n      production_year.append(year)\\nprint(Format)\\nprint(production_year)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we defined a function for this purpose. It's clear though that we're facing some problems. Which is that eventhough there were not any empty cells fot the edition there exists some missing values. like the word \"Import\".\n",
        "We have to handle it in some way."
      ],
      "metadata": {
        "id": "oK8ly_fGm6gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for ed in cat_df['New_Edition'].values.tolist():\n",
        "   if \"Import\" in ed :\n",
        "    count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhkRdbajbTbn",
        "outputId": "1d685307-850c-490d-f8ad-030321500b06"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for ed in cat_df['New_Edition'].values.tolist():\n",
        "   if \"HardcoverUnabridged\" in ed :\n",
        "    count += 1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJTe6LgzdUtV",
        "outputId": "f3fae033-956d-4059-b214-87df73cfda80"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''cat_df = cat_df[~cat_df['New_Edition'].str.contains('Import')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('CombNTSC')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('Facsimile')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('Paperbackset')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('PaperbackEdition')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('Hardcoverset')]\n",
        "cat_df = cat_df[~cat_df['New_Edition'].str.contains('HardcoverUnabridged')]\n",
        "\n",
        "cat_df.shape'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OebnWzRuctBy",
        "outputId": "54cd2864-fc1e-471f-f793-46f0f8e9d3e4"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"cat_df = cat_df[~cat_df['New_Edition'].str.contains('Import')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('CombNTSC')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('Facsimile')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('Paperbackset')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('PaperbackEdition')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('Hardcoverset')]\\ncat_df = cat_df[~cat_df['New_Edition'].str.contains('HardcoverUnabridged')]\\n\\ncat_df.shape\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the number of these values that can cause us difficulty is fairly large eventhough the number of the occurence of each one is few. So it's better to somehow fill those in instead of dropping their rows."
      ],
      "metadata": {
        "id": "K4om1yHlrfM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def divide(string):\n",
        "    global prev_year\n",
        "    match = re.match(r\"([a-zA-Z ]+)([0-9]+)\", string)\n",
        "    if match:\n",
        "        format = match.group(1)\n",
        "        year = match.group(2)\n",
        "        prev_year = year\n",
        "        return format, year\n",
        "    else:\n",
        "        format = string\n",
        "        year = prev_year\n",
        "        return format, year"
      ],
      "metadata": {
        "id": "bykl-UrhoAF8"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Format = []\n",
        "production_year=[]\n",
        "for ed in cat_df[\"New_Edition\"].values.tolist():\n",
        "      print(ed)\n",
        "      format , year = divide(ed)\n",
        "      Format.append(format)\n",
        "      production_year.append(year)\n",
        "print(Format)\n",
        "print(production_year)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjgG9EMoHt5",
        "outputId": "8cb20525-5ca9-45bb-ce75-6c5bba8af664"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback1998\n",
            "Paperback2012\n",
            "Hardcover2008\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Loose Leaf2015\n",
            "Paperback2017\n",
            "Mass Market Paperback2008\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback1997\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback1990\n",
            "Paperback2014\n",
            "Paperback1998\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback1971\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2003\n",
            "Mass Market Paperback2006\n",
            "Paperback1992\n",
            "Hardcover2017\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Hardcover2008\n",
            "Paperback2013\n",
            "Hardcover2008\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Sheet music2017\n",
            "Mass Market Paperback2014\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2004\n",
            "Mass Market Paperback2000\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback1997\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Sheet music2018\n",
            "Paperback2014\n",
            "Paperback1996\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2009\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Mass Market Paperback2015\n",
            "Hardcover2004\n",
            "Hardcover2012\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2008\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2005\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Mass Market Paperback1983\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Flexibound2016\n",
            "Hardcover2019\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Mass Market Paperback2016\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Mass Market Paperback2015\n",
            "Paperback2012\n",
            "Paperback2002\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback1993\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback1998\n",
            "Paperback2007\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback1994\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Hardcover2014\n",
            "Paperback2018\n",
            "Mass Market Paperback2016\n",
            "Paperback2004\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Hardcover2012\n",
            "Paperback1998\n",
            "Paperback2017\n",
            "Paperback2002\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Hardcover2009\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Paperback1997\n",
            "Paperback1987\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Mass Market Paperback2006\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Mass Market Paperback1994\n",
            "Hardcover1999\n",
            "Hardcover2005\n",
            "Paperback2010\n",
            "Paperback2006\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2007\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Hardcover2012\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback1993\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2003\n",
            "Paperback2015\n",
            "Paperback2003\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2002\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2006\n",
            "Paperback1985\n",
            "Hardcover2016\n",
            "Paperback2008\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2003\n",
            "Hardcover2014\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Hardcover2019\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Hardcover2012\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Hardcover2019\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2006\n",
            "Paperback1998\n",
            "Paperback2006\n",
            "PaperbackImport\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2001\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Mass Market Paperback2014\n",
            "Hardcover2009\n",
            "Paperback2004\n",
            "Paperback2002\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Paperback2003\n",
            "Paperback1992\n",
            "Paperback2005\n",
            "Paperback1986\n",
            "Paperback1989\n",
            "Paperback2001\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback1998\n",
            "Hardcover2016\n",
            "Paperback2006\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Hardcover2009\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Hardcover2014\n",
            "Paperback2003\n",
            "Paperback2015\n",
            "Paperback2007\n",
            "Paperback2002\n",
            "Hardcover2018\n",
            "Paperback2003\n",
            "Paperback2008\n",
            "Paperback1996\n",
            "Mass Market Paperback2018\n",
            "Paperback2019\n",
            "Paperback2006\n",
            "Paperback1999\n",
            "Paperback2017\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2000\n",
            "Hardcover2016\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Hardcover2014\n",
            "Hardcover2018\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Mass Market Paperback1986\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback1999\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback1990\n",
            "Paperback1994\n",
            "Paperback1996\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Mass Market Paperback1991\n",
            "Paperback2013\n",
            "Paperback1997\n",
            "Paperback2018\n",
            "Mass Market Paperback1983\n",
            "Paperback2001\n",
            "Paperback2018\n",
            "Hardcover1991\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Paperback2000\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Paperback2014\n",
            "Paperback2006\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Mass Market Paperback2017\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2004\n",
            "Mass Market Paperback2011\n",
            "Paperback2003\n",
            "Paperback2012\n",
            "Mass Market Paperback1995\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Mass Market Paperback1985\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2006\n",
            "Paperback2010\n",
            "Hardcover2019\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback1995\n",
            "Hardcover2014\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback1993\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Mass Market Paperback2007\n",
            "Hardcover1999\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Hardcover2011\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2008\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Hardcover2019\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback1999\n",
            "Paperback2010\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback1971\n",
            "Paperback2016\n",
            "Paperback2000\n",
            "Mass Market Paperback2010\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback1999\n",
            "Mass Market Paperback2011\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Hardcover2018\n",
            "PaperbackImport\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2007\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Mass Market Paperback2019\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Hardcover2014\n",
            "Paperback2016\n",
            "Paperback1988\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2001\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2001\n",
            "Paperback2002\n",
            "Hardcover2012\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Sheet music2017\n",
            "Paperback2018\n",
            "Hardcover2000\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Mass Market Paperback2014\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Mass Market Paperback1986\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Mass Market Paperback1996\n",
            "Hardcover2017\n",
            "Hardcover2002\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2000\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2003\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2006\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Mass Market Paperback2005\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Mass Market Paperback1989\n",
            "Paperback1995\n",
            "Paperback2013\n",
            "Paperback2006\n",
            "Paperback2014\n",
            "Paperback1997\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Hardcover2014\n",
            "Paperback2009\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2004\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2010\n",
            "Paperback2008\n",
            "Paperback2009\n",
            "Paperback1995\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Mass Market Paperback2008\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback2012\n",
            "Hardcover2013\n",
            "Paperback2017\n",
            "Mass Market Paperback2016\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback1994\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback1989\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2010\n",
            "Paperback2003\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Hardcover2009\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Hardcover2014\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback1998\n",
            "Paperback2004\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Hardcover2011\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Hardcover2004\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Hardcover2001\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2006\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback1997\n",
            "Mass Market Paperback2015\n",
            "Paperback2005\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Hardcover2012\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback1989\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Hardcover2016\n",
            "Hardcover2009\n",
            "Paperback2018\n",
            "Paperback1982\n",
            "Hardcover2015\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2000\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback1978\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Hardcover2019\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2006\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "HardcoverFacsimile\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Paperback1997\n",
            "Mass Market Paperback2018\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2001\n",
            "Paperback2001\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback1992\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Mass Market Paperback2018\n",
            "Paperback2010\n",
            "Paperback2000\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback1985\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2004\n",
            "Paperback2008\n",
            "Paperback2006\n",
            "Paperback2012\n",
            "Hardcover2003\n",
            "Paperback2013\n",
            "Mass Market Paperback1995\n",
            "Paperbackset\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Mass Market Paperback1997\n",
            "Hardcover2002\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Hardcover2014\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Paperback2009\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "PaperbackImport\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "PaperbackEdition\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Mass Market Paperback1985\n",
            "Hardcover1997\n",
            "Hardcover2015\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback1992\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2007\n",
            "Paperback2015\n",
            "Paperback1988\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Sheet music2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Mass Market Paperback1995\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Hardcover2018\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback1992\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Hardcover2019\n",
            "Hardcover2018\n",
            "Mass Market Paperback2009\n",
            "Hardcover2010\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback1997\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2005\n",
            "Paperback2000\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Sheet music2017\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Hardcover2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Hardcover2015\n",
            "Paperback2013\n",
            "Hardcover2019\n",
            "Hardcover2011\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Hardcover2010\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Mass Market Paperback1994\n",
            "Paperback1998\n",
            "Hardcover2019\n",
            "Paperback1993\n",
            "Paperback1987\n",
            "Hardcover2018\n",
            "Paperback2000\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Hardcover2014\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Hardcover2011\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Hardcover2016\n",
            "Hardcover2014\n",
            "Hardcoverset\n",
            "Paperback2000\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Hardcover2013\n",
            "Hardcover2003\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback1995\n",
            "Board book2013\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover1964\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Paperback2015\n",
            "Hardcover2010\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Mass Market Paperback1984\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2007\n",
            "Paperback2004\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2004\n",
            "Mass Market Paperback2016\n",
            "Paperback1982\n",
            "Paperback2018\n",
            "Paperback1992\n",
            "Paperback2016\n",
            "Paperback1993\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback1994\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback1999\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback1990\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback1996\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2003\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2010\n",
            "Paperback1999\n",
            "Hardcover2009\n",
            "Mass Market Paperback1983\n",
            "Paperback2012\n",
            "Paperback1993\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Mass Market Paperback2016\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Hardcover2012\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Hardcover2009\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Mass Market Paperback2000\n",
            "Paperback2010\n",
            "Hardcover2010\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Flexibound2013\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2001\n",
            "Paperback2006\n",
            "Hardcover2016\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Mass Market Paperback1985\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2014\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback1992\n",
            "Paperback2004\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback1998\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback1986\n",
            "Paperback2005\n",
            "Hardcover2017\n",
            "Paperback1992\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback1992\n",
            "Paperback2015\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2002\n",
            "Paperback2013\n",
            "Paperback1991\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2000\n",
            "Paperback2006\n",
            "Mass Market Paperback2007\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Hardcover2003\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Mass Market Paperback1971\n",
            "Paperback2013\n",
            "Paperback2002\n",
            "Paperback1988\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback1994\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2011\n",
            "Hardcover2002\n",
            "Hardcover2019\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Hardcover2019\n",
            "Mass Market Paperback1988\n",
            "Hardcover2012\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2011\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Mass Market Paperback2003\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Mass Market Paperback2013\n",
            "Paperback1983\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2009\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Hardcover2015\n",
            "Paperback2003\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Mass Market Paperback2019\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Hardcover2019\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2007\n",
            "Paperback2012\n",
            "Paperback2002\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback1988\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback1994\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback1980\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback1996\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "HardcoverUnabridged\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2017\n",
            "Paperback1993\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2009\n",
            "Paperback2006\n",
            "Paperback1984\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback1964\n",
            "Paperback2010\n",
            "Hardcover2016\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Paperback2003\n",
            "Paperback2014\n",
            "Paperback1979\n",
            "Hardcover2019\n",
            "Paperback2008\n",
            "Mass Market Paperback1989\n",
            "Hardcover2014\n",
            "Paperback2008\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Hardcover2002\n",
            "Paperback2006\n",
            "Hardcover2009\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Hardcover1997\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2003\n",
            "Paperback2013\n",
            "Mass Market Paperback2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Mass Market Paperback1984\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback1993\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2012\n",
            "Mass Market Paperback2015\n",
            "Paperback2012\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback1988\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2007\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback1995\n",
            "Paperback1995\n",
            "Paperback2000\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback1993\n",
            "Hardcover2012\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Hardcover2008\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2011\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Mass Market Paperback2006\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Sheet music2018\n",
            "Paperback2019\n",
            "Paperback1971\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "PaperbackPrint\n",
            "Paperback2013\n",
            "Paperback1993\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2000\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Cards2016\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2004\n",
            "Mass Market Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback1984\n",
            "Hardcover2017\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2004\n",
            "Paperback2002\n",
            "Hardcover2009\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Spiral-bound2016\n",
            "Paperback2011\n",
            "Hardcover2009\n",
            "Hardcover2013\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Hardcover1997\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback1991\n",
            "Paperback2012\n",
            "Paperback2004\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Hardcover2019\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Hardcover2012\n",
            "Paperback2006\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback1994\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Hardcover2017\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Paperback1993\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2002\n",
            "Paperback1990\n",
            "(Kannada),Paperback2014\n",
            "Paperback1991\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2001\n",
            "Paperback2008\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2009\n",
            "Mass Market Paperback2011\n",
            "Paperback2019\n",
            "Paperback2003\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback1998\n",
            "Mass Market Paperback1987\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2001\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Flexibound2016\n",
            "Paperback1995\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback1984\n",
            "Paperback2001\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Mass Market Paperback1992\n",
            "Hardcover2017\n",
            "Hardcover2014\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback1998\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback1994\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Hardcover2016\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2006\n",
            "Hardcover2019\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2002\n",
            "Hardcover2015\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Hardcover2008\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback1991\n",
            "Paperback2018\n",
            "Paperback1976\n",
            "Hardcover2016\n",
            "Product Bundle2016\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Hardcover2006\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback1995\n",
            "Paperback1999\n",
            "Paperback2012\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Paperback2011\n",
            "Mass Market Paperback1984\n",
            "Paperback2009\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2001\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Library Binding2013\n",
            "Paperback2010\n",
            "Paperback2003\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Hardcover2002\n",
            "Paperback2005\n",
            "Hardcover2009\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Paperback2010\n",
            "Paperback2010\n",
            "Paperback1974\n",
            "Hardcover2009\n",
            "Paperback2019\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Hardcover2015\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback1993\n",
            "Cards2005\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback1998\n",
            "Hardcover2015\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Hardcover2013\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback2002\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Hardcover2016\n",
            "Hardcover2013\n",
            "Paperback2001\n",
            "Paperback2016\n",
            "Hardcover2015\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Paperback2010\n",
            "Hardcover2016\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Flexibound2011\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2007\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2008\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2001\n",
            "Paperback2006\n",
            "Paperback2008\n",
            "Paperback1996\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback1991\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2011\n",
            "Hardcover2009\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Hardcover2011\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Hardcover2016\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2015\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback1997\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback1994\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback1977\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Sheet music2017\n",
            "Paperback2016\n",
            "Paperbackset\n",
            "Paperback2019\n",
            "Hardcover2012\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2003\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2019\n",
            "Mass Market Paperback2009\n",
            "Paperback2016\n",
            "Paperback1999\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1989\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Hardcover1995\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Hardcover2008\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1994\n",
            "Paperback2011\n",
            "Paperback1997\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2006\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Sheet music1987\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Hardcover2007\n",
            "Hardcover2011\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2002\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2008\n",
            "Paperback2007\n",
            "Mass Market Paperback1986\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback1992\n",
            "Paperback2012\n",
            "Paperback2007\n",
            "Hardcover2016\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback1960\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Mass Market Paperback2016\n",
            "Paperback2001\n",
            "Paperback1960\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback1999\n",
            "Paperback2006\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback1992\n",
            "PaperbackImport\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Mass Market Paperback2002\n",
            "Hardcover2016\n",
            "Paperback1999\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Mass Market Paperback2015\n",
            "Paperback1989\n",
            "Paperback2008\n",
            "Paperback2006\n",
            "Mass Market Paperback2002\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Mass Market Paperback1997\n",
            "Paperback1998\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Hardcover2015\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback1990\n",
            "Hardcover2011\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2007\n",
            "Paperback2002\n",
            "Paperback2005\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback2017\n",
            "Paperback2001\n",
            "Paperback2018\n",
            "Cards2015\n",
            "Paperback2001\n",
            "Paperback2019\n",
            "Paperback2005\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Hardcover2017\n",
            "Paperback2019\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2000\n",
            "Mass Market Paperback2007\n",
            "Hardcover2014\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Hardcover2017\n",
            "Paperback2009\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2010\n",
            "Paperback2000\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Paperback1974\n",
            "Hardcover2016\n",
            "Paperback2004\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback2008\n",
            "Paperback1995\n",
            "Paperback2016\n",
            "Paperback1983\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2009\n",
            "Hardcover2018\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Mass Market Paperback2015\n",
            "Paperback2011\n",
            "Paperback1999\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Flexibound2015\n",
            "Hardcover2015\n",
            "Paperback1998\n",
            "Paperback1992\n",
            "Paperback2017\n",
            "Paperback2008\n",
            "Paperback2003\n",
            "Hardcover2015\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback1994\n",
            "Hardcover2011\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Hardcover2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2010\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback1983\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Flexibound2018\n",
            "Paperback2012\n",
            "Paperback2009\n",
            "Paperback1990\n",
            "Paperback2019\n",
            "Paperback2004\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover2009\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Hardcover2019\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Hardcover2014\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Hardcover2009\n",
            "Mass Market Paperback2017\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Hardcover2014\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback1990\n",
            "Paperback2006\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Flexibound2016\n",
            "Paperback2010\n",
            "Hardcover2016\n",
            "Paperback2006\n",
            "Paperback2007\n",
            "Paperback2012\n",
            "Sheet music2018\n",
            "Paperback2015\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Mass Market Paperback2013\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Hardcover2011\n",
            "Paperback2004\n",
            "Hardcover2016\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Hardcover2014\n",
            "Paperback1991\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Mass Market Paperback1989\n",
            "Paperback2016\n",
            "Paperback1994\n",
            "Paperback2010\n",
            "Hardcover2005\n",
            "Hardcover2009\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Hardcover2016\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Paperback1970\n",
            "Paperback2013\n",
            "Paperback1997\n",
            "Paperback2019\n",
            "Hardcover2000\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Hardcover1998\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback1997\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Hardcover2018\n",
            "Paperback2004\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2004\n",
            "Hardcover2008\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2012\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2002\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Flexibound2014\n",
            "Hardcover2014\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback1997\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback1995\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback2006\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Hardcover2013\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback1995\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2002\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Hardcover2016\n",
            "Hardcover2009\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Hardcover2008\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2001\n",
            "Paperback2015\n",
            "Hardcover2003\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Hardcover2014\n",
            "Hardcover2010\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Hardcover2014\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2019\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Hardcover2014\n",
            "Hardcover2019\n",
            "Paperback2004\n",
            "Hardcover2017\n",
            "Mass Market Paperback1996\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback1986\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Sheet music2017\n",
            "Paperback2014\n",
            "Mass Market Paperback1989\n",
            "(German),Paperback2014\n",
            "Paperback2009\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Paperback1991\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2004\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Hardcover2015\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2006\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2002\n",
            "Paperback2017\n",
            "Paperback1980\n",
            "Tankobon Softcover2017\n",
            "Paperback1994\n",
            "Hardcover2015\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Mass Market Paperback2016\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover1998\n",
            "Hardcover2008\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback1990\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2014\n",
            "Cards2014\n",
            "Mass Market Paperback1986\n",
            "Paperback2017\n",
            "Paperback1996\n",
            "Hardcover2014\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback1975\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback1998\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Mass Market Paperback2009\n",
            "Paperback2016\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2013\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperback2002\n",
            "Paperback1992\n",
            "Paperback1987\n",
            "Paperback2000\n",
            "Paperback2010\n",
            "Paperback2006\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Hardcover2018\n",
            "Paperback2003\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Hardcover2013\n",
            "Paperback2013\n",
            "Cards2017\n",
            "Paperback2012\n",
            "Hardcover2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback1996\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Hardcover2008\n",
            "Paperback2017\n",
            "Mass Market Paperback2017\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback1985\n",
            "Paperback1984\n",
            "Paperback2011\n",
            "Paperback1992\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2013\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Cards2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Hardcover2012\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Mass Market Paperback2001\n",
            "Paperback2000\n",
            "Paperback1991\n",
            "Hardcover2009\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperbackset\n",
            "Hardcover2008\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2012\n",
            "Paperback2003\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Mass Market Paperback2013\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Hardcover2018\n",
            "Paperback2010\n",
            "Paperback2006\n",
            "Paperback2000\n",
            "Paperback1997\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Mass Market Paperback2018\n",
            "Paperback2008\n",
            "Mass Market Paperback2002\n",
            "Hardcover2018\n",
            "Hardcover2019\n",
            "Paperback2006\n",
            "Paperback1998\n",
            "Hardcover2018\n",
            "Mass Market Paperback2015\n",
            "Hardcover2019\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Hardcover2006\n",
            "Paperback2016\n",
            "Hardcover2014\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback1992\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback1995\n",
            "Hardcover2015\n",
            "Paperback1999\n",
            "Paperback2010\n",
            "Mass Market Paperback1989\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Mass Market Paperback1991\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Mass Market Paperback1986\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback1996\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2001\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Hardcover2009\n",
            "Flexibound2016\n",
            "Paperback2011\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Hardcover2009\n",
            "Paperback2017\n",
            "Hardcover2016\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback2000\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback1905\n",
            "Hardcover2019\n",
            "Paperback2002\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback1996\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Hardcover2014\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback1983\n",
            "Paperback2017\n",
            "Hardcover2013\n",
            "Paperback2016\n",
            "Paperback1994\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Hardcover2013\n",
            "Paperback2015\n",
            "Paperback2007\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Hardcover2013\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Mass Market Paperback2005\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Hardcover2017\n",
            "Paperback1995\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Hardcover2015\n",
            "Mass Market Paperback2015\n",
            "Hardcover2014\n",
            "Paperback2016\n",
            "Flexibound2018\n",
            "Paperback2000\n",
            "Hardcover2013\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Hardcover2009\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Mass Market Paperback1983\n",
            "Paperback2010\n",
            "Paperback2000\n",
            "Hardcover2016\n",
            "Paperback2002\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback1997\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2012\n",
            "Hardcover2017\n",
            "Paperback2011\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Hardcover2011\n",
            "Hardcover1988\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback1900\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1993\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Sheet music2017\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2008\n",
            "Paperback1964\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Mass Market Paperback1994\n",
            "Sheet music2007\n",
            "Paperback2017\n",
            "Paperback1995\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Hardcover2014\n",
            "Hardcover2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2011\n",
            "Paperback2004\n",
            "Paperback2008\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback1996\n",
            "Paperback2014\n",
            "Hardcover2012\n",
            "Paperback1997\n",
            "Paperback2008\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2019\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Sheet music2017\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Hardcover2013\n",
            "Paperback2011\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2019\n",
            "Hardcover2019\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2010\n",
            "Paperback1998\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "HardcoverImport\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Hardcover1999\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback1996\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback1989\n",
            "Paperback2011\n",
            "Paperback1994\n",
            "Paperback2019\n",
            "Hardcover2011\n",
            "Paperback2015\n",
            "Hardcover2013\n",
            "Hardcover2010\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1987\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Sheet music2018\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2000\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Hardcover2016\n",
            "Hardcover2017\n",
            "Paperback1992\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Hardcover2016\n",
            "Mass Market Paperback1984\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2009\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Hardcover2019\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2002\n",
            "Paperback2014\n",
            "PaperbackAudiobook\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2005\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2001\n",
            "Paperback2009\n",
            "Hardcover2014\n",
            "Hardcover2016\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Mass Market Paperback2014\n",
            "Paperback2009\n",
            "Paperback2010\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Leather Bound2017\n",
            "Hardcover2013\n",
            "Paperback2001\n",
            "Hardcover2013\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback1997\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Mass Market Paperback2013\n",
            "Paperback2012\n",
            "Mass Market Paperback2017\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2002\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "PaperbackImport\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback1982\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2006\n",
            "Paperback2014\n",
            "Paperback1999\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback1987\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Hardcover2008\n",
            "Paperback2015\n",
            "Paperback1999\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2004\n",
            "Mass Market Paperback1982\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback1990\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Hardcover2018\n",
            "Paperback2006\n",
            "Hardcover2014\n",
            "Hardcover2012\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback1994\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Hardcover2002\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Hardcover2016\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Hardcover2008\n",
            "Paperback2000\n",
            "Paperback2019\n",
            "Hardcover1999\n",
            "Paperback1991\n",
            "Paperback2005\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2009\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Paperback2003\n",
            "Hardcover2018\n",
            "Paperback1988\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2001\n",
            "Paperback2002\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2012\n",
            "Hardcover2016\n",
            "Paperback1987\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Hardcover2009\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Hardcover2011\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2006\n",
            "Paperback2015\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Paperback2010\n",
            "Paperback2010\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2006\n",
            "Paperback1992\n",
            "Hardcover2015\n",
            "Mass Market Paperback1985\n",
            "Paperback2012\n",
            "Hardcover2009\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2003\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Mass Market Paperback2011\n",
            "Paperback2016\n",
            "Hardcover2016\n",
            "Mass Market Paperback2017\n",
            "Paperback2016\n",
            "Paperback2006\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Paperback2011\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback1996\n",
            "Hardcover2014\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Spiral-bound1986\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2003\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2001\n",
            "Hardcover2019\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2003\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2009\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Paperback2006\n",
            "Hardcover2012\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Paperback2006\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback1994\n",
            "Paperback2000\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2002\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Cards2011\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Hardcover2015\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2003\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2003\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback1999\n",
            "Paperback2015\n",
            "Paperback2009\n",
            "Hardcover2009\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2008\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback1994\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Hardcover2009\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Sheet music2007\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Paperback1984\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Mass Market Paperback1986\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Mass Market Paperback2008\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2010\n",
            "Paperback2013\n",
            "Hardcover2009\n",
            "Paperback2016\n",
            "Paperback1982\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Hardcover2019\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2009\n",
            "Paperback2009\n",
            "Paperback2014\n",
            "HardcoverImport\n",
            "Paperback2010\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Hardcover2014\n",
            "Paperback2013\n",
            "Paperback2007\n",
            "Paperback2015\n",
            "Hardcover2004\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2003\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2014\n",
            "Paperback2002\n",
            "Paperback2014\n",
            "Paperback1985\n",
            "Hardcover2003\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Hardcover2016\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback1991\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Mass Market Paperback2014\n",
            "Paperback1997\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2002\n",
            "Paperback2005\n",
            "Paperback2003\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2002\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2015\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback1998\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback1994\n",
            "Paperback2007\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback1997\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2000\n",
            "Paperback2013\n",
            "Hardcover2014\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Hardcover2016\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback2004\n",
            "Hardcover2018\n",
            "Paperback2011\n",
            "Paperback2004\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Hardcover2015\n",
            "Paperback2010\n",
            "Paperback2005\n",
            "Paperback1999\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Mass Market Paperback1997\n",
            "Hardcover2007\n",
            "Paperback2014\n",
            "Hardcover2013\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2012\n",
            "Hardcover2009\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Hardcover2003\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover2016\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Mass Market Paperback2018\n",
            "Mass Market Paperback2013\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Hardcover2002\n",
            "Hardcover2015\n",
            "Paperback2015\n",
            "Hardcover2017\n",
            "Hardcover2005\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback1994\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2019\n",
            "Paperback2009\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Spiral-bound2007\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Paperback2015\n",
            "Hardcover2004\n",
            "Hardcover2018\n",
            "Mass Market Paperback2015\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Paperback2004\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Hardcover2010\n",
            "Paperback2010\n",
            "Paperback1995\n",
            "Hardcover1992\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2003\n",
            "Hardcover2018\n",
            "Hardcover2016\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2010\n",
            "Paperback2015\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2002\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Mass Market Paperback2011\n",
            "Hardcover2018\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback1998\n",
            "Paperback1990\n",
            "Paperback2000\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Hardcover1993\n",
            "Paperback2015\n",
            "Hardcover2009\n",
            "Paperback1996\n",
            "Paperback2013\n",
            "Paperback2001\n",
            "Paperback1998\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Sheet music2018\n",
            "Hardcover2014\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Hardcover2017\n",
            "Paperback1999\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Hardcover2019\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2014\n",
            "Paperback1999\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Hardcover2019\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback1995\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2004\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2019\n",
            "Paperback1998\n",
            "Paperback2018\n",
            "Hardcover2010\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Hardcover2002\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback1988\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Hardcover2019\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Paperback2015\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Hardcover2016\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback1997\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Hardcover2018\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Hardcover2004\n",
            "Paperback2018\n",
            "Paperback1991\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback1975\n",
            "Paperback2019\n",
            "Hardcover2017\n",
            "Paperback2003\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Hardcover2013\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback1988\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2003\n",
            "Spiral-bound2012\n",
            "Paperback2010\n",
            "Paperback2011\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2006\n",
            "Paperback2011\n",
            "Hardcover2001\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Hardcover2013\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback1995\n",
            "Paperback2012\n",
            "Hardcover2000\n",
            "Paperback2017\n",
            "Paperback1999\n",
            "Paperback1982\n",
            "Paperback2011\n",
            "Hardcover2012\n",
            "Paperback1995\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Hardcover2018\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Hardcover2007\n",
            "Paperback2008\n",
            "Paperback2015\n",
            "Hardcover2014\n",
            "Hardcover2017\n",
            "Hardcover2014\n",
            "Hardcover2002\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Mass Market Paperback1961\n",
            "Paperback1976\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback1994\n",
            "Hardcover2017\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Hardcover2016\n",
            "Paperback2013\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Mass Market Paperback1996\n",
            "Paperback2011\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2006\n",
            "Paperback1989\n",
            "Hardcover2013\n",
            "Paperback2001\n",
            "Paperback2005\n",
            "Paperback2008\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2006\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback1996\n",
            "Paperback2014\n",
            "Hardcover2018\n",
            "Paperback2008\n",
            "Paperback2006\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Hardcover2017\n",
            "Paperback2011\n",
            "Hardcover2016\n",
            "Mass Market Paperback2016\n",
            "Paperback2011\n",
            "Paperback1991\n",
            "Paperback2009\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Hardcover2013\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2002\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Mass Market Paperback2004\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2007\n",
            "Paperback2010\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Hardcover2011\n",
            "Paperback2009\n",
            "Paperback2003\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Hardcover2016\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2009\n",
            "Paperback2008\n",
            "Hardcover2017\n",
            "Paperback2006\n",
            "Paperback2008\n",
            "Hardcover2019\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Paperback2019\n",
            "Mass Market Paperback2018\n",
            "Hardcover2003\n",
            "Paperback2003\n",
            "Paperback1994\n",
            "Paperback2018\n",
            "Paperback2000\n",
            "Paperback2009\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Mass Market Paperback1984\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Mass Market Paperback2017\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback1987\n",
            "Paperback2013\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Paperback2004\n",
            "Hardcover2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1975\n",
            "Paperback2001\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback1999\n",
            "Paperback1994\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2001\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Paperback2011\n",
            "Mass Market Paperback2006\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2009\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback2014\n",
            "Paperback2013\n",
            "Hardcover2011\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2015\n",
            "Hardcover2010\n",
            "Hardcover2018\n",
            "Paperback2010\n",
            "Paperback2001\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Hardcover2017\n",
            "Paperback2011\n",
            "Paperback2002\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback1986\n",
            "Paperback2018\n",
            "Mass Market Paperback2005\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Hardcover2016\n",
            "Paperback1999\n",
            "Paperback2007\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback1998\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Mass Market Paperback2017\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Paperback2005\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Mass Market Paperback1993\n",
            "Paperback2013\n",
            "Paperback2011\n",
            "Paperbackset\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2011\n",
            "Mass Market Paperback2010\n",
            "Paperback2008\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Paperback1983\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2019\n",
            "Paperback2002\n",
            "Hardcover2011\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Flexibound2016\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2013\n",
            "Mass Market Paperback1976\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2005\n",
            "Hardcover1925\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Hardcover2014\n",
            "Paperback2015\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback1999\n",
            "Paperback2013\n",
            "(French),Paperback2010\n",
            "Paperback2009\n",
            "Paperback1992\n",
            "Paperback2011\n",
            "Hardcover2018\n",
            "Sheet music2017\n",
            "Paperback2008\n",
            "Paperback2006\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2019\n",
            "Paperback2011\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Hardcover2008\n",
            "Paperback1986\n",
            "Paperback2014\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2016\n",
            "Paperback2000\n",
            "Hardcover2018\n",
            "Paperback2007\n",
            "Hardcover2009\n",
            "Paperback2005\n",
            "Paperback2019\n",
            "Paperback2014\n",
            "Hardcover2015\n",
            "Paperback2004\n",
            "Paperback2013\n",
            "Paperback2002\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2011\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Paperback1992\n",
            "Paperback2011\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Hardcover2014\n",
            "Paperback2006\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback1976\n",
            "Paperback2009\n",
            "Paperback2010\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback1995\n",
            "Paperback1994\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2011\n",
            "Paperback2001\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Hardcover2009\n",
            "Paperback2014\n",
            "Hardcover2013\n",
            "Paperback2012\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Paperback2016\n",
            "Sheet music2018\n",
            "Hardcover2019\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Hardcover2019\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2004\n",
            "Hardcover2018\n",
            "Paperback2013\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2009\n",
            "Paperback2011\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Paperback1990\n",
            "Paperback1992\n",
            "Paperback2010\n",
            "Paperback2000\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2009\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2011\n",
            "Paperback2012\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback1982\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback1975\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2010\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Hardcover2015\n",
            "Paperback2006\n",
            "Paperback2005\n",
            "Paperback1997\n",
            "Paperback2009\n",
            "Paperback1986\n",
            "Hardcover2018\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Mass Market Paperback2009\n",
            "Hardcover2015\n",
            "Paperback2007\n",
            "Paperback2012\n",
            "Paperback2008\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2015\n",
            "Paperback2019\n",
            "Mass Market Paperback2009\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Hardcover2018\n",
            "Paperback2015\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2015\n",
            "Paperback2016\n",
            "Paperback2006\n",
            "Paperback2002\n",
            "Paperback2010\n",
            "Paperback2018\n",
            "Paperback1999\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2012\n",
            "Paperback2015\n",
            "Paperback2015\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Mass Market Paperback2014\n",
            "Paperback2013\n",
            "Paperback2015\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2013\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2014\n",
            "Paperback2010\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Hardcover2015\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Hardcover2019\n",
            "Paperback2003\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2009\n",
            "Paperback2019\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2006\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback1990\n",
            "Hardcover2016\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2000\n",
            "Paperback2017\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback1999\n",
            "Paperback2003\n",
            "Paperback2008\n",
            "Paperback2011\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2017\n",
            "Paperback2006\n",
            "Hardcover2017\n",
            "Hardcover2017\n",
            "Paperback2005\n",
            "Paperback2015\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2010\n",
            "Paperback2012\n",
            "Paperback2016\n",
            "Hardcover2018\n",
            "Paperback2006\n",
            "Paperback2002\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2007\n",
            "Paperback2007\n",
            "Paperback2019\n",
            "Paperback2001\n",
            "Paperback2014\n",
            "Hardcover2001\n",
            "Paperback2007\n",
            "Paperback2002\n",
            "Paperback2014\n",
            "Paperback1987\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Mass Market Paperback2010\n",
            "Hardcover2016\n",
            "Paperback2012\n",
            "Paperback2004\n",
            "Hardcover2014\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Hardcover2015\n",
            "Paperback2018\n",
            "Paperback1996\n",
            "Hardcover2018\n",
            "Paperback2001\n",
            "Sheet music2017\n",
            "Flexibound2011\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2014\n",
            "Paperback2011\n",
            "Paperback2001\n",
            "Hardcover2016\n",
            "Paperback2011\n",
            "Hardcover2016\n",
            "Paperback2018\n",
            "Paperback2012\n",
            "Hardcover2018\n",
            "Hardcover2017\n",
            "Paperback2017\n",
            "Paperback2013\n",
            "Paperback2003\n",
            "Paperback2000\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2007\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Hardcover2017\n",
            "Paperback2014\n",
            "Hardcover2007\n",
            "Paperback2017\n",
            "Paperback1999\n",
            "Paperback2011\n",
            "Paperback2010\n",
            "Paperback2016\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Hardcover2016\n",
            "Paperback2010\n",
            "Hardcover2019\n",
            "Paperback2012\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2014\n",
            "Hardcover2009\n",
            "Paperback2012\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2005\n",
            "Paperback2014\n",
            "Paperback2005\n",
            "Paperback2000\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2013\n",
            "Hardcover2017\n",
            "Paperback2000\n",
            "Paperback1973\n",
            "Mass Market Paperback1996\n",
            "Paperback2004\n",
            "Paperback2007\n",
            "Paperback2014\n",
            "Hardcover2019\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback1997\n",
            "Paperback2015\n",
            "Hardcover2016\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback2014\n",
            "Paperback1989\n",
            "Paperback2018\n",
            "Sheet music2007\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback1995\n",
            "Hardcover2015\n",
            "Hardcover2018\n",
            "Paperback2008\n",
            "Paperback2017\n",
            "Paperback2010\n",
            "Paperback2015\n",
            "Paperback2005\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2012\n",
            "Paperback2016\n",
            "Paperback1999\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Mass Market Paperback2002\n",
            "Hardcover2019\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2001\n",
            "Paperback2017\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2018\n",
            "Paperback2011\n",
            "Paperback2006\n",
            "Paperback2013\n",
            "Paperback1980\n",
            "Paperback2017\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Paperback2010\n",
            "Paperback2012\n",
            "Paperback2013\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Paperback2004\n",
            "Paperback2001\n",
            "Paperback2011\n",
            "Hardcover2017\n",
            "Hardcover2016\n",
            "Hardcover2016\n",
            "Paperback2001\n",
            "Paperback2007\n",
            "Paperback1993\n",
            "Paperback2007\n",
            "Paperback2013\n",
            "Paperback2003\n",
            "Paperback2017\n",
            "Paperback1989\n",
            "Paperback1985\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2019\n",
            "Paperback2019\n",
            "Hardcover2014\n",
            "Paperback2018\n",
            "Paperback2016\n",
            "Paperback2012\n",
            "Paperback2017\n",
            "Paperback2002\n",
            "Paperback2016\n",
            "Paperback2018\n",
            "Hardcover2013\n",
            "Paperback2005\n",
            "Paperback2011\n",
            "Paperback2017\n",
            "Paperback2014\n",
            "Paperback2017\n",
            "Paperback1998\n",
            "Paperback2016\n",
            "Paperback2010\n",
            "Hardcover2011\n",
            "Paperback2015\n",
            "Paperback2010\n",
            "Hardcover2013\n",
            "Paperback2009\n",
            "Paperback2006\n",
            "Mass Market Paperback1992\n",
            "Hardcover2017\n",
            "Paperback1993\n",
            "Paperback2014\n",
            "Paperback2018\n",
            "Hardcover2018\n",
            "Flexibound2016\n",
            "Paperback2004\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2014\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2019\n",
            "Paperback2012\n",
            "Hardcover2011\n",
            "Paperback1998\n",
            "Paperback2012\n",
            "Hardcover2017\n",
            "Paperback2018\n",
            "Paperback2008\n",
            "Paperback2018\n",
            "Paperback2010\n",
            "Paperback2009\n",
            "Paperback2018\n",
            "Paperback2017\n",
            "Hardcover2018\n",
            "Paperback2019\n",
            "Paperback2017\n",
            "Paperback2018\n",
            "Paperback2005\n",
            "Paperback2016\n",
            "Paperback2016\n",
            "Paperback2008\n",
            "Paperback2016\n",
            "['Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Plastic CombNTSC', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Loose Leaf', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Mass Market Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Tankobon Softcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Perfect Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Loose Leaf', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Sheet music', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'PaperbackImport', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'HardcoverFacsimile', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperbackset', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'PaperbackEdition', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcoverset', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Board book', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'HardcoverUnabridged', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'PaperbackPrint', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Cards', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Spiral-bound2016', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', '(Kannada),Paperback2014', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Product Bundle', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Library Binding', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Cards', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperbackset', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Cards', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Mass Market Paperback', '(German),Paperback2014', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Tankobon Softcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Cards', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Cards', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Cards', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperbackset', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Flexibound', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'HardcoverImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'PaperbackAudiobook', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Leather Bound', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'PaperbackImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Spiral-bound1986', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Cards', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'HardcoverImport', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Spiral-bound2007', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Spiral-bound2012', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperbackset', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', '(French),Paperback2010', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Sheet music', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Sheet music', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Sheet music', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Hardcover', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Mass Market Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Flexibound', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Hardcover', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback', 'Paperback']\n",
            "['2016', '2012', '1982', '2017', '2006', '2009', '2017', '2018', '2015', '2013', '2017', '2018', '1999', '2002', '2011', '2015', '2018', '2016', '1991', '2016', '2018', '2018', '2012', '2014', '2012', '2011', '2016', '2014', '2014', '1989', '2013', '2015', '2000', '2005', '2016', '2019', '2014', '2009', '2006', '2013', '2013', '2013', '2008', '2015', '2019', '2014', '2006', '2014', '2012', '2012', '2017', '2016', '2018', '2000', '2018', '2019', '2017', '2016', '2015', '2013', '2016', '2016', '2004', '2018', '2014', '1999', '2010', '2013', '2014', '2011', '2016', '2016', '2018', '2016', '2016', '2015', '2019', '2007', '2008', '2018', '2015', '2012', '2016', '2016', '2018', '2015', '2001', '2018', '2018', '2019', '2015', '2017', '2017', '2016', '2019', '1969', '2019', '2008', '2011', '2016', '2018', '2014', '2009', '2019', '2016', '2017', '2016', '2017', '2013', '2004', '2014', '2004', '2019', '2018', '2010', '2015', '2016', '2001', '1993', '2016', '2009', '2014', '2015', '2018', '2016', '2015', '2013', '2002', '2000', '2013', '2011', '2017', '2015', '2008', '2009', '2009', '1999', '2008', '2013', '2019', '2018', '2017', '2016', '2015', '1992', '2018', '2016', '2016', '2019', '2012', '2018', '2009', '2016', '2006', '2018', '2019', '2011', '2010', '2003', '2003', '2017', '1992', '2018', '2017', '2017', '1996', '2009', '2018', '2018', '2018', '2018', '2011', '2009', '2000', '2012', '2012', '1982', '2014', '2019', '2017', '2019', '2018', '2009', '2013', '2014', '2015', '2018', '1997', '2011', '2015', '2016', '2015', '2010', '2008', '2016', '1989', '2014', '2015', '2019', '2015', '2017', '2015', '2018', '2017', '2015', '2015', '2017', '2018', '2018', '2010', '2018', '2016', '2018', '2011', '2017', '1982', '2010', '2011', '1995', '2018', '2007', '2004', '2013', '2014', '2011', '2011', '2012', '2017', '2009', '2017', '2014', '2019', '2016', '2016', '2017', '2017', '2012', '2007', '2015', '2013', '1987', '1999', '2013', '2018', '2011', '2003', '2017', '2017', '2013', '2017', '2016', '2015', '2015', '2018', '2003', '2014', '2017', '2001', '2013', '2011', '2016', '2011', '2003', '2012', '2018', '2017', '2017', '2002', '2002', '2016', '2012', '2002', '2003', '2019', '2018', '2007', '2014', '2010', '2017', '2018', '2019', '2014', '2010', '2017', '2019', '2018', '2014', '2010', '1986', '2016', '2013', '2003', '2012', '2017', '2014', '2014', '2012', '1990', '2016', '1991', '2009', '2013', '2016', '2018', '2016', '2018', '2019', '2014', '2016', '2018', '2017', '2010', '2017', '2017', '2012', '2015', '2013', '2016', '2012', '2018', '2007', '2012', '2014', '2016', '2018', '2010', '2016', '2014', '2018', '2015', '2008', '2009', '2017', '2016', '2011', '2017', '2017', '2015', '2013', '2018', '2006', '2019', '2017', '2015', '1988', '2003', '2016', '2017', '2007', '2017', '2018', '2004', '2017', '2006', '2003', '2016', '2012', '2017', '2017', '2011', '2016', '1991', '2013', '2013', '2017', '2016', '2017', '2019', '2010', '2014', '2018', '2015', '2017', '1991', '1993', '1981', '2019', '2015', '2018', '2009', '2012', '2000', '1988', '1989', '2018', '1976', '2005', '2008', '2008', '2019', '2014', '2013', '1994', '2017', '2017', '2016', '2012', '2007', '2016', '2014', '2016', '2016', '2004', '2015', '2017', '2009', '2010', '2011', '2015', '2014', '2011', '2010', '2016', '2019', '2012', '2009', '2017', '2017', '2014', '1999', '2005', '2019', '2015', '1992', '1996', '2017', '2015', '2016', '2013', '2008', '2009', '2015', '2010', '2018', '2017', '2018', '2016', '2018', '2016', '2017', '2000', '2014', '2012', '2018', '1998', '2013', '1992', '2018', '1982', '2019', '2014', '1996', '2016', '2018', '2015', '2017', '2018', '1996', '2009', '2005', '2016', '2019', '2010', '2018', '2009', '2018', '2015', '2019', '2017', '2017', '2008', '2015', '2012', '2018', '2016', '2018', '2017', '2019', '2013', '2013', '2017', '2000', '2016', '2007', '2019', '2010', '2018', '2014', '2015', '2010', '1998', '2008', '2016', '2014', '2011', '2018', '2013', '2016', '2011', '2015', '2015', '2018', '2018', '2012', '2004', '2013', '2018', '2009', '2019', '2015', '2011', '2007', '1994', '2005', '2019', '1992', '2015', '2003', '2019', '2015', '2011', '2008', '2013', '2003', '2012', '2015', '2018', '2019', '2009', '2017', '2014', '2016', '2018', '2012', '2011', '2014', '2003', '2010', '2017', '1977', '2017', '2015', '1988', '2017', '2018', '2018', '2017', '2019', '2015', '2006', '2017', '2016', '2014', '2013', '2004', '2015', '2018', '2015', '2016', '2004', '2016', '2018', '2018', '1974', '2019', '2015', '2016', '2018', '2012', '2007', '1997', '2017', '2017', '2015', '1983', '2016', '2008', '2009', '2017', '2015', '2016', '2015', '2015', '1988', '2005', '2015', '2010', '2018', '2007', '2016', '2010', '1996', '2010', '2015', '2010', '2013', '2016', '2013', '2015', '2014', '1976', '2018', '2012', '2017', '2016', '2013', '2015', '2008', '2018', '2013', '2016', '1995', '2018', '2017', '2018', '1982', '2003', '1994', '2011', '2006', '2010', '2017', '2014', '2018', '2000', '2010', '2015', '2015', '2006', '2014', '2018', '2008', '2005', '2018', '2008', '2013', '2014', '2011', '2013', '2016', '2017', '2016', '2017', '2005', '2018', '2015', '2010', '2016', '2016', '2016', '1995', '2014', '2014', '2018', '2014', '2017', '2015', '2013', '2015', '2014', '2017', '2019', '2015', '2018', '2002', '2008', '2016', '1998', '2016', '2017', '2015', '2016', '2017', '2014', '2016', '2010', '2019', '2018', '2016', '2003', '2016', '2016', '2019', '2018', '2011', '2010', '2008', '2015', '1997', '2018', '2019', '2018', '2017', '2019', '2000', '2018', '2017', '1988', '2008', '1982', '2016', '2015', '2017', '2018', '2016', '2010', '1998', '2012', '2008', '2016', '2019', '2011', '2012', '2017', '2017', '2015', '2019', '2016', '2015', '2017', '2008', '2016', '2019', '2006', '2017', '2013', '2016', '2007', '2019', '2011', '1997', '2017', '2016', '2015', '2016', '2015', '1990', '2014', '1998', '2011', '2011', '2016', '2015', '2013', '2016', '2018', '2019', '2017', '2015', '2017', '2019', '2017', '1971', '2017', '2018', '2018', '2018', '2006', '2003', '2006', '1992', '2017', '2015', '2018', '2008', '2013', '2008', '2018', '2015', '2015', '2018', '2017', '2014', '2012', '2013', '2010', '2018', '2004', '2000', '2017', '2018', '2017', '2016', '2017', '2004', '2018', '2011', '1997', '2011', '2015', '2016', '2016', '2012', '2018', '2014', '1996', '2008', '2011', '2010', '2018', '2009', '2018', '2016', '2016', '2018', '2014', '2015', '2004', '2012', '2010', '2016', '2018', '2017', '2008', '2003', '2016', '2013', '2015', '2008', '2005', '2018', '2018', '2003', '1983', '2014', '2012', '2017', '2016', '2016', '2019', '2009', '2013', '2015', '2016', '2016', '2017', '2016', '2015', '2012', '2002', '2006', '2019', '2017', '2018', '1993', '2016', '2005', '2013', '1998', '2007', '2014', '2016', '2014', '2010', '1994', '2013', '2014', '2014', '2018', '2016', '2004', '2013', '2008', '2012', '1998', '2017', '2002', '2012', '2015', '2009', '2019', '2014', '2014', '2004', '2016', '1997', '1987', '2010', '2019', '2018', '2017', '2017', '2006', '2019', '2016', '1994', '1999', '2005', '2010', '2006', '2016', '2018', '2012', '2007', '2012', '2017', '2012', '2016', '2018', '2006', '2017', '2001', '2017', '1993', '2017', '2017', '2010', '2018', '2015', '2003', '2015', '2003', '2011', '2015', '2017', '2017', '2018', '2015', '2002', '2005', '2013', '2018', '2016', '2014', '2018', '2012', '2011', '2006', '1985', '2016', '2008', '2007', '2016', '2019', '2015', '2016', '2014', '2018', '2015', '2003', '2014', '2015', '2018', '2013', '2014', '2013', '2014', '2019', '2019', '2019', '2019', '2011', '2016', '2019', '2012', '2015', '2012', '2018', '2012', '2017', '2003', '2019', '2019', '2019', '2015', '2011', '2005', '2017', '2018', '2006', '1998', '2006', '2006', '2015', '2019', '2017', '2007', '2011', '2001', '2018', '2017', '2011', '2017', '2005', '2016', '2012', '2015', '2013', '2017', '2019', '2011', '2014', '2009', '2004', '2002', '2016', '2014', '2012', '2017', '2013', '2014', '2004', '2016', '2016', '2014', '2016', '2002', '2003', '1992', '2005', '1986', '1989', '2001', '2006', '2017', '2018', '1998', '2016', '2006', '2012', '2015', '2018', '2009', '2018', '2019', '2011', '2011', '2015', '2015', '2005', '2014', '2003', '2015', '2007', '2002', '2018', '2003', '2008', '1996', '2018', '2019', '2006', '1999', '2017', '2008', '2014', '2000', '2016', '2019', '2018', '2019', '2018', '2019', '2017', '2017', '2010', '2014', '2018', '2011', '2014', '2019', '2009', '2015', '2017', '1986', '2011', '2011', '2014', '1999', '2011', '2016', '2013', '2019', '2016', '2016', '2010', '2013', '1990', '1994', '1996', '2018', '2012', '2015', '1991', '2013', '1997', '2018', '1983', '2001', '2018', '1991', '2016', '2004', '2000', '2016', '2012', '2017', '2016', '2002', '2016', '2019', '2014', '2006', '2018', '2019', '2018', '2014', '2018', '2015', '2010', '2013', '2013', '2012', '2016', '2017', '2018', '2014', '2017', '2012', '2018', '2009', '2018', '2018', '2016', '2018', '2004', '2011', '2003', '2012', '1995', '2013', '2016', '2019', '2007', '2013', '2016', '2017', '1985', '2017', '2016', '2015', '2014', '2018', '2006', '2018', '2015', '2018', '2012', '2013', '2015', '2019', '2018', '2013', '2011', '2017', '2013', '2011', '2013', '2016', '2017', '2010', '2017', '2014', '2006', '2010', '2019', '2019', '2014', '2019', '2014', '2018', '1995', '2014', '2015', '2018', '2016', '1993', '2013', '2017', '2005', '2013', '2016', '2015', '2008', '2007', '1999', '2014', '2016', '2013', '2018', '2011', '2014', '2014', '2018', '2014', '2008', '2018', '2017', '2018', '2018', '2017', '2017', '2008', '2019', '2014', '2014', '2019', '2009', '2015', '2006', '2017', '2015', '1999', '2010', '2008', '2016', '2019', '2018', '1971', '2016', '2000', '2010', '2019', '2016', '2011', '2018', '1999', '2011', '2016', '2016', '2015', '2012', '2018', '2018', '2016', '2005', '2007', '2018', '2017', '2012', '2018', '2014', '2008', '2005', '2016', '2017', '2019', '2017', '2017', '2009', '2014', '2014', '2016', '1988', '2018', '2010', '2015', '2010', '2013', '2016', '2003', '2016', '2019', '2018', '2018', '2012', '2005', '2014', '2010', '2016', '2013', '2014', '2001', '2018', '2003', '2019', '2016', '2001', '2018', '2018', '2016', '2017', '2016', '2015', '2016', '2010', '2013', '2018', '2006', '2017', '2013', '2001', '2002', '2012', '2019', '2016', '2017', '2005', '2018', '2018', '2017', '2017', '2018', '2000', '2017', '2015', '2018', '2014', '2018', '2017', '2018', '2011', '2015', '1986', '2015', '2018', '1996', '2017', '2002', '2001', '2014', '2015', '2019', '2010', '2017', '2000', '2014', '2009', '2019', '2013', '2010', '2017', '2018', '2015', '2013', '2016', '2018', '2017', '2017', '2019', '2003', '2018', '2009', '2017', '2019', '2006', '2015', '2011', '2016', '2005', '2013', '2017', '2017', '2017', '2003', '2018', '2017', '2010', '2019', '2018', '1989', '1995', '2013', '2006', '2014', '1997', '2009', '2018', '2014', '2009', '2007', '2016', '2010', '2017', '2009', '2004', '2012', '2016', '2010', '2011', '2015', '2014', '2019', '2010', '2008', '2009', '1995', '2018', '2016', '2008', '2004', '2017', '2018', '2017', '2017', '2017', '2018', '2007', '2012', '2013', '2017', '2016', '2017', '2016', '2014', '2011', '2018', '2010', '2018', '1994', '2016', '2007', '2013', '2016', '2015', '2008', '2017', '2014', '1989', '2013', '2019', '2011', '2010', '2007', '2018', '2014', '2016', '2007', '2010', '2003', '2013', '2016', '2009', '2013', '2019', '2017', '2016', '2014', '2009', '2010', '2013', '2014', '2015', '2018', '2015', '1998', '2004', '2014', '2015', '2005', '2013', '2013', '2017', '2013', '2017', '2016', '2015', '2017', '2014', '2017', '2009', '2018', '2013', '2018', '2011', '2018', '2015', '2018', '2016', '2014', '2004', '2014', '2015', '2017', '2001', '2018', '2016', '2017', '2014', '2001', '2014', '2013', '2012', '2015', '2016', '2018', '2006', '2013', '2008', '2018', '2015', '1997', '2015', '2005', '2012', '2017', '2012', '2015', '2013', '2018', '2016', '2014', '1989', '2012', '2012', '2018', '2014', '2013', '2017', '2009', '2015', '2018', '2005', '2016', '2009', '2018', '1982', '2015', '2018', '2005', '2014', '2017', '2000', '2005', '2018', '2017', '2010', '2013', '2019', '2009', '2016', '1978', '2016', '2017', '2018', '2017', '2016', '2016', '2014', '2019', '2018', '2016', '2013', '2018', '2008', '2018', '2018', '2016', '2018', '2016', '2000', '2015', '2017', '2018', '2006', '2018', '2017', '2016', '2016', '2017', '2008', '1997', '2018', '2019', '2018', '2017', '2018', '2015', '2016', '2017', '2016', '2001', '2001', '2010', '2017', '2015', '1992', '2015', '2017', '2009', '2006', '2019', '2012', '2018', '2010', '2000', '2009', '2019', '2018', '2012', '1985', '2014', '2015', '2013', '2015', '2011', '2018', '2013', '2004', '2008', '2006', '2012', '2003', '2013', '1995', '1995', '2016', '2009', '2015', '2018', '2005', '2015', '2017', '2019', '2015', '2017', '2014', '1997', '2002', '2019', '2017', '2019', '2013', '2014', '2018', '2011', '2012', '2009', '2005', '2019', '2012', '2012', '2017', '2004', '2016', '2016', '2012', '2012', '2019', '2011', '2016', '2015', '2014', '2014', '2001', '2014', '2012', '2017', '2015', '1985', '1997', '2015', '2017', '2016', '2014', '2005', '2017', '2018', '1992', '2017', '2009', '2014', '2009', '2014', '2012', '2017', '2012', '2009', '2018', '2006', '2018', '2014', '2007', '2015', '1988', '2015', '2018', '2018', '2015', '2017', '2018', '2013', '2017', '1995', '2016', '2014', '2018', '2007', '2017', '2016', '2014', '2009', '2014', '2003', '2014', '2014', '2015', '2016', '2017', '1992', '2012', '2013', '2019', '2018', '2009', '2010', '2016', '2014', '2017', '2005', '2016', '2014', '2009', '1997', '2015', '2006', '2005', '2000', '2018', '2015', '2007', '2018', '2017', '2017', '2006', '2017', '2007', '2017', '2010', '2016', '2016', '2010', '2015', '2013', '2019', '2011', '2005', '2018', '2016', '2017', '2016', '2014', '2014', '2015', '2010', '2016', '2012', '1994', '1998', '2019', '1993', '1987', '2018', '2000', '2010', '2018', '2014', '2018', '2012', '2018', '2000', '2012', '2005', '2011', '2008', '2011', '2015', '2016', '2014', '2014', '2000', '2007', '2011', '2015', '2013', '2003', '2007', '2017', '2015', '2013', '1995', '2013', '2016', '2017', '2017', '2013', '1964', '2011', '2014', '2013', '2019', '2018', '2016', '2004', '2015', '2010', '2009', '2016', '2013', '2011', '2017', '1984', '2015', '2019', '2016', '2016', '2015', '2014', '2003', '2013', '2019', '2007', '2018', '2013', '2017', '2011', '2013', '2007', '2004', '2018', '2016', '2018', '2004', '2016', '1982', '2018', '1992', '2016', '1993', '2013', '2014', '2018', '2018', '2017', '2019', '2014', '2015', '2017', '2007', '2011', '2015', '2014', '2017', '2012', '2015', '2018', '2012', '2005', '2017', '2011', '2014', '2008', '2017', '2009', '2019', '1994', '2013', '2015', '2015', '2015', '2006', '1999', '2014', '2019', '2016', '2017', '1990', '2011', '2012', '2016', '2018', '2016', '2018', '2000', '1996', '2009', '2016', '2013', '2003', '2015', '2011', '2018', '2005', '2010', '1999', '2009', '1983', '2012', '1993', '2013', '2011', '2013', '2017', '2018', '2016', '2008', '2018', '2012', '2017', '2010', '2017', '2016', '2018', '2009', '2015', '2010', '2019', '2019', '2018', '2018', '2013', '2019', '2014', '2017', '2012', '2017', '2000', '2010', '2010', '2008', '2011', '2013', '2014', '2017', '2001', '2006', '2016', '2019', '2018', '2015', '2018', '2017', '2018', '1985', '2017', '2018', '2017', '2015', '2014', '2000', '2017', '2017', '2013', '2013', '2016', '2012', '1992', '2004', '2015', '2012', '1998', '2017', '2010', '1986', '2005', '2017', '1992', '2019', '2015', '2008', '2013', '2017', '1992', '2015', '2009', '2014', '2015', '2002', '2013', '1991', '2015', '2015', '2000', '2006', '2007', '2018', '2015', '2018', '2018', '2003', '2010', '2019', '2015', '2015', '2015', '2016', '2008', '1971', '2013', '2002', '1988', '2018', '2017', '2015', '2000', '2015', '1994', '2012', '2013', '2016', '2003', '2011', '2002', '2019', '2014', '2014', '2008', '2012', '2018', '2005', '2019', '1988', '2012', '2014', '2003', '2011', '2008', '2011', '2003', '2017', '2014', '2009', '2011', '2013', '2013', '2010', '2013', '1983', '2016', '2016', '2017', '2016', '2003', '2009', '2016', '2018', '2014', '2015', '2003', '2018', '2018', '2017', '2018', '2017', '2003', '2000', '2015', '2018', '2015', '2010', '2018', '2014', '2001', '2019', '2009', '2014', '2015', '2019', '2010', '2018', '2009', '2013', '2016', '2010', '2007', '2012', '2002', '2013', '2016', '2015', '2012', '2010', '1988', '2018', '2013', '2016', '2009', '2019', '2017', '2015', '2015', '2005', '2005', '2019', '1994', '2019', '2018', '2015', '1980', '2010', '2019', '2002', '2018', '2016', '2016', '2019', '2013', '2014', '1996', '2018', '2014', '2014', '2017', '2009', '2017', '1993', '2018', '2016', '2009', '2006', '1984', '2012', '2016', '2019', '2018', '2017', '1964', '2010', '2016', '2019', '2016', '2012', '2018', '2005', '2013', '2019', '2016', '2013', '2003', '2014', '1979', '2019', '2008', '1989', '2014', '2008', '2010', '2016', '2018', '2002', '2006', '2009', '2015', '2017', '2012', '2018', '2016', '2017', '2015', '1997', '2015', '2011', '2003', '2013', '2016', '2017', '2017', '2019', '1984', '2016', '2009', '2010', '2014', '2013', '2015', '2018', '2012', '2018', '2012', '2014', '2016', '2009', '2018', '2009', '1993', '2015', '2006', '2012', '2015', '2012', '2015', '2010', '2016', '1988', '2007', '2013', '2007', '2019', '2017', '2014', '2017', '2015', '1995', '1995', '2000', '2011', '2017', '2019', '1993', '2012', '2019', '2019', '2017', '2015', '2008', '2017', '2017', '2011', '2018', '2017', '2017', '2017', '2014', '2011', '2006', '2012', '2008', '2008', '2017', '2018', '2018', '2019', '1971', '2015', '2010', '2015', '2010', '2018', '2017', '2017', '2013', '1993', '2018', '2011', '2000', '2013', '2019', '2016', '2018', '2016', '2017', '2014', '2004', '2018', '2016', '2018', '2014', '2011', '2008', '2014', '2018', '2019', '2004', '2018', '2018', '2015', '2017', '2006', '2016', '2017', '2010', '2016', '1984', '2017', '2011', '2019', '2017', '2015', '2014', '2018', '2004', '2002', '2009', '2016', '2014', '2017', '2018', '2010', '2010', '2011', '2009', '2013', '2015', '2012', '1997', '2018', '2019', '2013', '2006', '2013', '2015', '1991', '2012', '2004', '2012', '2019', '2019', '2019', '2013', '2018', '2012', '2006', '2012', '2017', '2016', '1994', '2016', '2018', '2018', '2005', '2010', '2019', '2015', '2016', '2019', '2017', '2018', '2018', '2013', '2017', '2019', '2016', '2017', '2018', '2018', '2018', '2015', '2017', '2016', '2008', '2017', '2016', '2012', '1993', '2016', '2018', '2002', '1990', '1990', '1991', '2017', '2013', '2018', '2013', '2012', '2001', '2008', '2015', '2016', '2018', '2016', '2010', '2014', '2018', '2016', '2010', '2014', '2014', '2012', '2008', '2012', '2017', '2012', '2016', '2016', '2014', '2011', '2018', '2016', '2012', '2009', '2011', '2019', '2003', '2012', '2013', '1998', '1987', '2019', '2018', '2008', '2012', '2016', '2015', '2007', '2018', '2010', '2010', '2018', '2015', '2001', '2015', '2017', '2010', '2016', '1995', '2012', '2015', '2013', '1984', '2001', '2016', '2010', '2017', '2015', '1992', '2017', '2014', '2016', '2007', '2014', '2013', '2015', '1998', '2017', '2006', '1994', '2019', '2016', '2017', '2013', '2014', '2016', '2011', '2017', '2016', '2013', '2006', '2019', '2015', '2008', '2002', '2015', '2015', '2018', '2017', '2013', '2018', '2015', '2008', '2011', '2018', '2017', '2017', '2012', '2017', '2018', '2011', '2011', '2018', '2015', '1991', '2018', '1976', '2016', '2016', '2016', '2012', '2018', '2006', '2014', '2015', '2017', '2017', '1995', '1999', '2012', '2002', '2018', '2015', '2011', '2017', '2018', '2015', '2011', '1984', '2009', '2018', '2019', '2017', '2015', '2016', '2013', '2018', '2018', '2014', '2004', '2018', '2005', '2018', '2001', '2006', '2019', '2013', '2010', '2003', '2001', '2014', '2002', '2005', '2009', '2017', '2018', '2016', '2011', '2010', '2010', '2010', '1974', '2009', '2019', '2007', '2013', '2002', '2018', '2016', '2019', '2015', '2007', '2016', '2017', '2011', '2017', '1993', '2005', '2013', '2018', '2016', '2015', '2014', '1998', '2015', '2005', '2016', '2011', '2013', '2018', '2010', '2016', '2018', '2015', '2018', '2016', '2011', '2019', '2002', '2010', '2019', '2016', '2013', '2001', '2016', '2015', '2013', '2018', '2015', '2017', '2014', '2010', '2016', '2010', '2018', '2013', '2008', '2010', '2016', '2015', '2015', '2011', '2013', '2011', '2014', '2012', '2018', '2015', '2008', '2018', '2018', '2017', '2007', '2014', '2017', '2011', '2014', '2004', '2008', '2015', '2019', '2012', '2013', '2017', '2011', '2018', '2014', '2011', '2001', '2006', '2008', '1996', '2009', '2018', '2011', '2019', '1991', '2017', '2016', '2011', '2009', '2017', '2018', '2007', '2018', '2005', '2014', '2013', '2017', '2018', '2005', '2017', '2018', '2016', '2005', '2017', '2017', '2018', '2010', '2013', '2011', '2018', '2015', '2008', '2016', '2017', '2015', '2008', '2016', '2007', '2018', '2016', '2015', '2012', '2016', '2009', '2015', '1997', '2014', '2017', '2018', '2007', '1994', '2010', '2018', '2016', '1977', '2012', '2014', '2016', '2004', '2017', '2016', '2016', '2019', '2012', '2018', '2012', '2008', '2017', '2010', '2019', '2015', '2008', '2016', '2019', '2003', '2018', '2008', '2019', '2009', '2016', '1999', '2019', '2013', '2013', '2017', '2013', '2018', '2017', '2000', '2017', '2018', '2018', '2011', '2018', '2018', '1989', '2012', '2019', '2017', '2015', '2015', '2016', '2017', '1995', '2017', '2015', '2018', '2018', '2016', '2013', '2008', '2017', '2018', '2017', '2018', '2018', '2018', '1994', '2011', '1997', '2016', '2003', '2012', '2017', '2014', '2006', '2018', '2013', '2018', '2015', '2012', '2017', '2015', '2017', '1987', '2018', '2015', '2011', '2007', '2011', '2008', '2016', '2017', '2017', '2002', '2015', '2013', '2017', '2007', '2008', '2007', '1986', '2019', '2017', '1992', '2012', '2007', '2016', '2017', '2019', '2017', '2019', '2016', '2016', '2012', '1960', '2018', '2017', '2016', '2001', '1960', '2017', '2017', '2017', '1999', '2006', '2018', '2012', '2017', '1992', '1992', '2013', '2017', '2017', '2018', '2013', '2015', '2015', '2015', '2014', '2010', '2011', '2013', '2012', '2013', '2018', '2002', '2016', '1999', '2012', '2015', '2015', '2016', '2018', '2018', '2015', '1989', '2008', '2006', '2002', '2018', '2019', '2011', '2014', '1997', '1998', '2019', '2012', '2013', '2010', '2018', '2009', '2018', '2016', '2002', '2015', '2019', '2018', '1990', '2011', '2012', '2005', '2011', '2015', '2007', '2002', '2005', '2008', '2017', '2003', '2016', '2008', '2013', '2015', '2017', '2001', '2018', '2015', '2001', '2019', '2005', '2017', '2008', '2017', '2019', '2005', '2013', '2011', '2000', '2007', '2014', '2008', '2018', '2007', '2009', '2015', '2016', '2016', '2007', '2017', '2009', '2008', '2014', '2018', '2006', '2010', '2000', '2013', '2017', '2013', '2015', '2017', '2014', '1974', '2016', '2004', '2011', '2019', '2008', '1995', '2016', '1983', '2012', '2016', '2010', '2009', '2018', '2011', '2015', '2018', '2018', '2019', '2001', '2017', '2015', '2011', '1999', '2019', '2014', '2003', '2017', '2014', '2016', '2015', '2015', '2015', '1998', '1992', '2017', '2008', '2003', '2015', '2012', '2012', '2016', '2016', '2013', '1994', '2011', '2015', '2018', '2005', '2015', '2016', '2016', '2018', '2010', '2016', '2017', '2012', '2019', '2018', '2012', '2010', '2017', '2013', '2018', '2005', '2018', '2016', '2013', '2018', '2015', '2005', '1983', '2015', '2016', '2005', '2014', '2017', '2013', '2015', '2008', '2016', '2015', '2013', '2018', '2018', '2012', '2009', '1990', '2019', '2004', '2010', '2014', '2013', '2009', '2019', '2012', '2017', '2015', '2012', '2019', '2002', '2016', '2018', '2017', '2015', '2013', '2010', '2015', '2011', '2014', '2018', '2018', '2014', '2017', '2009', '2017', '2018', '2007', '2014', '2013', '2011', '2011', '2011', '2014', '2018', '2018', '1990', '2006', '2009', '2018', '2018', '2016', '2010', '2016', '2006', '2007', '2012', '2018', '2015', '2002', '2018', '2015', '2017', '2009', '2013', '2018', '2018', '2016', '2015', '2014', '2011', '2004', '2016', '2001', '2017', '2003', '2017', '2018', '2015', '2015', '2016', '2012', '2017', '2017', '2014', '2011', '2017', '2019', '2014', '1991', '2006', '2019', '2013', '1989', '2016', '1994', '2010', '2005', '2009', '2012', '2012', '2018', '2017', '2017', '2012', '2010', '2009', '2014', '2016', '2016', '2013', '2017', '2019', '2017', '2004', '2019', '2014', '2010', '2016', '2011', '2016', '2018', '2013', '2012', '2019', '2016', '1970', '2013', '1997', '2019', '2000', '2016', '2018', '2016', '2016', '2012', '2010', '2010', '2013', '1998', '2018', '2012', '2005', '2014', '2016', '1997', '2018', '2013', '2015', '2017', '2015', '2018', '2016', '2014', '2018', '2004', '2007', '2018', '2018', '2018', '2015', '2015', '2013', '2014', '2009', '2018', '2009', '2018', '2017', '2004', '2004', '2008', '2007', '2017', '2018', '2018', '2017', '2007', '2016', '2018', '2012', '2005', '2018', '2015', '2012', '2015', '2019', '2012', '2017', '2015', '2016', '2013', '2002', '2012', '2011', '2017', '2018', '2014', '2014', '2012', '2010', '2017', '2018', '2018', '2017', '2018', '2017', '2009', '2019', '2015', '2013', '2013', '1997', '2009', '2013', '1995', '2014', '2004', '2013', '2016', '2013', '2018', '2006', '2010', '2017', '2017', '2013', '2015', '2011', '2017', '1995', '2018', '2018', '2018', '2019', '2010', '2018', '2010', '2019', '2014', '2002', '2012', '2019', '2016', '2009', '2014', '2017', '2016', '2017', '2017', '2010', '2017', '2008', '2017', '2010', '2016', '2016', '2015', '2011', '2011', '2001', '2015', '2003', '2016', '2015', '2018', '2000', '2014', '2017', '2004', '2017', '2006', '2005', '2018', '2015', '2013', '2016', '2015', '2016', '2017', '2012', '2004', '2017', '2017', '2017', '2014', '2008', '2007', '2018', '2014', '2010', '2013', '2011', '2014', '2016', '2018', '2019', '2015', '2010', '2017', '2014', '2014', '2019', '2004', '2017', '1996', '2015', '2019', '2012', '2016', '2008', '2014', '2012', '2019', '2019', '1986', '2012', '2017', '2010', '2016', '2005', '2016', '2018', '2019', '2018', '2010', '2017', '2014', '1989', '1989', '2009', '2016', '2012', '1991', '2015', '2014', '2010', '2004', '2014', '2016', '2002', '2018', '2018', '2014', '2013', '2017', '2010', '2015', '2012', '2018', '2013', '2006', '2011', '2016', '2014', '2018', '2015', '2011', '2018', '2002', '2017', '1980', '2017', '1994', '2015', '2019', '2019', '2015', '2012', '2016', '2013', '2012', '2015', '2014', '2013', '1998', '2008', '2005', '2017', '2016', '2011', '2018', '2018', '2016', '2018', '2007', '2018', '2017', '2013', '2016', '2018', '2014', '2005', '2019', '2013', '2018', '1990', '2017', '2004', '2014', '2014', '1986', '2017', '1996', '2014', '2003', '2016', '2007', '2016', '2014', '2018', '2013', '1975', '2013', '2015', '2018', '2017', '2017', '2018', '2015', '2015', '2010', '2012', '2011', '2017', '1998', '2004', '2016', '2019', '2009', '2016', '2006', '2019', '2016', '2008', '2018', '2017', '2019', '2018', '2018', '2017', '2013', '2019', '2018', '2017', '2018', '2017', '2006', '2013', '2011', '2002', '1992', '1987', '2000', '2010', '2006', '2015', '2016', '2019', '2014', '2017', '2016', '2005', '2011', '2018', '2017', '2003', '2018', '2003', '2018', '2016', '2018', '2017', '2017', '2009', '2016', '2016', '2017', '2016', '2018', '2014', '2015', '2013', '2013', '2017', '2012', '2015', '2016', '2016', '2008', '1996', '2016', '2002', '2017', '2010', '2018', '2017', '2005', '2019', '2000', '2017', '2008', '2017', '2017', '2018', '2019', '2013', '2018', '2000', '1985', '1984', '2011', '1992', '2017', '2018', '2013', '2018', '2015', '2016', '2018', '2000', '2019', '2017', '2010', '2014', '2005', '2015', '2008', '2018', '2011', '2018', '2016', '2017', '2016', '2015', '2012', '2019', '2018', '2012', '2001', '2000', '1991', '2009', '2000', '2017', '2017', '2014', '2010', '2013', '2013', '2008', '2018', '2015', '2017', '2010', '2012', '2003', '2018', '2013', '2015', '2013', '2018', '2014', '2017', '2018', '2010', '2006', '2000', '1997', '2007', '2017', '2006', '2018', '2008', '2002', '2018', '2019', '2006', '1998', '2018', '2015', '2019', '2014', '2012', '2003', '2016', '2006', '2016', '2014', '2017', '2009', '1992', '2016', '2019', '2016', '2019', '2015', '2016', '2017', '2012', '2011', '2010', '2017', '2017', '2017', '2016', '2016', '2018', '2017', '2017', '2012', '2016', '1995', '2015', '1999', '2010', '1989', '2011', '2016', '2015', '2008', '2018', '2018', '2008', '2009', '2019', '2016', '2017', '2018', '2015', '2016', '2018', '2016', '1991', '2015', '2006', '2017', '2003', '2017', '2017', '2015', '2013', '2014', '2018', '2010', '1986', '2015', '2018', '2018', '2014', '2018', '2008', '1996', '2008', '2018', '2011', '2018', '2000', '2016', '2018', '2012', '2001', '2010', '2017', '2018', '2005', '2013', '2014', '2015', '2015', '2017', '2015', '2018', '2009', '2016', '2011', '2003', '2017', '2005', '2009', '2015', '2017', '2009', '2014', '2016', '2015', '2017', '2004', '2012', '2016', '2019', '2017', '2015', '2009', '2017', '2016', '2015', '2012', '2014', '2000', '2011', '2011', '1905', '2019', '2002', '2011', '2015', '1996', '2011', '2018', '2015', '2008', '2016', '2008', '2016', '2019', '2014', '2014', '2013', '2015', '2010', '2007', '2018', '2018', '2010', '2016', '2014', '1983', '2017', '2013', '2016', '1994', '2017', '2005', '2017', '2013', '2015', '2007', '2005', '2019', '2018', '2013', '2019', '2014', '2018', '2003', '2005', '2008', '2017', '2013', '2014', '2014', '2019', '2017', '1995', '2017', '2011', '2018', '2015', '2015', '2014', '2016', '2018', '2000', '2013', '2013', '2017', '2015', '2010', '2017', '2014', '2009', '2017', '2016', '2011', '1983', '2010', '2000', '2016', '2002', '2015', '2015', '2016', '2009', '1997', '2018', '2003', '2013', '2013', '2015', '2017', '2009', '2012', '2017', '2011', '2007', '2018', '2011', '1988', '2017', '2018', '2017', '1900', '2019', '2018', '2012', '2018', '2018', '1993', '2014', '2018', '2017', '2001', '2017', '2005', '2018', '2009', '2017', '2013', '2018', '2017', '2017', '2015', '2016', '2007', '2011', '2013', '2019', '2008', '1964', '2017', '2015', '1994', '2007', '2017', '1995', '2014', '2003', '2014', '2001', '2017', '2012', '2014', '2014', '2013', '2017', '2012', '2014', '2016', '2017', '2017', '2018', '2011', '2004', '2008', '2019', '2018', '1996', '2014', '2012', '1997', '2008', '2017', '2008', '2016', '2018', '2017', '2017', '2019', '2011', '2015', '2005', '2011', '2018', '2017', '2018', '2013', '2013', '2017', '2011', '2018', '2011', '2017', '2017', '2015', '2013', '2017', '2013', '2011', '2018', '2013', '2018', '2012', '2007', '2011', '2017', '2012', '2018', '2017', '2019', '2019', '2008', '2011', '2006', '2013', '2017', '2019', '2010', '1998', '2014', '2017', '2018', '2018', '2018', '2018', '2011', '2012', '2016', '2012', '2013', '2017', '2005', '2010', '2018', '2010', '2013', '2004', '2017', '2007', '2011', '2011', '1999', '2014', '2013', '2015', '2018', '2013', '1996', '2012', '2016', '2015', '2005', '2016', '2019', '1989', '2011', '1994', '2019', '2011', '2015', '2013', '2010', '2018', '2018', '1987', '2014', '2012', '2018', '2006', '2017', '2000', '2018', '2014', '2013', '2013', '2016', '2017', '1992', '2017', '2016', '2017', '2016', '1984', '2014', '2018', '2000', '2016', '2015', '2009', '2017', '2006', '2017', '2016', '2016', '2007', '2017', '2011', '2011', '2018', '2017', '2017', '2017', '2019', '2019', '2011', '2015', '2002', '2014', '2014', '2008', '2017', '2014', '2019', '2016', '2017', '2013', '2005', '2017', '2016', '2018', '2011', '2013', '2018', '2003', '2009', '2018', '2017', '2001', '2009', '2014', '2016', '2006', '2017', '2018', '2015', '2017', '2013', '2012', '2014', '2009', '2010', '2017', '2008', '2018', '2014', '2015', '2013', '2019', '2014', '2015', '2016', '2013', '2017', '2013', '2001', '2013', '2013', '2018', '2016', '2013', '2018', '1997', '2017', '2011', '2015', '2011', '2016', '2002', '2017', '2012', '2016', '2014', '2019', '2013', '2012', '2017', '2012', '2017', '2016', '2002', '2018', '2015', '2011', '2013', '2013', '2015', '2008', '1982', '2017', '2016', '2012', '2012', '2016', '2004', '2014', '2004', '2006', '2017', '2018', '2018', '2016', '2003', '2016', '2018', '2017', '2012', '2006', '2014', '1999', '2018', '2010', '2015', '2014', '2017', '2013', '1987', '2016', '2012', '2008', '2015', '1999', '2012', '2018', '2015', '2015', '2018', '2015', '2004', '1982', '2019', '2019', '2013', '2017', '2017', '1990', '2012', '2016', '2012', '2013', '2018', '2009', '2013', '2005', '2016', '2010', '2018', '2006', '2014', '2012', '2015', '2016', '1994', '2016', '2019', '2002', '2017', '2005', '2015', '2017', '2012', '2016', '2011', '2012', '2008', '2000', '2019', '1999', '1991', '2005', '2010', '2014', '2015', '2019', '2006', '2013', '2018', '2012', '2010', '2017', '2017', '2011', '2015', '2018', '2019', '2018', '2017', '2015', '2009', '2017', '2015', '2018', '2016', '2015', '2018', '2012', '2005', '2003', '2018', '1988', '2018', '2019', '2001', '2002', '2009', '2018', '2012', '2015', '2005', '2012', '2016', '1987', '2014', '2017', '2018', '2019', '2017', '2017', '2012', '2015', '2009', '2002', '2016', '2011', '2018', '2016', '2011', '2017', '2017', '2016', '2006', '2015', '2009', '2016', '2012', '2015', '2018', '2018', '2015', '2017', '2014', '2005', '2010', '2010', '2008', '2017', '2010', '2006', '1992', '2015', '1985', '2012', '2009', '2016', '2015', '2008', '2016', '2003', '2019', '2011', '2018', '2018', '2017', '2017', '2015', '2012', '2011', '2016', '2016', '2017', '2016', '2006', '2015', '2018', '2013', '2017', '2017', '2018', '2005', '2011', '2010', '2011', '2005', '2016', '2016', '1996', '2014', '2015', '2012', '2016', '2018', '2012', '2017', '2004', '2015', '2017', '2016', '2016', '2018', '2018', '2018', '2018', '2013', '2013', '2003', '2014', '2016', '2018', '2010', '2016', '2018', '2001', '2019', '2010', '2017', '2017', '2018', '2018', '2006', '2019', '2003', '2010', '2019', '2009', '2015', '2010', '2006', '2012', '2015', '2016', '2016', '2013', '2014', '2003', '2006', '2016', '2019', '2018', '2016', '1994', '2000', '2016', '2017', '2015', '2018', '2014', '2017', '2005', '2015', '2019', '2018', '2018', '2019', '2002', '2014', '2010', '2017', '2018', '2016', '2014', '2018', '2019', '2012', '2015', '2018', '2019', '2011', '2012', '2016', '2009', '2015', '2013', '2008', '2011', '2011', '2015', '2018', '2003', '2012', '2011', '2003', '2015', '2017', '2016', '2017', '2016', '2019', '2016', '2018', '2008', '2017', '2019', '1999', '2015', '2009', '2009', '2009', '2014', '2011', '2008', '2012', '2019', '2016', '2012', '2012', '2014', '1994', '2016', '2019', '2009', '2019', '2013', '2015', '2019', '2017', '2017', '2007', '2016', '2009', '1984', '2018', '2014', '2002', '2016', '1986', '2017', '2007', '2017', '2016', '2008', '2016', '2017', '2007', '2010', '2013', '2009', '2016', '1982', '2013', '2018', '2018', '2019', '2019', '2018', '2019', '2017', '2015', '2005', '2019', '2009', '2009', '2014', '2014', '2010', '2002', '2016', '2017', '2014', '2013', '2007', '2015', '2004', '2017', '2016', '2015', '2018', '2019', '2016', '2018', '2018', '2003', '2013', '2012', '2014', '2002', '2014', '1985', '2003', '2012', '2016', '2016', '2018', '2016', '2016', '2015', '2014', '2016', '1991', '2003', '2016', '2004', '2017', '2018', '2013', '2016', '2015', '2014', '1997', '2011', '2015', '2012', '2002', '2005', '2003', '2015', '2015', '2015', '2012', '2002', '2019', '2018', '2016', '2015', '2004', '2016', '2016', '2013', '2018', '1998', '2018', '2015', '2016', '2018', '1994', '2007', '2017', '2004', '2018', '2018', '2015', '2016', '2008', '2013', '2016', '2017', '2015', '2018', '2009', '2018', '2010', '1997', '2018', '2008', '2000', '2013', '2014', '2016', '2016', '2005', '2016', '2009', '2013', '2004', '2018', '2011', '2004', '2017', '2008', '2016', '2015', '2015', '2010', '2005', '1999', '2010', '2015', '2017', '2014', '2016', '2015', '1997', '2007', '2014', '2013', '2018', '2017', '2012', '2009', '2018', '2014', '2008', '2017', '2015', '2017', '2003', '2017', '2013', '2016', '2010', '2019', '2019', '2018', '2019', '2019', '2014', '2015', '2017', '2018', '2018', '2013', '2010', '2018', '2002', '2015', '2015', '2017', '2005', '2018', '2019', '2011', '2015', '2015', '2018', '2017', '1994', '2018', '2015', '2019', '2009', '2008', '2016', '2012', '2011', '2016', '2018', '2014', '2017', '2018', '2011', '2011', '2010', '2016', '2016', '2013', '2015', '2004', '2018', '2015', '2012', '2008', '2017', '2011', '2013', '2013', '2016', '2019', '2019', '2019', '2015', '2012', '2016', '2011', '2004', '2015', '2018', '2015', '2018', '2016', '2018', '2019', '2014', '2014', '2017', '2014', '2010', '2010', '1995', '1992', '2016', '2016', '2017', '2003', '2018', '2016', '2010', '2015', '2012', '2018', '2017', '2010', '2015', '2019', '2018', '2014', '2015', '2010', '2017', '2018', '2018', '2007', '2013', '2018', '2018', '2002', '2009', '2013', '2000', '2015', '2013', '2018', '2011', '2018', '2008', '2014', '2016', '2017', '2017', '2013', '2012', '2015', '2017', '2009', '2016', '2014', '2012', '2018', '1998', '1990', '2000', '2005', '2015', '2014', '2014', '1993', '2015', '2009', '1996', '2013', '2001', '1998', '2013', '2013', '2018', '2014', '2015', '2012', '2012', '2018', '2005', '2012', '2012', '2019', '2011', '2018', '2006', '2015', '2018', '2019', '2012', '2019', '2017', '1999', '2018', '2013', '2018', '2010', '2016', '2005', '2011', '2017', '2004', '2010', '2015', '2015', '2018', '2019', '2013', '2017', '2013', '2018', '2016', '2014', '1999', '2012', '2018', '2012', '2017', '2011', '2019', '2000', '2015', '2006', '2017', '2018', '1995', '2018', '2014', '2005', '2014', '2004', '2016', '2017', '2019', '1998', '2018', '2010', '2018', '2018', '2002', '2017', '2013', '2015', '2013', '2017', '2019', '1988', '2018', '2016', '2019', '2019', '2015', '2019', '2015', '2013', '2018', '2018', '2016', '2017', '2018', '2008', '1997', '2018', '2013', '2018', '2010', '2015', '2015', '2010', '2004', '2018', '1991', '2018', '2014', '2018', '2014', '2017', '2015', '1975', '2019', '2017', '2003', '2017', '2017', '2011', '2007', '2016', '2013', '2015', '2018', '2018', '2019', '2016', '2019', '2011', '1988', '2017', '2016', '2014', '2003', '2003', '2010', '2011', '2018', '2012', '2018', '2006', '2011', '2001', '2010', '2018', '2011', '2013', '2014', '2015', '2014', '2016', '2012', '2016', '2017', '2018', '2013', '2019', '2019', '2016', '2008', '1995', '2012', '2000', '2017', '1999', '1982', '2011', '2012', '1995', '2019', '2018', '2019', '2018', '2017', '2015', '2007', '2008', '2015', '2014', '2017', '2014', '2002', '2017', '2019', '2017', '2014', '2010', '1961', '1976', '2015', '2017', '2018', '2000', '1994', '2017', '2013', '2017', '2017', '2011', '2005', '2015', '2012', '2017', '2011', '2018', '2016', '2013', '2017', '2016', '2013', '2012', '2018', '2017', '2010', '2016', '2012', '2016', '2019', '2018', '1996', '2011', '2009', '2015', '2016', '2016', '2014', '2016', '2016', '2014', '2006', '1989', '2013', '2001', '2005', '2008', '2019', '2017', '2013', '2015', '2018', '2018', '2006', '2001', '2017', '1996', '2014', '2018', '2008', '2006', '2003', '2017', '2017', '2008', '2018', '2017', '2011', '2016', '2016', '2011', '1991', '2009', '2005', '2016', '2008', '2013', '2013', '2016', '2018', '2002', '2018', '2017', '2012', '2008', '2017', '2017', '2004', '2018', '2018', '2014', '2007', '2010', '2019', '2018', '2015', '2016', '2018', '2011', '2009', '2003', '2016', '2015', '2016', '2017', '2019', '2009', '2008', '2017', '2006', '2008', '2019', '2017', '2013', '2013', '2018', '2013', '2019', '2018', '2003', '2003', '1994', '2018', '2000', '2009', '2018', '2014', '2008', '2016', '1984', '2013', '2016', '2018', '2017', '2013', '2016', '1987', '2013', '2016', '2008', '2008', '2016', '2004', '2018', '2018', '2018', '1975', '2001', '2013', '2017', '2014', '2012', '1999', '1994', '2018', '2017', '2019', '2001', '2016', '2018', '2017', '2017', '2018', '2017', '2016', '2019', '2012', '2010', '2011', '2006', '2014', '2012', '2009', '2010', '2017', '2009', '2012', '2019', '2018', '2015', '2016', '2018', '2014', '2017', '2015', '2011', '2011', '2019', '2012', '2018', '2017', '2006', '2014', '2013', '2011', '2010', '2016', '2014', '2009', '2015', '2010', '2018', '2010', '2001', '2016', '2017', '2017', '2011', '2002', '2015', '2018', '2010', '2015', '2016', '1986', '2018', '2005', '2010', '2018', '2018', '2012', '2008', '2018', '2018', '2016', '2013', '2003', '2017', '2010', '2016', '2011', '2016', '1999', '2007', '2018', '2010', '2016', '2001', '2017', '2017', '1998', '2014', '2018', '2017', '2016', '2013', '2005', '2013', '2017', '2015', '1993', '2013', '2011', '2011', '2013', '2014', '2016', '2011', '2010', '2008', '2015', '2017', '2006', '1983', '2010', '2015', '2014', '2019', '2012', '2017', '2011', '2017', '2013', '2001', '2014', '2017', '2017', '2018', '2019', '2002', '2011', '2012', '2019', '2017', '2016', '2017', '2009', '2013', '1976', '2016', '2018', '2005', '2019', '2012', '2017', '2018', '2015', '2016', '2012', '2005', '1925', '2015', '2016', '2015', '2018', '2007', '2014', '2015', '2018', '2015', '2014', '2015', '2012', '2016', '1999', '2013', '2013', '2009', '1992', '2011', '2018', '2017', '2008', '2006', '2016', '2017', '2013', '2008', '2011', '2015', '2018', '2011', '2019', '2011', '2013', '2018', '2015', '2005', '2008', '1986', '2014', '2000', '2017', '2012', '2018', '2012', '2019', '2016', '2000', '2018', '2007', '2009', '2005', '2019', '2014', '2015', '2004', '2013', '2002', '2017', '2017', '2011', '2008', '2014', '2018', '2012', '1992', '2011', '2011', '2016', '2010', '2014', '2008', '2017', '2018', '2018', '2007', '2016', '2014', '2006', '2019', '2012', '2016', '1976', '2009', '2010', '2017', '2017', '2017', '1995', '1994', '2016', '2018', '2016', '2011', '2001', '2016', '2016', '2013', '2017', '2009', '2014', '2013', '2012', '2019', '2013', '2016', '2018', '2018', '2016', '2018', '2019', '2015', '2014', '2019', '2018', '2017', '2016', '2014', '2019', '2008', '2018', '2014', '2015', '2018', '2018', '2018', '2008', '2016', '2017', '2010', '2015', '2014', '2012', '2016', '2016', '2018', '2018', '2018', '2018', '2004', '2018', '2013', '2013', '2014', '2017', '2014', '2009', '2011', '2017', '2015', '1990', '1992', '2010', '2000', '2015', '2016', '2009', '2018', '2012', '2011', '2012', '2018', '2015', '2014', '2014', '2019', '1982', '2018', '2017', '2016', '2017', '2018', '2018', '1975', '2018', '2012', '2010', '2017', '2017', '2015', '2006', '2005', '1997', '2009', '1986', '2018', '2017', '2014', '2009', '2015', '2007', '2012', '2008', '2014', '2016', '2015', '2019', '2009', '2017', '2015', '2018', '2015', '2011', '2018', '2017', '2017', '2013', '2017', '2015', '2017', '2016', '2014', '2017', '2018', '2015', '2016', '2006', '2002', '2010', '2018', '1999', '2014', '2016', '2018', '2012', '2015', '2015', '2017', '2016', '2014', '2013', '2015', '2018', '2017', '2016', '2017', '2013', '2008', '2017', '2005', '2014', '2014', '2010', '2017', '2018', '2015', '2016', '2017', '2003', '2017', '2017', '2019', '2003', '2019', '2017', '2009', '2011', '2017', '2009', '2019', '2018', '2011', '2006', '2005', '2016', '2017', '1990', '2016', '2014', '2019', '2013', '2000', '2017', '2017', '2018', '2014', '1999', '2003', '2008', '2011', '2018', '2011', '2016', '2016', '2016', '2017', '2013', '2017', '2006', '2017', '2017', '2005', '2015', '2006', '2013', '2010', '2012', '2016', '2018', '2006', '2002', '2012', '2018', '2007', '2007', '2019', '2001', '2014', '2001', '2007', '2002', '2014', '1987', '2017', '2018', '2014', '2018', '2016', '2018', '2014', '2010', '2016', '2012', '2004', '2014', '2017', '2015', '2015', '2018', '1996', '2018', '2001', '2017', '2011', '2014', '2016', '2010', '2015', '2014', '2011', '2001', '2016', '2011', '2016', '2018', '2012', '2018', '2017', '2017', '2013', '2003', '2000', '2014', '2017', '2007', '2016', '2014', '2001', '2017', '2014', '2007', '2017', '1999', '2011', '2010', '2016', '2017', '2018', '2016', '2010', '2019', '2012', '2012', '2017', '2010', '2014', '2009', '2012', '2018', '2013', '2017', '2015', '2005', '2005', '2014', '2005', '2000', '2019', '2019', '2013', '2018', '2013', '2017', '2000', '1973', '1996', '2004', '2007', '2014', '2019', '2017', '2016', '2016', '2014', '2017', '2019', '2017', '2018', '1997', '2015', '2016', '2006', '2013', '2014', '1989', '2018', '2007', '2014', '2017', '1995', '2015', '2018', '2008', '2017', '2010', '2015', '2005', '2018', '2017', '2012', '2016', '1999', '2017', '2018', '2002', '2019', '2018', '2014', '2001', '2017', '2016', '2018', '2018', '2018', '2011', '2006', '2013', '1980', '2017', '2019', '2019', '2010', '2012', '2013', '2018', '2017', '2004', '2001', '2011', '2017', '2016', '2016', '2001', '2007', '1993', '2007', '2013', '2003', '2017', '1989', '1985', '2012', '2017', '2014', '2019', '2019', '2014', '2018', '2016', '2012', '2017', '2002', '2016', '2018', '2013', '2005', '2011', '2017', '2014', '2017', '1998', '2016', '2010', '2011', '2015', '2010', '2013', '2009', '2006', '1992', '2017', '1993', '2014', '2018', '2018', '2016', '2004', '2017', '2018', '2014', '2016', '2016', '2019', '2012', '2011', '1998', '2012', '2017', '2018', '2008', '2018', '2010', '2009', '2018', '2017', '2018', '2019', '2017', '2018', '2005', '2016', '2016', '2008', '2016']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have add these two lists to our data frame."
      ],
      "metadata": {
        "id": "tTg3m-TSr5-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['Format'] = Format\n",
        "cat_df['Year_of_publish'] = production_year\n",
        "cat_df.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KBM5S1wGr5nu",
        "outputId": "1a282671-9216-40d8-e16b-65b2bdc7d1e4"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title           Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2         Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3   A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "3  A handful of grain is found in the pocket of a...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "2                                Humour  299.00            6          4.8   \n",
              "3             Crime, Thriller & Mystery  180.00           13          4.1   \n",
              "\n",
              "     New_Edition     Format Year_of_publish  \n",
              "0  Paperback2016  Paperback            2016  \n",
              "1  Paperback2012  Paperback            2012  \n",
              "2  Paperback1982  Paperback            1982  \n",
              "3  Paperback2017  Paperback            2017  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a9efa1bc-b574-4cf4-90b6-81b2cdc4dd9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>New_Edition</th>\n",
              "      <th>Format</th>\n",
              "      <th>Year_of_publish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Paperback2016</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Paperback2012</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>Paperback1982</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>1982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Paperback2017</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9efa1bc-b574-4cf4-90b6-81b2cdc4dd9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a9efa1bc-b574-4cf4-90b6-81b2cdc4dd9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a9efa1bc-b574-4cf4-90b6-81b2cdc4dd9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c947ed8e-d258-4777-a1b3-3c7215a42209\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c947ed8e-d258-4777-a1b3-3c7215a42209')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c947ed8e-d258-4777-a1b3-3c7215a42209 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since we have our two divided columns, there's no need to keep \"New_Edition\" , so we just drop it."
      ],
      "metadata": {
        "id": "6T30kTg9sjaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop([\"New_Edition\"],axis=1)\n",
        "cat_df.head(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LwBczbbisrmu",
        "outputId": "45f8a37b-3e68-4da9-a57f-0ffb15fc948a"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title          Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)  Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts    Arun Khopkar   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "\n",
              "      Format Year_of_publish  \n",
              "0  Paperback            2016  \n",
              "1  Paperback            2012  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-830bc9a2-d361-4424-9a5a-08ebac5c2315\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Format</th>\n",
              "      <th>Year_of_publish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-830bc9a2-d361-4424-9a5a-08ebac5c2315')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-830bc9a2-d361-4424-9a5a-08ebac5c2315 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-830bc9a2-d361-4424-9a5a-08ebac5c2315');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-de8998fd-91f0-4980-9a5f-4569ac27c791\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de8998fd-91f0-4980-9a5f-4569ac27c791')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-de8998fd-91f0-4980-9a5f-4569ac27c791 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Format**"
      ],
      "metadata": {
        "id": "DXaxsVJ4T6v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_BC = pd.get_dummies(cat_df['Format'], prefix='Format')\n",
        "cat_df = pd.concat([cat_df, onehot_BC], axis=1)\n",
        "cat_df.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gQda6X07T-7x",
        "outputId": "4a082ff8-3276-41a5-8dda-2e61502ed823"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title           Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2         Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3   A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "3  A handful of grain is found in the pocket of a...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "2                                Humour  299.00            6          4.8   \n",
              "3             Crime, Thriller & Mystery  180.00           13          4.1   \n",
              "\n",
              "      Format Year_of_publish  Format_(French),Paperback2010  ...  \\\n",
              "0  Paperback            2016                              0  ...   \n",
              "1  Paperback            2012                              0  ...   \n",
              "2  Paperback            1982                              0  ...   \n",
              "3  Paperback            2017                              0  ...   \n",
              "\n",
              "   Format_Paperbackset  Format_Perfect Paperback  Format_Plastic CombNTSC  \\\n",
              "0                    0                         0                        0   \n",
              "1                    0                         0                        0   \n",
              "2                    0                         0                        0   \n",
              "3                    0                         0                        0   \n",
              "\n",
              "   Format_Product Bundle  Format_Sheet music  Format_Spiral-bound1986  \\\n",
              "0                      0                   0                        0   \n",
              "1                      0                   0                        0   \n",
              "2                      0                   0                        0   \n",
              "3                      0                   0                        0   \n",
              "\n",
              "   Format_Spiral-bound2007  Format_Spiral-bound2012  Format_Spiral-bound2016  \\\n",
              "0                        0                        0                        0   \n",
              "1                        0                        0                        0   \n",
              "2                        0                        0                        0   \n",
              "3                        0                        0                        0   \n",
              "\n",
              "   Format_Tankobon Softcover  \n",
              "0                          0  \n",
              "1                          0  \n",
              "2                          0  \n",
              "3                          0  \n",
              "\n",
              "[4 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abe1457c-2380-4c90-9dca-df46752a240c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Format</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Format_Paperbackset</th>\n",
              "      <th>Format_Perfect Paperback</th>\n",
              "      <th>Format_Plastic CombNTSC</th>\n",
              "      <th>Format_Product Bundle</th>\n",
              "      <th>Format_Sheet music</th>\n",
              "      <th>Format_Spiral-bound1986</th>\n",
              "      <th>Format_Spiral-bound2007</th>\n",
              "      <th>Format_Spiral-bound2012</th>\n",
              "      <th>Format_Spiral-bound2016</th>\n",
              "      <th>Format_Tankobon Softcover</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>Paperback</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 39 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abe1457c-2380-4c90-9dca-df46752a240c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-abe1457c-2380-4c90-9dca-df46752a240c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-abe1457c-2380-4c90-9dca-df46752a240c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-86a2c4ba-4781-4404-8342-57cb7697d0d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-86a2c4ba-4781-4404-8342-57cb7697d0d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-86a2c4ba-4781-4404-8342-57cb7697d0d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop(['Format'],axis=1)\n",
        "cat_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Jz-VAXFgUqsZ",
        "outputId": "3679f929-1e3d-4b61-e862-cf1f072d0632"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Title          Author  \\\n",
              "0  The Prisoner's Gold (The Hunters 3)  Chris Kuzneski   \n",
              "1   Guru Dutt: A Tragedy in Three Acts    Arun Khopkar   \n",
              "2         Leviathan (Penguin Classics)   Thomas Hobbes   \n",
              "\n",
              "                                            Synopsis  \\\n",
              "0  THE HUNTERS return in their third brilliant no...   \n",
              "1  A layered portrait of a troubled genius for wh...   \n",
              "2  \"During the time men live without a common Pow...   \n",
              "\n",
              "                           BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "0                    Action & Adventure  220.00            8          4.0   \n",
              "1  Biographies, Diaries & True Accounts  202.93           14          3.9   \n",
              "2                                Humour  299.00            6          4.8   \n",
              "\n",
              "  Year_of_publish  Format_(French),Paperback2010  \\\n",
              "0            2016                              0   \n",
              "1            2012                              0   \n",
              "2            1982                              0   \n",
              "\n",
              "   Format_(German),Paperback2014  ...  Format_Paperbackset  \\\n",
              "0                              0  ...                    0   \n",
              "1                              0  ...                    0   \n",
              "2                              0  ...                    0   \n",
              "\n",
              "   Format_Perfect Paperback  Format_Plastic CombNTSC  Format_Product Bundle  \\\n",
              "0                         0                        0                      0   \n",
              "1                         0                        0                      0   \n",
              "2                         0                        0                      0   \n",
              "\n",
              "   Format_Sheet music  Format_Spiral-bound1986  Format_Spiral-bound2007  \\\n",
              "0                   0                        0                        0   \n",
              "1                   0                        0                        0   \n",
              "2                   0                        0                        0   \n",
              "\n",
              "   Format_Spiral-bound2012  Format_Spiral-bound2016  Format_Tankobon Softcover  \n",
              "0                        0                        0                          0  \n",
              "1                        0                        0                          0  \n",
              "2                        0                        0                          0  \n",
              "\n",
              "[3 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb3bce51-5ef2-4b35-b116-4fbdaca97338\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>...</th>\n",
              "      <th>Format_Paperbackset</th>\n",
              "      <th>Format_Perfect Paperback</th>\n",
              "      <th>Format_Plastic CombNTSC</th>\n",
              "      <th>Format_Product Bundle</th>\n",
              "      <th>Format_Sheet music</th>\n",
              "      <th>Format_Spiral-bound1986</th>\n",
              "      <th>Format_Spiral-bound2007</th>\n",
              "      <th>Format_Spiral-bound2012</th>\n",
              "      <th>Format_Spiral-bound2016</th>\n",
              "      <th>Format_Tankobon Softcover</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>Action &amp; Adventure</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>Biographies, Diaries &amp; True Accounts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 38 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb3bce51-5ef2-4b35-b116-4fbdaca97338')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb3bce51-5ef2-4b35-b116-4fbdaca97338 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb3bce51-5ef2-4b35-b116-4fbdaca97338');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5a1dcc8d-9acc-4b4c-9a8f-7c3939e7859d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a1dcc8d-9acc-4b4c-9a8f-7c3939e7859d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5a1dcc8d-9acc-4b4c-9a8f-7c3939e7859d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Book Category"
      ],
      "metadata": {
        "id": "28RCPiHK-VMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df[\"BookCategory\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzHBmMLitlDN",
        "outputId": "3a955c6e-e43d-499e-f9cf-b22a9c404ba6"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Action & Adventure', 'Biographies, Diaries & True Accounts',\n",
              "       'Humour', 'Crime, Thriller & Mystery', 'Arts, Film & Photography',\n",
              "       'Sports', 'Language, Linguistics & Writing',\n",
              "       'Computing, Internet & Digital Media', 'Romance',\n",
              "       'Comics & Mangas', 'Politics'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's a good thing that there only a few unique values for this feature. This means that we may be able to use onehot encoding for this one and since the number of our columns is not large , it's okay to use this method."
      ],
      "metadata": {
        "id": "gJcBQKt9t2fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_BC = pd.get_dummies(cat_df['BookCategory'], prefix='BookCategory')\n",
        "cat_df = pd.concat([cat_df, onehot_BC], axis=1)"
      ],
      "metadata": {
        "id": "b3ftWgVvtiVj"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ueoIrwv0uLKf",
        "outputId": "ddf70bf7-2aa1-4675-8b70-b726f5793736"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title            Author  \\\n",
              "5694  Who Ordered This Truckload of Dung?: Inspiring...       Ajahn Brahm   \n",
              "5695              PostCapitalism: A Guide to Our Future        Paul Mason   \n",
              "5696                             The Great Zoo Of China    Matthew Reilly   \n",
              "5697                                            Engleby  Sebastian Faulks   \n",
              "5698  Only Dull People Are Brilliant at Breakfast (P...       Oscar Wilde   \n",
              "\n",
              "                                               Synopsis  \\\n",
              "5694  “Laugh your way to enlightenment” with this in...   \n",
              "5695  'The most important book about our economy and...   \n",
              "5696  The Chinese government has been keeping a secr...   \n",
              "5697  Mike Engleby has a secret...\\n\\nThis is the st...   \n",
              "5698  'It would be unfair to expect other people to ...   \n",
              "\n",
              "                   BookCategory   Price  New_Ratings  New_Reviews  \\\n",
              "5694                     Humour  1009.0            9          4.9   \n",
              "5695                   Politics   781.0            2          4.1   \n",
              "5696  Crime, Thriller & Mystery   449.0           28          4.1   \n",
              "5697  Crime, Thriller & Mystery   108.0            1          1.0   \n",
              "5698                     Humour    99.0            7          4.5   \n",
              "\n",
              "     Year_of_publish  Format_(French),Paperback2010  \\\n",
              "5694            2005                              0   \n",
              "5695            2016                              0   \n",
              "5696            2016                              0   \n",
              "5697            2008                              0   \n",
              "5698            2016                              0   \n",
              "\n",
              "      Format_(German),Paperback2014  ...  \\\n",
              "5694                              0  ...   \n",
              "5695                              0  ...   \n",
              "5696                              0  ...   \n",
              "5697                              0  ...   \n",
              "5698                              0  ...   \n",
              "\n",
              "      BookCategory_Arts, Film & Photography  \\\n",
              "5694                                      0   \n",
              "5695                                      0   \n",
              "5696                                      0   \n",
              "5697                                      0   \n",
              "5698                                      0   \n",
              "\n",
              "      BookCategory_Biographies, Diaries & True Accounts  \\\n",
              "5694                                                  0   \n",
              "5695                                                  0   \n",
              "5696                                                  0   \n",
              "5697                                                  0   \n",
              "5698                                                  0   \n",
              "\n",
              "      BookCategory_Comics & Mangas  \\\n",
              "5694                             0   \n",
              "5695                             0   \n",
              "5696                             0   \n",
              "5697                             0   \n",
              "5698                             0   \n",
              "\n",
              "      BookCategory_Computing, Internet & Digital Media  \\\n",
              "5694                                                 0   \n",
              "5695                                                 0   \n",
              "5696                                                 0   \n",
              "5697                                                 0   \n",
              "5698                                                 0   \n",
              "\n",
              "      BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "5694                                       0                    1   \n",
              "5695                                       0                    0   \n",
              "5696                                       1                    0   \n",
              "5697                                       1                    0   \n",
              "5698                                       0                    1   \n",
              "\n",
              "      BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "5694                                             0                      0   \n",
              "5695                                             0                      1   \n",
              "5696                                             0                      0   \n",
              "5697                                             0                      0   \n",
              "5698                                             0                      0   \n",
              "\n",
              "      BookCategory_Romance  BookCategory_Sports  \n",
              "5694                     0                    0  \n",
              "5695                     0                    0  \n",
              "5696                     0                    0  \n",
              "5697                     0                    0  \n",
              "5698                     0                    0  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce809a07-9cfd-4ff9-a767-42737a0336e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>BookCategory</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Arts, Film &amp; Photography</th>\n",
              "      <th>BookCategory_Biographies, Diaries &amp; True Accounts</th>\n",
              "      <th>BookCategory_Comics &amp; Mangas</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>Who Ordered This Truckload of Dung?: Inspiring...</td>\n",
              "      <td>Ajahn Brahm</td>\n",
              "      <td>“Laugh your way to enlightenment” with this in...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>1009.0</td>\n",
              "      <td>9</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>PostCapitalism: A Guide to Our Future</td>\n",
              "      <td>Paul Mason</td>\n",
              "      <td>'The most important book about our economy and...</td>\n",
              "      <td>Politics</td>\n",
              "      <td>781.0</td>\n",
              "      <td>2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>The Great Zoo Of China</td>\n",
              "      <td>Matthew Reilly</td>\n",
              "      <td>The Chinese government has been keeping a secr...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>449.0</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>Engleby</td>\n",
              "      <td>Sebastian Faulks</td>\n",
              "      <td>Mike Engleby has a secret...\\n\\nThis is the st...</td>\n",
              "      <td>Crime, Thriller &amp; Mystery</td>\n",
              "      <td>108.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>Only Dull People Are Brilliant at Breakfast (P...</td>\n",
              "      <td>Oscar Wilde</td>\n",
              "      <td>'It would be unfair to expect other people to ...</td>\n",
              "      <td>Humour</td>\n",
              "      <td>99.0</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce809a07-9cfd-4ff9-a767-42737a0336e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce809a07-9cfd-4ff9-a767-42737a0336e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce809a07-9cfd-4ff9-a767-42737a0336e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e275823-c879-410d-866b-37163cc1e405\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e275823-c879-410d-866b-37163cc1e405')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e275823-c879-410d-866b-37163cc1e405 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving to the next step we should drop the old bookcategory column."
      ],
      "metadata": {
        "id": "4jj3VnULuXvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop([\"BookCategory\"],axis=1)\n",
        "cat_df.tail(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CUK3ZWO4ufWd",
        "outputId": "7e1f25da-75b9-4bfc-a3fe-4d8dbdbf58cd"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title            Author  \\\n",
              "5696                             The Great Zoo Of China    Matthew Reilly   \n",
              "5697                                            Engleby  Sebastian Faulks   \n",
              "5698  Only Dull People Are Brilliant at Breakfast (P...       Oscar Wilde   \n",
              "\n",
              "                                               Synopsis  Price  New_Ratings  \\\n",
              "5696  The Chinese government has been keeping a secr...  449.0           28   \n",
              "5697  Mike Engleby has a secret...\\n\\nThis is the st...  108.0            1   \n",
              "5698  'It would be unfair to expect other people to ...   99.0            7   \n",
              "\n",
              "      New_Reviews Year_of_publish  Format_(French),Paperback2010  \\\n",
              "5696          4.1            2016                              0   \n",
              "5697          1.0            2008                              0   \n",
              "5698          4.5            2016                              0   \n",
              "\n",
              "      Format_(German),Paperback2014  Format_(Kannada),Paperback2014  ...  \\\n",
              "5696                              0                               0  ...   \n",
              "5697                              0                               0  ...   \n",
              "5698                              0                               0  ...   \n",
              "\n",
              "      BookCategory_Arts, Film & Photography  \\\n",
              "5696                                      0   \n",
              "5697                                      0   \n",
              "5698                                      0   \n",
              "\n",
              "      BookCategory_Biographies, Diaries & True Accounts  \\\n",
              "5696                                                  0   \n",
              "5697                                                  0   \n",
              "5698                                                  0   \n",
              "\n",
              "      BookCategory_Comics & Mangas  \\\n",
              "5696                             0   \n",
              "5697                             0   \n",
              "5698                             0   \n",
              "\n",
              "      BookCategory_Computing, Internet & Digital Media  \\\n",
              "5696                                                 0   \n",
              "5697                                                 0   \n",
              "5698                                                 0   \n",
              "\n",
              "      BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "5696                                       1                    0   \n",
              "5697                                       1                    0   \n",
              "5698                                       0                    1   \n",
              "\n",
              "      BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "5696                                             0                      0   \n",
              "5697                                             0                      0   \n",
              "5698                                             0                      0   \n",
              "\n",
              "      BookCategory_Romance  BookCategory_Sports  \n",
              "5696                     0                    0  \n",
              "5697                     0                    0  \n",
              "5698                     0                    0  \n",
              "\n",
              "[3 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c831cea5-2120-411a-88d0-94e7509fa6d2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Arts, Film &amp; Photography</th>\n",
              "      <th>BookCategory_Biographies, Diaries &amp; True Accounts</th>\n",
              "      <th>BookCategory_Comics &amp; Mangas</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>The Great Zoo Of China</td>\n",
              "      <td>Matthew Reilly</td>\n",
              "      <td>The Chinese government has been keeping a secr...</td>\n",
              "      <td>449.0</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>Engleby</td>\n",
              "      <td>Sebastian Faulks</td>\n",
              "      <td>Mike Engleby has a secret...\\n\\nThis is the st...</td>\n",
              "      <td>108.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>Only Dull People Are Brilliant at Breakfast (P...</td>\n",
              "      <td>Oscar Wilde</td>\n",
              "      <td>'It would be unfair to expect other people to ...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c831cea5-2120-411a-88d0-94e7509fa6d2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c831cea5-2120-411a-88d0-94e7509fa6d2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c831cea5-2120-411a-88d0-94e7509fa6d2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d1548eaf-82da-4852-9d66-5ccf88bcec1c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1548eaf-82da-4852-9d66-5ccf88bcec1c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d1548eaf-82da-4852-9d66-5ccf88bcec1c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Synopsis**"
      ],
      "metadata": {
        "id": "9Yp5sJK4CbEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synopsis = cat_df['Synopsis'].values.tolist()\n",
        "synopsis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEhRYFR_CpwV",
        "outputId": "564470df-15eb-488d-810d-d03bd94e8efd"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"THE HUNTERS return in their third brilliant novel from the Sunday Times Top Ten bestselling author Chris Kuzneski, whose writing James Patterson says has 'raw power'. The team are hunting Marco Polo's hidden treasure, but who is on their tail?\\nTHE HUNTERS\\nIf you seek, they will find...\\n\\nThe travels of Marco Polo are known throughout the world.\\nBut what if his story isn't complete?\\nWhat if his greatest adventure has yet to be discovered?\\nGuided by a journal believed to have been dictated by Polo himself,\\nthe Hunters set out in search of his final legacy:\\nthe mythical treasure gathered during Polo's lifetime of exploration.\\nBut as every ancient clue brings them closer to the truth,\\neach new step puts them in increasing danger...\\nExplosive action. Killer characters. Classic Kuzneski.\",\n",
              " 'A layered portrait of a troubled genius for whom art was not merely a thing of beauty but a vital part of living itself.\\nSelling Points: The original Marathi book won the National Award for Best Book on Cinema (in 1986), now available for the first time in English.\\nThe Book: Guru Dutt is now named along with the masters of world cinema—like Orson Welles, Mizoguchi, Hitchcock, Jancso, Ophuls—for his innovative cinematic form and his deep humanism and compassion. In Guru Dutt: A Tragedy in Three Acts, renowned film-maker and scholar Arun Khopkar sheds new light on Dutt’s genius through a close examination of Dutt’s three best-known films—Pyaasa, Kaagaz Ke Phool and Sahib Biwi Aur Ghulam. With a nuanced eye, Khopkar explores the historical context which influenced Dutt’s deeply melancholic style while also analyzing the intricacies of the medium—acting, lighting, music, editing, rhythm—that Dutt carefully deployed to create his masterpieces. Originally written in Marathi, this exquisite English translation paints a layered portrait of a troubled genius for whom art was not merely a thing of beauty but a vital part of living itself.',\n",
              " '\"During the time men live without a common Power to keep them all in awe, they are in that condition which is called Warre\"\\n\\nWritten during the turmoil of the English Civil War, Leviathan is an ambitious and highly original work of political philosophy. Claiming that man\\'s essential nature is competitive and selfish, Hobbes formulates the case for a powerful sovereign—or \"Leviathan\"—to enforce peace and the law, substituting security for the anarchic freedom he believed human beings would otherwise experience. This worldview shocked many of Hobbes\\'s contemporaries, and his work was publicly burnt for sedition and blasphemy when it was first published. But in his rejection of Aristotle\\'s view of man as a naturally social being, and in his painstaking analysis of the ways in which society can and should function, Hobbes opened up a whole new world of political science.\\n\\nBased on the original 1651 text, this edition incorporates Hobbes\\'s own corrections, while also retaining the original spelling and punctuation, to read with vividness and clarity. C. B. Macpherson\\'s introduction elucidates one of the most fascinating works of modern philosophy for the general reader.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.',\n",
              " 'A handful of grain is found in the pocket of a murdered businessman…\\n\\nRex Fortescue, king of a financial empire, was sipping tea in his ‘counting house’ when he suffered an agonising and sudden death. On later inspection, the pockets of the deceased were found to contain traces of cereals.\\n\\nYet, it was the incident in the parlour which confirmed Jane Marple’s suspicion that here she was looking at a case of crime by rhyme…',\n",
              " 'For seven decades, \"Life\" has been thrilling the world with its unrivalled presentation of the very best photography to be found. Here, the editors have assembled the creme de la creme from the magazine\\'s vast collection of images. Because \"Life\" has always dealt with matters of every sort, the entire spectrum of society is represented in these pages. One after another, there are unforgettable photos from Hollywood\\'s greatest stars, from the wonders of small-town America, from the terrible wars, as well as from the zestful years of childhood. \"Life\" has always represented the apex in photojournalism and its roster of great photographers is unequalled.',\n",
              " 'The revised edition of the bestselling ChiRunning, a groundbreaking program from ultra-marathoner and nationally-known coach Danny Dreyer, that teaches you how to run faster and farther with less effort, and to prevent and heal injuries for runners of any age or fitness level.\\n\\nIn ChiRunning, Danny and Katherine Dreyer, well-known walking and running coaches, provide powerful insight that transforms running from a high-injury sport to a body-friendly, injury-free fitness phenomenon. ChiRunning employs the deep power reserves in the core muscles, an approach found in disciplines such as yoga, Pilates, and T’ai Chi.\\n\\nChiRunning enables you to develop a personalized exercise program by blending running with the powerful mind-body principles of T’ai Chi:\\n\\n1. Get aligned. Develop great posture and reduce your potential for injury while running, and make knee pain and shin splints a thing of the past.\\n\\n2. Engage your core. Shift the workload from your leg muscles to your core muscles, for efficiency and speed.\\n\\n3. Add relaxation to your running. Learn to focus your mind and relax your body to increase speed and distance.\\n\\n4. Make it a Mindful Practice. Maintain high performance and make running a mindful, enjoyable life-long practice.\\n\\n5. It’s easy to learn. Transform your running with the ten-step ChiRunning training program.',\n",
              " 'Agatha Christie’s most exotic murder mystery\\n\\nThe tranquillity of a cruise along the Nile is shattered by the discovery that Linnet Ridgeway has been shot through the head. She was young, stylish and beautiful, a girl who had everything – until she lost her life.\\n\\nHercule Poirot recalls an earlier outburst by a fellow passenger: ‘I’d like to put my dear little pistol against her head and just press the trigger.’ Yet in this exotic setting’ nothing is ever quite what it seems…',\n",
              " 'Achieve a healthy body, mental alertness, and inner serenity through the practice of hatha yoga.\\nCombining step-by-step asanas, detailed illustrations, programmes, mindfulness techniques, diet advice, and recipes, Yoga: Your Home Practice Companion is the complete practice and lifestyle guide for students of all abilities.\\nMaster breathing and mindfulness techniques to recharge your energy levels and combat stress. Embrace a healthier more yogic way of eating with diet advice and more than 40 nutritious plant-based recipes. Written by the experts from the world-renowned Sivananda Yoga Vedana Centres, Yoga: Your Home Practice Companion has everything you need in one complete package.',\n",
              " \"Karmayogi is the dramatic and inspiring story of E. Sreedharan, the much-admired engineer and technocrat who won accolades for finishing the Delhi Metro project within budget and on time, in the face of severe constraints. Known for his efficiency and discipline and regarded the world over for his productivity standards, Sreedharan has, surprisingly, never spent more than the eight-hour workday in office. This fascinating book looks back on an extraordinary career full of sterling achievements-Sreedharan's years with the Railways, the building of the Kolkata Metro and the Konkan Railway, followed by the Delhi Metro, and the many metro projects he is involved with now. Translated from a bestselling biography in Malayalam, this is the uplifting story of a very private person who has become an icon of modern India because of his uncompromising work ethic.\",\n",
              " '‘This is the original game of thrones’ George R.R. Martin\\nFrom the publishers that brought you A Game of Thrones comes the series that inspired George R.R. Martin’s epic work.\\n“Accursed! Accursed! You shall be accursed to the thirteenth generation!”\\nThe Iron King – Philip the Fair – is as cold and silent, as handsome and unblinking as a statue. He governs his realm with an iron hand, but he cannot rule his own family: his sons are weak and their wives adulterous; while his red-blooded daughter Isabella is unhappily married to an English king who prefers the company of men.\\nA web of scandal, murder and intrigue is weaving itself around the Iron King; but his downfall will come from an unexpected quarter. Bent on the persecution of the rich and powerful Knights Templar, Philip sentences Grand Master Jacques Molay to be burned at the stake, thus drawing down upon himself a curse that will destroy his entire dynasty…',\n",
              " 'There is a new awakening in India that is challenging the ongoing Westernization of the discourse about India. Battle for Sanskrit seeks to alert traditional scholars of Sanskrit and sanskriti - Indian civilization - concerning an important school of thought that has its base in the US and that has started to dominate the discourse on the cultural, social and political aspects of India. The scholars of this field hold that many Sanskrit texts are socially oppressive and serve as political weapons in the hands of the ruling elite; that the sacred aspects need to be refuted; and that Sanskrit has long been dead. The traditional Indian experts would outright reject or at least question these positions. Is each view exclusive of the other, or can there be a bridge between them?',\n",
              " \"THE DEFINITIVE BOOK ON HOW THE TECHNOLOGY BEHIND BITCOIN AND CRYPTOCURRENCY IS CHANGING THE WORLD.\\n'Blockchain Revolution is a highly readable introduction to a bamboozling but increasingly important field' - Guardian\\nBlockchain is the ingeniously simple technology that powers Bitcoin. But it is much more than that, too. It is a public ledger to which everyone has access, but which no single person controls. It allows for companies and individuals to collaborate with an unprecedented degree of trust and transparency. It is cryptographically secure, but fundamentally open. And soon it will be everywhere.\\nIn Blockchain Revolution, Don and Alex Tapscott reveal:\\n· how this game-changing technology will shape the future of the world economy\\n· how it will improve everything from healthcare records to online voting, and from insurance claims to artist royalty payments\\nBrilliantly researched and highly accessible, this is the essential text on the next major paradigm shift. Read it, or be left behind.\\n-----\\n'This book has had an enormous impact on the evolution of blockchain in the world' Satya Nadella, CEO, Microsoft\\n\\n'Spectacular. Mind-blowing in its expansiveness and profundity' Steve Wozniak, co-founder of Apple\\n\\n'Iconic' Clay Christensen, author of The Innovator's Dilemma\\n'Occasionally a book comes along that changes the global discourse. This is likely to be one of those books.' Klaus Schwab, Founder and Executive Chairman of the World Economic Forum\",\n",
              " 'Set in the turbulent days of the founding of Hong Kong in the 1840s, Tai-Pan is the story of Dirk Struan, the ruler - the Tai-Pan - of the most powerful trading company in the Far East. He is also a pirate, an opium smuggler, and a master manipulator of men. This is the story of his fight to establish himself and his dynasty as the undisputed masters of the Orient.',\n",
              " 'The Art of Shaolin Kung Fu is the ultimate guide to Kung Fu, from theory to practical application.\\n\\nThis unique martial arts book, by a renowned Grandmaster, is a complete and comprehensive introduction to Kung Fu and all other aspects of ancient Shaolin wisdom. You will learn the ancient art of the Shaolin monks from the famous Shaolin monastery. It will prove invaluable to everyone interested in martial arts, chi kung, and meditation, showing how Kung Fu and other Shaolin arts can bring you health, vitality, mental focus, and spiritual joy.\\n\\nChapters include:\\nWhat is Kung Fu?—Four Aspects of Kung Fu; The Importance of Force Training; Application for Combat and Daily Living; Kung Fu Philosophy for Deeper Understanding; The Benefits of Kung Fu\\nThe Historical Development of Chinese Martial Arts—Kung Fu in Prehistoric and Ancient Times; The Glorious Han and Tang; The Modern Period\\nFrom Shaolin to Taijiquan—Shaolin Kung Fu; The Various Styles of Taijiquan; Soft and Hard, Internal and External\\nA Comparative Study of Kung Fu—Contrasting Shaolin and Wudang Kung Fu; Xingyi Kung Fu and Taoist Concepts; Tanglangquan or Praying Mantis Kung Fu; The Spread of Southern Kung Fu\\nDefining Aims and Objectives—Setting Aims for Kung Fu Training; Personal Objectives; Course Objectives\\nThe Foundation of Shaolin Kung Fu—Inheritance from Past Masters; Various Shaolin Hand Forms; Basic Shaolin Patterns\\nFrom Form to Combat Application—The Four Directions of Attack; The Principles of Effective Combat; Specific Techniques Against Kicks\\nCombat Sequences and Set Practice—Relieving Injuries Sustained in Sparring; Composing Your Own Kung Fu Sets\\nShaolin Five Animals—Understanding Characteristics and Essence; The Five-Animal Set; The Names of the Five-Animal Patterns\\nFive-Animal Combination Set—How to Improve Combat Efficiency; Spacing and Timing in Combat\\nThe Internal Force of Shaolin Kung Fu—The Relationship Between Technique and Force; The Compassionate Art of Qin-Na; The Internal Force of Tiger Claw\\nTactics and Strategies—Using Continuous Attack Effectively; A Tactic to Distract Your Opponent; Selecting Strategies to Suit Particular Situations\\nClassical Kung Fu Weapons—Staffs; Whips, Knives and Other Weapons; Light and Heavy Weapons\\nUnderstanding and Developing Chi—The Various Genres of Chi Kung; Lohan Embracing Buddha; Abdominal Breathing\\nShaolin Kung Fu and Zen—Culitvating Heart, Nourishing Nature; Bodhidharma and Taoism in Zen\\nThe Shaolin Way to Enlightenment—Attaining a Focused Mind; Meditation to Train Awareness; Shaolin Kung Fu for Spiritual Development',\n",
              " \"Anil's Ghost transports us to Sri Lanka, a country steeped in centuries of tradition, now forced into the late twentieth century by the ravages of a bloody civil war. Enter Anil Tissera, a young woman and forensic anthropologist born in Sri Lanka but educted in the West, sent by an international human rights group to identify the victims of the murder campaigns sweeping the island.\\n\\nWhen Anil discovers that the bones found in an ancient burial site are in fact those of a much more recent victim, her search for the terrible truth hidden in her homeland begins. What follows is a story about love, about family, about identity - a story driven by a riventing mystery.\",\n",
              " \"One day, an alien orphan crash-lands on Earth and is adopted by a human couple, the Kents. Many years later, that little baby becomes Superman, protector of Earth! But how did the Man of Steel go from orphan to super hero? Experience Clark Kent's incredible journey in this action-packed chapter book for early readers featuring vibrant art by DC Comics illustrators.\",\n",
              " 'London is one of the most exciting cities in the world! But if you are a young visitor where should you go and what should you look out for?\\nGo for a ride on the London Eye, watch street performer spectacles at Covent Garden and stand under a life-sized model of a blue whale at the Natural History Museum. In this revised and updated edition of the much-loved My First Book of London, a fun question-and-answer format introduces young readers to the famous sights of this fascinating city. Stunning illustrations bring each feature to life and provide an ideal way for your child to learn about London and the fun of exploring it!',\n",
              " 'A new series of prose novels, straight from the worldwide Naruto franchise. Naruto’s allies and enemies take center stage in these fast-paced adventures, with each volume focusing on a particular clan mate, ally, team…or villain.\\n\\nUchiha Itachi, four years of age. With the hell of war burned into his eyes, the boy makes a resolution: he will rid this world of all violence. The birth of Sasuke, meeting his friend Shisui, the academy, genin, chunin, and then the Anbu—Itachi races down the path of glory toward his dream of becoming the first Uchiha Hokage, unaware of the darkness that lies ahead… ',\n",
              " 'A brilliant and concise account of the lives and ideas of the great philosophers—Plato, Aristotle, Bacon, Spinoza, Voltaire, Kant, Schopenhauer, Spencer, Nietzsche, Bergson, Croce, Russell, Santayana, James, and Dewey—The Story of Philosophy is one of the great books of our time. Few write for the non-specialist as well as Will Durant, and this book is a splendid example of his eminently readable scholarship. Durant’s insight and wit never cease to dazzle; The Story of Philosophy is a key book for any reader who wishes to survey the history and development of philosophical ideas in the Western world.',\n",
              " 'Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. You’ll explore data visualization, graph databases, the use of NoSQL, and the data science process. You’ll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it.',\n",
              " \"A stunning hardback edition of this surprise hit - perfect for Christmas gifting.\\nRecommended By Thousands of International Readers - the tender feelgood story of a man's journey around Japan with a streetcat. Translated by Philip Gabriel, a translator of Murakami.\\nIncludes Specially Commissioned Line-Drawings\\nA Watersones Paperback of The Year 2017\\n'Anyone who has ever unashamedly loved an animal will read this book with gratitude, for its understanding of an emotion that ennobles us as human beings, whether we value it or not 'Lynne Truss, Guardian\\n‘Bewitching as self-possessed and comforting as – well, a cat’ Sunday Telegraph\\nIt's not the journey that counts, but who's at your side.\\nNana is on a road trip, but he is not sure where he is going. All that matters is that he can sit beside his beloved owner Satoru in the front seat of his silver van. Satoru is keen to visit three old friends from his youth, though Nana doesn’t know why and Satoru won’t say.\\nSet against the backdrop of Japan’s changing seasons and narrated with a rare gentleness and humour, Nana’s story explores the wonder and thrill of life’s unexpected detours. It is about the value of friendship and solitude and knowing when to give and when to take. Travelling Cat has already demonstrated its power to move thousands of readers with a message of kindness and truth. It shows, above all, how acts of love, both great and small, can transform our lives.\\n'Arikawa has a lightness of touch that elevates her story to a tale about loyalty and friendship while speaking to our basic human need for companionship' john Boyne, Irish Times\\n‘It has the warmth, painterly touch and tenderness of a Studio Ghibli film – and it is a delight to read’ Financial Times\\n'A delightful tale of loyalty and friendship' John Boyne, Irish Times.\",\n",
              " \"FROM THE BESTSELLING AUTHOR OF RONALDO AND NEYMAR Prolific, cool-headed and unerringly consistent, Lionel Messi is one of the most revered footballers in history. But did you know that his transfer to Barcelona was first agreed on a paper napkin? Or that an x-ray of his hand was to thank for identifying his growth hormone deficiency? And do you know why he refused to collect his first ever Champions League winner's medal? Find out all this and more in Luca Caioli's classic portrait of a footballing icon, featuring exclusive interviews with those who know him best and even Messi himself. Includes all the action from the 2017/18 season and the 2018 World Cup\",\n",
              " \"MARIO PUZO'S FIRST ACCLAIMED NOVEL, BEFORE HIS CREATION OF THE INTERNATIONAL BESTSELLER, THE GODFATHER. AN ASTOUNDING STORY OF CORRUPTION AND BETRAYAL.\\n\\nHardened by the brutality and desceration of three long years of war, Walter Mosca returns to America a changed man. But he has no sooner arrived than he knows he must run back to the land of the enemy, to find the woman who accepts the rage and cruelty of the world around her.\\n\\nIn Germany, where the bitter aftermath of war is everything apparent, American cigarettes will buy almost anything. Mosca now faces a different kind of war, one in which he must make a fateful decision - between love and ambition, passion and greed, life and death...\",\n",
              " 'Step by Step Screenshots Guided Handholding Approach to Learning\\nExplaining FI, CO Modules & Concepts to guide Consultants, Users, End Users gain confidence, get comfortable with and improve productivity using SAP FICO. Beginners who are in their First & Second year of career with SAP FICO will find this book beneficial the most.\\nHow the Chapters are arranged\\nCHAPTER I - Helps YOU begin using SAP FICO on a strong note.\\nCHAPTER II - Grasp the concepts for a theoretical foundation on which SAP FICO is designed and built.\\nCHAPTERS III & IV - Get introduced to Transaction Codes & Standard Reports in SAP FICO.\\nCHAPTER V - Navigation in SAP FICO put YOU at complete ease with SAP Navigation and a strong footing to move forward confidently.\\nCHAPTERS VI & VII - Essential SAP Tips & Layout make YOURSAP FICO experience, a pleasure.\\nCHAPTER VIII - How YOU can work with Standard Reports.\\nCHAPTERS IX, X & XI - Experience authority in using Standard Reports.\\nTopics Covered\\nChapter I Navigation in SAP (Part \\x96 I)\\nChapter II Concepts\\nChapter III Data Entry\\nChapter IV Standard Reports in FICO\\nChapter V Navigation in SAP \\x96 (Part II)\\nChapter VI SAP Tips & Tricks\\nChapter VII Customizing SAP Layout\\nChapter VIII Report Parameter Selections\\nChapter IX List Functions\\nChapter X ABC Analysis\\nChapter XI Extract Management\\nBegin your journey with this book to understand and optimize using SAP FICO to take your career to greater heights.',\n",
              " 'Comprehensive and clear explanations of key grammar patterns and structures are reinforced and contextualized through authentic materials. You will not only learn how to construct grammar correctly, but when and where to use it so you sound natural and appropriate. German Grammar You Really Need to Know will help you gain the intuition you need to become a confident communicator in your new language.',\n",
              " '• Thousands of Africans head to China each year to buy cell phones, auto parts, and other products that they will import to their home countries through a clandestine global back channel.\\n \\n• Hundreds of Paraguayan merchants smuggle computers, electronics, and clothing across the border to Brazil.\\n \\n• Scores of laid-off San Franciscans, working without any licenses, use Twitter to sell home-cooked foods.\\n \\n• Dozens of major multinationals sell products through unregistered kiosks and street vendors around the world.\\n \\nWhen we think of the informal economy, we tend to think of crime: prostitution, gun running, drug trafficking. Stealth of Nations opens up this underground realm, showing how the worldwide informal economy deals mostly in legal products and is, in fact, a ten-trillion-dollar industry, making it the second-largest economy in the world, after that of the United States.\\n \\nHaving penetrated this closed world and persuaded its inhabitants to open up to him, Robert Neuwirth makes clear that this informal method of transaction dates back as far as humans have existed and traded, that it provides essential services and crucial employment that fill the gaps in formal systems, and that this unregulated market works smoothly and effectively, with its own codes and unwritten rules.\\n \\nCombining a vivid travelogue with a firm grasp on global economic strategy—along with a healthy dose of irreverence and skepticism toward conventional perceptions—Neuwirth gives us an eye-opening account of a world that is always operating around us, hidden in plain sight.',\n",
              " \"Who killed Hansie Cronje and Bob Woolmer? Have players from the national squad been involved in match-fixing? Is suspending IPL teams punishment enough for erring franchise owners? Should betting be made legal in India as advised by the Lodha Committee? From S. Sreesanth to Chris Cairns, Lalit Modi to N. Srinivasan, Hamid Cassim to Tinku Mandi, Shantanu Guha Ray examines the allegations of corruption against players, cricket administrators and bookies alike. He interviews the myriad people who linger in the shadows of players' dressing rooms - the middle men, agents, 'friends' of IPL franchise owners - placing bets on games and enticing cricketers to reveal inside information for money, sex or, worse, fear for their lives. Also under the spotlight are the roles of the police and the government, who have, at best, made patchy efforts to stem the rot. Fixed!: Cash and Corruption in Cricket is an incisive, unflinching look at the underbelly of what once used to be a gentleman's game.\",\n",
              " \"The classic eight volume graphic novel series from the godfather of the genre an irreverent and humourous rendition of the life and times of Buddha is now available in Box Set. Originated in the 1970s, Buddha is Osamu Tezuka's unparalleled rendition of the life and times of Prince Siddhartha. Tezuka's storytelling genius and consummate skill at visual expression blossom fully as he contextualizes the Buddha's ideas, with an emphasis on action, emotion, humour and conflict as Prince Siddhartha runs away from home, travels across India and questions Hindu practices such as ascetic self-mutilation and caste oppression. Rather than recommend resignation and impassivity, Tezuka's Buddha predicates enlightenment upon recognizing the interconnectedness of life, having compassion for the suffering and ordering one's life sensibly. Furthermore, his approach is slightly irreverent in that it incorporates something that Western commentators often eschew, namely, humour.\\nThe Box Set includes\\nBuddha: Kapilvastu - Vol. 1\\nBuddha: The Four Encounters - Vol. 2\\nBuddha: Devadatta - Vol. 3\\nBuddha: The Forest of Uruvela - Vol. 4\\nBuddha: Deer Park - Vol. 5\\nBuddha: Ananda - Vol. 6\\nBuddha: Prince Ajatasattu - Vol. 7\\nBuddha: Jetavana - Vol. 8\",\n",
              " 'From the creators of WWE 50 and the official WWE Encyclopedia, 30 Years of WrestleMania gives you an in-depth, behind-the-scenes look at the show of shows from its inception to the current day. Relive each exciting match with detailed information, exclusive interviews, never-before-seen shots, and much more!\\nThe history of each WrestleMania, from both sides of the curtain\\nSpecial tributes to Undertaker\\'s Streak, Mr. WrestleMania, records, stats & more\\nStories from influential figures, including Vince McMahon, Hulk Hogan, Bret Hart & others\\nBehind the scenes photos and anecdotes\\nArtifacts such as tickets, VIP passes, programs, promotional items and other unique surprises\\nCoverage of all the definitive Superstars and celebrities\\nForeward by \"Mr. WrestleMania\" Shawn Michaels\\nExclusive Topps collectible trading card featuring the Undertaker\\nOne of five exclusive collectible bookmarks\\nWhether you\\'ve never missed a \\'Mania or you are new to WWE, 30 Years of WrestleMania is guaranteed to enhance your fanhood and enrich your enjoyment of the WWE\\'s annual worldwide phenomenon.',\n",
              " \"An eye-opening biography of one of the most influential psychiatrists of the modern age, drawing from his lectures, conversations, and own writings.\\n\\nIn the spring of 1957, when he was eighty-one years old, Carl Gustav Jung undertook the telling of his life story. Memories, Dreams, Reflections is that book, composed of conversations with his colleague and friend Aniela Jaffé, as well as chapters written in his own hand, and other materials. Jung continued to work on the final stages of the manuscript until shortly before his death on June 6, 1961, making this a uniquely comprehensive reflection on a remarkable life. Fully corrected, this edition also includes Jung's VII Sermones ad Mortuos.\",\n",
              " \"The Hit is David Baldacci's blockbuster follow up to The Innocent, the smash-hit bestseller featuring U.S. government assassin, Will Robie. When government hit man Will Robie is given his next target he knows he's about to embark on his toughest mission yet. He is tasked with killing one of their own, following evidence to suggest that fellow assassin Jessica Reel has been turned. She's leaving a trail of death in her wake including her handler. The trap is set. To send a killer to catch a killer. But what happens when you can't trust those who have access to the nation's most secret intelligence? The Hit is the second thriller in the Will Robie series by David Baldacci, preceding The Target.\",\n",
              " 'No matter what cases private eye Lew Archer takes on—a burglary, a runaway, or a disappeared person—the trail always leads to tangled family secrets and murder. Widely considered the heir to Sam Spade and Philip Marlowe, Archer dug up secrets and bodies in and around Los Angeles. \\n\\nHere, The Archer Files collects all the Lew Archer short stories ever published, along with thirteen unpublished “case notes” and a fascinating biographical profile of Archer by Edgar Award finalist Tom Nolan. Ross Macdonald’s signature staccato prose is the real star throughout this collection, which is both a perfect introduction for the newcomer and a must-have for the Macdonald aficionado.',\n",
              " 'Light on Life brings the insight and wisdom of Indian astrology to the Western reader. Jyotish or Indian astrology is an ancient and complex method of exploring the nature of time and space and its effect upon the individual. Formerly a closed book to the West, the subject has now been clarified and explained by Hart deFouw and Robert Svoboda, two experts and long-term practitioners. In Light on Life they have created a complete and thorough handbook that can be appreciated and understood by those with very little knowledge of astrology. Jyotish states that by considering the state of the cosmos when an event occurs, we can begin to understand its nature- and to prepare an appropriate response. Although there are similarities with Western astrology there are also profound differences. Jyotish is, above all, infused with the religious, psychological and physical spirit of India. This comprehensive and enlightening book on the subject will prove a necessity to every astrologer or student of Indian thought.',\n",
              " 'The Doomsday Conspiracy, by Sidney Sheldon, is a suspense-thriller novel, that revolves around an American naval personnel, Robert Bellamy, who experiences some mysterious happenings during his investigation around a balloon accident. This special assignment gets the naval officer into a secret service. While the officer tries to find out the story behind the accident, he realises that his own life is in danger. With a nail-biting mystery, The Doomsday Conspiracy succeeds in keeping the readers glued.\\nRobert is given an assignment to find out a balloon that is declared missing after an accident. The people who were inside the balloon also have to be tracked down, while maintaining secrecy of the entire mission. As Robert unfolds each layer of this investigation, he gets a news of his wife having left him for another man. While he is dealing with this emotional trauma, he finds out that the missing people are being killed one by one, after which he slowly realises that he himself is being hunted. Who is trying to kill him, why is the entire mission woven so secretly, and what’s all hidden for Robert to know more are some questions that keep revolving in the readers’ minds with each page turned.\\nThe book is full of vivid characters and a fresh plot. This story has proven to be an inspiration to several popular authors to write similar thriller fictional work. Reissue edition of The Doomsday Conspiracy was published by Harpercollins in 2005 in paperback.',\n",
              " \"Journey alongside Nathan Drake once again, as Naughty Dog and Dark Horse Books team up to bring you this comprehensive and breathtaking exploration into Uncharted 4: A Thief's End! Numerous never-before-seen designs and concept art accompanied by commentary from the developers give you behind-the-scenes access to the creation of this gaming masterwork. This beautiful oversized hardcover is a must-have for any fan of the Uncharted franchise and high quality video game art.\\nThe Art of Unchartend 4 will be released by Dark Horse simultaneoulsy with the new game, Uncharted 4.\",\n",
              " \"_________________________ hannibal lecter wasn't born a monster. he was made one. hannibal lecter emerges from the nightmare of the Eastern front of world War II, a boy in the snow, mute, with a chain around his neck. He will not speak of what happened to him and his family. He seems utterly alone, but he has brought his demons with him. Hannibal's uncle, a noted painter, finds him in a Soviet orphanage and brings him to France. There, hannibal lives with his uncle and his uncle's beautiful and exotic wife, lady murasaki, who helps him to heal - and flourish. But hannibal's demons are not so easily defeated. Throughout his young life, They visit him and torment him. When he is old enough, he visits them in turn - and in the fog of traumatic memory, he discovers that he has gifts far beyond what he imagined...\",\n",
              " 'This second edition of Data Structures Using C has been developed to provide a comprehensive and consistent coverage of both the abstract concepts of data structures as well as the implementation of these concepts using C language. It begins with a thorough overview of the concepts of C programming followed by introduction of different data structures and methods to analyse the complexity of different algorithms. It then connects these concepts and applies them to the study of various data structures such as arrays, strings, linked lists, stacks, queues, trees, heaps, and graphs.\\n\\nThe book utilizes a systematic approach wherein the design of each of the data structures is followed by algorithms of different operations that can be performed on them, and the analysis of these algorithms in terms of their running times.\\n\\nEach chapter includes a variety of end-chapter exercises in the form of MCQs with answers, review questions, and programming exercises to help readers test their knowledge.',\n",
              " \"Exploring a karmic network in 25,320 kilometres After twenty years in the Indian Administrative Service, P.G. Tenzing throws off the staid life of a bureaucrat to roar across India on an Enfield Thunderbird, travelling light with his possessions strapped on the back of his bike. On the nine-month motorcycle journey without a pre-planned route or direction, he encounters acquaintances who appear to be from his karmic past: from the roadside barber to numerous waiters and mechanics― fleeting human interactions and connections that seem pre-ordained. Life on the road is full of pot holes in more ways than one, but Tenzing acquires a wheelie’s sixth sense. He is unfazed by suspicious hotel receptionists or other unkarmic sceptics who take one look at his dishevelled, unkempt appearance and ask for an advance, or a deposit or both. Tenzing’s views on life and death, friendship and love are informed by a certain dark humour. But his conviction that everything revolves around the sacred bond that humans share with each other and with the universe is deeply felt and inspiring. Sometime singer with a Gangtok band, a dabbler in vipassana meditation and a supporter of a monk's school at Mangan, Sikkim, P.G. Tenzing is self-confessedly at a mid-life crisis point and ready for all the adventures this world has to offer.\",\n",
              " \"On a bright morning in Rome, a terrible explosion rips a hole in the Israeli embassy. Moments later, four gunmen cut down survivors as they stagger from the burning building. Gabriel Allon is hastily recalled to Israel and drawn once more into the heart of the secret service he'd hoped to leave behind. For the blast has led to a disturbing revelation: a dossier that strips away Allon's secrets and lays bare his history. A dossier that had fallen into terrorist hands . . .\",\n",
              " \"NEW from the bestselling HBR's 10 Must Reads series. Learn why bad decisions happen to good managers--and how to make better ones. If you read nothing else on decision making, read these 10 articles. We've combed through hundreds of articles in the Harvard Business Review archive and selected the most important ones to help you and your organization make better choices and avoid common traps. Leading experts such as Ram Charan, Michael Mankins, and Thomas Davenport provide the insights and advice you need to: * Make bold decisions that challenge the status quo * Support your decisions with diverse data * Evaluate risks and benefits with equal rigor * Check for faulty cause-and-effect reasoning * Test your decisions with experiments * Foster and address constructive criticism * Defeat indecisiveness with clear accountability Looking for more Must Read articles from Harvard Business Review? Check out these titles in the popular series: HBR's 10 Must Reads: The Essentials HBR's 10 Must Reads on Communication HBR's 10 Must Reads on Collaboration HBR's 10 Must Reads on Innovation HBR's 10 Must Reads on Leadership HBR's 10 Must Reads on Managing Yourself HBR's 10 Must Reads on Strategic Marketing HBR's 10 Must Reads on Teams\",\n",
              " \"'Politics and the English Language' is widely considered Orwell's most important essay on style. Style, for Orwell, was never simply a question of aesthetics; it was always inextricably linked to politics and to truth.'All issues are political issues, and politics itself is a mass of lies, evasions, folly, hatred and schizophrenia.When the general atmosphere is bad, language must suffer.'Language is a political issue, and slovenly use of language and cliches make it easier for those in power to deliberately use misleading language to hide unpleasant political facts. Bad English, he believed, was a vehicle for oppressive ideology, and it is no accident that 'Politics and the English Language' was written after the close of World War II.\",\n",
              " \"One book. Two readers. A world of mystery, menace and desire A young woman picks up a book left behind by a stranger. Inside it are his margin notes, which reveal a reader entranced by the story and by its mysterious author. She responds with notes of her own, leaving the book for the stranger, and so begins an unlikely conversation that plunges them both into the unknown. THE BOOK: Ship of Theseus, the final novel by a prolific but enigmatic writer named V. M. Straka, in which a man with no past is shanghaied onto a strange ship with a monstrous crew and launched on a disorienting and perilous journey. THE WRITER: Straka, the incendiary and secretive subject of one of the world's greatest mysteries, a revolutionary about whom the world knows nothing apart from the words he wrote and the rumours that swirl around him. THE READERS: Jennifer and Eric, a college senior and a disgraced grad student, both facing crucial decisions about who they are, who they might become, and how much they're willing to trust another person with their passions, hurts and fears. S., conceived by filmmaker J.J. Abrams and written by award-winning novelist Doug Dorst, is the chronicle of two readers finding each other in the margins of a book and enmeshing themselves in a deadly struggle between forces they don't understand. It is also Abrams and Dorst's love letter to the written word.\",\n",
              " 'A dictionary of synonyms and opposites that helps learners of English distinguish between similar words and use them correctly.',\n",
              " \"Did you know that the Japanese have a word to express the way sunlight filters through the leaves of trees?\\n\\nOr, that there’s a Swedish word that means a traveller’s particular sense of anticipation before a trip?\\n\\nLost in Translation, a New York Times bestseller, brings the nuanced beauty of language to life with over 50 beautiful ink illustrations.\\n\\nThe words and definitions range from the lovely, such as goya, the Urdu word to describe the transporting suspension of belief that can occur in good storytelling, to the funny, like the Malay word pisanzapra, which translates as 'the time needed to eat a banana' .\\n\\nThis is a collection full of surprises that will make you savour the wonderful, elusive, untranslatable words that make up a language.\",\n",
              " \"picked as < u> one to watch in 2019</u> by <the times, I news, stylist, heat magazine, Elle, cosmopolitan, red, image magazine </and Thea> Irish independent. </for a while, Daisy Jones & The six were everywhere. Their albums were on every turntable, they sold out arenas from Coast to Coast, their sound defined an era. And then, on 12 July 1979, They split. nobody ever knew why. Until now. they were lovers and friends and brothers and rivals. They couldn't believe their luck, until it ran out. This is their story of the early days and the wild nights, but everyone remembers the truth differently. The only thing they all know for sure is that from the moment Daisy Jones walked barefoot, on to the stage at the whisky, the band were irrevocably changed. Making music is never just about the music. And sometimes it can be hard to tell where the sound stops and the feelings begin. _________________ <A tremendously engaging, and completely believable tale of rock and roll excess... Inventive, persuasive and completely satisfying.’ </Dylan Jones <I spent a lost weekend in this book. Daisy Jones is an instant icon.’ erin Kelly “daisy Jones & The six is a transporting novel – at once a love story, a glimpse into the combustible inner workings of a rock-and-roll band, and a pitch-perfect recreation of the music scene of the Fleetwood Mac era. You’ll never want it to end.” cecilia Ahern ‘once in a Blue moon you get to discover a book you end up pressing upon many other people to read. Taylor Jenkins Reid has got every nuance, every detail exact and Right. I loved every word.’ Paul Rees ‘so brilliantly written I thought all the characters were real... I couldn't put it down’ Edith bowman ‘the heady haze of the 70s music scene, and a perfectly flawed Daisy, combine to create a fresh, rock N roll read. I loved it.’ Ali land, author of Good me bad me.\",\n",
              " \"Bear Grylls is one of the world's most famous survivors. Bear Grylls: Two All-Action Adventures combines two of his greatest adventures told in Facing Up and Facing the Frozen Ocean. At the age of twenty-three, Bear Grylls became one of the youngest Britons to reach the summit of Mount Everest. At extreme altitude youth holds no advantage over experience, nevertheless, only two years after breaking his back in a freefall parachuting accident, he overcame severe weather conditions, fatigue and dehydration to stand on top of the world's highest mountain. Facing Up is the story of his adventure, his courage and humour, his friendship and faith. Facing the Frozen Ocean tells of a carefully calculated attempt to complete the first unassisted crossing of the frozen north Atlantic in an open rigid inflatable boat. But this expedition became a terrifying battle against extreme elements and icebergs as large as cathedrals. Starting from the remote north Canadian coastline, Bear Grylls and his crew crossed the infamous Labrador Sea, pushed on through ice-strewn waters to Greenland and then found themselves isolated in a perfect storm 400 miles from Iceland. This is a compelling, vivid and inspirational tale.\",\n",
              " 'Specially arranged and simplified, these pieces offer beginners the pleasure and satisfaction of playing Beethoven. Students and teachers alike will delight in such popular melodies as \"Für Elise,\" \"Ode to Joy,\" and the haunting opening of the Moonlight Sonata.\\nA First Book of Beethoven features 24 selections, including \"Romance in F,\" \"Turkish March,\" \"Minuet in G,\" and \"Bagatelle in G Minor.\" It includes excerpts from the sonatas\\x97such as the opening of the 2nd movement of Sonata No. 14, the \"Adagio\" from Piano Sonata No. 5, and \"Adagio Cantabile\" from Sonata No. 8\\x97as well as highlights from the symphonies.\\nAlong with the twenty-four piano arrangements, this collection includes a free MP3 download for every piece, which will help beginning pianists develop an ear for the melodies. The MP3s may be downloaded individually or collectively.',\n",
              " 'The much-awaited sequel to Dongri to Dubai After the huge success of Dongri to Dubai, here comes its much-awaited sequel, Byculla to Bangkok. Chota Rajan, Arun Gawli and Ashwin Naik are among those whose lives Hussain Zaidi recounts with his characteristic flair for narrativizing the Mumbai underworld. Violence and deceit one expects to read of, but the strength of this book is also its ability to capture the mundane and almost naive beginnings of what later became organized crime and brutal vendettas which held Mumbai to ransom through the last decades of the twentieth century. Unputdownable.',\n",
              " \"The new novelization based on the bestselling games franchise Assassin's Creed Publication will be timed to coincide with the release of the eagerly awaited first instalment of the new Assassin's Creed III game. Assassin's Creed: Forsaken is the latest thrilling novelisation by Oliver Bowden based on the phenomenally successful game series. The new game, Assassin's Creed III, takes one of gaming's most popular franchises to new heights in its most realistic world yet and introduces a brand new Assassin, Connor. Set during the American Revolution, Connor has sworn to secure liberty for his people and his nation and unleashes his powerful skills on the chaotic, blood-soaked battlefields of the hostile American wilderness. Assassin's Creed: Forsaken is the story behind who Connor really is and how he has become a deadly killer. The world of the Assassin's has become far more lethal than ever before.\",\n",
              " 'It\\'s THE book on manga from YouTube\\'s most popular art instruction Guru! There\\'s more to manga than big, shiny eyes and funky hair. In these action-packed pages, graphic novelist Mark Crilley shows you step-by-step how to achieve an authentic manga style-from drawing faces and figures to laying out awesome, high-drama spreads. You\\'ll learn how a few basic lines will help you place facial features in their proper locations and simple tricks for getting body proportions right. Plus, you\\'ll find inspiration for infusing your work with expression, attitude and action. This is the book fans have been requesting for years, packed with expert tips on everything from hairstyles and clothing to word bubbles and sound effects, delivered in the same friendly, easy-to-follow style that has made Mark Crilley one of the \"25 Most Subscribed to Gurus on YouTube.\" Take this opportunity to turn the characters and stories in your head into professional-quality art on the page! Packed with everything you need to make your first (or your best-ever) manga stories! 30 step-by-step demonstrations showing how to draw faces and figures for a variety of ages and body types Inspirational galleries featuring 101 eyes, 50 ways to draw hands, 40 hairstyles, 12 common expressions, 30 classic poses and more! Tutorials to create a variety of realistic settings Advanced lessons on backgrounds, inking, sequencing and layout options',\n",
              " 'In the ninth gripping thriller in the world famous Dirk Pitt series, the adventurer finds himself in a deadly battle against the darkest forces of international terrorism.\\nTHE HIGH-EXPLOSIVE SECRET OF THE AGES…\\n391 AD:\\nFanatics destroy the greatest storehouse of knowledge and treasure in the ancient world – the mighty library and museum of Alexandria. But a few conspirators secretly remove its most precious items and hide them deep in a specially excavated stronghold…\\n1991 AD:\\nA UN plane, with the Secretary-General aboard, crashes in the icy waters of Greenland, brought down by a murderous conspiracy. And trouble-shooter Dirk Pitt is drawn into a deadly battle, against the darkest forces of international terrorism, that could reshape the balance of world power forever.',\n",
              " 'A searing indictment of the suspension of democracy In June 1975, a state of Emergency was declared, where civil liberties were suspended and the press muzzled. In the dark days that followed, Coomi Kapoor, then a young journalist, personally experienced the full fury of the establishment. Meanwhile, Indira Gandhi, her son Sanjay and his coterie unleashed a reign of terror that saw forced sterilizations, brutal evictions in the thousands, and wanton imprisonment of many, including Opposition leaders. This gripping eyewitness account vividly recreates the drama, the horror, as well as the heroism of a few during those nineteen months when democracy was derailed.',\n",
              " \"This book contains nine pieces from ABRSM's Grade 2 Piano Syllabus for 2019 & 2020, three pieces chosen from each of Lists A, B and C. The pieces have been carefully selected to offer an attractive and varied range of styles, creating a collection that provides an excellent source of repertoire to suit every performer. The book also contains helpful footnotes and, for those preparing for exams, useful syllabus information. Recordings of all 18 pieces on the Grade 2 syllabus are available. These can be purchased as part of the Piano Exam Pieces with CD package or as audio downloads (see www.abrsmdownloads.org for more details). ABRSM also offers a range of apps to support musical learning, available from www.abrsm.org/apps.\",\n",
              " \"Sometimes, it seems like you can reach out and touch the past...\\n\\nAn old man wearing a brown robe is found wandering disoriented in the Arizona desert. He is miles from any human habitation and has no memory of how he got to be there, or who he is. The only clue to his identity is the plan of a medieval monastery in his pocket.\\n\\nIn France, Professor Edward Johnston and his students are studying the ruins of a medieval town. Suspicious of the knowledge of the site shown by their mysterious financier, he returns to the US to investigate. But in his absence, the students make a disturbing discovery in the ruins: the long-decayed remains of Johnston's glasses - and a message in modern English.\\n\\nThe implications are staggering. The consequences are earth-shaking. And the distant past isn't so distant any more.\\n______________________\\n\\nIncreasingly considered an underappreciated classic that stands proudly alongside his more famous works like Jurassic Park and Westworld, Timeline confirms Michael Crichton as the king of the high-concept thriller, and a master storyteller to boot.\",\n",
              " \"Heartbreak and grief touch every soul at least once in a lifetime and Ranata Suzuki translates those raw emotions into words. The Longest Night combines strikingly poignant quotations, powerfully emotive poetry and captivating silhouette imagery to form a mournful lover's journal that explores a side of love that is deep, dark and hauntingly beautiful.\\nEach of the book's elements are skilfully woven together to reveal fragments of thoughts and feelings that seem almost to belong to the reader as years of painful longing are condensed into the context of a single night.\\nThe journal begins with 'Sunset', in which poems convey the initial feelings of shock and loss first felt when a relationship with a loved one ends. As the poetry descends into an emotional downward spiral, the book progresses into its next chapter, 'Darkness', in which emptiness, jealousy, sorrow and despair are passionately portrayed.\\nThe concluding chapter, 'First Light', sees the gradual dawning of a new outlook. The final poems express a gratitude for what once was, an acceptance of what now is, and come to the uplifting conclusion that even though a relationship can be fated to end tragically, the memories gained and lessons learned from it are, in their own way, treasured gifts that will last a lifetime.\\nA book for anyone who has found themselves separated from someone they love no matter the circumstance, The Longest Night is a companion for the broken heart on the painful emotional journey that is losing someone you love from your life. Its words serve as a comforting reminder, whether you are travelling this road or have recently completed this journey yourself, that despite the loneliness you may sometimes feel along the way none of us walk this path alone.\",\n",
              " \"Besharam is a book on young Indian women and how to be one, written from the author's personal experience in several countries. It dissects the many things that were never explained to us and the immense expectations placed on us. It breaks down the taboos around sex and love and dating in a world that's changing with extraordinary rapidity. It tackles everything, from identity questions like what should our culture mean to us? to who are we supposed to be on social media? Are we entitled to loiter in public spaces like men do? Why do we have so many euphemisms for menstruation? Like an encyclopedia, or a really good big sister, Besharam teaches young Indian women something that they almost never hear: it's okay to put ourselves first and not feel guilty for it.\\nPart memoir, part manual, Besharam serves up ambitious feminism for the modern Indian woman.\",\n",
              " 'The first time Veer set his eyes on Maia, he felt such an intense attraction that he was swept away by the magic. Strangely, Maia felt it too. Soon it became their lives. It was as if the only reality was this inexplicable force that drew them together.\\nHowever, just like the whirlwind that it was, it tore them apart. A tragedy caught them unawares and jolted them out of their stupor. What it left in its wake was hatred—as potent as their love. Things did not end there. Several years later, fate played its dirty trick again and brought them face to face.\\nThis time the choice was theirs: to let their hatred destroy them or to give love another chance.',\n",
              " 'A New York Times Bestseller\\n\\nFrom the beloved New Yorker cartoonist comes a collection of paintings and stories from some of the world’s most cherished bookstores. \\n\\nThis collection of 75 evocative paintings and colorful anecdotes invites you into the heart and soul of every community: the local bookshop, each with its own quirks, charms, and legendary stories. \\n\\nThe book features an incredible roster of great bookstores from across the globe and stories from writers, thinkers and artists of our time, including David Bowie, Tom Wolfe, Jonathan Lethem, Roz Chast, Deepak Chopra, Bob Odenkirk, Philip Glass, Jonathan Ames, Terry Gross, Mark Maron, Neil Gaiman, Ann Patchett, Chris Ware, Molly Crabapple, Amitav Ghosh, Alice Munro, Dave Eggers, and many more.  \\n\\nPage by page, Eckstein perfectly captures our lifelong love affair with books, bookstores, and book-sellers that is at once heartfelt, bittersweet, and cheerfully confessional.',\n",
              " \"The million-copy bestselling phenomenon, Fredrik Backman's heartwarming debut is a funny, moving, uplifting tale of love and community that will leave you with a spring in your step. Perfect for fans of Rachel Joyce's The Unlikely Pilgrimage of Harold Fry, Graeme Simsion's The Rosie Project and David Nicholl's US.\\nNew York Times bestseller\\n\\n'Warm, funny, and almost unbearably moving' Daily Mail\\n\\n'Rescued all those men who constantly mean to read novels but never get round to it' Spectator Books of the Year\\nAt first sight, Ove is almost certainly the grumpiest man you will ever meet. He thinks himself surrounded by idiots - neighbours who can't reverse a trailer properly, joggers, shop assistants who talk in code, and the perpetrators of the vicious coup d'etat that ousted him as Chairman of the Residents' Association. He will persist in making his daily inspection rounds of the local streets.\\nBut isn't it rare, these days, to find such old-fashioned clarity of belief and deed? Such unswerving conviction about what the world should be, and a lifelong dedication to making it just so?\\nIn the end, you will see, there is something about Ove that is quite irresistible...\",\n",
              " 'A messenger in India leads Tintin and Snowy to a chess game in Hong Kong, where they join forces with a society dedicated to eliminating smuggling.',\n",
              " \"Mastering VBA for Microsoft Office 2016 helps you extend the capabilities of the entire Office suite using Visual Basic for Applications (VBA). Even if you have no programming experience , you'll be automating routine computing processes quickly using the simple, yet powerful VBA programming language. Clear, systematic tutorials walk beginners through the basics, while intermediate and advanced content guides more experienced users toward efficient solutions.\",\n",
              " 'Despite his grand ‘secular’ statements in Parliament that bordered on the Nehruvian, Atal Bihari Vajpayee has often taken brief excursions into the hard-line camp. In 1983, he made an incendiary speech during the Assam elections in which the presence of ‘Bangladeshi foreigners’ in the state was a big issue. Even the BJP disowned Vajpayee’s speech, which possibly inspired the massacre of over 2000 people, mostly Muslims, in Nellie, Assam, the same year.\\nVajpayee, one of the shrewdest politicians of India, is known for negotiating multiple paradoxes: from militant nationalism to his secret family life; his stint as a communist; his indulgence in food; and his attempt to project himself as a moderate, if not a liberal. Exploring crucial milestones of Vajpayee’s career and his traits as a seasoned politician, the book looks at his relationship with leaders of his party and his love–hate association with the RSS and its feeder organizations. Thoroughly researched, supported by hard facts and accompanied by inside stories and anecdotes, insightful interviews and archival photographs, The Untold Vajpayee will open a window to the life and times of a poet-politician.',\n",
              " \"In his 1932 classic dystopian novel, Brave New World, Aldous Huxley depicted a future society in thrall to science and regulated by sophisticated methods of social control. Nearly thirty years later in Brave New World Revisited, Huxley checked the progress of his prophecies against reality and argued that many of his fictional fantasies had grown uncomfortably close to the truth. Brave New World Revisited includes Huxley's views on overpopulation, propaganda, advertising and government control, and is an urgent and powerful appeal for the defence of individualism still alarmingly relevant today.\",\n",
              " 'Prisha Khatri is a regular college graduate, focused on her career, desperate to finally move out of her parents’ house and freshly dumped by her successful fiancé. When she lands a job at a prestigious media house, she’s glad to have something to take her mind off her heartbreak. What she doesn’t expect is to be landed on a business trip with a famously fiery reporter Rajesh Lagheri. He’s travelling to a business conference for a story and doesn’t seem impressed by her involvement. But as soon as they’re out of the office, things change and it becomes clear that there is more to Rajesh’s trip than meets the eye. As Prisha is drawn into the story he’s trying to hide from their editor, their hunt for the story grows more intense and she finds herself growing closer to Rajesh. As their chemistry threatens to overwhelm them and Prisha is pulled deeper into the Seductive Affair, she must decide what matters most to her matters of the head or of the heart.',\n",
              " \"From GQ's 'Nerd of the Year' to one of Time's most influential people in the world, Biz Stone represents different things to different people but he is known to all as the creative, effervescent, funny, charmingly positive and remarkably savvy co-founder of Twitter, the social media platform that singlehandedly changed the way the world works. Now, Biz tells fascinating, pivotal and personal stories from his early life and his careers at Google and Twitter, sharing his knowledge about the nature and importance of ingenuity today. In Biz's world:\\nOpportunity can be manufactured\\nGreat work comes from abandoning a linear way of thinking\\nCreativity never runs out\\nAsking questions is free\\nEmpathy is core to personal and global success\\nIn this book, Biz also addresses failure, the value of vulnerability, ambition, and corporate culture. Whether seeking behind-the-scenes stories, advice or wisdom and principles from one of the most successful businessmen of the new century, Thinks a Little Bird Told Me will satisfy every reader.\",\n",
              " 'In this classic social commentary from Dickens, Mr. Samuel Pickwick, retired business man and confirmed bachelor, is determined that after a quiet life of enterprise the time has come to go out into the world. Together with the other members of the Pickwick Club: Tracy Tupman, Augustus Snodgrass and Nathaniel Winkle, the portly innocent embarks on a series of hilariously comic adventures. But can Pickwick retain his good will towards his fellow humans once he discovers the evils of the world?\\n\\nCharles Dickens’s satirical masterpiece, The Pickwick Papers, catapulted the young writer into literary fame when it was first serialized in 1836–37. It recounts the rollicking adventures of the members of the Pickwick Club as they travel about England getting into all sorts of mischief.\\n\\nLaugh-out-loud funny and endlessly entertaining, the book also reveals Dickens’s burgeoning interest in the parliamentary system, lawyers, the Poor Laws, and the ills of debtors’ prisons.\\n\\nAs G. K. Chesterton noted, “Before [Dickens] wrote a single real story, he had a kind of vision . . . a map full of fantastic towns, thundering coaches, clamorous market-places, uproarious inns, strange and swaggering figures. That vision was Pickwick.”',\n",
              " 'WITH A NEW FOREWORD BY THE AUTHOR\\n\\nOn his third birthday Oskar decides to stop growing. Haunted by the deaths of his parents and wielding his tin drum Oskar recounts the events of his extraordinary life; from the long nightmare of the Nazi era to his anarchic adventures is post-war Germany.',\n",
              " \"In this spellbinding book, the man described by the Daily Telegraph as 'possibly the best living writer in Britain' takes on his biggest challenge yet: unlocking the film that has obsessed him all his adult life. Like the film Stalker itself, it confronts the most mysterious and enduring questions of life and how to live.\",\n",
              " \"Acclaimed bestselling novelist Kunihiko Hidaka is found brutally murdered in his home on the night before he's planning to leave Japan and relocate to Vancouver. His body is found in his office, in a locked room, within his locked house, by his wife and his best friend, both of whom have rock solid alibis. Or so it seems.\\nPolice Detective Kyochiro Kaga recognizes Hidaka's best friend. Years ago when they were both teachers, they were colleagues at the same high school. Kaga went on to join the police force while Osamu Nonoguchi left to become a full-time writer, though with not nearly the success of his friend Hidaka. But Kaga thinks something is a little bit off with Nonoguchi's statement and investigates further, ultimately executing a search warrant on Nonoguchi's apartment. There he finds evidence that shows that the two writers' relationship was very different than the two claimed. Nonoguchi confesses to the murder, but that's only the beginning of the story. In a brilliantly realized tale of cat and mouse, the detective and the writer battle over the truth of the past and how events that led to the murder really unfolded. Which one of the two writers was ultimately guilty of malice?\",\n",
              " \"Killer 2-for-1 value on hit thriller Death Note!\\n\\nReads R to L (Japanese Style), for audiences T+\\nContains Volumes 7 and 8 of Death Note! Light Yagami is an ace student with great prospects--and he’s bored out of his mind. But all that changes when he finds the Death Note, a notebook dropped by a rogue Shinigami death god. Any human whose name is written in the notebook dies, and now Light has vowed to use the power of the Death Note to rid the world of evil. Will Light's noble goal succeed, or will the Death Note turn him into the very thing he fights against?\",\n",
              " 'The Five Language Visual Dictionary puts the perfect English, French, German, Spanish or Italian translation at your fingertips.\\nFully revised and updated, This illustrated dictionary is organised into 15 different themes, including eating out, work, transport and entertainment, giving you quick and easy access to the word you need. Including over 10,000 terms labelled in English, French, German, Spanish and Italian, plus feature panels which list essential abstract nouns, verbs and useful phrases.\\nOffering the most user-friendly and intuitive reference for language learning, the Five Language Visual Dictionary makes it simple to learn and retain important vocabulary.\\nPrevious edition ISBN 9780751336818',\n",
              " 'Four of Ibsen’s most important plays in superb modern translations, part of the new Penguin Ibsen series.\\n     With her assertion that she is “first and foremost a human being,” rather than a wife, mother or fragile doll, Nora Helmer sent shockwaves throughout Europe when she appeared in Henrik Ibsen’s greatest and most famous play, A Doll’s House. Ibsen’s follow-up, Ghosts, was no less radical, with its unrelenting investigation into religious hypocrisy, family secrets, and sexual double-dealing. These two masterpieces are accompanied here by The Pillars of Society and An Enemy of the People, both exploring the tensions and dark compromises at the heart of society.',\n",
              " 'THE SUNDAY TIMES BESTSELLER\\n\\nA dazzling new collection of short stories from the beloved, internationally acclaimed Haruki Murakami\\n\\nAcross seven tales, Haruki Murakami brings his powers of observation to bear on the lives of men who, in their own ways, find themselves alone. Here are vanishing cats and smoky bars, lonely hearts and mysterious women, baseball and the Beatles, woven together to tell stories that speak to us all.\\n\\nMarked by the same wry humor that has defined his entire body of work, in this collection Murakami has crafted another contemporary classic.\\n\\n‘Supremely enjoyable, philosophical and pitch-perfect new collection of short stories…Murakami has a marvelous understanding of youth and age’ Observer\\n\\n‘Murakami at his whimsical, romantic best’ Financial Times',\n",
              " 'WINNER OF A 2018 TEEN CHOICE AWARD FOR BEST ANIMATED SERIES! Hawk Moth is at it again! Follow Ladybug and Cat Noir as they save Paris from akumatized Paris citizens such as, Roger Cop, who claims to embody the law by ensuring a tyrannical justice in Paris, and Mr. Pigeon, who wants to turn the capital into a huge nature reserve for the birds he communicates with. Will Paris ever be safe from Hawk Moth and his akuma? SPOTS ON, CLAWS OUT!',\n",
              " \"The Autobiography of One of the World's Finest Ever Batsman.\\n'AB has become the most valuable cricketer on the planet' Adam Gilchrist\\nAB de Villiers is one of the finest batsmen ever to play cricket, and yet his achievement extends beyond his outrageous armoury of drives, pulls, paddles, scoops and flicks.\\nWhether he is delighting home crowds at the Wanderers or Newlands or setting new records in Bengaluru or Sydney, he plays the game in a whole-hearted manner that projects a positive image of his country around the world, and also makes millions of South Africans feel good about themselves.\\nThis is AB's story, in his own words. The story of the youngest of three talented, sports-mad brothers growing up in Warmbaths, of a boy who excelled at tennis, rugby and cricket, of a youngster who made his international debut at the age of twenty and was then selected in every single test played by South Africa for the next eleven seasons, of a batsman who has started to redefine the art, being ranked among the world's very best in test, ODI and T20.\\nThrough all the pyrotechnics and consistency, AB de Villiers has remained a true sportsman - quick to deflect praise, swift to praise opponents, eager to work hard, to embrace the team's next challenge and to relish what he still regards as the huge privilege of representing his country.\\nThis is the story of a modern sporting phenomenon.\",\n",
              " '\"It\\'s the book I tell everyone who asks me about drones to to buy\"\\nThomas Greer, organisers of FPVLeague.co.uk - the drone racing league\\n\\n\"Awesome. The book captures every topic and is accessible for everyone from journos needing proper info, through newbies to experienced fliers and photo/videographers. Clear images and well humoured i am delighted!\"\\n- Dominic Robinson, organiser of the International Drone Day (UK), part of a global event.\\nThe only thing growing faster than the drone market is the amount of misinformation about them. Almost overnight popular drones like the instantly reconginsable DJI Phantom have created a billion-dollar industry, with a foothold in everything from movie-making to the toy market.\\nThis book shows you everything there is to know about drones/multicopters/UAVs (including what really is the correct term for them) in plain, jargon-free English.\\nFind out how to:\\n· Choose the right drone for you\\n· Build them\\n· Fly them\\n· Capture amazing photos and videos from above\\nThe text is accompanied with clear illustrations and brand-new photography.\\nAlso included is a complete step-by-step project to build your own modestly-priced drone - great as a first step into the world of drones; fun for one, perfect as an educational project for parent and child.\\nIn addition there is a layman\\'s guide to the crucial legal issues around drone flying, and there is an accompanying website with video clips and commmunity links.\\nThis book is all you need to take to the skies!',\n",
              " 'The India–Pakistan border in Jammu & Kashmir has witnessed repeated ceasefire violations (CFVs) over the past decade. As relations between India and Pakistan have deteriorated, CFVs have increased exponentially. It is imperative to gain a deeper understanding of these violations owing to their potential to not only cause a crisis but also escalate an ongoing one.\\nLine on Fire, part of the Oxford International Relations in South Asia series, postulates that the incorrect diagnosis of the reasons behind CFVs has led to wrong policies being adopted by both India and Pakistan to deal with the recurrent violations. Using fresh empirical data and first-hand accounts, the volume attempts to understand the reason why CFVs continue to take place between India and Pakistan despite consistent efforts to reduce the tension between the two nations. In doing so, it recontextualizes and enriches the prevailing arguments in contemporary literature on escalating dynamics and unenduring ceasefire agreements between the two South Asian nuclear rivals.',\n",
              " \"This lavishly illustrated and super-condensed history of world art is the perfect gift for any art lover. A pocket-sized book bursting with 900 illustrations, it takes the reader from the beginnings of art in prehistory to the contemporary scene. Complete with concise introductions to art movements, time lines, capsule biographies of great artists, and spreads that spotlight masterworks, it's a museum's worth of looking and learning that fits effortlessly into a purse or backpack.\",\n",
              " 'The 50th anniversary edition of this classic World War 2 adventure set in south-east Asia.\\nFebruary, 1942: Singapore lies burning and shattered, defenceless before the conquering hordes of the Japanese Army, as the last boat slips out of the harbour into the South China Sea. On board are a desperate group of people, each with a secret to guard, each willing to kill to keep that secret safe.\\nWho or what is the dissolute Englishman, Farnholme? The elegant Dutch planter, Van Effen? The strangely beautiful Eurasian girl, Gudrun? The slave trader, Siran? The smiling and silent Nicholson who is never without his gun? Only one thing is certain: the rotting tramp steamer is a floating death trap, carrying a cargo of human TNT.\\nDawn sees them far out to sea but with the first murderous dive bombers already aimed at their ship. Thus begins an ordeal few are to survive, a nightmare succession of disasters wrought by the hell-bent Japanese, the unrelenting tropical sun and by the survivors themselves, whose hatred and bitterness divides them one against the other.\\nWritten after the acclaimed and phenomenally successful HMS Ulysses and The Guns of Navarone, this was MacLean’s third book, and it contains all the hallmarks of those other two classics. Rich with stunning visual imagery, muscular narrative power, brutality, courage and breathtaking excitement, the celebration of the 50th anniversary of South by Java Head offers readers a long-denied chance to enjoy one of the greatest war novels ever written.',\n",
              " \"If you've ever laughed your way through David Sedaris's cheerfully misanthropic stories, you might think you know what you're getting with Calypso. You'd be wrong. When he buys a beach house on the Carolina coast, Sedaris envisions long, relaxing vacations spent playing board games and lounging in the sun with those he loves most. And life at the Sea Section, as he names the vacation home, is exactly as idyllic as he imagined, except for one tiny, vexing realization: it's impossible to take a vacation from yourself. With Calypso, Sedaris sets his formidable powers of observation toward middle age and mortality. Make no mistake: these stories are very, very funny - it's a book that can make you laugh 'til you snort, the way only family can. Sedaris's writing has never been sharper and his ability to shock readers into laughter unparalleled. But much of the comedy here is born out of that vertiginous moment when your own body betrays you and you realize that the story of your life is made up of more past than future. This is beach reading for people who detest beaches, required reading for those who loathe small talk and love a good tumour joke. Calypso is simultaneously Sedaris's darkest and warmest book yet - and it just might be his very best.\",\n",
              " 'A New York Times bestseller!\\n\\nThe only one thing more terrifying than the madness currently gripping Gotham City is the method behind it.\\n \\nGang wars. Viral outbreaks. Riots. Hauntings. Terrorist strikes. Batman and his army of allies have battled them all one by one, taking casualties at every turn.\\n \\nAnd in the eye of this vortex of chaos stands one man. His face is wrapped in darkness. His name is spoken only in whispers.\\n \\nHe is Hush. And he is the Dark Knight’s darkest nightmare.\\n \\nOnce Bruce Wayne’s best friend, this maniacal mastermind is out to prove that he is the Batman’s better in every way. To do it, he will burn Gotham City to the ground and crown himself king of the ashes.\\n \\nDistrust and disaster have torn them apart, but now Batman and his family of crimefighters must join closer than they ever have before. Only then can they stop Hush from sinking the city into the silence of the grave…\\n \\nSuperstar BATMAN writer Scott Snyder leads an all-star team of creators — including James Tynion IV, Ray Fawkes, Kyle Higgins, Tim Seeley, Jason Fabok, Fernando Pasarin, R.M. Guéra, and more — in BATMAN ETERNAL VOL. 2, the second landmark chapter in the unprecedented weekly Batman series! Collects issues #22-34.',\n",
              " \"The Sunday Times bestselling autobiography from the greatest tennis player of his generation\\n'A winner' Independent\\n'A terrific sporting memoir, full of memorable anecdotes' New Statesman\\n'As exciting as Rafa himself' Woman's Own\\nNo tennis player since Andre Agassi has captivated the world like Rafael Nadal. He's a rarity in today's sporting arena - a true sportsman who chooses to let his raw talent, dedication and humility define him. With a remarkable 16 grand slam victories under his belt, and with friend and rival Roger Federer's record haul of 20 in his sights, Nadal is an extraordinary competitor whose ferocity on court is made even more remarkable by his grace off it.\\nThis book takes us to the heart of Nadal's childhood, his growth as a player, and his incredible career. It includes memorable highs and lows, from victory in the 2008 Wimbledon final - a match that John McEnroe called the 'greatest game of tennis ever played' - to the injury problems that have frequently threatened his dominance of the sport, to becoming the youngest player of the open era to complete a career Grand Slam in 2010. It transports us from Nadal's lifelong home on the island of Majorca to the locker room of Centre Court as he describes in detail the pressures of competing in the greatest tournament in the world. It offers a glimpse behind the racquet to learn what really makes this intensely private person - who has never before talked about his home life - tick. And it provides us with a story that is personal, revealing and every bit as exciting as Nadal himself.\",\n",
              " 'Easy to use, just cut page and you know what to do. The ULTIMATE TARGET for all firearms and calibers, including; Airsoft, BB, pellet guns, even shotguns. Our targets make it EASY to see your shots and work flawlessly shooting indoors or outdoors, short range or long range.USED & RECOMMENDED BY LAW ENFORCEMENT Nationwide - Our premium gun range targets are DESIGNED FOR SUPERIOR VISIBILITY using all caliber weapons. PERFECT 8.5\" x 11\" SIZE: Works perfectly for pistols, rifles, airsoft, BB, pellet guns and even shotguns. PROUDLY MADE IN THE USA - Makes a great gift',\n",
              " 'Eliminate stress and feel calm again with The Little Book of More Calm Colouring, a perfectly formed antidote to a busy life. From David Sinden and Victoria Kay, the creators of the bestselling The Little Book of Calm Colouring, this pocket-sized book is filled with more soothing, hand-illustrated artworks to colour in. Take a short relaxing breather from your day to colour the calming images and feel inspired by the poignant quotations that accompany each elegant artwork.',\n",
              " \"SHORTLISTED FOR THE ORWELL PRIZE FOR POLITICAL WRITING 2019\\n'A near miracle' Ha-Joon Chang, author of 23 Things They Don't Tell You About Capitalism\\n\\nIn The Growth Delusion, author and prize-winning journalist David Pilling explores how economists and their cult of growth have hijacked our policy-making and infiltrated our thinking about what makes societies work. Our policies are geared relentlessly towards increasing our standard measure of growth, Gross Domestic Product. By this yardstick we have never been wealthier or happier. So why doesn't it feel that way? Why are we living in such fractured times, with global populism on the rise and wealth inequality as stark as ever?\\nIn a book that is simultaneously trenchant, thought-provoking and entertaining, Pilling argues that we need to measure our successes and failures using different criteria. While for economic growth, heroin consumption and prostitution are worth more than volunteer work or public services, in a rational world we would learn how to value what makes economies better, not just what makes them bigger. So much of what is important to our wellbeing, from clean air to safe streets and from steady jobs to sound minds, lies outside the purview of our standard measure of success. We prioritise growth maximisation without stopping to think about the costs.\\nIn prose that cuts through the complex language so often wielded by a priesthood of economists, Pilling argues that our steadfast loyalty to growth is informing misguided policies - and contributing to a rising mistrust of experts that is shaking the very foundations of our democracy.\",\n",
              " 'Most of us want to be fit and healthy, but get stuck in a rut—we just don’t have the will power to get up and move. What is the incentive for you to get off that couch and work out when you have all three seasons of Game of Thrones waiting for you? Almost everyone wants to be fit, but they just can’t muster up the effort to do so. If you are like them, then this book is for you.\\nThe Lazy Girl’s Guide to Being Fit is about the first few steps you need to take to go from a sedentary lifestyle to an active one, because that’s the biggest challenge for a couch potato—movement! It’s all about finding the balance in your life. This book will show you how exercise can take the guise of several daily activities—be it shopping or going on a picnic—and how eating right can solve half your problems. The easy and effective exercise routines contained here will get you fit in no time. The body can be beautiful if you know how to put it to use and have fun doing so. And this is exactly what this book will show you.',\n",
              " \"In the cloud-washed airspace between the cornfields of Illinois and blue infinity, a man puts his faith in the propeller of his biplane. For disillusioned writer and itinerant barnstormer Richard Bach, belief is as real as a full tank of gas and sparks firing in the cylinders ... until he meets Donald Shimoda - former mechanic and self-described messiah who can make wrenches fly and Richard's imagination soar...\\n\\nIn Illusions, the unforgettable follow-up to his phenomenal New York Times bestseller Jonathan Livingston Seagull, Richard Bach takes to the air to discover the ageless truths that give our souls wings: that people don't need airplanes to soar ... that even the darkest clouds have meaning once we lift ourselves above them ... and that messiahs can be found in the unlikeliest places - like hay fields, one-traffic-light midwestern towns, and most of all, deep within ourselves.\",\n",
              " \"Our personal space is dear to us all. We live our lives in full public view on social media - posting photos of the food we just ate or even expressing intimate feelings for our loved ones - but there are still things we would rather not share with the world. Indeed, it is privacy that sets man apart from the animals who must stick together in the wild for their own safety. But mankind was not born private. Our primitive ancestors too lived in large groups, every member of which knew all there was to know about the others. Privacy evolved over time as man developed technologies to wall himself off, even as he remained part of the society at large. But just as some technologies enhanced privacy, others - such as the printing press or the portable camera - chipped away at it. Every time this happened, man opposed the technology at first but made his peace with it eventually to benefit from the obvious good it could do. We are at a similar crossroads today with data technologies. Aadhaar is one example of the many ways in which we have begun to use data in everything we do. While it has made it far easier to avail of services from the government and private enterprises than ever before, there are those who rightly worry about people's private data being put to ill use - and, worse, without consent. But this anxiety is no different from that which we felt during the teething troubles of every previous technology we adopted. What we really need is a new framework that unlocks the full potential of a data-driven future while still safeguarding what we hold most dear - our privacy. In this pioneering work, technology lawyer Rahul Matthan traces the changing notions of privacy from the earliest times to its evolution through landmark cases in the UK, US and India. In the process, he re-imagines the way we should be thinking about privacy today if we are to take full advantage of modern data technologies, cautioning against getting so obsessed with their potential harms that we design our laws to prevent us from benefiting from them at all.\",\n",
              " \"Logophilia Education (estd. August, 2010) is the first and only Etymology Education organisation in the world. We're passionate in our belief that a strong and sophisticated vocabulary is the most essential skill that students should possess. Why? Anything you study is written in words. All subjects have their own terminology. Without a strong understanding of English words you will never really study well, and will end up finding shortcuts to learning (e.g. rote-memorising, using mnemonics, and so forth).\\nLogophilia, therefore, teaches the structure of English words with the intent of getting students to become very strong in vocabulary, and dictionary-independent. This method of teaching, done through Logophilia's unique paedagogy, is called Etymology Education.\\nWe promote Etymology awareness as a life skill, and aim to get it into mainstream school curricula worldwide, thereby simplifying and sophisticating language comprehension for everyone. We teach (experiential vocabulary programmes), quiz (the Logo+philia(TM) Gala Olympiad), & write (books, blogs, apps, and vocabulary-learning aids), to help you see the logic of English words.\",\n",
              " \"The Tatas is the story of one of India's leading business families. It starts in the nineteenth century with Nusserwanji Tata - a middle-class Parsi priest from the village of Navsari in Gujarat, and widely regarded as the Father of Indian Industry - and ends with Ratan Tata - chairman of the Tata Group until 2012. But it is more than just a history of the industrial house; it is an inspiring account of India in the making. It chronicles how each generation of the family invested not only in the expansion of its own business interests but also in nation building. For instance, few know that the first hydel project in the world was conceived and built by the Tatas in India. Nor that some radical labour concepts such as eight-hour work shifts were born in India, at the Tata mill in Nagpur. The National Centre for the Performing Arts, the Tata Cancer Research Centre, the Tata Institute of Fundamental Research - the list about the Tatas' contribution to India is a long one. A bestseller in Marathi when it was first published in 2015, this is the only book that tells the complete Tata story over two hundred years.\",\n",
              " \"'Riveting...Honnold is neither crazy nor reckless. Alone on the Wall reveals him to be an utterly unique and extremely appealing young man.' Jon Krakauer A twelve-year-old kid in the audience raised his hand and asked, 'Aren't you afraid you're gonna die?' Without missing a beat, Alex shot back, 'We've all gotta die sometimes. You might as well go big.' Alex Honnold is 28 years old, and perhaps the world's best 'free solo' climber, scaling impossible rock faces without ropes, pitons or and support of any kind. There is a purity to Alex's climbs that is easy to comprehend, but impossible to fathom; in the last forty years, only a handful of climbers have pushed 'free soloing' to the razor edge of risk. Half of them are dead. From Yosemite's famous Half Dome to the frighteningly difficult El Sendero Luminoso in Mexico, Alone on the Wall is structured around Alex's seven most extraordinary climbing achievements so far. These are tales to make your palms sweat and your feet curl with vertigo. Together, they get to the heart of how - and why - Alex does what he does. Exciting, uplifting and truly awe-inspiring, Alone on the Wall is a book about the essential truths of risk and reward, and the ability to maintain a singular focus, even in the face of extreme danger.\",\n",
              " 'Calligraphy Practice Notebook\\nUpper and Lowercase Calligraphy Alphabet Pages for Lettering Practice.\\nDurable matte cover.\\n124 pages, 31 practice pages per Letter case.\\nMeasures 8\" x 10\" (20.32 x 25.4 cm).\\nDesigned in USA.',\n",
              " \"A thrilling account of the incredible journey of Indian football\\nFew football-crazy fans today may be aware that India was once called the 'Brazil of Asia'. Or that the competition between East Bengal and Mohun Bagan is ranked among the top fifty club rivalries in the world. Brilliant players, world-class coaches and passionate fans-including political leaders and film celebrities-once combined to make India a football-worshipping nation. A major source of sports entertainment in India, football has thrived in several places apart from Kolkata. The game has also been inextricably linked to community identity, shrewdly used towards political ends and contributed richly to our cultural heritage.\\nIn Barefoot to Boots, renowned journalist Novy Kapadia reveals Indian football's glorious legacy through riveting descriptions of on-field action, stories of memorable matches, lively anecdotes, and exclusive conversations with legendary players and officials. Having witnessed the evolution of the sport for over fifty years, Novy charts its eventful journey up to the present, to enthral old and new fans alike. The book will offer invaluable insight into the future of the game as the Indian Super League dramatically changes the face of domestic football and India hosts the FIFA U-17 World Cup for the first time.\\n'Novy is like an encyclopaedia on Indian football and this book reveals his passion for the game'-Syed Nayeemuddin, Dronacharya and Arjuna award-winner\",\n",
              " 'Validate your AWS skills. This is your opportunity to take the next step in your career by expanding and validating your skills on the AWS cloud. AWS has been the frontrunner in cloud computing products and services, and the AWS Certified Solutions Architect Official Study Guide for the Associate exam will get you fully prepared through expert content, and real-world knowledge, key exam essentials, chapter review questions, access to Sybex s interactive online learning environment, and much more. This official study guide, written by AWS experts, covers exam concepts, and provides key review on exam topics, including: * Mapping Multi-Tier Architectures to AWS Services, such as web/app servers, firewalls, caches and load balancers * Understanding managed RDBMS through AWS RDS (MySQL, Oracle, SQL Server, Postgres, Aurora) * Understanding Loose Coupling and Stateless Systems * Comparing Different Consistency Models in AWS Services * Understanding how AWS CloudFront can make your application more cost efficient, faster and secure * Implementing Route tables, Access Control Lists, Firewalls, NAT, and DNS * Applying AWS Security Features along with traditional Information and Application Security * Using Compute, Networking, Storage, and Database AWS services * Architecting Large Scale Distributed Systems * Understanding of Elasticity and Scalability Concepts * Understanding of Network Technologies Relating to AWS * Deploying and Managing Services with tools such as CloudFormation, OpsWorks and Elastic Beanstalk. Learn from the AWS subject-matter experts, review with proven study tools, and apply real-world scenarios. If you are looking to take the AWS Certified Solutions Architect Associate exam, this guide is what you need for comprehensive content and robust study tools that will help you gain the edge on exam day and throughout your career.',\n",
              " \"'It was 1947, and life was about to change quite dramatically for most of us'\\nThirteen-year-old Ruskin is back at school, doing what he loves-reading, goal-keeping, spending time with his friends and eating lots of jalebis. But things seem to be rapidly changing all around him. Whispers of a partition haunt the corridors of his school. Does the formation of a new, independent India mean saying goodbye to old friends-and, with it, the shenanigans they got up to?\\nOn the heels of Looking for the Rainbow and Till the Clouds Roll By, Coming Round the Mountain is yet another look at the past, in particular one memorable year, 1947, during which a lot happened to Ruskin and those around him. It is a fitting finale to a journey down memory lane, one about accepting change and finding hope in the unknown days to come.\",\n",
              " 'Plains Indians from different tribes speaking different languages were nevertheless able to communicate facts and feelings of considerable complexity when they met. They used a language composed of gestures made almost entirely with the hands and fingers, probably the most highly developed gesture language to be found in any part of the world.\\nWith this book, you will find it simple to use this language, which the author learned in the late nineteenth and early twentieth centuries, principally from Sioux Indians in Wyoming. Drawings and short descriptions make clear the proper positions and motions of the hands to convey the meaning of over 870 alphabetically arranged common words — hungry, camp, evening, angry, fire, laugh, owl, cat, many times, brave, cold, heart, rain, spotted, together, river, etc. The words are then used in sample sentences. There are also brief sections on the pictography and ideography of the Sioux and Ojibway tribes, and on smoke signals.\\nThis is a book for anyone who wants to learn or teach Indian sign language — scouts, school teachers, camp counselors, scout leaders, parents, linguists, and students of Indian culture. To help counselors and teachers, the last chapters give instructions on how to conduct the Indian ceremony for opening a council fire, an Indian initiation ceremony, and suggestions for sign language tests and exercises.',\n",
              " \"In 2011, Tim cook took on an impossible task - following in the footsteps of one of history's greatest business visionaries, Steve Jobs. Facing worldwide scrutiny, cook (who was often described as shy, unassuming and unimaginative) defied all expectations. Under cook's leadership Apple has soared: its stock has nearly tripled to become the world's first trillion-dollar company. From the massive growth of the I phone to new victories like the Apple watch, cook is leading Apple to a new era of success. But he's also spearheaded a cultural revolution within the company. Since becoming CEO, cook has introduced a new style of management that emphasise kindness, collaboration and honesty and has quietly pushed Apple to support sexual and racial equal rights and invest heavily in renewable energy. Drawing on authorized access with several Apple insiders, kahney, the world's leading reporter on Apple, tells the inspiring story of how one man attempted to replace the irreplaceable and succeeded better than anyone thought possible.\",\n",
              " \"Susan Sontag's On Photography is a seminal and groundbreaking work on the subject.\\nSusan Sontag's groundbreaking critique of photography asks forceful questions about the moral and aesthetic issues surrounding this art form. Photographs are everywhere, and the 'insatiability of the photographing eye' has profoundly altered our relationship with the world. Photographs have the power to shock, idealize or seduce, they create a sense of nostalgia and act as a memorial, and they can be used as evidence against us or to identify us. In these six incisive essays, Sontag examines the ways in which we use these omnipresent images to manufacture a sense of reality and authority in our lives.\\n'Sontag offers enough food for thought to satisfy the most intellectual of appetites'The Times\\n'A brilliant analysis of the profound changes photographic images have made in our way of looking at the world, and at ourselves'Washington Post\\n'The most original and illuminating study of the subject'New Yorker\\nOne of America's best-known and most admired writers, Susan Sontag was also a leading commentator on contemporary culture until her death in December 2004. Her books include four novels and numerous works of non-fiction, among them Regarding the Pain of Others, On Photography, Illness as Metaphor, At the Same Time, Against Interpretation and Other Essays and Reborn: Early Diaries 1947-1963, all of which are published by Penguin. A further eight books, including the collections of essays Under the Sign of Saturn and Where the Stress Falls, and the novels The Volcano Lover and The Benefactor, are available from Penguin Modern Classics.\",\n",
              " \"SAP is a great software. One needs to fully understand its features in order to effectively exploit them for the benefit of customers. Mr. Agrawal's books on SAP HR have a unique approach. A chapter usually focuses on a single business concept, and discusses the user interface as well as its associated configuration. This logical division makes it easier for readers to understand the functionality. Another important feature of these books is the level of detail. Each screen and each field in a screen is explained. Explanation includes meaning, use case and in some cases guidelines. Details are balanced by overviews explaining the concepts and their relationships. While explaining functionality, Mr. Agrawal has made efforts to highlight what can be done and how it is to be done. This is particularly important for less experienced users and consultants. Indicating chapter numbers against each menu and configuration item is a very useful innovation, as it establishes direct link between the SAP system and the book. Another useful feature is that these books can be read not only by consultants, but also by users, business process owners and even by senior managers. The importance of each topic for each category of users is specified. Mr. Agrawal has taken considerable pains in writing these books, and I congratulate Mr. Agrawal on his achievement and thank him for his contribution to the SAP community.\",\n",
              " 'The PTE Academic Expert coursebook for B1 supports students by giving them: Step-by-step approach to speaking and writing - Tasks to prepare students for the exam from the start - Strategies for approaching the exam task types - Vocabulary sections based on the Academic Collocations and Word lists (ACL and AWL).',\n",
              " 'In every angel a demon hides, And in every demon, an angel strides. Neel is a self-proclaimed demon, a slave to his desires, putting at stake even the purest of relationships for it. He lives for himself, takes life as it comes, and considers people who love as emotional fools. When he first sets his eyes on his new landlady, a widow who is eleven years elder to him, all he can see is an opportunity. He has a plan to get rich and is working hard to achieve it, until he bumps into Pihu. She is an immature teenager who likes Neel for no apparent reason, and blindly believes that he is an angel who will take away all her life’s troubles. Neel hates Pihu for her unexplained obsession, and her being a hindrance in his plan, but her firm resolve to see a good person in him shakes Neel to the core. Will Pihu make a difference? Does inner transformation come to a man who has gone to a point of no return? A Girl to Remember is an emotional roller coaster which will make you believe that confession is the best punishment.',\n",
              " \"First published in 1956, this collection of articles covers Wodehouse's feelings on United States, his adopted homeland all collected into one edition. Features a collection of articles originally from Punch magazine as well as America, I Like You, all with Wodehouse's usual wit and personality\",\n",
              " \"Hailed as a masterpiece from the moment of its first publication, Revolutionary Road is the story of Frank and April Wheeler, a bright, young couple who are bored by the banalities of suburban life and long to be extraordinary. With heartbreaking compassion and clarity, Richard Yates shows how Frank and April's decision to change their lives for the better leads to betrayal and tragedy.\",\n",
              " 'A sun-drenched story of desire and murder with a conclusion you’ll never see coming. ‘the best Agatha Christie since and then there were none’ observer the moment Arlena Stuart steps through the door, every eye in the resort is on her. She is beautiful. She is famous. And in less than 72 hours she will be dead. On this luxury retreat, cut off from the outside world, everyone is a suspect. The wandering husband. The jealous wife. The bitter step-daughter. They all had a reason to kill Arlena Stuart. But who hated her enough to do it?',\n",
              " 'When a famous cobra called Fu-xi attacks a village, will Viper defend her fellow snake as a hero or will she help her friends?\\nFor over thirty-five years, the best-selling Read it yourself with Ladybird has helped children learn to read.\\nAll stories feature essential key words. Story-specific words are repeated to practice throughout.\\nDesigned to be read independently at home or used in a guided reading session at school.\\nAll titles include comprehension puzzles, guidance notes and book band information for schools.\\nThis Level 3 title is suitable for children who are developing reading confidence and are eager to start reading longer stories with a wider vocabulary.',\n",
              " \"What does it take to win success and influence?\\n\\nIn a world where we are constantly connected, it's those with the best people skills who win the day. Those who build the right relationships. Those who truly understand and connect with their colleagues, their customers, their partners. Those who others like, respect and trust.\\n\\nThe Art of People reveals the eleven people skills that will get you more of what you want at work, at home and in life. Accessible, easy to execute and often counter-intuitive, these eleven principles will show you how to charm and win over anyone, no matter who you are or what profession you're in.\",\n",
              " \"Publisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product.\\n\\nThis bestselling on-the-job reference and test preparation guide has been fully revised for the new 2015 CompTIA A+ exam objectives for exams 901 & 902\\nWritten by the leading authority on CompTIA A+ certification and training, this self-study book and CD has been thoroughly updated to cover 100% of the exam objectives on the 2015 CompTIA A+ exams. New topics include managing and maintaining cellular devices, including tablets; configuring operating systems, including Windows 8, Android, and iOS; and enhanced, mobile-centered security and troubleshooting procedures. The All-in-One Exam Guide enables you to take the test with complete confidence. It also serves as a practical reference for IT support and technical personnel.\\nElectronic content includes:\\nPractice exams download for both exam 901 and 902 with hundreds of accurate practice exam questions\\nMore than an hour of online video training featuring Mike Meyers\\nOnline performance-based simulations that prepare you for the performance-based exam questions\\nA link to Mike's favorite PC tools\\nA PDF copy of the book\\nKey Features:\\nWritten with the “in the trenches” voice and clarity Mike Meyers is known for\\nFeatures pre-assessment tests, exam tips, and “Try This!” sections to reinforce difficult topics\\nIncludes a coupon for 10% off of the exam fee, a $37 value\",\n",
              " \"More than 150 million copies sold worldwide\\n\\nDaunted by the dark secrets of the tormented young entrepreneur Christian Grey, Ana Steele has broken off their relationship to start a new career with a US publishing house.\\nBut desire for Grey still dominates her every waking thought and when he proposes a new arrangement, she cannot resist. Soon she is learning more about the harrowing past of her damaged, driven and demanding Fifty Shades than she ever thought possible.\\n\\nBut while Grey wrestles with his inner demons, Ana must make the most important decision of her life. And it's a decision she can only make on her own ..\",\n",
              " \"NEW from the bestselling HBR's 10 Must Reads series. To innovate profitably, you need more than just creativity. Do you have what it takes? If you read nothing else on inspiring and executing innovation, read these 10 articles. We've combed through hundreds of articles in the Harvard Business Review archive and selected the most important ones to help you innovate effectively. Leading experts such as Clayton Christensen, Peter Drucker, and Rosabeth Moss Kanter provide the insights and advice you need to: * Decide which ideas are worth pursuing * Innovate through the front lines--not just from the top * Adapt innovations from the developing world to wealthier markets * Tweak new ventures along the way using discovery-driven planning * Tailor your efforts to meet customers' most pressing needs * Avoid classic pitfalls such as stifling innovation with rigid processes Looking for more Must Read articles from Harvard Business Review? Check out these titles in the popular series: HBR's 10 Must Reads: The Essentials HBR's 10 Must Reads on Communication HBR's 10 Must Reads on Collaboration HBR's 10 Must Reads on Leadership HBR's 10 Must Reads on Making Smart Decisions HBR's 10 Must Reads on Managing Yourself HBR's 10 Must Reads on Strategic Marketing HBR's 10 Must Reads on Teams\",\n",
              " 'Choosing a name for your child has never been easier The Penguin Book of Hindu Names has sold over 50,000 copies since it was published almost a decade ago. The product of several years of research, it is an exhaustive and user-friendly compilation, with information on sources and usage. For the first time, this classic work is available in a two-volume set, divided into names for boys and those for girls, making it more accessible. Including modern names and those which are popular, The Penguin Book of Hindu Names for Boys serves as a practical guide for choosing the perfect name for your son. It is also a precise and invaluable sourcebook for scholars and lay readers alike who would like to know what familiar (and not so familiar) Hindu names actually mean.',\n",
              " \"This is translated from bestselling English book Don't Lose Your Mind, Lose Your Weight written by Rujuta Diwekar.\\n‘Rujuta has not just changed my body but also my mind and soul. She is the best thing to have happened to my life!’ Kareena Kapoor\\nWant to know how Kareena Kapoor managed to achieve the perfect figure? Let Rujuta Diwekar tell you how. India’s top celebrity fitness guru has worked with the who’s who of Bollywood including Kareena, Karishma, Saif, Lisa and Sonali. Now she lets you in on her secret―you can eat anything you want just as long as you plan for it. No crash dieting, no carb deprivation, no unbidden cravings. Rujuta teaches you the three simple steps to dieting heaven: learn about your body, create the right plan for it, and slowly adjust your food habits. What’s more, she even lets you in on Bebo’s secret, in a special chapter on how exactly our favourite heroine got that phenomenally fit bikini bod for Tashan.\\nSo whether you’re apple or pear-shaped, soon you’ll be eating all you want―including those irresistible parathas―and still shedding those kilos. Don’t Lose Your Mind, Lose Your Weight is the ultimate diet for daily life. It’s worked for the stars―now make it work for you.\",\n",
              " \"He may live in Madrid but he continues to make front-page headlines. This is David Beckham's own story of his career to date, for Manchester United, Real Madrid and England, and of his childhood, family and private life. Featuring David's first full account of a turbulent year in Spain, on and off the field, and England's fortunes in Euro 2004.\\nThis is Beckham's fascinating life story in his own words. His rise through the ranks at the biggest club side in the world. His complex relationship with United boss Alex Ferguson. The England story, from being vilified by the nation before returning as the prodigal son to eventually captaining his country. His acrimonious falling-out with his manager and departure from Old Trafford in June 2003. And starting a new chapter of his life on foreign soil in the glare of the world’s press.\\nNow from Beckham himself, we gain a vivid and eye-opening insight into the family man behind the famous footballer, the international model and fashion leader. He describes how he first met and then married ex-Spice girl Victoria Adams, and the upbringing of their two children Brooklyn and Romeo. How his family's every step is monitored by a posse of newshounds and paparazzi. Also, the influence of his parents, growing up as a shy youngster in the family home, and how their subsequent split affected him.\\nIntimate and soul-searching, this is the real David Beckham like we have never seen before.\\nNEW FOR THIS PAPERBACK EDITION:\\n- Beckham’s first season with Real Madrid from within the dressing room, with key stories on the likes of Figo, Roberto Carlos and Zidane.\\n- His exclusive reaction to the sensational allegations about his private life; their effect on his relationship with Victoria and a reappraisal of their living arrangements.\\n- England and Euro 2004: the players’ threatened strike in support of Rio Ferdinand; Eriksson as England boss; and all the behind the scenes stories leading up to and including the Finals in Portugal.\\n- One year down the line, does Beckham have any regrets about leaving Manchester United? And is there any truth in the rumours that he is unsettled in Madrid?\",\n",
              " \"Cram session working hard finally feels like it's paying off! After an exciting fireworks festival, futaro feels like he's come to an understanding with the sisters. Unfortunately for the study Group, midterm exams are right around the corner and the stakes are higher than ever. The quintuplets' Father gives futaro an ultimatum: if even one of My daughters fails. You're fired! But to futaro's surprise, after a sleep over at the girls' House, some of them are actually stepping up to help him!.\",\n",
              " \"What makes the ultimate Sunday League footballer? YouTube star ChrisMD has turned amateur Sunday afternoon kick-arounds into a social media event - and now, for the first time, he's sharing all of his secrets. Join Chris as he reveals the characters, training tricks and top-level dietary regime (greasy sausage roll from the village café) that has got him where he is today. Chris's debut book also includes behind-the-scenes photos from his life as a YouTuber, plus his proudest moments, biggest embarrassments and favourite memories from his years of making videos with everyone from the Sidemen to his own family. Includes links to exclusive, never-before-seen ChrisMD videos.\",\n",
              " \"Universal Principles of Design, Revised and Updated is a comprehensive, cross-disciplinary encyclopedia covering 125 laws, guidelines, human biases, and general considerations important to successful design. Richly illustrated and easy to navigate, it pairs clear explanations of every design concept with visual examples of the ideas applied in practice. From the 80/20 Rule to the Weakest Link, every major design concept is defined and illustrated.\\n\\nWhether a marketing campaign or a museum exhibit, a video game or a complex control system, the design we see is the culmination of many concepts and practices brought together from a variety of disciplines. Because no one can be an expert on everything, designers have always had to scramble to find the information and know-how required to make a design work—until now.\\n\\nJust a few of the principles that will broaden your design knowledge, promote brainstorming, and help you check the quality of your work:\\nBaby-Face Bias\\nExpectation Effect\\nGolden Ration\\nOckham's Razor\\nProximity\\nScaling Fallacy\\nThe book is organized alphabetically so that principles can be easily and quickly referenced by name. For those interested in addressing a specific problem of design, the principles havealso been indexed by questions commonly confronting designers (How can I help people learn from my design? How can I enhance the usability of a design? How can I make better design decisions? ...).\\n\\nEach principle is presented in a two-page format. The left-hand page contains a succinct definition, a full description of the principle, examples of its use, and guidelines for use. Side notes appear to the right of the text, and provide elaborations and references. The right-hand page contains visual examples and related graphics to support a deeper understanding of the principle.\\n\\nThis landmark reference is the standard for designers, engineers, architects, and students who seek to broaden and improve their design expertise.\",\n",
              " \"Yoga for Runners uses yoga postures to both stretch and lengthen those short, tight running muscles in a way that is time-efficient and highly targeted.\\nRunning is a fantastic cardiovascular workout that floods the body with endorphins and many runners find its rhythmic, meditative nature a natural stress reliever. The downside of repetitive motion, however, is that it taxes select groups of muscles (hamstrings, quadriceps, hip flexors and Achilles) and neglects the remainder. This leads to muscular imbalances and, eventually, injury.\\nYoga for Runners addresses these imbalances by using yoga postures to both effectively stretch and lengthen these over-taxed muscles.\\nAs in yoga, it's a union of all these physical and mental aspects that affect the performance of the runner as a whole. Practice the techniques together to become a stronger, smoother runner who is less prone to injury. Breathe deeper and more efficiently to train for longer. Gain a clear head to tackle negative distractions as you train or race, and in life generally.\",\n",
              " \"Everyone's favourite troublemaker is still up to mischief! William's natural desire to do the right thing leads him into serious trouble, as usual, and when blackmail and kidnapping are involved, it's no surprise. Even when he turns over a new leaf, the consequences are dire. But it's his new neighbour, Violet Elizabeth Bott, who really causes chaos – and no one will believe that it's not William's fault . . . Richmal Crompton's Still William is a collection of fourteen brilliant Just William stories with an introduction by Sir Tony Robinson, appealing contemporary cover art by Chris Judge, along with the original inside illustrations by Thomas Henry. There is only one William. This tousle-headed, snub-nosed, hearty, lovable imp of mischief has been harassing his unfortunate family and delighting his hundreds of thousands of admirers since 1922. Enjoy more of William's adventures in William the Conqueror and William in Trouble.\",\n",
              " \"Sunday Times 2012 Books of The Year Mail on Sunday's 2012 Books of The Year Independent's 2012 Books of The Year The Times 2012 Books of The Year During the US book tour for his memoir, Hitch-22, Christopher Hitchens collapsed in his New York hotel room to excoriating pain in his chest and thorax. As he would later write in the first of a series of deeply moving Vanity Fair pieces, he was being deported 'from the country of the well across the stark frontier that marks off the land of malady.' Over the next year he underwent the brutal gamut of modern cancer treatment, enduring catastrophic levels of suffering and eventually losing the ability to speak. Mortality is the most meditative collection of writing Hitchens has ever produced; at once an unsparingly honest account of the ravages of his disease, an examination of cancer etiquette, and the coda to a lifetime of fierce debate and peerless prose. In this eloquent confrontation with mortality, Hitchens returns a human face to a disease that has become a contemporary cipher of suffering.\",\n",
              " \"Spaceman Spliff, Stupendous Man, the ferocious tiger Hobbes, and the rest of Calvin's imaginary friends return in this book. Other books featuring these characters include Something Under the Bed is Drooling, Weirdos from Another Planet and Scientific Progress Goes Boink.\\n'Beautifully drawn, with hilarious and often thought-provoking text' Jersey Evening Post\",\n",
              " \"New York Times best selling authors, Sherry & John Petersik of the popular home blog Young House Love, have teamed up with Paige Tate & Co. to create a fresh and fun adult coloring book full of interior design eye candy.\\nDive into beautiful rooms and home decor inspiration illustrated by the talented Joan Borawski. Get lost in intricate design details like built-in bookcases, patterned pillows, and ornate rugs - and bring them to life with your own color schemes. Each page is printed on white premium paper and offers countless opportunities to experiment with different looks without having to repaint a room or buy new furniture, so you can test drive different styles and color combinations before committing to them at home. Whether you're exploring new decorating ideas or just looking for a way to escape reality and get lost in some lovely spaces, this book is for home decor enthusiasts of all kinds.\",\n",
              " 'Piercing pioneer Elayne Angel has performed over 40,000 piercings since the 1980s and has brought many practices, such as tongue-piercing, into the mainstream. She brings her exhaustive knowledge to this groundbreaking manual that covers everything you need to know about the process, including:\\n\\n• The best piercings and placements for various body parts and body types\\n• Terminology, tools, and techniques of the trade\\n• Vital sterility, sanitation, and hygiene information\\n• Jewelry designs, shapes, and materials\\n• Advice for people with stretch marks, plastic surgery, and unique anatomy\\n• Healing, aftercare, and troubleshooting for problem-free piercing\\n\\n\"As a piercer, nurse, and educator, I can say without a doubt that this is the most complete book ever written for all people in our industry.\"\\n--DAVID A. VIDRA, FOUNDER AND PRESIDENT OF HEALTH EDUCATORS, INC.\\n\\n\"No one is more qualified to write this book than Elayne Angel. With more than three decades of personal experience to her credit, no one knows the subject better.\"\\n--JIM WARD, FOUNDER OF GAUNTLET, THE WORLD\\'S FIRST BODY PIERCING STUDIO\\n\\n\"This is an exciting book for a field that has exploded in the last two decades.\\nNational industry standards are needed, and Elayne provides important data.\"\\n--MYRNA L. ARMSTRONG, RN, EdD',\n",
              " \"'Seek and ye shall find.'\\nWith these words echoing in his head, eminent Harvard symbologist Robert Langdon awakes in a hospital bed with no recollection of where he is or how he got there. Nor can he explain the origin of the macabre object that is found hidden in his belongings. A threat to his life will propel him and a young doctor, Sienna Brooks, into a breakneck chase across the city of Florence. Only Langdon's knowledge of hidden passageways and ancient secrets that lie behind its historic facade can save them from the clutches of their unknown pursuers. With only a few lines from Dante's dark and epic masterpiece, The Inferno, to guide them, they must decipher a sequence of codes buried deep within some of the most celebrated artefacts of the Renaissance - sculptures, paintings, buildings - to find the answers to a puzzle which may, or may not, help them save the world from a terrifying threat. Set against an extraordinary landscape inspired by one of history's most ominous literary classics, Inferno is Dan Brown's most compelling and thought-provoking novel yet, a breathless race-against-time thriller that will grab you from page one and not let you go until you close the book.\",\n",
              " \"Vinod Mehta distils his observations and insights into the Sanjay Gandhi phenomenon and its impact on the national scene. In his compelling, honest style, Mehta sifts facts from rumours, and gets to the core of Sanjay's dramatic emergence after the declaration of Emergency. Containing a new introduction by the author, The Sanjay Story allows readers to look with the benefit of hindsight at the rise and fall of one of independent India's most controversial figures.\",\n",
              " '\"I need you to understand something. I wrote this for you. I wrote this for you and only you. Everyone else who reads it, doesn\\'t get it.\"\\n\\nStarted 2007, I Wrote This For You is an acclaimed exploration of hauntingly beautiful words, photography and emotion that\\'s unique to each person who reads it. This book gathers together nearly 200 of the most beautiful entries into four distinct chapters; Sun, Moon, Stars, Rain. Together with several new and exclusive poems that don\\'t appear anywhere else, each chapter of I Wrote This For You focuses on a different facet of life, love, loss, beginnings and endings.',\n",
              " \"Whether you're a school or university student or studying for work or pleasure this is the ultimate Urdu reference dictionary.\\nThis dictionary is aimed at speakers of English who want to learn Urdu.\\n- Two-way dictionary: Urdu-English, English- Urdu including transliteration throughout.\\n- 4000 headwords.\\n- Contains handy sections teaching Urdu script and grammar.\\n- Insights from the author give extra help and guidance on usage.\\n- Covers everyday idioms and expressions.\\n- Includes new terms related to media and internet Urdu.\\nWritten by a university professor with years of teaching experience this dictionary will help support your studies.\\nWant to learn more? Why not check out Teach Yourself Languages Online? Over 65 languages all available online from Teach Yourself.\",\n",
              " \"During his storied career as head coach of the Chicago Bulls and Los Angeles Lakers, Phil Jackson won more championships than any coach in the history of professional sports. Even more important, he succeeded in never wavering from coaching his way, from a place of deep values. Jackson was tagged as the 'Zen master' half in jest by sportswriters but the nickname speaks to an important truth: this is a coach who inspired, not goaded, who led by awakening and challenging the better angels of his players' nature, not their egos, fear or greed.\\nThis is the story of a preacher's kid from North Dakota who grew up to be one of the most innovative leaders of our time. In his quest to reinvent himself, Jackson explored everything from humanistic psychology and Native American philosophy to Zen meditation. In the process, he developed a new approach to leadership based on freedom, authenticity and selfless teamwork that turned the hyper-competitive world of professional sports on its head.\\nIn Eleven Rings, Jackson candidly describes how he:\\nLearned the secrets of mindfulness and team chemistry while playing for the champion New York Knicks in the 1970s\\nManaged Michael Jordan, the greatest player in the world and got him to embrace selflessness, even if it meant losing a scoring title\\nForged successful teams out of players of varying abilities by getting them to trust one another and perform in sync\\nInspired Dennis Rodman and other 'uncoachable' personalities to devote themselves to something larger than themselves\\nTransformed Kobe Bryant from a rebellious teenager into a mature leader of a championship team.\\nEleven times, Jackson led his teams to the ultimate goal: the NBA championship six times with the Chicago Bulls and five times with the Los Angeles Lakers. We all know the legendary stars on those teams or think we do. What Eleven Rings shows us, however, is that when it comes to the most important lessons, we don't know very much at all. This book is full of revelations: about fascinating personalities and their drive to win, about the wellsprings of motivation and competition at the highest levels and about what it takes to bring out the best in ourselves and others.\",\n",
              " \"A book on 'Politics (Palgrave Foundations Series)'.\",\n",
              " \"The Art of Spirited Away collects colour illustrations of Spirited Away for the first time in an English edition! This book includes paintings and designs from the new animated film from the director of Kiki's Delivery Service and Princess Mononoke. Large-size, hardcover coffee-table book featuring artwork from the renowned animated film, Spirited Away, directed by Hayao Miyazaki. Features commentary, colour stills, sketches, storyboards, and illustrations used to envision the rich fantasy world of the film. Also includes a complete English-language script.\",\n",
              " 'Drawn from 3,000 years of the history of power, this is the definitive guide to help readers achieve for themselves what Queen Elizabeth I, Henry Kissinger, Louis XIV and Machiavelli learnt the hard way. Law 1: Never outshine the master Law 2: Never put too much trust in friends; learn how to use enemies Law 3: Conceal your intentions Law 4: Always say less than necessary. The text is bold and elegant, laid out in black and red throughout and replete with fables and unique word sculptures. The 48 laws are illustrated through the tactics, triumphs and failures of great figures from the past who have wielded - or been victimised by - power.',\n",
              " 'Swaying hips, praying lips and flying tips: \"A Hilarious Take on Just about Every-Thing\" associated with air travel. This book provides invaluable perspectives on some of the most common situations encountered by fliers.\\nAwesome icebreakers to start conversations with attractive co-passengers.\\nGetting seamlessly upgraded to Business Class.\\nSure shot strategies for winning the affections of flight attendants.\\nHow a water bottle dramatically improves flight safety?\\nHow smart executives deal with the “Walk of Shame” to the economy section?\\nAction plans to counter the airline’s “technical snag” routine.\\nThe how\\'s and whys of micro-managing the pilots.\\nBreezing through immigration, customs and always making your flight connections.\\nRest assured, your flying experience will never be the same again. A must read as you prepare for, or take your next flight!\\nRishi Piparaiya is an over-worked and over travelled corporate executive based in the skies, 38,000 feet over India.',\n",
              " 'International English Language Testing System or IELTS is an English proficiency test for students preparing to apply for undergraduate and graduate studies. It is especially important for international students. IELTS Practice Tests: with Explanatory Key and Audio CD by Peter May is a book consisting of essential practice material for the IELTS examination. Along with reading and writing manuals for improving your writing skills and pronunciations, the book also has several key concepts and tips to answer tricky questions.\\nIELTS Practice Tests puts an emphasis on the strategy that lies behind answering select problems, it goes on to describe the correct approach to completing each section of the test. This book focusses on improving the ability of the students to identify errors and be quick with their choices in every task set out in front of them.\\nWith detailed explanatory keys providing sample answers to the academic writing tests, students get additional assistance in form of audio scripts and two discs. IELTS Practice Tests also contains four practice tests with sections on reading and writing. The answer sheets in the book are printable for easier usage.\\nIELTS Practice Tests: with Explanatory Key and Audio CD was published by OUP Oxford in 2004 and is available in paperback.\\nKey Features:\\nThe book comes with an Audio CD containing listening material for the four practice tests.\\nIt provides an access to an online practice test link with automatic marking and an online dictionary lookup.',\n",
              " \"Trinity College London's Rock and Pop graded songbooks contain real songs from legendary artists, with specially written performance notes for the Rock and Pop exams from 2018. This book contains material intended for Drums at grade 1.\",\n",
              " \"Nowadays, the world's money is traded by computer code, inside black boxes in heavily guarded buildings. Even the experts entrusted with your cash don't know what's happening to it. In Flash Boys, Michael Lewis tells the explosive story of how one group of ingenious oddballs and misfits set out to expose what was going on. It's the story of what it's like to declare war on some of the richest and most powerful people in the world. It's about taking on an entire system. And it's about the madness that has taken hold of the financial markets today.\",\n",
              " \"Captain Blake seems to have lost the first round in Part 1, having been assassinated at Athens airport. But a furious Mortimer swears that he'll never stop trying to avenge his friend. He goes on the hunt, but information is scarce. Strange happening occur when he comes under the protection of Sheik Abdel Razek and Mortimer soon feels like he's losing his way in his investigation which will lead him to the darkest depths of the Great Pyramid...\",\n",
              " 'Recycling is good, isn’t it?\\n\\nIn this visionary book, chemist Michael Braungart and architect William McDonough challenge this status quo and put forward a manifesto for an intriguing and radically different philosophy of environmentalism.\\n\\n\"Reduce, reuse, recycle”. This is the standard “cradle to grave” manufacturing model dating back to the Industrial Revolution that we still follow today. In this thought-provoking read, the authors propose that instead of minimising waste, we should be striving to create value. This is the essence of Cradle to Cradle: waste need not to exist at all. By providing a framework of redesign of everything from carpets to corporate campuses, McDonough and Braungart make a revolutionary yet viable case for change and for remaking the way we make things.',\n",
              " 'The book challenges the popular notion of the Pahari painter being an anonymous craftsman plying predetermined strokes. When first published in German, it was widely acclaimed and has come to occupy the status of a classic.',\n",
              " \"The greatest economic thinkers of the world have influenced their times and, to some extent, even the present times in their own unique ways. The Worldly Philosophers: The Lives, Times And Ideas Of The Great Economic Thinkers is a book that enumerates various economists and their valuable and some not-so-valuable ideas and principles, in accordance with their own times.\\nThe book tries to find common ground among the different thinkers and the common ground is nothing else but the desire to understand the mechanism of a capitalist society, especially when the world is going through a state of economic confusion.\\nThis book doesn't teach you economics like textbooks, it expands your economic vision by giving you insights about the thinking process of various economic thinkers, from Adam Smith to Karl Marx and John Maynard Keynes. It brings in the past to look at the present and the future, as ideas of great thinkers always remain relevant to some extent in society.\\nThe Worldly Philosophers: The Lives, Times And Ideas Of The Great Economic Thinkers is a wonderful book to enlighten people about great people and their priceless ideas, that shaped the economic history of nations. It is a book that explores the past and questions the future. The 7th revised edition of this book was published by Touchstone in 1999 and it is available in paperback.\\nKey Features:\\nThe book gives people an insight into the varied ideas of the great economic thinkers, from different times.\\nThrough their ideas, the book makes an attempt to understand the workings of a capitalist society.\",\n",
              " '\"If you\\'re ready to graduate from the boy-meets-girl league of screenwriting, meet John Truby . . . [his lessons inspire] epiphanies that make you see the contours of your psyche as sharply as your script.\"\\n―LA Weekly\\nJohn Truby is one of the most respected and sought-after story consultants in the film industry, and his students have gone on to pen some of Hollywood\\'s most successful films, including Sleepless in Seattle, Scream, and Shrek. The Anatomy of Story is his long-awaited first book, and it shares all his secrets for writing a compelling script. Based on the lessons in his award-winning class, Great Screenwriting, The Anatomy of Story draws on a broad range of philosophy and mythology, offering fresh techniques and insightful anecdotes alongside Truby\\'s own unique approach to building an effective, multifaceted narrative.',\n",
              " \"NEW from the bestselling HBR's 10 Must Reads series. The best leaders know how to communicate clearly and persuasively. How do you stack up? If you read nothing else on communicating effectively, read these 10 articles. We've combed through hundreds of articles in the Harvard Business Review archive and selected the most important ones to help you express your ideas with clarity and impact--no matter what the situation. Leading experts such as Deborah Tannen, Jay Conger, and Nick Morgan provide the insights and advice you need to: * Pitch your brilliant idea--successfully * Connect with your audience * Establish credibility * Inspire others to carry out your vision * Adapt to stakeholders' decision-making styles * Frame goals around common interests * Build consensus and win support Looking for more Must Read articles from Harvard Business Review? Check out these titles in the popular series: HBR's 10 Must Reads: The Essentials HBR's 10 Must Reads on Collaboration HBR's 10 Must Reads on Innovation HBR's 10 Must Reads on Leadership HBR's 10 Must Reads on Making Smart Decisions HBR's 10 Must Reads on Managing Yourself HBR's 10 Must Reads on Strategic Marketing HBR's 10 Must Reads on Teams\",\n",
              " 'Ever suffered the collective flatulence of eighty co-passengers while sailing on a serene Asian river? Or called out for rescue in true Bollywood style while locked up in a minaret in Persia? Or had to cross a pack of hyenas en route to the loo?\\nDreaming of glorious sunrises and architectural marvels in exotic places, Sudha often landed up in situations that were uproariously bizarre or downright dangerous. Tongue firmly in cheek, she recounts her journeys through the raw wildernesses of Borneo and the African savannah, into the deserts of Iran and Uzbekistan, and up the Annapurna and the Pamirs, revealing the quirky side of solo travel to side-splitting effect. Punctuating her droll stories with breathtaking descriptions and stunning photographs, Sudha invites readers on an unexpected and altogether memorable tour around the world!',\n",
              " '\\'I couldn’t put down this thriller . . . the perfect book to read by the fire this winter.\\' Bill Gates, \\'5 books I loved in 2018\\' WINNER OF THE FINANCIAL TIMES/MCKINSEY BUSINESS BOOK OF THE YEAR AWARD 2018 The full inside story of the breathtaking rise and shocking collapse of Theranos, the multibillion-dollar biotech startup, by the prize-winning journalist who first broke the story and pursued it to the end, despite pressure from its charismatic CEO and threats by her lawyers. In 2014, Theranos founder and CEO Elizabeth Holmes was widely seen as the female Steve Jobs: a brilliant Stanford dropout whose startup \"unicorn\" promised to revolutionize the medical industry with a machine that would make blood testing significantly faster and easier. Backed by investors such as Larry Ellison and Tim Draper, Theranos sold shares in a fundraising round that valued the company at more than $9 billion, putting Holmes\\'s worth at an estimated $4.7 billion. There was just one problem: The technology didn\\'t work. In Bad Blood, John Carreyrou tells the riveting story of the biggest corporate fraud since Enron, a tale of ambition and hubris set amid the bold promises of Silicon Valley. Now to be adapted into a film, with Jennifer Lawrence to star. \\'Chilling . . . Reads like a West Coast version of All the President’s Men.\\' New York Times Book Review',\n",
              " 'Mandala is a symbol made from circles, squares or triangles that represents the cosmos. Colouring is the best calming tool that unleash your inner creativity. The Mandala Colouring Book invites you to relax, find tranquility and balance in your life as you fill in colours on each page of this book with your own creative essence.',\n",
              " 'Why is it that airline tickets booked well in advance are always cheaper?\\nHow would Phoolan Devi and Veerappan react to a case of Prisoners’ Dilemma?\\nProfessor Satish Y. Deodhar explains the dynamics of pricing with respect to demand and supply and the various market structures like perfect competition, monopoly, monopolistic competition and oligarchy through everyday examples and case studies.',\n",
              " \"This book is written as a textbook for educational programs at colleges and universities. It can also be used by IoT (Internet of Things) vendors and service providers for training their program developers. The authors have used an immersive 'hands on' approach, similar to the one adopted in the companion book, Cloud Computing: A Hands-on Approach, to help readers gain expertise in developing working code for real-world IoT applications.\",\n",
              " 'A Time To Kill is available in paperback. The book has 672 pages. The language of the book is English, and the author of the book is John Grisham.\\nA ten-year old African American girl is brutally raped and beaten by two white racist drunken men. Inspite of committing such a heinous crime, the perpetrators remain unrepentant. This horrendous act unleashes mayhem in the town of Clayton, Mississippi where the story is set.\\nCarl Lee, the father of the wronged girl, is driven into a mad rage and decides to take the law in his hands in order to bring justice to his daughter. Armed with an assault rifle, he sets out to avenge the gruesome crime committed on his daughter and kills the culprits. He is arrested and is to be tried for capital murder.\\nThis is a moving tale about a young defence lawyer, Jake Brigance, who decides to defend the black Vietnam war hero. The people of the town are divided in their opinion about the the crime committed by Carl. The book takes you through a journey of the lawyer’s struggle to save his client. Also at stake is the young attorney’s life. The novel was published in the year 1992 by RHUK.\\nKey Features:\\nThe book sensitively explores the relationship between black and whites. It is a legal suspense thriller and keeps you on an edge at all times.',\n",
              " 'The great Himalayan National Park conservation area (ghnpca), a UNESCO world heritage site, is one of the most important protected areas in the Himalayas, one of the world’s great biological realms. The book is intended both as a history and an ecological overview of the park as well as a plea for continuing conservation of the rich legacy of Himalayan plants and animals. In addition to descriptions of the ecology, the book includes local history and Culture and a review of current development in the region. The inscription of the park into the UNESCO world heritage list in 2014 confirmed the outstanding universal values of the area, which contains the most important and significant natural habitats for in-situ conservation of Western Himalayan biological diversity. The pictures, taken by the authors and their collaborators, vividly illustrate the grandeur and diversity of the area. The book has universal appeal: to naturalists, scholars, resource managers, trekkers, arm-chair traveller. Success and failure along the road to creating today’s Park are discussed frankly to inform future management efforts and there are numerous examples of conservation in action that will motivate a new generation of naturalists and ecologists to continue the fight to protect the ecology of the greatest mountains on Earth.',\n",
              " '2018 Eisner Award winner, Best Writer 2018 Eisner Award winner, Best Painter/Multimedia Artist 2018 Eisner Award winner, Best Continuing Series 2018 Eisner Award winner, Best Publication for Teens 2018 Eisner Award winner, Best Cover Artist 2018 Harvey Award winner, Book of the Year 2018 Hugo Award winner, Best Graphic Story 2018 British Fantasy Award winner, Best Comic/Graphic Novel 2018, 2016, 2015 Entertainment Weekly\\'s The Best Comic Books of the Year 2018, Newsweek\\'s Best Comic Books of the Year 2018, The Washington Post\\'s 10 Best Graphic Novels of the Year 2018, Barnes & Noble\\'s Best Books of the Year 2018, YALSA\\'s Great Graphic Novels for Teens 2018, Thrillist\\'s Best Comics & Graphic Novels of the Year 2018, Powell\\'s Best Science-Fiction, Fantasy, Horror, and Graphic Novels of the Year Set in an alternate matriarchal 1900\\'s Asia, in a richly imagined world of art deco-inflected steam punk, MONSTRESS tells the story of a teenage girl who is struggling to survive the trauma of war, and who shares a mysterious psychic link with a monster of tremendous power, a connection that will transform them both and make them the target of both human and otherworldly powers. About the Creators: New York Times bestselling and award-winning writer Marjorie Liu is best known for her fiction and comic books. She teaches comic book writing at MIT, and leads a class on Popular Fiction at the Voices of Our Nation (VONA) workshop. Ms. Liu\\'s extensive work includes the bestselling \"Astonishing X-Men\" for Marvel Comics, which featured the gay wedding of X-Man Northstar and was subsequently nominated for a GLAAD Media Award for outstanding media images of the lesbian, gay, bisexual and transgender community. Prior to writing full-time, Liu was a lawyer. She currently resides in Boston. Sana Takeda is an illustrator and comic book artist who was born in Niigata, and now resides in Tokyo, Japan. At age 20 she started out as a 3D CGI designer for SEGA, a Japanese video game company, and became a freelance artist when she was 25. She is still an artist, and has worked on titles such as \"X-23\" and \"Ms. Marvel\" for Marvel Comics, and is an illustrator for trading card games in Japan.',\n",
              " 'The new edition of bestselling CCNA Cert Library by Wendell Odom is a comprehensive review and practice package for the latest CCNA exams. The two books contained in this package, CCENT/CCNA ICND1 Official Cert Guide, and CCNA ICND2 Official Cert Guide, present complete reviews and a more challenging and realistic preparation experience. The books will be fully updated to cover the latest CCNA exam topics.\\n \\nThe companion DVDs contains the powerful Pearson IT Certification Practice Test engine, complete with hundreds of well-reviewed, exam-realistic questions. The assessment engine offers you a wealth of customization options and reporting features, laying out a complete assessment of your knowledge to help you focus your study where it is needed most. This new edition also includes a free copy of the CCNA Network Simulator Lite edition complete with meaningful lab exercises, which help you hone your hands-on skills with the Cisco user interface for routers and switches. The DVDs also contain more than 60 minutes of personal video mentoring from the author focused on subnetting.\\n \\nWell-regarded for its level of detail, assessment features, and challenging review questions and exercises, these official study guides help you master the concepts and techniques that will enable you to succeed on the exam the first time.\\n This package includes the following two products:\\n1. CCENT/CCNA ICND1 Official Cert Guide\\n2. CCNA ICND2 Official Cert Guide',\n",
              " \"A schoolteacher who killed multiple paramours with cyanide; a mother who trained her daughters to kill children; a thug from the 1800s who slaughtered more than 900 people, a manservant who killed girls and devoured their body parts.\\nIf you thought serial killers was a Western phenomenon, think again!\\nThese bone-chilling stories in The Deadly Dozen will take you into the hearts and heads of India's most devious murderers and schemers, exploring what made them kill and why?\",\n",
              " \"How can you bring out MySQLs full power? With High Performance MySQL, you'll learn advanced techniques for everything from designing schemas, indexes and queries to tuning your MySQL server, operating system and hardware to their fullest potential. This guide also teaches you safe and practical ways to scale applications through replication, load balancing, high availability and failover.\\nUpdated to reflect recent advances in MySQL and InnoDB performance, features and tools, this third edition not only offers specific examples of how MySQL works, it also teaches you why this system works as it does, with illustrative stories and case studies that demonstrate MySQLs principles in action. With this book, you'll learn how to think in MySQL.\\nLearn the effects of new features in MySQL 5.5, including stored procedures, partitioned databases, triggers and views.\\nImplement improvements in replication, high availability and clustering.\\nAchieve high performance when running MySQL in the cloud.\\nOptimize advanced querying features, such as full-text searches.\\nTake advantage of modern multi-core CPUs and solid-state disks.\\nExplore backup and recovery strategies including new tools for hot online backups.\",\n",
              " \"A President's affair to remember soon becomes a nightmare he wishes he could forget in James Patterson's gripping new stand-alone thriller. As he steps out of his hotel in Atlanta with the love of his life, the last thing President Tucker expects is to be greeted by the flashing cameras of the media. Or for his affair to be revealed to the world. The President must act to stop the scandal from spiralling out of control, just weeks before the vote for a second term. But not before he figures out how to keep the First Lady on his side. Grace Tucker, however, has other plans. Betrayed and deeply hurt, she heads for a safe haven outside the capital. But when her security detail loses all trace of her, all hell breaks loose. The First Lady is missing. Did she run away. Or is she in more danger than they could have imagined.\",\n",
              " \"Python is fast becoming the programming language of choice for hackers, reverse engineers, and software testers because it's easy to write quickly, and it has the low-level support and libraries that make hackers happy. But until now, there has been no real manual on how to use Python for a variety of hacking tasks. You had to dig through forum posts and man pages, endlessly tweaking your own code to get everything working. Not anymore.\\nGray Hat Python explains the concepts behind hacking tools and techniques like debuggers, trojans, fuzzers, and emulators. But author Justin Seitz goes beyond theory, showing you how to harness existing Python-based security tools - and how to build your own when the pre-built ones won't cut it.\\nYou'll learn how to:\\nAutomate tedious reversing and security tasks\\nDesign and program your own debugger\\nLearn how to fuzz Windows drivers and create powerful fuzzers from scratch\\nHave fun with code and library injection, soft and hard hooking techniques, and other software trickery\\nSniff secure traffic out of an encrypted web browser session\\nUse PyDBG, Immunity Debugger, Sulley, IDAPython, PyEMU, and more\\nThe world's best hackers are using Python to do their handiwork. Shouldn't you?\",\n",
              " 'Microservices can have a positive impact on your enterpriseójust ask Amazon and Netflixóbut you can fall into many traps if you donít approach them in the right way. This practical guide covers the entire microservices landscape, including the principles, technologies and methodologies of this unique, modular style of system building. Youíll learn about the experiences of organizations around the globe that have successfully adopted microservices.\\nIn three parts, this book explains how these services work and what it means to build an application the Microservices Way. Youíll explore a design-based approach to microservice architecture with guidance for implementing various elements. And youíll get a set of recipes and practices for meeting practical, organizational and cultural challenges to microservice adoption.\\n\\nLearn how microservices can help you drive business objectives\\nExamine the principles, practices and culture that define microservice architectures\\nExplore a model for creating complex systems and a design process for building a microservice architecture\\nLearn the fundamental design concepts for individual microservices\\nDelve into the operational elements of a microservices architecture, including containers and service discovery\\nDiscover how to handle the challenges of introducing microservice architecture in your organization',\n",
              " \"Although the story of Uncle Dynamite concerns Bill Oakshott's struggle to find ways of getting his girl while financing his inheritance at Ashenden Manor, the real hero of the book is Frederick Altamont Cornwallis, fifth Earl of Ickenham. This noble lord describes himself as 'one of the hottest earls that ever donned a coronet' and he was also one of his creator's favourite characters, featuring in three other novels. Lord Ickenham sees it as his mission to bring a little joy into the lives of others, and on this occasion he surpasses himself.\",\n",
              " \"Venkat Iyer was living a fast-paced life in the IT world in Mumbai when he decided to stop and take a long, hard look at where he was headed. Disheartened by his stressful existence in the city, he decided to give it all up and take up organic farming in a small village near Mumbai. But it wasn't easy. With no experience in agriculture, his journey was fraught with uncertainty. He soon went from negotiating tough clients, strict deadlines and traffic to looking forward to his first bumper crop of moong. As he battled erratic weather conditions and stubborn farm animals, he discovered a world with fresh air and organic food, one where he could lead a more wholesome existence. At times hilarious, and other times profound, this book follows his extraordinary story.\",\n",
              " 'This comprehensive textbook addresses the role that the agencies of administrative development play in the functioning of a government, especially in a developing country like India.\\nWritten specifically as a textbook for undergraduate and postgraduate students of public administration, Development Administration in India traces and analyses the concerns and challenges faced by administrations and governments to achieve inclusive and sustainable development in India. The book dwells extensively on the importance of NGOs and the role of women and self-help groups in the development of administration in India. Further, it discusses the administrative theories, concepts and functions in the international context.\\nThe highlight of this book is the inclusion of current topics and evaluation of emerging issues such as environmental sustainability, tribal development, ethical deficit and e-governance, which makes this text equally relevant for Civil and State Services Examination aspirants.\\nKey Features:\\nCritical and exhaustive coverage of administrative development from both theoretical and practical perspectives\\nThorough insight into the concepts, concerns and new challenges of development administration in India\\nRecent literature, latest data and illustrations in each chapter.',\n",
              " 'Introduces readers to the names and starting moves of the main chess openings, while highlighting several devastating opening traps.',\n",
              " \"It's the wedding of the century and the story you've waited a lifetime for! Archie finally marries Betty! ...and Veronica? Almost 80 years in the making! But is this really the end of the classic love triangle between Archie, Veronica, and Betty? Will the Riverdale gang ever be the same? Do Archie and Veronica live happily ever after? Follow the celebrated story - originally published in the pages of Archie #600-605 - from proposal to wedding and beyond! Watch as Archie and Veronica start a family and navigate the ups and downs of married life as one of the most famous comic couples of all time! Written by life-long Archie Comics fan and movie producer Michael Ulsan, and with art by acclaimed Archie artist Stan Goldberg, this timeless story about growing up is jam-packed with the hilarious antics and touching sentiment only Archie & Co. can bring you.\",\n",
              " 'There are approximately six thousand languages on Earth today, each a descendant of the tongue first spoken by Homo sapiens some 150,000 years ago. While laying out how languages mix and mutate over time, linguistics professor John McWhorter reminds us of the variety within the species that speaks them, and argues that, contrary to popular perception, language is not immutable and hidebound, but a living, dynamic entity that adapts itself to an ever-changing human environment.\\nFull of humor and imaginative insight, The Power of Babel draws its illustrative examples from languages around the world, including pidgins, Creoles, and nonstandard dialects.',\n",
              " \"Fyodor Dostoyevsky's powerful meditation on faith, meaning and morality, The Brothers Karamazov is translated with an introduction and notes by David McDuff in Penguin Classics. When brutal landowner Fyodor Karamazov is murdered, the lives of his sons are changed irrevocably: Mitya, the sensualist, whose bitter rivalry with his father immediately places him under suspicion for parricide; Ivan, the intellectual, whose mental tortures drive him to breakdown; the spiritual Alyosha, who tries to heal the family's rifts; and the shadowy figure of their bastard half-brother Smerdyakov. As the ensuing investigation and trial reveal the true identity of the murderer, Dostoyevsky's dark masterpiece evokes a world where the lines between innocence and corruption, good and evil, blur and everyone's faith in humanity is tested. This powerful translation of The Brothers Karamazov features and introduction highlighting Dostoyevsky's recurrent themes of guilt and salvation, with a new chronology and further reading. Fyodor Mikhailovich Dostoyevsky (1821-1881) was born in Moscow. From 1849-54 he lived in a convict prison, and in later years his passion for gambling led him deeply into debt. His other works available in Penguin Classics include Crime & Punishment, The Idiot and Demons. If you enjoyed The Brothers Karamazov you might like Nikolai Gogol's Dead Souls, also available in Penguin Classics. 'There is no writer who better demonstrates the contradictions and fluctuations of the creative mind than Dostoyevsky, and nowhere more astonishingly than in The Brothers Karamazov' Joyce Carol Oates 'Dostoyevsky was the only psychologist from whom I had anything to learn: he belongs to the happiest windfalls of my life' Friedrich Nietzsche 'The most magnificent novel ever written' Sigmund Freud\",\n",
              " 'Captain Hastings recounts 18 of Poirot’s early cases from the days before he was famous…\\n\\nHercule Poirot delighted in telling people that he was probably the best detective in the world. So turning back the clock to trace eighteen of the cases which helped establish his professional reputation was always going to be a fascinating experience. With his career still in its formative years, the panache with which Hercule Poirot could solve even the most puzzling mystery is obvious.\\n\\nChronicled by his friend Captain Hastings, these eighteen early cases - from theft and robbery to kidnapping and murder - were all guaranteed to test Poirot’s soon-to-be-famous ‘little grey cells’ to their absolute limit.',\n",
              " 'The second volume in a series of comic cartoons starring the Calvin and Hobbes pair.\\nCalvin, cheeky, hyperactive and mischievous, and Hobbes, his cuddly toy tiger who, as far as Calvin is concerned is very much alive and kicking, are two of the most loveable and hilarious characters to grace the comic strip in years.\\nSit back and enjoy . . .',\n",
              " 'From the 1 New York Times bestselling author of Selp-Helf comes the “leaked” diary of YouTube comedic personality Miranda Sings. Taped together so the world can read all about her life through her eyes, My Diarrhe includes pages from Miranda’s baby book, poems from her years as an emotional teen, secrets from her dating life and stories from her rise to fame. As Miranda herself says, “It has every single secret about my life in it. My first kiss, my first period, stories about secret family members, secret photos of other celebrities, etc. so don’t read it!”',\n",
              " 'He loved French cookbooks, invented a new way of making khichdi, was interested in the engineering behind ship-building and the technology that makes ammunition. More than 100 years after his death, do we really know or understand the bewildering, fascinating, complex man Swami Vivekananda was? Vivekananda is one of the most important figures in the modern imagination of India. He is also an utterly modern man, consistently challenging his own views and embracing diverse, even conflicting arguments. It is his modernity that appeals to us today. He is unlike any monk we have known. He is confined neither by history nor by ritual and is constantly questioning everything around him, including himself. It is in Vivekananda’s contradictions, his doubts, his fears and his failings that he recognize his profoundly compelling divinity—he teaches us that to try and understand God, first one must truly comprehend one’s own self. This book is an argument that it is not just because he is close to God but also because he is so tantalizingly immersed in being human that keeps us returning to Vivekananda and his immortal wisdom.',\n",
              " \"The Sunday Times Number One Bestseller\\n\\nTrudy has betrayed her husband, John. She's still in the marital home – a dilapidated, priceless London townhouse – but not with John. Instead, she's with his brother, the profoundly banal Claude, and the two of them have a plan. But there is a witness to their plot: the inquisitive, nine-month-old resident of Trudy's womb.\",\n",
              " \"In this companion volume to his classic 'Chess Endings: Essential Knowledge, ' Grandmaster Averbakh takes the reader through the various steps to success in the middlegame. He explains the tactical skills and positional strategies that every chessplayer needs to master. Topics covered include coordinating the pieces, attack and defence, combinations and sacrifices, and much more. Yuri Averbakh is a former Soviet champion and world championship candidate. He is also the author of the five-volume treatise 'Comprehensive Chess Endings' (also translated by Ken Neat) and the perennially popular 'Chess Endings: Essential Knowledge.\",\n",
              " \"Key Words with Peter and Jane uses the most frequently met words in the English language as a starting point for learning to read successfully and confidently. The Key Words reading scheme is scientifically researched and world renowned. In book 5b, Peter and Jane have fun playing outside in 46 new words including 'how', 'from', 'soon' and 'street'. Once this book has been completed, the child moves on to book 6b. The Key Words with Peter and Jane books work because each of the key words is introduced gradually and repeated frequently. This builds confidence in children when they recognise these key words on sight (also known as the 'look and say' method of learning). Examples of key words are: the, one, two, he. There are 12 levels, each with 3 books: a, b, and c. Series a: Gradually introduces new words. Series b: Provides further practise of words featured in the 'a' series. Series c: Links reading with writing and phonics. All the words that have been introduced in each 'a' and 'b' book are also reinforced in the 'c' books.\",\n",
              " 'Now a Richard & Judy Book Club pick, End Game is the fifth book in the thrilling Will Robie series by international number one bestselling author David Baldacci. Will Robie, highly trained assassin and the US government’s most indispensable asset, is called to London. An imminent terrorist attack threatens the Underground and with the US next in line, Robie is the perfect choice to stop it before it begins. He knows he has one chance to succeed. One chance to save London. One chance to make it safely home to find out what has happened to fellow agent Jessica Reel following their last deadly mission together. But Robie is about to learn that even if he succeeds, the worst is yet to come. The game has started. Now only he can end it . . .',\n",
              " 'This is a sweet tale of finding true love. Iqbal, who works in IT company meets a beautiful girl Rubina. Iqbal falls in love with her, but Rubina does not reciprocate the same. Whether any mystery behind this girl Rubina or the destiny is waiting for something else?',\n",
              " 'Long Walk to Freedom by Nelson Mandela is the amazing story of a true hero of our times; his famous biography has been specially adapted for children in a beautiful illustrated picture book format. Discover how a little boy whose father called him \"troublemaker\" grew up to fight apartheid, become South Africa\\'s first black president and campaign for freedom and justice throughout the world. Adapted by poet Chris van Wyk and illustrated by South African artist Paddy Bouma, with an introduction from Archbishop Desmond Tutu, Long Walk to Freedom introduces children to the life of one of the world\\'s most beloved leaders.',\n",
              " \"The ultimate survival guide from the world’s leading survival expert.\\n\\nNobody knows survival like Bear Grylls. There is a barely a terrain he hasn’t conquered or an extreme environment he hasn’t experienced. Over the years — from his time in 21 SAS, through to his extraordinary expeditions climbing (and paragliding over) Everest, travelling through the Arctic's treacherous Northwest Passage, crossing the world’s oceans and taking part in expeditions to the toughest corners of each of the seven continents — Bear has accumulated an astonishing wealth of survival knowledge.\\n\\nNow, for the first time, he is putting all his expertise into one book. How To Stay Alive will teach you:\\n\\n- How to survive a bear attack\\n- How to fly a plane in an emergency\\n- How to make fire from virtually nothing\\n- How to drive off-road\\n- How to navigate using the stars\\n- How to administer first-aid\\n- How to escape a burning building\\n- How to survive the most extreme conditions\\n\\nAnd dozens of other essential skills to survive the modern world.\",\n",
              " 'Better English concentrates on the aim Teach Yourself -Test Yourself. This is a self-help book which a person with average English skills can consult without the help of any professional. Lessons on grammar, pronunciation and spelling are included in the book. Various types of practical examples are given to test the weaknesses of the readers at frequent sections of the book. Pronunciation of certain difficult words has been made easy.\\nThe words such as succinct-indict; genuine-guillotine; valet-bouquet; quay-querulous; homage-herb; chasm-chic. These types of lessons are very important for our everyday life since we interact with different kinds of people on a daily basis. Once a person is able to grab the basic principles and rules of the English language, better accuracy and efficiency can be achieved. The write-up is available online at Amazon India.\\nAbout the author:\\nNorman Lewis (June, 1908 – July, 2003) was an impressionable British journalist and a creative author. His travel write-ups were considered to be his best works. He authored 12 novels and several numbers of autobiographies. As per Graham Greene, Lewis was one of the best authors of the 20th century.',\n",
              " 'In a distant future where sentient humanoid robots pass for human, someone or some thing is out to destroy the seven great robots of the world. Europol’s top detective Gesicht is assigned to investigate these mysterious robot serial murders—the only catch is that he himself is one of the seven targets.',\n",
              " 'The classic guide that helps you communicate your thoughts clearly, concisely, and effectively. Essential for every professional, from entry level to the executive suite, Writing that Works includes advice on all aspects of written communication—including business memos, letters, reports, speeches and resumes, and e-mail—and offers insights into political correctness and tips for using non-biased language that won’t compromise your message.\\nConcise and easy-to-use, Writing that Works features an accessible, at-a-glance style, full of bulleted \"tips\" and specific examples of good vs. bad writing.\\nWith dozens of samples and useful tips for composition, Writing That Works will show you how to improve anything you write:\\nE-mails, memos and letters that get read—and get action\\nProposals, recommendations, and presentations that sell ideas\\nPlans and reports that get things done\\nFund-raising and sales letters that produce results\\nResumes and letters that lead to interviews\\nSpeeches that make a point\\nAnd much more.\\n  ',\n",
              " \"Once in a generation a woman comes along who changes everything. Tina Fey is not that woman, but she met that woman once and acted weird around her.\\nBefore 30 Rock, Mean Girls and 'Sarah Palin', Tina Fey was just a young girl with a dream: a recurring stress dream that she was being chased through a local airport by her middle-school gym teacher.\\nShe also had a dream that one day she would be a comedian on TV. She has seen both these dreams come true.\\nAt last, Tina Fey's story can be told. From her youthful days as a vicious nerd to her tour of duty on Saturday Night Live; from her passionately halfhearted pursuit of physical beauty to her life as a mother eating things off the floor; from her one-sided college romance to her nearly fatal honeymoon - from the beginning of this paragraph to this final sentence.\\nTina Fey reveals all, and proves what we've all suspected: you're no one until someone calls you bossy.\",\n",
              " \"The Stranger I Married is an erotic tale of love and awakened desire in Victorian England . . . perfect for fans of E. L. James . . .\\nThey are London's most scandalous couple.\\nIsabel, Lady Pelham, and Gerard Faulkner, Marquess of Grayson, are well matched in all things - lusty appetites, constant paramours, provocative reputations, and their absolute refusal to ruin a marriage of convenience by falling in love.\\nIt is a most agreeable sham - until a shocking event sends Gerard from her side.\\nWhen, four years later, Gerard returns, the boyish rogue is now a powerful, irresistible man determined to seduce Isabel. He is not the man she married - but is he the one to finally steal her heart?\\n\\nPraise for Sylvia Day:\\n'Move over Danielle Steel and Jackie Collins, this is the dawn of a new Day' Amuse\\n\\n'Several shades darker and a hundred degrees hotter than anything you've read before' Reveal\",\n",
              " 'Acclaimed by Henry James as Robert Louis Stevenson\\'s best novel, Kidnapped achieves what Stevenson called, \"the particular crown and triumph of the artist...not simply to convince, but to enchant.\"\\n\\nSpirited, romantic, and full of danger, Kidnapped is Robert Louis Stevenson\\'s classic of high adventure. Beloved by generations, it is the saga of David Balfour, a young heir whose greedy uncle connives to do him out of his inherited fortune and plots to have him seized and sold into slavery. But honor, loyalty, and courage are rewarded; the orphan and castaway survives kidnapping and shipwreck, is rescued by a daredevil of a rogue, and makes a thrilling escape to freedom across the wild highlands of Scotland.',\n",
              " \"The Garden of Words brings to the manga page all the beauty and mystery of the award-winning film from artful animator Makoto Shinkai. Beloved for the simple grace of its artwork as much as the poetic elegance of its text (adapted by Midori Motohashi), The Garden of Words begins with a chance, rainswept encounter between Takao, a young man who dreams of becoming a shoe designer, and Yukari, an enigmatic woman he finds sitting alone, nursing a beer on a park bench. The spare interaction of these two lonely souls sparks a spiritual transformation for the young man, and perhaps the woman as well. As this intriguing, understated story unfolds, their lives will become further intertwined amid rain, beer, school, and shoe cobbling. Words are not often necessary, but in this case just a few words can make a difference in one's heart.\",\n",
              " 'Everyone’s invited. Everyone’s a suspect. Bristling with tension, bitter rivalries and toxic friendships, get ready for the most hotly-anticipated thriller of 2019. In a remote hunting lodge, deep in the Scottish wilderness, old friends gather for new year. The beautiful one the Golden couple the volatile one the new parents the quiet one the city boy the outsider the victim.',\n",
              " \"'Why do they call you Baaz?' 'It means falcon,' he replies solemnly. 'Or bird of prey. Because I swoop down on the enemy planes just like a Baaz would.' Then he grins. The grey eyes sparkle. 'It's also short for bastard.' 1971. The USSR-backed India-Mukti Bahini alliance is on the brink of war against the America-aided Pakistani forces. As the Cold War threatens to turn red hot, handsome, laughing Ishaan Faujdaar, a farm boy from Chakkahera, Haryana, is elated to be in the IAF, flying the Gnat, a tiny fighter plane nicknamed 'Sabre Slayer' for the devastation it has wreaked in the ranks of Pakistan's F-86 Sabre Squadrons. Flanked by his buddies Raks, a MiG-21 Fighter, Maddy, a transport pilot who flies a Caribou, and fellow Gnatties Jana, Gana and Mana, Shaanu has nothing on his mind but glory and adventure - until he encounters Tehmina Dadyseth, famed bathing beauty and sister of a dead fauji, who makes him question the very concept of nationalism and whose eyes fill with disillusioned scorn whenever people wax eloquent about patriotism and war... Pulsating with love, laughter and courage, Baaz is Anuja Chauhan's tribute to our men in uniform.\",\n",
              " \"Siddhant meets Akriti during their medical residency in Delhi. Their connection is instant, blossoming from the many similarities between them. So, when Akriti faces a devastating loss, she leans on Siddhant for support. In the heat of an emotional moment, the two decide that this must be love. But as Akriti's depression begins to take a stronger hold over her, she spirals out of control, sinking deeper into an abyss of fear, insecurity and rage. And while Siddhant struggles to help her, it seems like everything he does is only making things worse. Meanwhile, Siddhant's life gets further complicated when Maahi, his ex-girlfriend whom he never stopped loving, re-enters his life. Nikita Singh returns with a stirring story - exploring emotional health, the boundaries of traditional relationships and second chances.\",\n",
              " \"Robotic Process Automation (RPA) enables automating business processes using software robots. Software robots interpret, trigger responses, and communicate with other systems just like humans do. Robotic processes and intelligent automation tools can help businesses improve the effectiveness of services faster and at a lower cost than current methods.\\nThis book is the perfect start to your automation journey, with a special focus on one of the most popular RPA tools: UiPath.\\nLearning Robotic Process Automation takes you on a journey from understanding the basics of RPA to advanced implementation techniques. You will become oriented in the UiPath interface and learn about its workflow. Once you are familiar with the environment, we will get hands-on with automating different applications such as Excel, SAP, Windows and web applications, screen and web scraping, working with user events, as well as understanding exceptions and debugging. By the end of the book, you'll not only be able to build your first software bot, but also you'll wire it to perform various automation tasks with the help of best practices for bot deployment.\",\n",
              " \"Key Words with Peter and Jane uses the most frequently met words in the English language as a starting point for learning to read successfully and confidently. The Key Words reading scheme is scientifically researched and world renowned. Book 6c provides the link with writing for the words used in Readers 6a and 6b. Once this book has been completed, the child moves on to book 7a. The Key Words with Peter and Jane books work because each of the key words is introduced gradually and repeated frequently. This builds confidence in children when they recognise these key words on sight (also known as the 'look and say' method of learning). Examples of key words are: the, one, two, he. There are 12 levels, each with 3 books: a, b, and c. Series a: Gradually introduces new words. Series b: Provides further practise of words featured in the 'a' series. Series c: Links reading with writing and phonics. All the words that have been introduced in each 'a' and 'b' book are also reinforced in the 'c' books.\",\n",
              " 'Excel at Excel with the help of this bestselling spreadsheet guide John Walkenbach\\'s name is synonymous with excellence in computer books that decipher the complexities of Microsoft Excel. Known as \"Mr. Spreadsheet,\" Walkenbach shows you how to maximize the power of Excel 2013 while bringing you up to speed on the latest features. This perennial bestseller is fully updated to cover all the new features of Excel 2013, including how to navigate the user interface, take advantage of various file formats, master formulas, analyze data with PivotTables, and more. Whether you\\'re an Excel beginner who is looking to get more savvy or an advanced user looking to become a power user, this latest edition provides you with comprehensive coverage as well as helpful tips, tricks, and techniques that you won\\'t find anywhere else. * Shares the invaluable insight of Excel guru and bestselling author \"Mr. Spreadsheet\" John Walkenbach as he guides you through every aspect of Excel 2013 * Provides essential coverage of all the newest features of Excel 2013 * Presents material in a clear, concise, logical format that is ideal for all levels of Excel experience * Features a website that includes downloadable templates and worksheets from the book Chart your path to fantastic formulas and stellar spreadsheets with Excel 2013 Bible!',\n",
              " 'The town of Badami, the nearby villages of Aihole and Pattadakal and the pilgrimage site of Mahakuta, in the Malprabha valley of central Karnataka, are celebrated for their magnificent rock-cut shrines and structural temples. These Hindu and Jain monuments are associated with the Early Chalukyas who reigned over this part of the Deccan during the 6th–8th centuries. Together with a profusion of magnificent sculptures, mostly found in situ, these shrines and temples may be considered among the earliest, best preserved vestiges of temple art in India.\\nThis guidebook, the first ever for the Badami region, is authored by a scholar whose PhD was on Early Chalukya architecture. The text is illustrated with regional and town maps, building plans and more than 130 splendid coloured photographs.',\n",
              " \"Teach Your Kids to Code is a parent's and teacher's guide to teaching kids basic programming and problem solving using Python, the powerful language used in college courses and by tech companies like Google and IBM.\\nStep-by-step explanations will have kids learning computational thinking right away, while visual and game-oriented examples hold their attention. Friendly introductions to fundamental programming concepts such as variables, loops, and functions will help even the youngest programmers build the skills they need to make their own cool games and applications.\\nWhether you've been coding for years or have never programmed anything at all, Teach Your Kids to Code will help you show your young programmer how to:\\nExplore geometry by drawing colorful shapes with Turtle graphics\\nWrite programs to encode and decode messages, play Rock-Paper-Scissors, and calculate how tall someone is in Ping-Pong balls\\nCreate fun, playable games like War, Yahtzee, and Pong\\nAdd interactivity, animation, and sound to their apps\\nTeach Your Kids to Code is the perfect companion to any introductory programming class or after-school meet-up, or simply your educational efforts at home. Spend some fun, productive afternoons at the computer with your kids—you can all learn something!\",\n",
              " \"THE TIMES BESTSELLER\\n'The most perceptive book I have ever read about the female interior' DOLLY ALDERTON\\n'It is so good! It is so honest about being a woman and all the things that are expected of us ... I enjoyed it very much.' MARIAN KEYES\\nAS DISCUSSED ON THE SARA COX SHOW AND BBC WOMAN'S HOUR\\nA STYLIST MUST READ BOOK OF 2018\\nA RED MAGAZINE BEST BOOK OF 2018\\n*****\\n'Turning thirty is like playing musical chairs. The music stops, and everyone just marries whoever they happen to be sitting on.'\\nWho the f*ck is Tori Bailey?\\nThere's no doubt that Tori is winning the game of life. She's inspired millions of women to stick two fingers up at convention with her bestselling memoir, and she has the perfect relationship to boot.\\nBut Tori Bailey has been living a lie.\\nEveryone around her is getting married and having babies, but her long-term boyfriend won't even talk about getting engaged. And when her best friend Dee - her plus one, the only person who understands the madness - falls in love, suddenly Tori's in terrifying danger of being left behind.\\nWhen the world tells you to be one thing and turning thirty brings with it a loud ticking clock, it takes courage to walk your own path.\\nIt's time for Tori to practise what she's preached, but the question is: is she brave enough?\\n*****\\n\\nThe debut adult novel by bestselling author Holly Bourne is a blisteringly funny, honest and moving exploration of love, friendship and navigating the emotional rollercoaster of your thirties.\\nEveryone is raving about this book!\\n'Truly a f***ing good novel' EVENING STANDARD\\n'Relatable for any woman navigating emotional time bombs' RED\\n'Bourne holds a mirror up to contemporary aspiration, deftly probing the dissonant dualism between the shinier selves projected online and people's offline reality.' SUNDAY TIMES\\n\\n'Identifiable, heart-breaking and wickedly funny. I'd say this is one of my favourite books of the year' GIOVANNA FLETCHER\\n'Funny, sad, honest, insightful, up-to-the-minute. Kept me guessing (and hoping) to the last page' ROISIN MEANEY\\n'Sure to resonate with anyone navigating the emotional minefield of their thirties' - RED ONLINE\\n'Smart, witty and perceptive. Razor-sharp on friendship, self-image and self-deception' LUCY DIAMOND\\n\\n'Sublime, perfectly observed' LAURA JANE WILLIAMS\\n'Bourne incinerates the lies we're all capable of telling ourselves in this raw and very funny book; it'll resonate with anyone trying to convince themselves that sticking it out is better than being alone.' EMERALD STREET\\n'Funny, real and heartbreaking. I haven't been this obsessed with a book in years.' LUCY VINE\\n'Injected with such reality it can't help but be hilarious' TIMES\\n'Well-written, genuinely funny and movingly honest. It could be a life-changing read for many.' - SHEERLUXE\",\n",
              " 'When psychiatrist and past-life therapist Dr. Brian Weiss began treating two new patients, Elizabeth and Pedro, he found that although they were strangers, they described the same past lives with a stunning similarity of detail and emotion. Their joint past unfolded quite separately in his office; they seemed to have loved each other across time. In this gripping book internationally bestselling author Dr. Brian Weiss tells the extraordinary story of their love and explains that each of us has a soulmate whom we have loved in past incarnations, and who waits to reunite with us now.',\n",
              " \"Target 3 Billion: Innovative Solutions Towards Sustainable Development talks about the 3 billion people across the globe who live in villages and are often deprived of basic resources. The authors explain how the global model of development has failed to eradicate poverty.\\nIndia's 750 million people living in villages constitutes the world's largest rural population. Target 3 Billion: Innovative Solutions Towards Sustainable Development integrates the challenges and opportunities of the present human civilization. It elaborates on providing Urban Amenities in Rural Areas (PURA), a sustainable and environment friendly system that will uplift the rural masses. Instead of relying on government subsidies, Dr. Kalam says that entrepreneurship with community participation can empower villagers. PURA is a blend of people, technology, entrepreneurial spirit, traditions and skills. Readers will come across many examples throughout the book which demonstrate how PURA can change the lives of millions. Some of these include Seed Club at Chitrakoot Pura and Warana Cooperative Sugar Factory.\\nTarget 3 Billion: Innovative Solutions Towards Sustainable Development explains how governments can use this system for the benefit of the rural masses. Some of the chapters in this book include The Other Half of Mankind, Effecting A Social Transformation and Agriculture and PURA. The authors have posed the question, what can I do to empower 3 billion? The answers have been provided from the different perspectives of citizens, students and senior citizens. This book was published by Penguin India in 2011, is available in paperback.\",\n",
              " '#1 NEW YORK TIMES BESTSELLER • NAMED ONE OF THE BEST BOOKS OF THE YEAR BY BOOKLIST\\n\\nIn her now classic novel Outlander, Diana Gabaldon told the story of Claire Randall, an English ex-combat nurse who walks through a stone circle in the Scottish Highlands in 1946, and disappears . . . into 1743. The story unfolded from there in seven bestselling novels, and CNN has called it “a grand adventure written on a canvas that probes the heart, weighs the soul and measures the human spirit across [centuries].” Now the story continues in Written in My Own Heart’s Blood.\\n \\n1778: France declares war on Great Britain, the British army leaves Philadelphia, and George Washington’s troops leave Valley Forge in pursuit. At this moment, Jamie Fraser returns from a presumed watery grave to discover that his best friend has married his wife, his illegitimate son has discovered (to his horror) who his father really is, and his beloved nephew, Ian, wants to marry a Quaker. Meanwhile, Jamie’s wife, Claire, and his sister, Jenny, are busy picking up the pieces.\\n \\nThe Frasers can only be thankful that their daughter Brianna and her family are safe in twentieth-century Scotland. Or not. In fact, Brianna is  searching for her own son, who was kidnapped by a man determined to learn her family’s secrets. Her husband, Roger, has ventured into the past in search of the missing boy . . . never suspecting that the object of his quest has not left the present. Now, with Roger out of the way, the kidnapper can focus on his true target: Brianna herself.\\n \\nWritten in My Own Heart’s Blood is the brilliant next chapter in a masterpiece of the imagination unlike any other.\\n\\nPraise for Written in My Own Heart’s Blood\\n \\n“[Written in My Own Heart’s Blood] features all the passion and swashbuckling that fans of this historical fantasy series have come to expect.”—People\\n \\n“Another breakneck, rip-roaring, oh-so-addictive page-turner from Gabaldon . . . Take a deep breath, jump aboard, and enjoy the ride.”—Library Journal\\n\\n“With her Outlander series, Gabaldon . . . successfully [juggles] a sizable and captivating cast of characters; developing thrilling plotlines that borrow equally from adventure, history, and romance; and meticulously integrating a wealth of fascinating period details into the story without slowing down the pace. The result is a sprawling and enthralling saga that is guaranteed to keep readers up long past their bedtimes.”—Booklist (starred review)\\n\\n\\nFrom the Hardcover edition.',\n",
              " \"Timeless Piece of Literature\\nThey say, 'the more you write personal, the more it becomes universal'. This is evident in this collection of letters that Jawaharlal Nehru sent to his daughter Indira when she was 10 years old. This book, 'Letters from a Father to His Daughter' is a collection of 30 letters sent in the year 1928 which has become a phenomenal piece of literature over the years because it puts a lot of light on the bond between a father and his daughter and the many things that Pandit Nehru tried to explain to her while being away on business. Originally written in English, these letters are still relevant over a span of ninety years and that is exactly where their beauty lies.\\nDiverse Letters\\nThe collection of these letters spans over a large range and has many topics covered in it. Nehru began writing to his 10-year-old daughter when she was in Mussourie and he was in Allahabad. In order to tell his daughter about all the wonderful things in the world and his many thoughts on people and the society, he wrote her diverse letters that initially spoke of how the earth was formed, how animal and human life came into existence and how societies entered the fray. Nehru further also writes about races, faith and beliefs that existed in those times. The letters cover a major portion of human history and the history of India as well.\\nHardcover Book\\nThis is one of the finest books published by Penguin India. The best thing about this book is that since it has stood the test of time and seems relevant ninety years later, it sure is something that can be handed down from one generation to another. For these reasons, Penguin has compiled the book in a rather solid manner with a hardcover.\\nAbout the Author\\nJawaharlal Nehru was the first Prime Minister of Independent India. Born on November 14, 1889, he was one of the most prominent political figures when India was at the peak of its struggle for freedom. Nehru was an alumnus of the Trinity College, Cambridge. He died on May 27, 1964.\\nThe book is available online for convenient shopping. You can bag this book from Amazon.in today by following a few easy steps.\",\n",
              " 'Spielberg makes his audience feel something, whether he\\x92s shooting a kids\\x92 \\xadadventure, a dramatic chase, or the darkest war scene. The auteur always employs a core set of techniques that make each shot crystal clear and evoke the most intense emotions from the audience. This book shows you how. From tension to tearjerker, these moves will make your scenes memorable enough to be talked about for years to come.Spielberg directs films that cover everything from childhood dreams to the horrors of war. He always hones in on the emotional center of a scene. This book unravels the secrets of his core techniques, and shows how you can use the same simple camera moves and setups to make your films full of wonder, thrills, and emotion.',\n",
              " \"HarperCollins is proud to present its new range of best-loved, essential classics.\\n'Phileas Fogg was one of those mathematically exact people, who, never hurried and always ready, are economical of their steps and their motions. He never made one stride too many, always going by the shortest route. He did not give an idle look. He did not allow himself a superfluous gesture.'\\nWhen Phileas Fogg wagers a bet that he can travel across the globe in just 80 days, little does he know about the epic journey that he is about to undertake. With his faithful French servant, Passepartout, Phileas Fogg embarks on the adventure of a lifetime, travelling across four continents by whatever means he can - train, elephant, steam ship - and experiencing endless surprises and mishaps along the way.\",\n",
              " 'A delectable offering from a writer who not only knows how to make us laugh but also knows how to laugh at himself. Playful tigers, ‘ghosts’, elephants, crows and old favoritess like Uncle Ken, Miss Bun, the author’s slightly eccentric grandfather and Bond himself weave in and out of the pages of this wildly eclectic, thoroughly delightful and absolutely irresistible anthology featuring previously unpublished pieces like ‘Respect Your Breakfast’ and ‘Uncle Ken Goes to Sea’ as well as beloved classics from Bond’s books. Marked by the signature charm and subtle wit of one of India’s best-loved writers, Ruskin Bond’s Book of humor, will make even the hardened among us crack a smile.',\n",
              " \"A software engineer goes missing in the Baltic.\\n\\nAn unscrupulous stockbroker is audaciously murdered in a crowded bar room.\\n\\nA hotshot CEO is accused of insider trading.\\n\\nWhen Shashi Kurva, self-made man and one of the country's most successful CEOs, is named in an insider trading scandal, he is stunned and utterly blindsided. In an attempt to prove his innocence and identify the real culprit, he stumbles upon a conspiracy that has far-reaching consequences for him, both at work and at home, as he realizes that his wife’s family is not what it seems.\\nMeanwhile, pursuing her husband’s killers, the newly widowed Chaaya begins to experience their ruthlessness first-hand, and is forced to make the most difficult choice of her life.\\nRacing between the boardroom, a stock broking firm and a shattered family, Insider is a tale of duplicity and avarice, manipulation and murder, that takes you into the murky depths of the Indian stock market, where profit is the only object, and money the only language.\",\n",
              " 'In his second collection, including the iconic and much-referenced title story featured in the Academy Award-winning film Birdman, Carver establishes his reputation as one of the most celebrated short-story writers in American literature—a haunting meditation on love, loss, and companionship, and finding one’s way through the dark.',\n",
              " \"Frank Miller's Sin City has set the gold standard for crime comics, both for Miller's unflinching stories and for his visceral, powerfully charged art. To honor the artist and his groundbreaking work, Dark Horse is proud to return Frank Miller: The Art of Sin City to print, now in an affordable softcover edition. An astonishing look into a master's process, containing pieces both published and unpublished, and featuring items ranging from preliminary sketches to promotional pieces, this beautiful artistic showcase holds everything a Sin City fan, or connoisseur of fine art.\",\n",
              " \"The quick and easy guide to improving your memory using simple memory techniques by the bestselling author of How to Become a Human Calculator?\\nCan we really memorize anything? The answer is, ‘Yes we can!' From Guinness World Record holders Aditi Singhal and Sudhir Singhal comes a book that will serve as a manual to explore the immense power of your memory. It will:\\nExplain concepts with simple illustrations\\nWhile teaching you memory techniques, it will also discuss their application in real life, like memorizing appointments, presentations, names and faces, long answers, spellings, formulae, vocabulary, foreign languages and general information.\\nGive the scientific interpretation of ancient memory-enhancing practices that will be particularly useful for professionals as well as the common man.\\nFollowing the unparalleled success of How to Become a Human Calculator, Aditi Singhal and Sudhir Singhal turn their hands to helping you master the right method to input any information using which you can easily memorize anything.\",\n",
              " \"Immediately upon completing his DPhil degree, young Mukund Rajan came back to India and joined the Tata group as Ratan Tata's executive assistant. Over the next twenty-three years, as he worked closely with Ratan Tata, he got an inside view of the ups and downs, the controversies and achievements of the Tata group. In this book, his memoirs, he talks of what really went on during those turbulent times and how the Tatas pulled through each of these situations. Along with that, this book offers a close portrait of the enigmatic Ratan Tata from his longest-serving executive assistant. The Brand Custodian is a study of the Tata group's evolution and explains the relevance of the conglomerate to the world we live in.\",\n",
              " 'NEW YORK TIMES BESTSELLER\\n\\nGOODREADS CHOICE AWARD WINNER FOR BEST MEMOIR/AUTOBIOGRAPHY\\n\\nFORBES TOP 5 BREAKTHROUGH BOOK OF 2015\\n\\nIn this intimate memoir of life beyond the camera, Connor Franta shares the lessons he has learned on his journey from small-town boy to Internet sensation—so far. Here, Connor offers a look at his Midwestern upbringing as one of four children in the home and one of five in the classroom; his struggles with identity, body image, and sexuality in his teen years; and his decision to finally pursue his creative and artistic passions in his early twenties, setting up his thrilling career as a YouTube personality, philanthropist, entrepreneur, and tastemaker.\\n\\nExploring his past with insight and humor, his present with humility, and his future with hope, Connor reveals his private struggles while providing heartfelt words of wisdom for young adults. His words will resonate with anyone coming of age in the digital era, but at the core is a timeless message for people of all ages: don’t be afraid to be yourself and to go after what you truly want.\\n\\nThis full-color collection includes photography and childhood clippings provided by Connor and is a must-have for anyone inspired by his journey.',\n",
              " 'In his lifetime and since his death, Paramhansa Yogananda has inspired thousands of people worldwide to adopt a spiritual lifestyle. He brought yoga to the West, where he established churches, spending nearly three decades teaching yoga and meditation. Autobiography of a Yogi tells the story of Yogananda’s search for spiritual enlightenment, his relationship with his guru and his teachings in Kriya Yoga. The book gives a compelling insight into the spiritual leader as he takes us on his journey from his early childhood in Gorakhpur to California in the 1940s. Along the way, we are introduced to the men and women who had a lasting impact on his life – and those whose lives he changed forever.',\n",
              " \"Happy City is the story of how the solutions to this century's problems lie in unlocking the secrets to great city living\\nThis is going to be the century of the city. But what actually makes a good city? Why are some cities a joy to live in?\\nAs Charles Montgomery reveals, it's not how much money your neighbours earn, or how pleasant the climate is that makes the most difference. Journeying to dozens of cities - from Atlanta to Bogotá to Vancouver - he talks to the new champions of the happy city to explore the urban innovations already transforming people's lives. He meets the visionary Colombian mayor who turned some of the world's most dangerous roads into an urban cycling haven; the Danish architect who brought the lessons of medieval Tuscan towns to modern-day Copenhagen; and the New York City transport commissioner who turned the gridlock of Times Square into a place to lounge in the sun.\\nDrawing on the lessons from their stories, from brain science, and from the fascinating realm of urban experimentation, Happy City offers solutions we can all use to improve our livesandshows that simple changes can make all the difference.\\n'Do we live in neighbourhoods that make us happy? Montgomery encourages us to ask without embarrassment, and to think intelligently about the answer' The New York Times Book Review\\n'Excellent . . . Montgomery believes in the importance of smart town planning and Happy City is a compendium of its major ideas' Will Dean, Independent\\nCharles Montgomery is a journalist and urban experimentalist from Vancouver, Canada. His writings on urban planning, psychology, culture, and history have appeared in magazines and journals on three continents. He is the author of one previous book, and was an original member of the BMW Guggenheim Lab.\",\n",
              " 'Edith Drummond owes her life to Niels Buchanan and his brothers. Waking after an illness to a castle overrun by rugged Highlanders is disconcerting, but so is learning that she’s slowly being poisoned. Niels insists on staying by her side, and Edith soon discovers that even more dangerous is her wild attraction to the fierce warrior.\\n\\nNiels has never met a more courageous―or enticing―woman than Lady Edith. The idea of such a bonny lass being forced to enter a nunnery is more than any red-blooded Scotsman could bear. He’ll gladly marry her himself. But while sweeping her off her feet is easy, it’ll take all his skill to defeat her family’s relentless enemies, and convince her to surrender to his sweet embrace. . . .',\n",
              " \"This volume combines six books by celebrated artist and lecturer George A. Bridgman, who taught figure and anatomy drawing at New York City's Art Students League. This fifth edition preserves Bridgman's comprehensive lessons and original sketches - featuring more than 1000 drawings and 22 expert lessons on mastering the human figure from every angle - but repackages it with a new cover that distinguishes it from the competition.\",\n",
              " \"The Queen of the Dorks is back in a brand new instalment of the internationally bestselling series! After a bump on the head, Nikki has a wild dream in which she, her BFF's Chloe and Zoey, her crush Brandon, and mean girl Mackenzie all end up playing the roles of some familiar classic fairy tale characters. Will Nikki's dream turn into a nightmare? The eighth book in the bestselling Dork Diaries series by Rachel Renee Russell, author of The Misadventures of Max Crumbly (publishing June 2016).\",\n",
              " 'One month after Akihiko Kayaba\\'s game of death began, the death toll continues to rise, two thousand players having already lost their lives to the ultra-difficult VRMMO world of Sword Art Online. On the day of the strategy meeting to plan out the first-floor boss battle, Kirito, a solo player who vows to fight alone to get stronger, runs into a rare, high-level female player. She gracefully dispatches powerful monsters with a single rapier that flashes like a shooting star in the night...\\nThis volume contains three stories, including \"Aria of a Starless Night,\" which details how Kirito came to be called the Black Swordsman, and \"Rondo of a Fragile Blade,\" the tragic tale of a young blacksmith that takes place before the second-floor boss fight.',\n",
              " \"READ THE INSPIRATION BEHIND THE THRILLING AMAZON PRIME SERIES JACK RYAN . . .\\nThe clock is ticking - and President Jack Ryan is running out of time...\\nA U.S. Navy Commander is attacked in a crowded restaurant while on leave. How did the assassin know his exact location?\\nIn Tehran airport, a Canadian businessman is taken away to be interrogated. A CIA operative for over a decade - why has his cover now been blown?\\nA massive information breach has compromised America's intelligence agencies and around the globe, the noose is tightening.\\nAnd the only man who can stop it, is the one closest to the danger: President Jack Ryan.\\n\\nPraise for Tom Clancy:\\n\\n'Constantly taps the current world situation for its imminent dangers and spins them into an engrossing tale' New York Times\\n\\n'Heart-stopping action . . . entertaining and eminently topical' Washington Post\\n'A virtuoso display of page-turning talent' Sunday Express\\n'Theres hardly another thriller writer alive who can fuel an adrenaline surge the way Clancy can' Daily Mail\",\n",
              " \"The Guardian, Daily Telegraph and Irish Times Book of the Year.\\n\\n'Something we've not seen before in contemporary crime fiction' GUARDIAN\\n'[An] uncomfortably close-to-home thriller' SUNDAY TIMES CRIME CLUB\\n'As intellectually stimulating as it is gripping' DAILY TELEGRAPH, BOOKS OF THE YEAR 2018\\n'Takes you right into the heart of darkness' MAIL ON SUNDAY\\n'A must-have new read' DAILY EXPRESS\\n'Wonderfully sinister' THE OBSERVER\\n'Frightening' THE TIMES\\n'Addictive' INDEPENDENT\\n'Terrific' JOANNE HARRIS\\n'Brilliantly done' FIONA BARTON\\n'A great achievement' HERMAN KOCH\\n'Claustrophobic and unsettling' BBC NEWS\\n'[A] creepy tale of obsession' SUNDAY MIRROR\\n'An unsettling tale of merciless self-scrutiny' RENEE KNIGHT\\n'A terrifying study of a family threatened by the tenant living downstairs' WOMAN & HOME\\n*********\\nHow far would you go to protect your family?\\nFamily is everything. So what if yours was being terrorised by a neighbour - a man who doesn't listen to reason, whose actions become more erratic and sinister with each passing day?\\nYou go to the police, but they can't help you. You become afraid to leave your family at home alone. But there's nothing more you can do to protect them.\\nOr is there...?\\n\\nFEAR is a brilliantly grippling, original psychological thriller - for fans of THE WOMAN IN THE WINDOW, ANATOMY OF A SCANDAL and THE DINNER.\\n\\n-------------------------\\nFEAR is translated from the German by Imogen Taylor\",\n",
              " 'THE SUNDAY TIMES BESTSELLER\\n\\nA dazzling new collection of short stories from the beloved, internationally acclaimed Haruki Murakami\\n\\nAcross seven tales, Haruki Murakami brings his powers of observation to bear on the lives of men who, in their own ways, find themselves alone. Here are vanishing cats and smoky bars, lonely hearts and mysterious women, baseball and the Beatles, woven together to tell stories that speak to us all.\\n\\nMarked by the same wry humor that has defined his entire body of work, in this collection Murakami has crafted another contemporary classic.\\n\\n‘Supremely enjoyable, philosophical and pitch-perfect new collection of short stories…Murakami has a marvelous understanding of youth and age’ Observer\\n\\n‘Murakami at his whimsical, romantic best’ Financial Times',\n",
              " 'Read stories from the continent of Africa. Meet the Zebra, learn about Anansi the spider and follow Masai Mara. Simple, funny and sometimes with a message for the reader, they will open your eyes to a different world.',\n",
              " \"Who really creates wealth in our world and how do we decide the value of what they do. In modern capitalism, value-extraction the siphoning off of profits, from shareholders' dividends to bankers' bonuses is rewarded more highly than value-creation, the productive process that drives a healthy economy and society. We misidentify takers as makers and have lost sight of what value really means. Yet, argues Mariana Mazzucato in this penetrating and passionate new book, if we are to reform capitalism we urgently need to rethink where wealth comes from. Who is creating it, who is extracting it and who is destroying it? the value of everything will reignite a long-needed debate about the kind of world we really want to live in.\",\n",
              " \"The Worldwide Number One Bestseller Wilbur Smith returns to Ancient Egypt in a captivating new novel that will transport you to extraordinary times.\\n\\n'Egypt is Under Attack!'\\n\\nPharaoh Tamose lies mortally wounded. The ancient city of Luxor is surrounded, All seems lost.\\n\\nTaita prepares for the enemy’s final, fatal push. The ex-slave, now general of Tamose’s armies, is never more ingenious than when all hope is dashed. And this is Egypt’s most desperate hour.\\n\\nWith the timely arrival of an old ally, the tide is turned and the Egyptian army feasts upon its retreating foe. But upon his victorious return to Luxor, Taita is seized and branded a traitor. Tamose is dead and a poisonous new era has begun. The new Pharaoh has risen.\\n\\nPharaoh Utteric is young, weak and cruel and threatened by Taita’s influence within the palace – especially his friendship with Utteric’s younger and worthier brother, Ramases. With Taita’s imprisonment, Ramases is forced to make a choice: help Taita escape and forsake his brother or remain silent and condone Utteric’s tyranny. To a good man like Ramases, there is no choice. Taita must be set free, Utteric must be stopped and Egypt must be reclaimed.\\n\\nFrom the glittering temples of Luxor to the Citadel of Sparta, PHARAOH is an intense and powerful novel magnificently transporting you to a time of threat, blood and glory. Master storyteller, Wilbur Smith, is at the very peak of his powers.\",\n",
              " \"Winner of the 2019 Hindu Young World-Goodbooks Award for Best Book (Non-Fiction)\\nEmbark on a vivid journey on which you'll learn about the origins and evolution of art in the country. Prepare to be amazed by the first pictures made by early humans; reflect upon the serenity of Buddhist cave paintings at Ajanta; marvel at the splendour of Mughal miniature art; delight in the religious depictions of Tanjore; study the hybrid Company and revivalist Bengal styles; and discover the best of modern and contemporary artists.\\nWatch it all come alive in intricate black-and-white sketches and stunning photographs of the most celebrated visuals across time.\\nA rich primer on the different schools of art and the most significant movements in Indian art history, A Brush with Indian Art might even nudge you into propping up your first canvas!\",\n",
              " \"An innocent man is days from execution. Only a guilty man can save him.\\n\\nTravis Boyette is a murderer. In 1998, in the small East Texas city of Sloan, he abducted, raped, and strangled a popular high-school cheerleader. He buried her body where it would never be found, then watched and waited as police and prosecutors arrested Donte Drumm, a black local football star with no connection to the crime. Tried, convicted and sentenced, Drumm was sent to death row.\\n\\nNine years later, Donte Drumm is four days from execution. Over 400 miles away in Kansas, Travis also faecs death, suffering from an inoperable brain tumour. At long last, he decides to do what's right. After years of silence, he is ready to confess.\\n\\nBut the law doesn't want to hear it. As far as they're concerned, they've got their man.\\n\\nSo how can a guilty man convince lawyers, judges and politicians that the man they're about to execute is innocent?\",\n",
              " 'Hit Refresh is about individual change, about the transformation happening inside of Microsoft and the technology that will soon impact all of our lives—the arrival of the most exciting and disruptive wave of technology humankind has experienced: artificial intelligence, mixed reality, and quantum computing. It’s about how people, organizations, and societies can and must transform and “hit refresh” in their persistent quest for new energy, new ideas, and continued relevance and renewal.\\nMicrosoft’s CEO tells the inside story of the company’s continuing transformation, tracing his own personal journey from a childhood in India to leading some of the most significant technological changes in the digital era. Satya Nadella explores a fascinating childhood before immigrating to the U.S. and how he learned to lead along the way. He then shares his meditations as a sitting CEO—one who is mostly unknown following the brainy Bill Gates and energetic Steve Ballmer. He tells the inside story of how a company rediscovered its soul—transforming everything from culture to their fiercely competitive landscape and industry partnerships. As much a humanist as engineer and executive, Nadella concludes with his vision for the coming wave of technology and by exploring the potential impact to society and delivering call to action for world leaders.\\n“Ideas excite me,” Nadella explains. “Empathy grounds and centers me.” Hit Refresh is a set of reflections, meditations, and recommendations presented as algorithms from a principled, deliberative leader searching for improvement—for himself, for a storied company, and for society.',\n",
              " '\"A man who has attained mastery of an art reveals it in his every action.\"--Samurai Maximum.\\n\\nUnder the guidance of such celebrated masters as Ed Parker and the immortal Bruce Lee, Joe Hyams vividly recounts his more than 25 years of experience in the martial arts. In his illuminating story, Hyams reveals to you how the daily application of Zen principles not only developed his physical expertise but gave him the mental discipline to control his personal problems-self-image, work pressure, competition. Indeed, mastering the spiritual goals in martial arts can dramatically alter the quality of your life-enriching your relationships with people, as well as helping you make use of all your abilities.',\n",
              " 'High jinx on the high seas! Buy three volumes of One Piece for the price of two!\\n\\nHigh jinx on the high seas! Buy three volumes of One Piece for the price of two!\\n\\nR to L (Japanese Style).\\n\\nFollow the beginning of Luffy’s search for the greatest treasure in the world… one guy alone in a rowboat, in search of the legendary \"One Piece.” Three volumes of One Piece for the price of two!  (Contains volumes 7,8,9).\\n\\nAs a child, Monkey D. Luffy dreamed of becoming the King of the Pirates. But his life changed when he accidentally gained the power to stretch like rubber…at the cost of never being able to swim again! Now Luffy, with the help of a motley collection of pirate wannabes, is setting off in search of the \"One Piece,\" said to be the greatest treasure in the world...',\n",
              " 'A Preposition is a word which shows relationship among other words in the sentence. The relationships include direction, place time, cause, manner and amount. A preposition comes before a noun or pronoun. A preposition phrase contains a preposition and object. Prepositional phrases are like idioms and are best learned through listening to and reading as much as possible, Little Red Book of Prepositions is a ready reference book with a check list of propositions.',\n",
              " 'The search for diamonds, a crucial scientific breakthrough and a mythical ruined city set off this adventure into the heart of the Congolese jungle.\\n\\nThe American expedition is led by Karen Ross, desperate to find her husband and recover the data he found before he disappeared. But there are other teams trying to get there first, and the way is strewn with life-threatening dangers -- plane crashes, civil wars and a dormant volcano awoken by dormant explosives.\\n\\nIn the tradition of Arthur Conan Doyle and H. Rider Haggard, Congo is a novel of high adventure from the master of the modern thriller.',\n",
              " 'Kargil, 1999. Two entire brigades of Pakistani army regulars infiltrated Indian territory and fortified themselves before the Indian army even realized they were there. The top army brass ignored warnings, downplayed the threat and the number of infiltrators till it was almost too late. They were also poorly prepared, operationally and in every other respect. Infantry soldiers were pushed up with inadequate maps, clothing and weapons and no information of either the enemy’s numbers or their weapon strength.\\nWith foreword by GL Batra, father of Capt. Vikram Batra, the kargil war hero and recipient of the highest gallantry award, Param Vir Chakra, this is the true story of Kargil as seen through the eyes of one of the front-line commanders. Written in the form of a diary, it offers the first really detailed and exclusive account of the events that led to the invasion and the subsequent battle to retake the peaks occupied by the intruders. Even after almost two decades, the book is still the most accurate account of the many Indian soldiers who laid down their lives in the line of duty.',\n",
              " \"A towering philosophical novel that is the summation of her Objectivist philosophy, Ayn Rand's Atlas Shrugged is the saga of the enigmatic John Galt, and his ambitious plan to 'stop the motor of the world', published in Penguin Modern Classics.\\nOpening with the enigmatic question 'Who is John Galt?', Atlas Shrugged envisions a world where the 'men of talent' - the great innovators, producers and creators - have mysteriously disappeared. With the US economy now faltering, businesswoman Dagny Taggart is struggling to get the transcontinental railroad up and running. For her John Galt is the enemy, but as she will learn, nothing in this situation is quite as it seems. Hugely influential and grand in scope, this story of a man who stopped the motor of the world expounds Rand's controversial philosophy of Objectivism, which champions competition, creativity and human greatness.\\nAyn Rand (1905-82), born Alisa Rosenbaum in St. Petersburg, Russia, emigrated to America with her family in January 1926, never to return to her native land. Her novel The Fountainhead was published in 1943 and eventually became a bestseller. Still occasionally working as a screenwriter, Rand moved to New York City in 1951 and published Atlas Shrugged in 1957. Her novels espoused what came to be called Objectivism, a philosophy that champions capitalism and the pre-eminence of the individual.\\nIf you enjoyed Atlas Shrugged, you might like Rand's The Fountainhead, also available in Penguin Modern Classics.\\n'A writer of great power ... she writes brilliantly, beautifully, bitterly'\\nThe New York Times\\n'Atlas Shrugged ... is a celebration of life and happiness'\\nAlan Greenspan\",\n",
              " \"Moab is My Washpot is in turns funny, shocking, tender, delicious, sad, lyrical, bruisingly frank and addictively readable.\\n\\nStephen Fry's bestselling memoir tells how, sent to a boarding school 200 miles from home at the age of seven, he survived beatings, misery, love, ecstasy, carnal violation, expulsion, imprisonment, criminal conviction, probation and catastrophe to emerge, at eighteen, ready to try and face the world in which he had always felt a stranger.\\n\\nFry writes with the wit to which we have become accustomed, but with shocking candour too. In an age of glossy celebrity autobiographies, Moab is My Washpot sets the high standard to which others should aspire.\",\n",
              " 'This is an competitive examination study material for RRB, Non Technical/Clerical Cadre etc.',\n",
              " 'ELT - Reference and Dictionaries.',\n",
              " 'Multi-million-selling Asterix is much loved across the world, and there is no better way to enjoy the antics of our indomitable hero and his friends than in this great value gift edition omnibus of the first three stories. Collect all of the Asterix omnibuses to build the fabulous artwork across their spines!\\nIn ASTERIX THE GAUL, we join Asterix, Obelix and co. as they try to defend one small village in Gaul from the mighty legionaries of Rome who surround them.\\nDisaster strikes the Gaulish village in ASTERIX AND THE GOLDEN SICKLE, as Getafix the druid breaks his golden sickle - which means no more magic potion...\\nIn ASTERIX AND THE GOTHS, Getafix is kidnapped by the Goths, so Asterix and Obelix have to ride to the rescue and save the day!\\nA perfect gift for both keen Asterix fans and those who have yet to join his hilarious adventures.',\n",
              " \"This basic book is designed as survival Korean language course book keeping in mind the needs of travelers, businessmen and students. This book is equally ideal for self learning and aims to familiarize the learners who plan to visit Korea or intend to interact with Koreans. The topics covered in this book ranges from formal to informal situations. A wide spectrum of the use of Korean language has been dealt with. The expressions and grammatical patterns used in this book are colloquial/conversational in nature which is frequently used by the native Korean speakers. An emphasis is laid on the direct usage of the sentences (oral communication) rather than complex grammatical explanations. It will help the beginners' level learners to memorize and use the frequently use sentences more effectively and much faster. For the of Korean vocabulary, grammar and pronunciation rules are also included in the book.\",\n",
              " 'This up-to-date, general purpose thesaurus offers over 300,000 synonyms and antonyms.\\n\\nIncludes a centre section containing thematic lists, for example of animals, games, and tools, designed to help you broaden your vocabulary, improve your general knowledge, and solve quizzes and puzzles.',\n",
              " 'Two years after vanishing into the Sudanese desert, the leader of a British archaeological expedition, Professor Harold McCabe, stumbles out of the sands, frantic and delirious, but he dies before he can tell his story. The mystery deepens when an autopsy uncovers that someone had begun to mummify the professor’s body—while he was still alive.\\n\\nWhen his remains are returned to London for further study, alarming news arrives from Egypt. The medical team who had performed the man’s autopsy has fallen ill with an unknown disease that is quickly spreading throughout Cairo. Fearing the\\nworst, a colleague of the professor reaches out to Painter Crowe, the director of Sigma Force. It appears that Professor McCabe had vanished into the desert while searching for proof of the ten plagues of Moses. As the pandemic grows, a disturbing\\nquestion arises.\\n\\nAre those plagues starting again?\\n\\nThen a mysterious group of assassins leaves behind a fiery wake of destruction and death, erasing all evidence. Sigma Force turns to the archaeologist’s daughter, Jane McCabe, for help. She discovers a puzzling connection to a shocking historical mystery that involves the travels of Mark Twain, the genius of Nikola Tesla and the adventures of explorer Henry Morton Stanley. Now Sigma Force must confront a danger that will unleash a cascading series of plagues, culminating in a scourge that could kill all of the world’s children . . . decimating humankind forever.',\n",
              " 'After years of study in Europe, the young narrator of Season of Migration to the North returns to his village along the Nile in the Sudan. It is the 1960s, and he is eager to make a contribution to the new postcolonial life of his country. Back home, he discovers a stranger among the familiar faces of childhood—the enigmatic Mustafa Sa’eed. Mustafa takes the young man into his confidence, telling him the story of his own years in London, of his brilliant career as an economist, and of the series of fraught and deadly relationships with European women that led to a terrible public reckoning and his return to his native land.\\n\\nBut what is the meaning of Mustafa’s shocking confession? Mustafa disappears without explanation, leaving the young man—whom he has asked to look after his wife—in an unsettled and violent no-man’s-land between Europe and Africa, tradition and innovation, holiness and defilement, and man and woman, from which no one will escape unaltered or unharmed.\\n\\nSeason of Migration to the North is a rich and sensual work of deep honesty and incandescent lyricism. In 2001 it was selected by a panel of Arab writers and critics as the most important Arab novel of the twentieth century.',\n",
              " 'Agatha Christie’s world-famous Miss Marple mystery\\n\\nIt’s seven in the morning. The Bantrys wake to find the body of a young woman in their library. She is wearing evening dress and heavy make-up, which is now smeared across her cheeks.\\n\\nBut who is she? How did she get there? And what is the connection with another dead girl, whose charred remains are later discovered in an abandoned quarry?\\n\\nThe respectable Bantrys invite Miss Marple to solve the mystery… before tongues start to wag.',\n",
              " \"Between 2000-2008 India's economic and political ascendancy were charted in the worlds press. This period of dynamism ushered in an increased sense of confidence, aspiration and pride in being Indian. This book is about the response of the design community to India's changing environment. It focuses on fashion, graphic and interior design as metaphors of the complex networks that constitute globalisation within key metropolitan centres of India. Examined within the context of their production and consumption, and through their economic, social, political and cultural underpinnings, a picture emerges which conveys how designers grapple with issues of identity and globalisation, nationhood and modernity, ethics and commerce.\",\n",
              " \"Saffron terrorism.\\nIs it a fact? Or, is this a myth? After all, do we know enough?\\nThe shocking blasts of Malegaon and Samjhauta were projected as 'saffron terrorism'. A new theory, terrorist attacks were tainted as such till a few years later, Kasab's confession offered solid proof of Pakistan's role in the 26/11 attacks. Though the police had concluded a Pakistani hand for the earlier blasts, it was saffron terrorism which prevented the perpetrators of these attacks from being brought to justice.\\nAs a theory, saffron terrorism is not just hurting Hindus sentiments but is also an obstacle to fight real terrorism sponsored by Pakistan and Islamic states. The term was coined by the erstwhile UPA government to garner minority votes and manipulate the vote bank. After all, why were the Malegaon-accused SIMI activists let off? Why did certain politicians declare not to oppose their bail? What was truly behind Aseemanand's confession? The reliability of these confessions was questionable given the police brutality that the National Investigative Agency exposed.\\nJournalist Praveen Tiwari explores saffron terrorism and reveals through exclusive interviews of senior National Investigative Agency officials, undercover agents and politicians how vote bank politics can compromise ethics and national security. Should the real masterminds behind the blasts be allowed to go scot-free? Should the manipulators of the Samjhauta Express bombings not be held accountable? Should we not investigate those who had exonerated Pakistan of its guilt? An extensive research on communal politics, the book offers indisputable evidence of the 'saffron terrorism' theory as the Great Indian Conspiracy.\",\n",
              " \"When Mitchell McDeere qualified third in his class at Harvard, offers poured in from every law firm in America. Bendini, Lambert and Locke were a small, well-respected firm, but their offer exceeded Mitch's wildest expectations: a fantastic salary, a new home and the keys to a brand new BMW.\\n\\nIt was his dream job – but it was to become his worst nightmare.\\nUnravelling a complex trail of secret files, undercover surveillance and millions of dollars of illegal mob money, Mitch stumbles across a shocking conspiracy and a horrifying truth: nobody has ever left Bendini, Lambert and Locke – and anybody who has ever tried has ended up dead.\",\n",
              " 'The manager everyone loves to hate\\x85 Mercurial Portuguese manager Jose Mourinho, who regards himself as football\\x92s equivalent of George Clooney, featured in his own blockbuster this summer when he took charge of Manchester United \\x96 the world\\x92s biggest club. The news sent shockwaves through the Old Trafford faithful \\x96 generations of whom have pledged their loyalty to a succession of managerial legends including Sir Matt Busby and Sir Alex Ferguson. At the very outset of what promises to be a tumultuous season for the Red Devils, Andrew J Kirby investigates in his latest book Jose Mourinho: The Art of Winning whether the latest controversial move by the club\\x92s owners is a marriage made in heaven or hell. Machiavellian schemer, marketing man\\x92s dream, inspirational leader and motivator, arrogant \\x93manager-lout\\x94, Super Coach. Anti-hero. Serial trophy-winner. Jose Mourinho\\x92s personality is a complex one. Jose Mourinho: The Art of Winning is an entertaining character study of the man who in many ways has come to define modern, elite-level football. Lionel Messi and Cristiano Ronaldo apart, the biggest, most marketable stars in world football today are the managers. The 2016-17 edition of the FA Premier League is being billed as the most exciting yet, and that\\x92s not because of the influx of star players after the European Championships in France. Though some eye-catching names have joined the league, none have the cache of a Messi or a Ronaldo. None have the standing of a Jose Mourinho or a Pep Guardiola either. And so it is on the manager\\x92s shoulders that the responsibility for delivering the customary Premier League spectacle falls. Incorporating interviews with fans (of United, and of Mourinho\\x92s former clubs), players (past and present), commentators and journalists, this book endeavours to determine what effect the Jose Mourinho show will have on Manchester United as a football club and the Premier League as a whole by tackling eight key questions including the following: Is Mourinho the best manager in the game?What is the management approach of Jose Mourinho? Can Manchester United tolerate the dark side of Jose Mourinho?What is the role of the modern day Premier League manager and how has this changed over the years?And:\\x93Once a Blue, always a Red?\\x94Can a manager who is so recognisably a Blue become a Red?Stuffed full of statistical detail and flavoured with a wistful view of a game, he sets the spectacular scene for a showdown of showdowns in this season\\x92s Premier League campaign where the ultimate prize for United will be the crucial return to the Champions League. With interviews with football writers, players and fans, this book is a must that every Manchester United fan \\x96 and indeed every football supporter \\x96 needs to read. Andrew J Kirby\\x92s sports-writing has featured in BBC Sport magazine and on the Radio Five Live website. He has held a Manchester United season ticket for over a quarter of a century and regularly follows the Reds across Europe and beyond. His other United books include, Fergie\\x92s Finest: Sir Alex Ferguson\\x92s First 11, The Pride of All Europe: Manchester United\\x92s Greatest Seasons in the European Cup and Louis van Gaal: Dutch Courage.PRAISE FOR FERGIE\\x92S FINESTA must read for any red. Scott the Red, Editor: Republik of Mancunia United BlogThe perfect gift for the red devil in your life! Sam Sharp, Manchester United fanBuy it now!Michael Hopkins, Manchester United fanPRAISE FOR THE PRIDE OF ALL EUROPEA very enjoyable trip down memory lane.Goodreads ReviewerThe book succeeds in encapsulating the hopes, dreams, disappointments and joys of so many different eras.Manchester United fan review on AmazonPRAISE FOR DUTCH COURAGEGreat research and insight make this a supreme football biography. Cool ReviewsExcellent insight into this complex character.Nigel Craig, Amazon Reviewer',\n",
              " 'A century before A GAME OF THRONES, two unlikely heroes wandered Westeros…\\nA KNIGHT OF THE SEVEN KINGDOMS compiles the first three official prequel novellas to George R.R. Martin’s ongoing masterwork, A SONG OF ICE AND FIRE.\\nAlmost a century before A Game of Thrones, two unlikely heroes wandered Westeros…\\nIn an age when the Targaryen line still holds the Iron Throne, and recollections of the last dragon have not yet passed from living memory, a naïve but courageous hedge knight, Ser Duncan the Tall, towers above his rivals – in stature if not experience.\\nTagging along with him is his diminutive squire, a boy called Egg, whose true identity must be kept hidden: for in reality he is Aegon Targaryen, and one day he will be king. Improbable heroes though they be, great destinies lie ahead for Dunk and Egg; as do powerful foes, royal intrigue, and outrageous exploits.',\n",
              " 'Alfred\\'s Basic Adult All-in-One Course is a greatly expanded version of Alfred\\'s Basic Adult Piano Course that includes lesson, theory, and technique in a convenient, \"all-in-one\" format. This comprehensive course adds such features as isometric hand exercises, finger strengthening drills, and written assignments that reinforce each lesson\\'s concepts. The accompanying DVD includes introductions to the lesson material in the book and performances of most of the pieces by well-known teacher, Gayle Kowalchyk. Book titles include: Alouette * Alpine Melody * Amazing Grace * Au Claire de la Lune * Aunt Rhody * Auld Lang Syne * Aura Lee * The Bandleader * Beautiful Brown Eyes * Blow the Man Down! * Blues for Wynton Marsalis * Brother John * Café Vienna * The Can-Can * Chasing the Blues Away * Chiapanecas * Cockles and Mussels * The Cuckoo * Day is Done * Dueling Harmonics * The Entertainer * A Friend Like You * Go Down, Moses * Good King Wenceslas * Good Morning to You! * Good People * Got Those Blues * Greensleeves * Happy Birthday to You! * Harmonica Rock * Harp Song * Here\\'s a Happy Song! * He\\'s Got the Whole World in His Hands * I\\'m Gonna Lay My Burden Down * Jericho * Jingle Bells * Joy to the World! * Kum-ba-yah! * Largo (Dvorak) * Lavender\\'s Blue * Lightly Row * Little Brown Jug * Liza Jane * London Bridge * Lone Star Waltz * Love Somebody * Lullaby * The Marine\\'s Hymn * Mary Ann * Merrily We Roll Along * Mexican Hat Dance * Michael, Row the Boat Ashore * Money Can\\'t Buy Everything * My Fifth * Ode to Joy * On Top of Old Smoky * O Sole Mio * Raisins and Almonds * Rock Along * Rockets * Rockin\\' Intervals * Rock It Away! * Scarborough Fair * Shoo, Fly, Shoo! * Skip to My Lou! * Standing in the Need of Prayer * The Stranger * Tisket, a Tasket * Waltzing Chords * Waltz Time * What Can I Share * When the Saints Go Marching In * Why Am I Blue?',\n",
              " 'What do sporting champions do, what makes winning teams, who is a good leader, why do only some teams keep winning while others win only for a while and then lose… Two IIMA alumni, Sports commentator and writer Harsha Bhogle and advertising and communication consultant Anita Bhogle dig into examples from sport to see how they can benefit managers. Contrary to popular perception ability is not a major distinguishing factor in success, especially as the level of competition increases. But if you combine your ability with the right attitude and the passion to excel, you too can become the best that you can be. That is the universal formula for winning that The Winning Way explores. For Anita and Harsha Bhogle, this book marks the completion of 300 successful corporate workshops of The Winning Way that they run.',\n",
              " \"The legend. In his own words.\\n\\nFrom the poverty-stricken streets of Sao Paulo to an international icon and one of the most celebrated footballers of all time, Pele's life story is as extraordinary as it is enrapturing. With his trademark wit and deference, the legend draws us into a wonderful story lit by insight and humour and encompassing everything you ever wanted to know about the great man himself.\\n\\nFrom shining shoes for extra pennies at the Baru Athletic Club to triumph in several World Cups, the glory of being on top of the world -- and staying there -- is shared in what is undoubtedly one of the must-read autobiographies of the year. On top of his athletic achievements, Pele has also been a staunch campaigner for human rights and in particular the plight of street children in his home country, leading to an appointment as a UN Ambassador and an honorary knighthood from the British monarchy.\\n\\nBy turns addictive, moving and enlightening, this is the ultimate story of the rise of a star and an amazing testimony to how even the lowliest of society's people can reach the dizzying heights of worldwide adoration and success.\",\n",
              " \"He's back . . . Private investigator Harry Bosch confronts a villain who's long been in hiding - a fiend known as The Poet.\\nFormer FBI agent Rachel Walling is working a dead-end stint in South Dakota when she gets the call she's been dreading for four years. The Poet is back. And he has not forgotten Rachel. He has a special present for her.\\nHarry Bosch is adjusting to life in Las Vegas as a private investigator and a new father. He gets a call, too, from the widow of a friend who died recently. Previously in his FBI career, the friend worked on the famous case tracking the killer known as The Poet. This fact alone makes some of the elements of his death doubly suspicious.\\nAnd Harry Bosch is heading straight into the path of the most ruthless and inventive murderer he has ever encountered. . .\",\n",
              " \"David Baldacci's heart-stopping Hour Game is the second fast-paced thriller in the King and Maxwell series. Following their collaboration in Split Second, ex-Secret Service agents Sean King and Michelle Maxwell have gone into partnership and are investigating the robbery of some secret documents at the residence of the incredibly wealthy Battle family. It seems like a straightforward case of domestic burglary, but soon they begin to suspect links to larger, more terrifying events now shaking the prosperous town of Wrightsburg . . . The unidentified corpse of an attractive young woman turns up in the woods; two high school kids, one shot in the back, the other in the face, are found dead in their car; a successful lawyer is discovered stabbed to death in her own home. A serial killer is on the loose. The murderer kills in the manner of famous killers of the past but takes care to leave a stopped watch at the scene of each crime – corresponding to the victim's position on his hit list. As the killing spree escalates it seems that the fractured Battle family are somehow involved and Maxwell and King suddenly find themselves racing to solve an intricate puzzle, one that is full of tantalizing clues but barren of solid evidence, and one that is leaving even the FBI confounded. And all the while, the body count is rising . . . Hour Game is followed by Simple Genius, First Family, The Sixth Man and King and Maxwell.\",\n",
              " 'This popular four-book series has been revised and updated, while preserving the features which have made it so successful. These include: Clear, simple explanations of key points of English grammar, using only essential technical terms. A large number of graded exercises lively illustrations enhancing understanding of the text. Book 1 is suitable for use with beginners and the series takes students upto intermediate level.',\n",
              " 'Whirlwind is the story of three weeks in Tehran in February 1979: three weeks of fanaticism, passion, self-sacrifice and heartbreak. Caught between the revolutionaries and the forces of international intrigue is a team of professional pilots. They are ordered to flee to safety with their helicopters. Two of them, both Europeans, have Iranian wives whom they love beyond safety and politics.',\n",
              " \"Red Rackham's Treasure (Tintin) is the twelfth volume in the Tintin series. In the previous volume, The Secret Of The Unicorn, Captain Haddock and Tintin discover what could be directions to the sunken ship, The Unicorn. They believe that The Unicorn contains Red Rackham's treasure.\\nThis adventure sees Tintin, Snowy and Captain Haddock head off into the sea, in a shark-proof submarine given to them by Professor Calculus in order to search for the fabled treasure of Red Rackham. On their way to the sea, the crew is joined by Thomson and Thompson, the detectives, who join the group to protect them from rival treasure hunters.\\nRed Rackham's Treasure (Tintin) is another thrilling adventure of the famous and much-loved journalist, Tintin and his friends. The graphic novel edition of Red Rackham's Treasure (Tintin) was published by Egmont in 2013. It is available in paperback.\\nKey Features:\\nThis book marks the first appearance of Professor Calculus.\",\n",
              " \"  Howl with laughter with the SIXTH book in the hilarious full-colour, illustrated series, Dog Man, from the creator of Captain Underpants!\\nIs Dog Man bad to the bone?\\nThe heroic hound is sent to the pound for a crime he didn't commit!\\nWhile his pals work to prove his innocence, Dog Man struggles to find his place among dogs and people.\\nBeing a part of both worlds, will he ever fully fit in with one?\\nDav Pilkey’s wildly popular Dog Man series appeals to readers of all ages and explores universally positive themes, including:\\nempathy,\\nkindness,\\npersistence,\\nand the importance of being true to one’s self.\\nFull colour pages throughout.\\n  OTHER BOOKS IN THE SERIES\\nDog Man (book 1)\\nDog Man: Unleashed (book 2)\\nDog Man: A Tale of Two Kitties (book 3)\\nDog Man and Cat Kid (book 4)\\nDog Man: Lord of the Fleas (book 5)\\nDog Man: For Whom the Ball Rolls (book 7)\\nDog Man: Fetch-22 (book 8)\",\n",
              " 'Idioms and their Storiesis the first of a four volume series, based on the popular column, Know Your English,which has been a regular feature in The Hindusince 1982. Teachers, students and those who are keen on honing their speaking and writing skills will find the series useful. This volume contains a selection of more than 300 idioms and each entry gives the meaning of the idiom, provides examples of its use and wherever possible, traces its origin.',\n",
              " 'Tired of menial tasks, Naruto, Sasuke and Sakura ask for a tougher assignment. But you should always be careful what you wish for! Along with their teacher, Kakashi, the trio must now guard a cranky old man from the Land of the Waves. But Tazuna the bridgebuilder is in more danger than anyone could have imagined. And now the young ninja are too!',\n",
              " \"What happens when a superhero can beat the snot out of every villain with just one punch? Can he find an opponent to give his life meaning, or is he doomed to a life of superpowered boredom?\\n\\nNothing about Saitama passes the eyeball test when it comes to superheroes, from his lifeless expression to his bald head to his unimpressive physique. However, this average-looking guy has a not-so-average problem—he just can’t seem to find an opponent strong enough to take on!'\\n\\nThe human monster and hero hunter Garo cranks up the intensity of his destruction. As the damage increases, throwing the hero world into turmoil, Saitama decides it’s the perfect time to develop an interest in martial arts and sneaks into a combat tournament. Meanwhile, the Class-S hero Metal Bat takes an assignment guarding a Hero Association executive and his son, and it isn’t long before trouble appears!\",\n",
              " 'The extraordinary and courageous journey of a transgender to define her identity and set new standards of achievement. When a boy was born in the Bandhopadhyay family, all rejoiced. Ason had been born after two girls and finally the conservative father could boast about having sired a son. However, it wasn’t long before the little boy began to feel inadequate in his own body and began questioning his own identity: Why did he constantly feel like he was a girl even when he had male parts? Why was he attracted to boys in a way that girls are? What could he do to stop feeling so incomplete?\\nIt was clearly a cruel joke of destiny which the family refused to acknowledge. But unknown to them, the boy had already begun his journey to becoming Manobi—the quintessential female, as nature meant for her to be.\\nWith unflinching honesty and deep understanding, Manobi tells the moving story of her transformation from a man to a woman; about how she continued to pursue her academics despite the severe upheavals and went on to become the first transgender principal of a girls’ college. And in doing so, she did not just define her own identity, but also inspired her entire community.',\n",
              " 'The confusing-yet-brilliant inventor known only as Crazy Dave helps his niece, Patrice, and young adventurer Nate Timely fend off a \"fun-dead\" neighborhood invasion in Plants vs. Zombies: Lawnmageddon! Winner of over thirty \"Game of the Year\" awards, Plants vs. Zombies is now determined to shuffle onto all-ages bookshelves to tickle funny bones and thrill... brains.\\n\\n* The first Plants vs. Zombies comic book!\\n\\n*',\n",
              " \"A spellbinding epic tale of ambition, anarchy, and absolute power set against the sprawling medieval canvas of twelfth-century England, The Pillars of the Earth is Ken Follett's classic historical masterpiece. A MASON WITH A DREAM 1135 and civil war, famine and religious strife abound. With his family on the verge of starvation, mason Tom Builder dreams of the day that he can use his talents to create and build a cathedral like no other. A MONK WITH A BURNING MISSION Philip, prior of Kingsbridge, is resourceful, but with money scarce he knows that for his town to survive it must find a way to thrive, and so he makes the decision to build within it the greatest Gothic cathedral the world has ever known. A WORLD OF HIGH IDEALS AND SAVAGE CRUELTY As Tom and Philip meet so begins an epic tale of ambition, anarchy and absolute power. In a world beset by strife and enemies that would thwart their plans, they will stop at nothing to achieve their ambitions in a struggle between good and evil that will turn church against state, and brother against brother . . . The Pillars of the Earth is the first in The Kingsbridge Novels series, followed by World Without End and A Column of Fire.\",\n",
              " 'Nothing about Saitama passes the eyeball test when it comes to superheroes, from his lifeless expression to his bald head to his unimpressive physique. However, this average-looking guy has a not-so-average problem—he just can’t seem to find an opponent strong enough to take on! An emergency summons gathers Class S heroes at headquarters...and Saitama tags along. There, they learn that the great seer Shibabawa left the following prophecy: “The Earth is in danger!\" What in the world is going to happen?!',\n",
              " \"In celebration of Penguin's 80th birthday, this box set of the 80 books in the Little Black Classics series showcases the many wonderful and varied writers in Penguin Black Classics. From India to Greece, Denmark to Iran and not forgetting Britain, this assortment of books will transport readers back in time to the furthest corners of the globe. With a choice of fiction, poetry, essays and maxims, by the likes of Chekhov, Balzac, Ovid, Austen, Sappho and Dante, it won't be difficult to find a book to suit your mood.\",\n",
              " 'An exciting thriller of espionage and murder! Classics Illustrated tells this wonderful tale in colorful comic strip form, providing an excellent introduction for younger readers. Also includes theme discussions and study questions.',\n",
              " \"When Aatish Taseer first came to Benares, he was eighteen, the Westernized child of an Indian journalist and a Pakistani politician, raised among the intellectual and cultural elite of New Delhi. Nearly two decades later, Taseer leaves his life in Manhattan to go in search of the Brahmins, wanting to understand his own estrangement from India through their ties to tradition. Known as the twice-born - first into the flesh, and again when initiated into their vocation - the Brahmins are a caste devoted to sacred learning. But what Taseer finds in Benares, the holy city of death, is a window on an India as internally fractured as his own continent-bridging identity. At every turn, the seductive, homogenizing force of modernity collides with the insistent presence of the past. From the narrow streets of the temple town to a Modi rally in Delhi, among the blossoming cotton trees and the bathers and burning corpses of the Ganges, Taseer struggles to reconcile magic with reason, faith in tradition with hope for the future and the brutalities of the caste system, all the while challenging his own myths about himself, his past, and his countries old and new. The Twice-born is a deeply individual, acutely perceptive, urgently relevant book: it revolves around questions of culture and politics that are going to define our future as a nation. But beyond the inherent interest of the stories it tells, it is a wonderfully written book, characterised by the music of Aatish Taseer's prose, which will haunt the reader long after the final page has been turned.\",\n",
              " \"The classic winner of the William Hill Sports Book of the Year Award\\nThroughout the world, football is a potent force in the lives of billions of people. Focusing national, political and cultural identities, football is the medium through which the world's hopes and fears, passions and hatreds are expressed.\\nSimon Kuper travelled to 22 countries from South Africa to Italy, from Russia to the USA, to examine the way football has shaped them. At the same time he tried to find out what lies behind each nation's distinctive style of play, from the carefree self-expression of the Brazilians to the anxious calculation of the Italians. During his journeys he met an extraordinary range of players, politicians and - of course - the fans themselves, all of whom revealed in their different ways the unique place football has in the life of the planet.\",\n",
              " 'The district of Lahaul-Spiti, west of Tibet and south of Ladakh, presents the most spectacular view to the human eye. Dotted with high snow peaks, glaciers, rivers and vistas of inner Himalayan desert landscapes, it is believed that this regions monasteries, friendly lamas and pristine beauty also hide ancient legends. The Mulkila Rakshasini, Barsi Nullah Bhoot, Chandrataal Fairy, a host of joginis and other mythical creatures have walked across its valleys. Manohar Singh Gills lifelong affair with this Himalayan wonderland has led him to collect its most enduring myths and folklore. Narrated in the gently prodding voice of a seasoned traveller, Tales from the Hills tells thirty-two stories that bring alive an enchanting world. An important repository of the culture and history of one of the worlds most beautiful spots.',\n",
              " \"'We should write because it is human nature to write' Julia Cameron\\nFor those jumping into the writing life for the first time and for those already living it, the art of writing will never be the same after reading this book. Provocative, thoughtful and exciting, The Right to Write will draw you back again and again as you seek to liberate and cultivate the writer residing within you.\\nThis isn't a book of rules, which can stifle creativity - it's a book about using writing to bring clarity and passion to the act of living. The secret is in breaking loose from the grip of your established thought process to unleash the wave of creativity which is striving to express itself.\\nWith the techniques and illustrative stories in The Right to Write, you'll learn how to make writing a natural and intensely personal part of your life. You'll also discover the details of Cameron's own writing processes, the ones she uses to create her poetry, plays, essays, novels and bestselling books, including the world famous\\nThe Artist's Way.\",\n",
              " 'This book provides simple but extremely useful guidelines for improving oral practice in English. The text is divided into two main parts: Part 1 contains conversations based on common daily situations, while Part 2 emphasizes particular structural or lexical features of the language. This book contains a large number of exercises designed in a programmed fashion; all the words or sentences are presented at the left of the page and the expected responses are given at the right. The book is ideal for home study and would be extremely useful to all those interested in improving their conversational skills.',\n",
              " \"In a land without magic, where the king rules with an iron hand, an assassin is summoned to the castle. She comes not to kill the king, but to win her freedom. If she defeats twenty-three killers, thieves, and warriors in a competition, she is released from prison to serve as the king's champion. Her name is Celaena Sardothien.\\nThe Crown Prince will provoke her. The Captain of the Guard will protect her. But something evil dwells in the castle of glass--and it's there to kill. When her competitors start dying one by one, Celaena's fight for freedom becomes a fight for survival, and a desperate quest to root out the evil before it destroys her world.\",\n",
              " \"Fortune magazine proclaimed Jobs 'the CEO of the decade'. Harvard Business Review called him 'the world's best-performing CEO'. And the Wall Street Journal praised him as a 'Person of the Decade'. Steve Jobs, the co-founder and longtime CEO of Apple, Inc., died on 5 October 2011, bringing to an end one of the greatest, most transformative business careers in history. Over the years Jobs gave countless interviews to the media, explaining what he called 'the vision thing'-his unmatched ability to envision, and successfully bring to the marketplace, consumer products that people find simply irresistible. Drawn from more than three decades of media coverage-print, electronic and online-this book serves up the best, most thoughtprovoking insights spoken by Steve Jobs: more than two hundred quotations that are essential reading for everyone who seeks innovative inspiration from the legend himself.\",\n",
              " \"The racing rules bible, completely updated for the latest 2017-2020 Racing Rules of Sailing. You don't have to know all the rules off by heart, but you do need to know your rights and obligations on the water - the rules can be looked up afterwards. This book takes you through the key situations that occur repeatedly on the racecourse showing, from the point of view of each boat in turn, what you may, must, or cannot do. Colour diagrams throughout ensure concepts are easily understood. The book also contains the new Racing Rules of Sailing in full, with all the Appendices. The latest rule changes are highlighted, along with their impact on you as a sailor. The Rules in Practice has been the racing rules bible for over 30 years, written by Bryan Willis who is an acknowledged rules expert.\",\n",
              " 'Data Structures And Algorithms For Gate: Solutions To All Previous Gate Questions Since 1991 by Narasimha Karumanchi is a compilation of GATE question papers from 1991 till 2010. The books features a special emphasis on the sections on data structures and algorithms, which are crucial topics for anybody studying computer science, programming, or similar subjects. Some of the important data structure topics covered by this book are linked lists, stacks, queues, trees, recursions, and backtracking. The algorithm topics covered are searching, sorting, graph algorithms, string algorithms, hashing techniques, and symbol tables, as well as more advanced topics like dynamic programming, divide and conquer algorithms, and greedy algorithms.\\nThis book looks at concepts of data structuring and algorithm formation, along with theorems and proofs on them that are most relevant from the point of view of interviews and competitive exams. It also contains a huge number of application-based problems, each with multiple solutions with different degrees of complexity, so that students can practise those they are comfortable with. They can also learn about other possible solutions to a problem, which will prepare them for interviews and GATE or higher studies. The first edition of Data Structures And Algorithms For Gate: Solutions To All Previous Gate Questions Since 1991 was published by CareerMonk Publications in 2011, and is available in paperback.\\nKey Features:\\nThis book can help one practise and master data structuring and algorithms at the level one is comfortable in, but also allows them to challenge their own abilities if they are ready for it.',\n",
              " 'Not many readers of Shashi Deshpande may be aware that her first experiments in writing fiction started with the short story. Over the years, she has published about a hundred stories in literary journals, magazines and newspapers, in between writing her immensely popular novels which are now read all over the world, and taught in universities wherever Indian writing has an audience. In this collection we find Shashi Deshpande at her best, writing with subtlety and a rare sensitivity about men and women trapped in relationships and situations often not of their making. The wife of a successful politician who must look to a long-lost past in order to keep up the pretense of contentment; a little girl who cannot comprehend why the very fact of her being born is a curse; a young man whose fantasy of love drives him to murder; a newly-wed couple with dramatically differing views on what it means to get to know each other—every one of the characters here is delineated with lucidity and compassion. Written over the past three decades, the stories in this volume provide an insight into often forgotten aspects of human feelings and relationships, weaving a magical web of emotions that is testimony to the unusual depth and range of Shashi Deshpande’s writing.',\n",
              " 'An instant classic in the vein of Jurassic Park, this boundary-pushing novel has all the hallmarks of Michael Crichton’s greatest adventures with its combination of pulse-pounding thrills, cutting-edge technology, and extraordinary research\\nThree men are found dead in a locked second-floor office in Honolulu. There is no sign of struggle, though their bodies are covered in ultra-fine, razor-sharp cuts. With no evidence, the police dismiss it as a bizarre suicide pact. But the murder weapon is still in the room, almost invisible to the human eye.\\nIn Cambridge, Massachusetts, seven graduate students at the forefront of their fields are recruited by a pioneering microbiology start-up company. Nanigen MicroTechnologies sends them to a mysterious laboratory in Hawaii, where they are promised access to tools that will open up a whole new scientific frontier.\\nBut this opportunity of a lifetime will teach them the true cost of existing at the cutting-edge…\\nThe group becomes prey to a technology of radical, unimaginable power and is thrust out into the teeming rainforest. Armed only with their knowledge of the natural world, the young scientists face a hostile wilderness that threatens danger at every turn.\\nTo survive, they must harness the awe-inspiring creative – and destructive – forces of nature itself.',\n",
              " \"Black Beauty meets many different people during his difficult life. Can Black Beauty ever be as happy as he was at Mr Gordon's farm? Ladybird Readers is a graded reading series of traditional tales, popular characters, modern stories, and non-fiction, written for young learners of English as a foreign or second language. Beautifully illustrated and carefully written, the series combines the best of Ladybird content with the structured language progression that will help children develop their reading, writing, speaking, listening and critical thinking skills. The eight levels of Readers and Activity Books follow the CEFR framework and include language activities that provide preparation for the Cambridge English: Young Learners (YLE) exams. Black Beauty, a Level 6 Reader, is A2+ in the CEFR framework and supports YLE Flyers and KET exams. The longer text is made up of sentences with up to four clauses, more complex past and future tense structures, passives and time clauses.\",\n",
              " '\"I don\\'t even feel like I\\'ve scratched the surface of what I can do with Python\"\\nWith Python Tricks: The Book you\\'ll discover Python\\'s best practices and the power of beautiful & Pythonic code with simple examples and a step-by-step narrative.\\nYou\\'ll get one step closer to mastering Python, so you can write beautiful and idiomatic code that comes to you naturally.\\nLearning the ins and outs of Python is difficult-and with this book you\\'ll be able to focus on the practical skills that really matter. Discover the \"hidden gold\" in Python\\'s standard library and start writing clean and Pythonic code today.\\nWho Should Read This Book:\\nIf you\\'re wondering which lesser known parts in Python you should know about, you\\'ll get a roadmap with this book. Discover cool (yet practical!) Python tricks and blow your coworkers\\' minds in your next code review.\\nIf you\\'ve got experience with legacy versions of Python, the book will get you up to speed with modern patterns and features introduced in Python 3 and backported to Python 2.\\nIf you\\'ve worked with other programming languages and you want to get up to speed with Python, you\\'ll pick up the idioms and practical tips you need to become a confident and effective Pythonista.\\nIf you want to make Python your own and learn how to write clean and Pythonic code, you\\'ll discover best practices and little-known tricks to round out your knowledge.\\nWhat Python Developers Say About The Book:\\n\"I kept thinking that I wished I had access to a book like this when I started learning Python many years ago.\" - Mariatta Wijaya, Python Core Developer\\n\"This book makes you write better Python code!\" - Bob Belderbos, Software Developer at Oracle\\n\"Far from being just a shallow collection of snippets, this book will leave the attentive reader with a deeper understanding of the inner workings of Python as well as an appreciation for its beauty.\" - Ben Felder, Pythonista\\n\"It\\'s like having a seasoned tutor explaining, well, tricks!\" - Daniel Meyer, Sr. Desktop Administrator at Tesla Inc.',\n",
              " \"How to be prepared no matter where running might take you\\nMillions of runners around the US are interested in special experiences, whether it means running a bucket-list event like the Boston Marathon, or competing in beautiful and challenging locales such as Rome or Death Valley. Whatever race you choose, there is no one better to guide you on your journey than Bart Yasso, chief running officer at Runner’s World magazine. Over the past 40 years, Yasso has run more than 1,000 races, across all seven continents, at every conceivable distance, from local 5Ks to grueling ultramarathons and Ironman triathlons. He’s truly done it all, and in Race Everything, he shares the secrets of how he trained, the particularities of each course, and the specific insights he has gleaned to help you run your best no matter the distance.\\nThis book offers tried-and-true advice on how to train and what to do on race day to make the best use of your training. It provides everything you need to know to succeed at the most popular race distances, including general training principles, targeted training plans for beginners and experienced runners alike, and insider tips based on Yasso’s own experiences and those of other top runners he has known and run with. The goal is to inform and inspire runners eager to challenge themselves by tackling the world’s signature races. You will also learn Yasso’s methods for winning the greatest race of all, longevity, so that you can remain healthy, fit, and able to race for decades to come.\\nWhether your goal is to complete a 5K or 10K race in your hometown or conquer the Antarctica Marathon, Runner's World Race Everything will be your guide.\",\n",
              " 'Within the pages of Striking Thoughts, you will find the secrets of Bruce Lee\\'s amazing success— as an actor, martial artist, and inspiration to the world. Consisting of eight sections, Striking Thoughts covers 72 topics and 825 aphorisms—from spirituality to personal liberation and from family life to filmmaking—all of which Bruce lived by.\\n\\nHis ideas helped energize his life and career, and made it possible for him to live a happy and assured life, overcoming difficult obstacles with seeming ease. His ideas also inspired his family, friends, students, and colleagues to achieve success in their own lives and this personal collection will help you in your journey too.\\n\\nSections include:\\nOn First Principles—including life, existence, time, and death\\nOn Being Human—including the mind, happiness, fear, and dreams\\nOn Matters of Existence—health, love, marriage, raising children, ethics, racism, and adversity\\nOn Achievement—work, goals, faith, success, money, and fame\\nOn Art and Artists—art, filmmaking, and acting\\nOn Personal Liberation—conditioning, Zen Buddhism, meditation, and freedom\\nOn the Process of Becoming—self-actualization, self-help, self-expression, and growth\\nOn Ultimate (Final) Principles—Yin-yang, totality, Tao, and the truth\\n\"A teacher is never a giver of truth—he is a guide, a pointer to the truth that each student must find for himself. A good teacher is merely a catalyst.\"—Bruce Lee',\n",
              " \"Combining audio and video with text, image, graphics and animation offers a more dynamic presentation than can be achieved through the use of text and image alone. This integration of media provides the possibility for a spectrum of new applications. Multimedia: Computing, Communications and Applications examines the challenges of this technology and probes today's developments toward fully integrated working systems.\",\n",
              " \"An incisive exploration of the Maoist insurgency in the heart of the country questions what India’s 'growth story' really means today.\\nAn innocent adivasi cut down in his prime by the unholy nexus of ruthless Maoist rebels and corrupt bureaucrats; a highly educated Maoist ideologue who had to die because he sought an end to bloody conflict; a contractor bitter at having been left in the lurch by his corporate paymaster; and a young adivasi woman, recently in the news, who dared to challenge the status quo to emerge as an authentic voice of her people...\\nIt is their compelling stories, among several others, that Rohit Prasad felt driven to explore while travelling in Chhattisgarh for over two years. The result is Blood Red River, an impassioned weaving together of narrated history and hard fact, first-person accounts of those who have witnessed terrible violence and encounters with keepers of the law, both in the Indian government as well as Maoist ranks. It offers, too, a startling glimpse of the so-far-unrevealed role that corporate rivalry has played in thwarting vital industrial projects in the name of insurgency.\\nUsing Chhattisgarh as a microcosm, this multi-layered narrative is an immersive inquiry into the roles of different stakeholders in the no-holds-barred war over natural resources that has continued to ravage some of India's mineral-rich states for more than three decades. Bold and unafraid to take sides, it leads the reader deep into a world where corruption and greed underlie ideological posturing and reveals the false dichotomies of India’s development paradigm.\",\n",
              " \"A History of the Jana Natya Manch chronicles the birth and growth of the Jana Natya Manch (Janam), a Delhi-based radical theater group which has been active since 1973.\\nBeginning in the early 1970s, when a group of young students in Delhi sought to continue the legacy of the Indian Peoples' Theatre Association, the book takes a close but critical look at the various phases in the four decades of the theatre collective. The author has also captured within these pages the functioning of Janam as an organization, its methods of attracting and training fresh talent, the process of scripting, interactions with mass organizations, the experience of performing almost skin-to-skin with its spectators in the grime of Indian streets, and much more.\\nThis book is not only a narration of Janam's history, development and functioning, it is also an attempt to throw fresh light on the practice of theater.\",\n",
              " '\"Without The Artist\\'s Way, there would have been no Eat, Pray, Love.” —Elizabeth Gilbert\\nThe Artist’s Way is the seminal book on the subject of creativity. An international bestseller, millions of readers have found it to be an invaluable guide to living the artist’s life. Still as vital today—or perhaps even more so—than it was when it was first published one decade ago, it is a powerfully provocative and inspiring work. In a new introduction to the book, Julia Cameron reflects upon the impact of The Artist’s Way and describes the work she has done during the last decade and the new insights into the creative process that she has gained. Updated and expanded, this anniversary edition reframes The Artist’s Way for a new century.',\n",
              " \"A fascinating discussion on sex, gender, and human instincts, as relevant today as ever\\n\\nIn the course of a lively drinking party, a group of Athenian intellectuals exchange views on eros, or desire. From their conversation emerges a series of subtle reflections on gender roles, sex in society and the sublimation of basic human instincts. The discussion culminates in a radical challenge to conventional views by Plato's mentor, Socrates, who advocates transcendence through spiritual love. The Symposium is a deft interweaving of different viewpoints and ideas about the nature of love—as a response to beauty, a cosmic force, a motive for social action and as a means of ethical education.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.\",\n",
              " 'Being born a princess, and raised by a loving father and three doting brothers would make life seem like a bed of roses to any woman. Born out of the sacred fire, Draupadi is no ordinary woman, and her destiny cannot be to walk the beaten path.\\nWitnessing estrangement and betrayal within her own family makes her perceptive and intuitive beyond her years. Complicated marital relationships, a meteoric rise and a fateful loss, humiliation unheard of and a pledge of revenge, all culminating in a bloody war—her ordeal seemed neverending. Yet she stands up to it all—never succumbing, never breaking. One of the most unforgettable characters of the Mahabharata, Draupadi shows what a woman is capable of.\\nTold with great sensitivity and passion, this book brings alive a character of epic proportions—one that resonates with every reader across space and time.',\n",
              " \"THE TIMES SCIENCE BOOK OF THE YEAR A Sunday Times Bestseller 'Thrilling . . . the best book on the subject written for the general reader since the 1980s.' The Sunday Times 66 million years ago the dinosaurs were wiped from the face of the earth. Today, Dr. Steve Brusatte, one of the leading scientists of a new generation of dinosaur hunters, armed with cutting edge technology, is piecing together the complete story of how the dinosaurs ruled the earth for 150 million years. The world of the dinosaurs has fascinated on book and screen for decades – from early science fiction classics like The Lost World, to Godzilla terrorizing the streets of Tokyo, and the monsters of Jurassic Park. But what if we got it wrong? In The Rise and Fall of the Dinosaurs, top dinosaur expert Brusatte, tells the real story of how dinosaurs rose to dominate the planet. Using the fossil clues that have been gathered using state of the art technology, Brusatte follows these magnificent creatures from their beginnings in the Early Triassic period, through the Jurassic period to their final days in the Cretaceous and the legacy that they left behind. Along the way, Brusatte introduces us to modern day dinosaur hunters and gives an insight into what it’s like to be a paleontologist. The Rise and Fall of the Dinosaurs is full of thrilling accounts of some of his personal discoveries, including primitive human-sized tyrannosaurs, monstrous carnivores even larger than T. rex, and feathered raptor dinosaurs preserved in lava from China. At a time when Homo sapiens has existed for less than 200,000 years and we are already talking about planetary extinction, The Rise and Fall of the Dinosaurs is a timely reminder of what humans can learn from the magnificent creatures who ruled the earth before us.\",\n",
              " \"This gripping and triumphant memoir from the author of The Mountain follows a living legend of extreme mountaineering as he makes his assault on history, one 8,000-meter summit at a time.\\n\\nFor eighteen years Ed Viesturs pursued climbing’s holy grail: to stand atop the world’s fourteen 8,000-meter peaks, without the aid of bottled oxygen. But No Shortcuts to the Top is as much about the man who would become the first American to achieve that goal as it is about his stunning quest. As Viesturs recounts the stories of his most harrowing climbs, he reveals a man torn between the flat, safe world he and his loved ones share and the majestic and deadly places where only he can go.\\n\\nA preternaturally cautious climber who once turned back 300 feet from the top of Everest but who would not shrink from a peak (Annapurna) known to claim the life of one climber for every two who reached its summit, Viesturs lives by an unyielding motto, “Reaching the summit is optional. Getting down is mandatory.” It is with this philosophy that he vividly describes fatal errors in judgment made by his fellow climbers as well as a few of his own close calls and gallant rescues. And, for the first time, he details his own pivotal and heroic role in the 1996 Everest disaster made famous in Jon Krakauer's Into Thin Air.\\n\\nIn addition to the raw excitement of Viesturs’s odyssey, No Shortcuts to the Top is leavened with many funny moments revealing the camaraderie between climbers. It is more than the first full account of one of the staggering accomplishments of our time; it is a portrait of a brave and devoted family man and his beliefs that shaped this most perilous and magnificent pursuit.\",\n",
              " \"Dawn broke fine on that fatal day. A couple of thousand feet above the tiny canvas tent the summit of the world's highest mountain stood impassively, waiting for someone to have the courage to approach. Inside the ice-crusted shelter, two forms lay still as death. Then there was a groan, a stirring and eventually the slow scratch of match against sandpaper. Low voices shared the high-altitude agonies of waking, the heating of water and the struggle with frozen boots. As the sun rose through wisps of cloud beyond the Tibetan hills to the east, one of the men emerged through the tent flaps. It was a fine morning for the attempt, with only a few clouds in the sky. The two of them stood for a while, shuffling their feet and blowing into their hands. Inside the tent lay a mess of sleeping bags and food. The men lifted oxygen sets onto their backs, then they turned towards the mountain and stamped off into history. On the 6th June, 1924, George Mallory and Sandy Irvine disappeared into the mists of history. George Mallory's body was discovered high on Everest in 1999. Sandy Irvine's body is still believed to be on the mountain having been rediscovered in 1975 by a Chinese climber who was killed the very next day. Combining personal experience, the physical evidence found on the mountain and an insight into the hearts and minds of the two climbers, Graham Holland produces the most compelling description of what actually happened on that day and the answer to that most intriguing of questions did they actually climb Everest?\",\n",
              " 'The third novel in the No. 1 bestselling Conqueror series, following the life and adventures of the mighty Genghis Khan and his descendants\\nGenghis Khan has fulfilled his dream of uniting the many warring tribes of his lands into one great nation. He has taken his armies against the mighty cities of their oldest enemies. Now he finds trouble rising west of the Mongolian plains. His emissaries are being mutilated or killed and his trading gestures rebuffed. He decides to divide his armies to conquer, using his sons as generals and sending them out simultaneously in many directions.\\nAs well as discovering new territories and laying waste the cities which resist, Genghis knows that the actions of his generals will help him decide who, from his rival sons and heirs, should succeed him as khan.',\n",
              " 'From the No. 1 New York Times bestselling author of The Black Widow comes the thrilling new summer blockbuster featuring legendary spy, assassin and art restorer Gabriel Allon. Legendary spy, assassin and art restorer Gabriel Allon is back and out for revenge –determined to hunt down the world’s most dangerous terrorist, a shadowy ISIS mastermind known only as Saladin. Four months after the deadliest attack on the American homeland since 9/11, terrorists leave a trail of carnage through London’s West End. The attack is a brilliant feat of planning and secrecy, but with one loose thread: the French-Moroccan street criminal and ISIS operative who supplied the combat assault rifles. The thread leads Gabriel Allon and his team to the south of France and to the doorstep of Jean-Luc Martel and Olivia Watson. A beautiful former fashion model, Olivia pretends not to know that the true source of Martel’s enormous wealth is drugs. And Martel, likewise, turns a blind eye to the fact he is doing business with a man whose objective is the very destruction of the West. Together, under Gabriel’s skilled hand, they will become an unlikely pair of heroes in the global war on terror.',\n",
              " '‘Keeps one reading long after the lights should have been out’ ROBIN HOBB\\nRead the explosive New York Times bestselling debut that’s captivated readers worldwide. Set to be a major motion picture, An Ember in the Ashes is the book everyone is talking about.\\nUnder the Martial Empire, defiance is met with death.\\nWhen Laia’s grandparents are brutally murdered and her brother arrested for treason by the empire, the only people she has left to turn to are the rebels.\\nBut in exchange for their help in saving her brother, they demand that Laia spy on the ruthless Commandant of Blackcliff, the Empire’s greatest military academy. Should she fail it’s more than her brother’s freedom at risk . . . Laia’s very life is at stake.\\nThere, she meets Elias, the academy’s finest soldier. But Elias wants only to be free of the tyranny he’s being trained to enforce. He and Laia will soon realize that their destinies are intertwined – and that their choices will change the fate of the Empire itself.',\n",
              " '3000 Idioms and Phrases by Sam Phillips is an incomparable compilation of three thousand most popular idioms and phrases. Each of the idioms and phrases has been explained in easy to understand language with their meanings and how to use them properly to fit the context. The book is exceptionally handy for everyone-general learners, students, teachers, writers, authors, editors, journalists and the like.',\n",
              " 'This book on basic computer knowledge is a simple, beginner’s guide into the world of computers. There are numerous examinations, held each year where the qualifying candidates are expected to have a basic knowledge of computers. To test the applicants on their knowledge of computer skills, most boards or institutions holding such recruitment exams add this section to the written test where the objective questions related to basic computing tasks are asked.\\nArihant Experts have published this edition of R.Pillai’s Objective Computer Awareness, which serves as a good compilation of a large number of possible objective questions on Computer awareness. The book has 12 chapters in total; it describes various aspects of computer basics and proceeds with providing questions that can be framed from each topic at the end of the chapter. To help a reader test his or her knowledge, the book provides model test papers; so, an aspirant can perform a self-assessment and know how much more efforts need to be put in for qualifying the exam.\\nWith over 1500 multiple choice questions based on computer awareness, this book is of good assistance for the candidates preparing for bank examinations for posts of PO, clerk, AAO and other examinations such as MCA, accountancy posts and other clerical and non-clerical posts where the recruiting body expects a candidate to have basic computer knowledge. The book also includes a detailed glossary containing important terminologies and abbreviations.\\nThe edition was published in 2014 and contains almost everything that one needs to be thorough about the basics on computers. All candidates who are preparing for any such competitive examination where computer skills are one of the sections in the syllabus, will require the assistance of such a book for a deep theoretical reading and practice.\\nAbout the Author:\\nAn experienced team of authors, editors, proof readers and teachers join together to bring out comprehensive guides for various public examinations that are held several times each year for filling up posts in various government departments. Backed with an experience of 15 years, Arihant Experts bring out revised editions of carefully designed, exam-oriented and exam-ready content after intensive research and analysis.',\n",
              " \"Set against the background of the Black Death of 1348, Giovanni Boccaccio's undisputed masterpiece recaptures both the tragedies and comedies of medieval life and is surely one of the greatest achievements in the history of literature.\",\n",
              " '\"One of the funniest books I\\'ve read this year.\"\\n—Boing Boing\\n\\nAbsurd comics for our absurd times, from the artist behind the wildly popular webcomic Poorly Drawn Lines.\\n\\nIn his follow up to the New York Times bestselling Poorly Drawn Lines, beloved webcomic artist Reza Farazmand returns with a new collection of comics that hilariously skewers our modern age. Comics for a Strange World takes readers through time, space, and alternate realities, reuniting fans with favorite characters and presenting them with even more bizarre scenarios. A child is arrested for plagiarism. A squirrel adapts to human society by purchasing a cell phone—and a gun. And an old man shares memories of the Internet with his granddaughter (“A vast network of millions of idiots. Together, the idiots created endless shitty ideas. It was a true renaissance of shit.”). In the world of Poorly Drawn Lines, nothing is too weird or too outlandish for parody.\\n\\nFeaturing 50% brand new content alongside some of the most popular comics of the past year, Comics for a Strange World is the perfect antidote to life’s absurdities.',\n",
              " \"It is 1974. Indu has inherited a flat from her grandmother and wants to turn it into a library for women. Her parents think this will keep her suitably occupied till she marries her fiancé, Rajat, who's away studying in London.\\nBut then she meets Rana, a young lawyer with sparkling wit and a heart of gold. He helps set up the library and their days light up with playful banter and the many Rajesh Khanna movies they watch together.\\nWhen the Emergency is declared, Indu's life turns upside down. Rana finds himself in trouble, while Rajat decides it's time to visit India and settle down. As the Emergency pervades their lives, Indu must decide not only who but what kind of life she will choose.\",\n",
              " \"This book is amazing ’ Malcolm Gladwell ‘if you want to gain insight into the mind of great athletes, adventurers and peak performers then prepare to be enthralled by Alex Hutchinson's endure. ’ – Bear Grylls\\nHow high or far or fast can humans go? And what about individual potential, what defines a person’s limits? From running a two-hour marathon to summiting mount Everest, we’re fascinated by the extremes of human endurance, constantly testing both our physical and psychological limits.\\nin endure Alex Hutchinson, ph. D., Reveals why our individual limits may be determined as much by our head and heart, as by our muscles. He presents an overview of science’s search for understanding human fatigue, from crude experiments with electricity and frogs’ legs to sophisticated brain imaging technology. Going beyond the traditional mechanical view of human limits, he instead argues that a key element in endurance is how the brain responds to distress signals whether heat or cold or muscles screaming with lactic acid and reveals that we can train to improve brain response.\\nAn Elite distance runner himself, Hutchinson takes us to the forefront of the new sports psychology brain electrode jolts, computer-based training, subliminal messaging and presents startling new discoveries enhancing the performance of athletes today, showing us how anyone can utilize these tactics to bolster their own performance and get the most out of their bodies.\",\n",
              " \"The best way to learn is by doing. The Photographer's Playbook features photography assignments, as well as ideas, stories, and anecdotes from many of the world's most talented photographers and photography professionals. Whether you're looking for exercises to improve your craft-alone or in a group-or you're interested in learning more about the medium, this playful collection will inspire fresh ways of engaging with photographic process. Inside you will find advice for better shooting and editing, creative ways to start new projects, games and activities, and insight into the practices of those responsible for our most iconic photographs-John Baldessari, Tina Barney, Philip-Lorca diCorcia, Jim Goldberg, Miranda July, Susan Meiselas, Stephen Shore, Alec Soth, Tim Walker, and many more. The book also features a Polaroid alphabet by Mike Slack, which divides each chapter, and a handy subject guide. Edited by acclaimed photographers Jason Fulford and Gregory Halpern, the assignments and project ideas in this book are indispensable for teachers and students, and great fun for everyone fascinated by taking pictures.\",\n",
              " 'From the Nobel Prize-winning author of The Remains of the Day and Never Let Me Go\\nIn his highly acclaimed debut, Kazuo Ishiguro tells the story of Etsuko, a Japanese woman now living alone in England, dwelling on the recent suicide of her daughter.\\nRetreating into the past, she finds herself reliving one particular hot summer night in Nagasaki, when she and her friends struggled to rebuild their lives after the war. But then as she recalls her strange friendship with Sachiko – a wealthy woman reduced to vagrancy – the memories take on a disturbing cast.',\n",
              " 'The genius of Francis Bacon is nowhere better revealed than in his essays.\\n\\nBacon’s education was grounded in the classical texts of ancient Greece and Rome, but he brought vividness and color to the arid scholasticism of medieval book-learning. Whatever their subject, whether it is something as personal as “Friendship” or as abstract as “Truth,” the essays combine a mixture of rhetoric and philosophy; and are perhaps the most complete and rounded examples of Bacon’s literary style.\\n\\nRather than merely summarizing popular philosophy or producing glib expositions of correct conduct, Bacon attempted to change the shape of the other men’s minds. He believed rhetoric, as the force eloquence and persuasion, could incline the mind towards the pure light of reason.',\n",
              " \"From the award-winning author of The Spring Bride comes the conclusion to the quartet about four sisters, four weddings, and a bride for every season...\\n\\nFiercely independent Daisy Chance has a dream—and it doesn’t involve marriage or babies (or being under any man’s thumb). Raised in poverty, she has a passion—and a talent—for making beautiful clothes. Daisy aims to become the finest dressmaker in London.\\n \\nDashing Irishman Patrick Flynn is wealthy and ambitious, and has entered society to find an aristocratic bride. Instead, he finds himself growing increasingly attracted to the headstrong, clever and outspoken Daisy. She’s wrong in every way—except the way she sets his heart racing.\\n \\nHowever, when Flynn proposes marriage, Daisy refuses. She won't give up her hard-won independence. Besides, she doesn't want to join the fine ladies of society—she wants to dress them. She might, however, consider becoming Flynn's secret mistress...\\n \\nBut Flynn wants a wife, and when he sets his heart on something, nothing can stand in his way...\",\n",
              " 'Rejected by Miss Milborne, the incomparable, for his unsteadiness of character, wild Lord Sheringham is bent on avenging fate. Vowing to marry the first woman to cross his way, who should he see but Hero Wantage, the young and charmingly unsophisticated girl, who has loved him since childhood.',\n",
              " 'CEO Louis V. Gerstner Jr.’s memoir about the extraordinary turnaround of IBM and his transformation of the company into the industry leader of the computer age – the great American business story of our time.\\nWhen Louis V. Gerstner became CEO of IBM in 1993, shares had slumped and the company was on the verge of collapse. Hired for his successful management of RJR Nabisco and American Express, Gerstner had no background in technology, but during his seven-year chairmanship, he transformed the company into the leading force of the computer age. In his frank, direct voice, Gerstner recalls the obstacles he faced: the plans to fragment the company, the inconsistent global policies, the stodgy white-shirt hierarchy and inter-departmental competitiveness and the rapidly declining sales. Within months of joining IBM, Gerstner presented his bold and controversial business strategy. Punitive towards office politics, he revolutionised the company from within, altering an entire corporate culture, divesting billions of dollars in unneeded assets and transforming IBM from a fractured, process-driven business into a nimble, customer-driven enterprise able to respond quickly to the volatile technology market and face down Microsoft and Intel in the internet era. Revealing his tactics step by step, Gerstner spins an engaging narrative that takes the reader behind the curtain into the unbelievable mess he inherited and into the office and mind of a CEO facing the challenge of a lifetime.',\n",
              " 'Over the years, Sudha Murty has come across some fascinating people whose lives make for interesting stories and have astonishing lessons to reveal. Take Vishnu, who achieves every material success but never knows happiness; or Venkat, who talks so much that he has no time to listen. In other stories, a young girl goes on a train journey that changes her life forever; an impoverished village woman provides bathing water to hundreds of people in a drought-stricken area; a do-gooder ghost decides to teach a disconsolate young man Sanskrit; and in the title story, a woman in a flooded village in Odisha teaches the author a life lesson she will never forget.',\n",
              " \"200 Data Structures & Algorithms Interview Questions\\n77 HR Interview Questions\\nReal life scenario based questions\\nStrategies to respond to interview questions\\n2 Aptitude Tests\\nData Structures & Algorithms Interview Questions You'll Most Likely Be Asked is a perfect companion to stand ahead above the rest in today's competitive job market. Rather than going through comprehensive, textbook-sized reference guides, this book includes only the information required immediately for job search to build an IT career. This book puts the interviewee in the driver's seat and helps them steer their way to impress the interviewer.\\nThe following is included in this book:\\n(a) 200 Data Structures & Algorithms Interview Questions, Answers and proven strategies for getting hired as an IT professional\\n(b) Dozens of examples to respond to interview questions\\n(c) 77 HR Questions with Answers and proven strategies to give specific, impressive, answers that help nail the interviews\\n(d) 2 Aptitude Tests download available on www.vibrantpublishers.com\",\n",
              " \"The fascinating life story of professional cricketer Kevin Pietersen, MBE, from his childhood in South Africa to his recent experiences as one of the leading lights in the world of international cricket.\\nKevin was dropped from the England squad in February of this year, seemingly calling time on an international career that began nearly ten years earlier. The decision puzzled many observers - although the England team had failed miserably in the Ashes tour of 2013-14, Kevin was the tourists' leading run scorer across the series, and he remains the country's highest run scorer of all time across all formats of the game.\\nThis autumn Kevin will reveal all in his autobiography, telling the stories behind the many other highs and lows of his incredible career. Giving readers the full story of his life, from his childhood in South Africa to his recent experiences as one of the leading lights in the world of international cricket, this will be an autobiography that entertains and fascinates readers in equal measure.\",\n",
              " \"Made into a major motion picture starring Toby Maguire and Jeff Daniels.\\nIn 1938 one figure received more press coverage than Mussolini, Hitler or Roosevelt. He was a cultural icon and a world-class athlete - and an undersized, crooked-legged racehorse by the name of Seabiscuit.\\nMisunderstood and mishandled, Seabiscuit had spent seasons floundering in the lowest ranks of racing until a chance meeting of three men. Together, they created a champion. This is a story which topped the bestseller charts for over two years a riveting tale of grit, grace, luck and an underdog's stubborn determination to win against all odds.\\nThe true story of three men and their dreams for a racehorse - Seabiscuit - which encompasses a pivotal moment in American history: its resurrection from the Depression.\",\n",
              " \"HBO’s hit series A GAME OF THRONES is based on George R R Martin’s internationally bestselling series A SONG OF ICE AND FIRE, the greatest fantasy epic of the modern age.\\nA DANCE WITH DRAGONS: DREAMS AND DUST is the FIRST part of the fifth volume in the series.\\n‘Richly satisfying and utterly engrossing’ Sunday Times\\nIn the aftermath of a colossal battle, new threats are emerging from every direction.\\nTyrion Lannister, having killed his father, and wrongfully accused of killing his nephew, King Joffrey, has escaped from King’s Landing with a price on his head.\\nTo the north lies the great Wall of ice and stone – a structure only as strong as those guarding it. Eddard Stark's bastard son Jon Snow has been elected 998th Lord Commander of the Night’s Watch. But Jon has enemies both inside and beyond the Wall. And in the east Daenerys Targaryen struggles to hold a city built on dreams and dust.\",\n",
              " \"A survey that includes India's environment and resources, its population and manpower, mythology and religion, history and culture, its economic framework, transport and communication, education and health, its freedom struggle and post-independence history, government and constitution, literature, science and technology, arts and entertainment and sports, India Quiz, in its multiple-choice format, provides fascinating snippets of information on all aspects of the country.\",\n",
              " \"'The F2 are unbelievable - what they do is not possible!' - Pele We're The F2 and this is our World of Football. Inside we give away the biggest secrets of the greatest footballers on the planet. Want tricks like Neymar? Or to hit free-kicks like Ronaldo? Or to dribble like Messi? We show you how. We've been travelling the world, meeting the biggest stars, like Gareth Bale, Ronaldinho, Mesut OEzil, Pele and Stevie G, and now we give you the lowdown on what they're really like, and how they got their edge. We'll also let you in on our journey from aspiring pros to YouTube superstars with over 10 million followers. Want to know how to become a social media star? That's inside too. There's a free app to download that will make these pages come to life with exclusive videos, tricks and games. So, what are you waiting for? Open, read, learn, download and get out on the pitch and practise. Love, peace and tekkers, Billy and Jez, aka The F2 Enter the F2 World of Football competition!! Submit your pre-order confirmation for a chance to win a selection of amazing prizes, including having your book delivered to your door by Billy and Jez and featuring on their Vlog! Go to www.F2playlikeapro.co.uk for all the details.\",\n",
              " 'Offers advice on how to be a friend by being more of a contributor than a taker, emphasizing the positive, and learning to assert oneself, express anger but avoid arguments, learn from mistakes, and establish rules',\n",
              " 'Surya Namaskara, or salutation to the sun, is an important Yogic practice, which dates back to the Ancient Vedic period when the Sun was worshipped as a powerful symbol of Spiritual consciousness. From its esoteric Origins Surya Namaskara has developed into a practice of twelve postures which weave together to generate Prana (subtle energy), aiming towards the purification and Rejuvenation of the practitioner. This book discusses in detail the full practice of Surya Namaskara, including the surya and bija mantras, points of concentration and extended guidelines to Aid both practitioners and teachers. An in-depth Physiological Study of Surya Namaskara supports its present day use as a powerful therapeutic practice.',\n",
              " \"IN THE BEGINNING WE WERE A GROUP OF NINE.\\nNine aliens who left our home planet of Lorien when it fell under attack from the deadly Mogodorians.\\nWe scattered on earth and went into hiding.\\nWe look like ordinary teenagers.\\nWe are not.\\nUntil I met John Smith, Number Four, I'd been on the run alone, hiding and fighting to stay alive. Together we are much more powerful. But it could only last so long before we had to separate.\\nThey caught Number One in Malaysia.\\nNumber Two in England.\\nAnd Number Three in Kenya.\\nThey caught me in New York - but I escaped.\\nI am Number Six. The Mogodorians want to finish what they started.\\nBut they'll have to fight us first.\",\n",
              " \"Building an empire is not easy, especially when there are enemies everywhere and no one you can trust. India, 326 BCE. The world's greatest conqueror, Alexander, the Greek emperor, is at its doorstep, having arrived at the Indus seeking to establish his dominion over the entire known world. In the east lies Magadha, ruled by the Nandas, a dynasty driven by greed, lust and hunger for power. From the embers of that lust and avarice a boy has been born, raised by a tribe of peacock-tamers – a boy named Moriya forced by the Nanda clan to be on the run. Aided by Chanakya, a political strategist at odds with his former rulers, who trains him in the ways of the world and christens him Chandragupta, the young man ventures across the vast Magadhan empire to form an army of his own and seek out the foreign invader. But being a warrior prince, he finds, comes at a heavy price – assassins appointed by the Nanda kings will stop at nothing to eliminate him, a rival prince seeks revenge through cruelty and friends are no longer what they seem… This is the story of a youth who must fight against all odds – within and without – to become one of the greatest emperors ever known. This is the story of Chandragupta Maurya.\",\n",
              " \"It was in 1973 that G.B.S. Sidhu, a young official with the newly set-up Research and Analysis Wing (R&AW), took charge of the field office in Gangtok in 1973. With an insider's view of the events that led to the Chogyal's ouster, he presents a first-hand account of the fledgling democracy movement and the struggle for reforms led by Kazi Lhendup Dorji in a society that was struggling to come to terms with the modern world.\\nIn his fast-paced, clear-sighted narrative, Sidhu tracks the reasons behind New Delhi's shift from a long-standing pro-Chogyal stand to a pro-democracy position and maps the political alignments on the ground in Sikkim. He outlines the interplay of personalities-Indira Gandhi, the Chogyal, the Kazi, and the Indian officials and intelligence agencies involved-to reveal the chain of events that led to the merger of the Himalayan kingdom with India.\",\n",
              " \"Phizzwhizzing new cover look and branding for the World's NUMBER ONE Storyteller! WHOOSH! Inside the Great Glass Elevator, Willy Wonka, Charlie Bucket and his family are cruising a thousand feet above the chocolate factory. They can see the whole world below them, but they're not alone. The American Space Hotel has just launched. Lurking inside are the Vernicious Knids - the most brutal, vindictive murderous beasts in the universe. So grab your gizzard! Hold your hats! Only Charlie and Willy Wonka can stop the Knids from destroying everything! Listen to CHARLIE AND THE GREAT GLASS ELEVATOR and other Roald Dahl audiobooks read by some very famous voices, including Kate Winslet, David Walliams and Steven Fry - plus there are added squelchy soundeffects from Pinewood Studios! Look out for new Roald Dahl apps in the App store and Google Play- including the disgusting TWIT OR MISS! and HOUSE OF TWITS inspired by the revolting Twits.\",\n",
              " \"WINNER OF THE PULITZER PRIZE FOR FICTION 2018\\n'You will sob little tears of joy' Nell Zink\\n'I recommend it with my whole heart' Ann Patchett\\n'I adore this book' Armistead Maupin\\n'Charming, languid and incredibly funny, I absolutely adored Arthur' Jenny Colgan\\n'Marvellously, endearingly, unexpectedly funny' Gary Shteyngart\\n'Bedazzling, bewitching and be-wonderful' New York Times Book Review\\n'A fast and rocketing read . . . a wonderful, wonderful book!' Karen Joy Fowler\\n'Hilarious, and wise, and abundantly funny' Adam Haslett\\nWHO SAYS YOU CAN'T RUN AWAY FROM YOUR PROBLEMS?\\nArthur Less is a failed novelist about to turn fifty. A wedding invitation arrives in the post: it is from an ex-boyfriend of nine years who is engaged to someone else. Arthur can't say yes - it would be too awkward; he can't say no - it would look like defeat. So, he begins to accept the invitations on his desk to half-baked literary events around the world.\\nFrom France to India, Germany to Japan, Arthur almost falls in love, almost falls to his death, and puts miles between him and the plight he refuses to face. Less is a novel about mishaps, misunderstandings and the depths of the human heart.\",\n",
              " \"The President for President Jonathan Bennett, reaching the White House was the realisation of lifetime's ambition. He's leader of the free world and the most powerful man on Earth. But public support for his administration is wearing thin. And if the truth about his rise to the top was exposed it would bury him. The assassin as a boy, evan Smoak was taken from his foster home and inducted into a top secret cold War programme. Code named orphan X, he was trained to become a lethal weapon, then despatched around the world to do whatever was required to keep his country safe. When evan discovered the mission was rotten to the core he got out, using his skills to hide in plain sight while helping those left behind by mainstream justice. The reckoning but evan knows about the president's dark past. And that's dangerous knowledge. To save himself and his country, evan must ask himself one simple question: how do you kill the most well-protected man on Earth? And, when he knows you're coming for him, how do you stay alive long enough to try? One thing is certain: a desperate call for help from another unfortunate in urgent need of evan's protection isn't going to make it any easier... Breathtakingly bold, brilliantly accomplished and blisteringly fast-paced, out of the dark is the day of the Jackal for the twenty-first century.\",\n",
              " 'One resource. All of your SAP Business Workflow needs. Reorganized and fine-tuned, the third edition of this #1 best-seller is packed with information and better than ever. Pick the sections or chapters that are most relevant to you; focus on the provided conceptual explanations, technical instructions, or both. You’ll find important topics such as configuration, administration and troubleshooting, design and enhancement. If you know the basics, you’ll find value in the coverage provided for SAP’s changed landscape such as SAPUI5, SAP Fiori, SAP HANA and much more.\\nGet the whole story on creating, maintaining and customizing\\nworkflows in SAP\\nLearn about workflow for all the major SAP applications\\nUpdate your skills with coverage of SAP HANA, SAP Fiori,\\nBRF+ and more\\n3rd edition, updated and expanded\\nWorkflow Concepts\\nAdministration Concepts\\nWorkflow Development\\nEnhancements\\nContents at a Glance\\nPART I Getting Started with Workflow in SAP\\nIntroduction\\nRequirements Gathering Strategy\\nConfiguring the System\\nWork Item Delivery\\nAgents\\nSetting Up an SAP-Provided SAP ERP Workflow\\nSAP Operational Process Intelligence Powered by SAP HANA\\nPART II Administering Workflows\\nWorkflow Administration\\nUsing SAP Business Warehouse for SAP Business Workflow Reporting\\nAdministration Troubleshooting Guide\\nAdvanced Diagnostics\\nUpgrading SAP Business Workflow\\nPART III Developing Workflows\\nCreating a Workflow\\nAdvanced Workflow Design Techniques\\nBusiness Objects\\nABAP Classes\\nAgent Determination Rules\\nUsing Events and Other Business Interfaces\\nCustom Programs\\nService-Enabling Workflows\\nBRFplus and SAP Decision Service Management\\nPART IV Enhancing Workflows\\n22 User Interface Options\\nUsing Web Dynpro ABAP\\nUsing Web Dynpro Java\\nUsing Business Server Pages\\nUsing Forms\\nUsing SAPUI5\\nPART V Using SAP Business Workflow in SAP Applications\\nArchiveLink\\nSAP Supplier Relationship Management\\nSAP Customer Relationship Management\\nSAP ERP Human Capital Management—Processes and\\nForms\\nSAP Governance, Risk and Compliance\\nSAP Fiori and Mobility\\nAP Master Data Governance',\n",
              " 'An extensive working vocabulary is a prerequisite for test-taking success on the Graduate Record Exam. This book presents 800 graduate-level words with definitions that frequently appear on the exam. Words are used in different contexts to familiarize test takers with their many variations. The book’s additional features include a pre-test that helps to diagnose weaknesses, a lengthy word list with extensive practice exercises, and a chapter that discusses and analyzes essential word roots. The book concludes with a post-test to assess progress. Answers are provided for all exercises and for all questions in the pre- and post-test.',\n",
              " 'In 1980, a brilliant young American scholar, George H. Gadbois, Jr., met five judges of the Supreme Court of India. The judges gave him astonishing details: about what they actually thought of their colleagues, about the inner workings and politics of the court, their interactions with the government and the judicial appointments process, among many other things. This was only the beginning. Over the course of that decade, Gadbois visited India on two more occasions and conducted over 116 interviews with more than sixty-six judges of the Supreme Court of India (nineteen of whom held the post of chief justice of India),\\nand others such as senior lawyers, politicians, relatives of deceased judges, and court staff. During each meeting, Gadbois diligently took down handwritten notes, which he later typed up on his typewriter, recording nearly every detail of what the judges had told him, sometimes to a fault.\\nRelying on these typewritten interviews, Abhinav Chandrachud sheds light on a decade of politics, decision-making and legal culture in the Supreme Court of India. This book yields a fascinating glimpse into the secluded world of the judges of the Supreme Court in the 1980s and earlier.',\n",
              " \"Following hot on the heels of last year's Top 10 bestseller, Ripley's Believe It or Not! 2018 offers a completely new assortment of strange-but-true facts and amazing stories! From the elephant who can play baseball, to the flowers that bloomed in space, to the weasel found inside the world's biggest computer, Ripley's compendium of hair-raising oddities will delight and fascinate the whole family.\\n\\nComplete with a mind-blowing 3D-effect cover, and packed with gob-smacking photos and illustrations, Ripley's Believe It or Not! 2018 offers another feast of mind-blowing tales of the extraordinary. Prepare yourself for this year's Ripley's bonanza!\",\n",
              " 'Designed for complete beginners, and tested for years with real learners, Complete Sanskrit offers a bridge from the textbook to the real world, enabling you to learn the grammar, understand the vocabulary and even how to translate the inscriptions and texts from this ancient and religiously significant Indian language.\\nStructured around authentic material, and introducing the Devangari script for those who wish to take their understanding further, this first updated new edition for some twenty years also features:\\n-15 learning units plus glossary and reference section\\n-Authentic materials - language taught through key texts\\n-Teaches the key skills - reading and understanding Sanskrit grammar and vocabulary\\n-Covers Devangari script\\n-A new Preface and updated further resources\\n-Additional learning activities\\n-Self tests and learning activities - see and track your own progress\\nRely on Teach Yourself, trusted by language learners for over 75 years.',\n",
              " \"This debut novel from US cartoonist Terri Libenson follows two girls who could not seem more different: shy, observant, wallflower Emmie; and loud, popular, cheery Katie. What both girls do have in common are their strong feelings for the same boy, Tyler Ross. Then Emmie's very private, very embarrassing scribbles fall into the wrong hands . . .\",\n",
              " \"**Over 1 million copies sold worldwide**\\nMAJOR NEW EDITION\\n\\nFrom Nobel laureate Joseph Stiglitz, Globalization and its Discontents is the bestselling exposé of the all-powerful organizations that control our lives.\\nJoseph Stiglitz's landmark book lifted the lid on how globalization was hurting those it was meant to help. Many of its predictions came true, and it became a touchstone in the debate. This major new edition looks afresh at the continuing mismanagement of globalization, and how it has led to our current political and economic discontents. Globalization can still be a force for good, Stiglitz argues. But the balance of power has to change. Here he offers real, tough solutions for the future.\\n'A massively important political as well as economic document ... we should listen to him urgently' Will Hutton, Guardian\\n'Stiglitz is a rare breed, an heretical economist who has ruffled the self-satisfied global establishment that once fed him. Globalization and its Discontents declares war on the entire Washington financial and economic establishment' Ian Fraser, Sunday Herald\\n'Gripping ... this landmark book shows him to be a worthy successor to Keynes' Robin Blackburn, Independent\",\n",
              " 'Following the bestselling success of Carlton\\'s man-size Commando anthologies, here is the latest of the pocket-format compilations each containing of three classic Commando war stories printed the same size as the original comics. \"Who Dares Wins\" features \"Sabotage\", \"The Secret Heroes\" and \"Ramsey\\'s Raiders\". Tales of extraordinary courage behind enemy lines make this action-packed but handily formatted collection of stories a thrill for Commando fans of all ages.',\n",
              " \"Explore the rich history of cinema like never before - from the golden age of black-and-white films to international art-house and 21st-century sci-fi - uncovering the key themes and big ideas behind more than 80 of the world's most celebrated cinematic gems.\\nBeginning with the iconic La Voyage Dans La Lune (1902) and ending with Richard Linklater's ground-breaking Boyhood (2014), The Movie Book chronicles more than 100 of the best films ever made - from comedies and dramas, to animations, documentaries, and brings cinema to life.\\nDiscover everything about your favourite movies, as well as celebrated classics, the films you need to see, through iconic quotes and film stills, posters, biographies, movie memorabilia and narrative timelines.\\nEssential for anyone with a passion for cinema, The Movie Book is ready for its close-up.\",\n",
              " 'Warren Buffett built Berkshire Hathaway into something remarkable— and Fortune journalist Carol Loomis had a front-row seat for it all.\\nWhen Carol Loomis first mentioned a little-known Omaha hedge fund manager in a 1966 Fortune article, she didn’t dream that Warren Buffett would one day be considered the world’s greatest investor—nor that she and Buffett would quickly become close personal friends. As Buf\\xadfett’s fortune and reputation grew over time, Loomis used her unique insight into Buffett’s thinking to chronicle his work for Fortune, writ\\xading and proposing scores of stories that tracked his many accomplishments—and also his occa\\xadsional mistakes.\\nNow Loomis has collected and updated the best Buffett articles Fortune published between 1966 and 2012, including thirteen cover stories and a dozen pieces authored by Buffett himself. Loomis has provided commentary about each major arti\\xadcle that supplies context and her own informed point of view. Readers will gain fresh insights into Buffett’s investment strategies and his thinking on management, philanthropy, public policy, and even parenting. Some of the highlights include:\\nThe 1966 A. W. Jones story in which Fortune first mentioned Buffett.\\nThe first piece Buffett wrote for the magazine, 1977’s “How Inf lation Swindles the Equity Investor.”\\nAndrew Tobias’s 1983 article “Letters from Chairman Buffett,” the first review of his Berk\\xadshire Hathaway shareholder letters.\\nBuffett’s stunningly prescient 2003 piece about derivatives, “Avoiding a Mega-Catastrophe.”\\nHis unconventional thoughts on inheritance and philanthropy, including his intention to leave his kids “enough money so they would feel they could do anything, but not so much that they could do nothing.”\\nBill Gates’s 1996 article describing his early impressions of Buffett as they struck up their close friendship.\\nScores of Buffett books have been written, but none can claim this work’s combination of trust between two friends, the writer’s deep under\\xadstanding of Buffett’s world, and a very long-term perspective.',\n",
              " 'What is the perfect prescription for broken heart? It is a full close of love-cetmol? Suturing the cut ends? A cardiac transplantation? Or a life time of love with hugs and smiles thrice a day?\\nHrudi, a heartbroken and calumniated gynecologist, decides that the perfect prescription to her unending endurance is to put a full stop to her life once and for all. Inebriated by a rush of nostalgia, she chooses to relieve the past once again, before finally finding her eternal in peace.\\nThis journey back in time takes her to her college days, where she falls into step with Aditi since day one and was always at loggerheads with the hunk, Hrishikesh Datta. But everything changed when the cupid struck his arrow on both the parties and life became even more beautiful, until the inevitable happened, crumpling down her world like a pack of cards.\\nWhen she was still coping with the heartache, another blow hit her like a wrecking ball and she was razed to the ground once again. As she prepares to end her life, Hrish awaits the one big confrontation that he feared for years.\\nWill she live through this?\\nWill her broken heart ever be mended?',\n",
              " 'प्रस्तुत शब्द कोष में उर्दू के बहु प्रचलित शब्दों को जमा किया गया है जो अच्छी हिंदी का आवश्यक अंग बन गए है जिनकी सहाय्यतासे उर्दू साहित्य का अच्छा दन्यांन प्राप्त किया जा सकता है! यह विशेष ध्यान रखा गया है कि सभी जरूरी शब्द इसमें अवश्य लिए जाएँ! यह आज की नयी पीढ़ी के उन पाठकों के लिए बहुत उपयोगी है जो उर्दू शब्दों, मुहावरों और शायरी को सुनकर दिलचस्पी का इजहार तो करते है, लेकिन उर्दू भाषा से अपरिचित होने का कारण उसका आनंद नहीं उठा पाते!',\n",
              " 'There is perhaps no political figure in modern history who did more to secure and protect the Indian nation than Sardar Vallabhbhai Patel. But, ironically, seventy years after Patel brought together piece by piece the map of India by fusing the princely states with British India to create a new democratic, independent nation, little is understood or appreciated about Patel\\'s enormous contribution to the making of India. Caricatured in political debate, all the nuances of Patel\\'s difficult life and the daring choices he made are often lost, or worse, used as mere polemic. If Mahatma Gandhi was the spiritual core of India\\'s freedom struggle and Jawaharlal Nehru its romantic idealism, it was Sardar Patel who brought in the vital pragmatism which held together the national movement and the first ideas of independent India. A naturally stoic man, Patel, unlike Gandhi or Nehru, wrote no personal history. He famously argued that its was better to create history than write it. This is why even his deepest misgivings and quarrels have been easily buried. But every warning that Patel left for India - from the dangers of allowing groups to create private militias to his thoughtful criticism on India\\'s approach to Kashmir, Pakistan and China - are all dangerously relevant today. It is impossible to read about Patel, who died in 1950, and not feel that had he lived on, India might have been a different country. It is also impossible to ignore Patel and understand not only what the idea of India is but also what it could have been, and might be in the future. The Man Who Saved India is a sweeping, magisterial retelling of Sardar Patel\\'s story. With fiercely detailed and pugnacious anecdotes, multiple award-winning, best-selling writer Hindol Sengupta brings alive Patel\\'s determined life of struggle and his furious commitment to keep India safe. This book brings alive all the arguments, quarrels and clashes between some of the most determined people in Indian history and their battle to carve out an independent nation. Through ravages of a failing body broken by decades of abuse in and outside prison, Patel stands out in this book as the man who, even on his death bed, worked to save India. Hindol Sengupta\\'s The Man Who Saved India is destined to define Patel\\'s legacy for future generations.\\nPraise for the book:\\nSardar Patel was the silent one of the trinity along with Gandhi and Nehru who dedicated his life. In the struggle for an independent India. His lasting legacy is a United India rather than the land which throughout history has-been split in rival warring kingdoms. Hindol Sengupta has given us the story of Sardar\\'s life for the new generations of India so that they could understand and admire a unique personality. Read this book and discover India\\'s history in the first half the last century. And reclaim your legacy.\\n- Lord Meghnad Desai, bestselling author and economist\\n\"It is dangerous to put dreamers in power. Sardar Patel\\'s pragmatism was the perfect antidote to Nehru\\'s idealism in the early years of Independence. If only Patel had lived longer, India would have been spared the excesses of the License Raj and the Kashmir problem. This is one of the messages of this lively, highly readable book.\"\\n- Gurcharan Das, bestselling author\\nThe Man Who Saved India is the most authoritative and accessible biography of Sardhar Vallabhbhai Patel, lovingly referred to as the \\'Iron Man of India\\'. In this book, written in his impeccable narrative style, Hindol Sengupta rescues the memory of the beloved leader of India from the vaults of obscurity. This book brings out the true story of independence as well as stability of India following it, which was achieved through sweat and blood of its leaders like Patel.\\nThe Man who Saved India reverses one of the historical ironies of modern India through bringing into light many of the unknown facts of the life of Patel, based on the author\\'s field visits, interviews, and extensive research, which is an onerous task in itself considering the fact that Patel neither maintained records of his work nor preserved his documents of communications.\\nHindol begins the narration of larger than life tale of Patel describing his visit to the birth home of Patel in Nadiad, now in dilapidated condition, an image not unlike the less than optimal public memory of Patel in modern India.\\nHindol discusses the numerous sacrifices Patel has made in public life bowing to the political ambitions of his peers, while never wavering from his duty to India. The Man who Saved India clearly charts the leadership skills and statesmanship of Patel during the numerous non-cooperation and civil disobedience movements such as Bardoli and Kheda Satyagrahas, and the timely military action in independent India. It was due to the sheer strength of resolve on the part of Patel, which has helped forge the Bharat that was to capture the imagination of the masses of independent India.\\nThe hitherto unknown details of Patel\\'s personal life and his complex relations with his peers and other contemporary national leaders including Gandhi, Nehru, and Bose, help understand the grace with which he gave up political positions more than once. In addition, the author also clearly describes the personal sacrifices Patel has made at the altar of mother India, not only of his personal life, but also of the life of his children, even to the chagrin of his family and friends.\\nThe amazing result of author\\'s copious research is the emergence of an exceptionally clear picture of Patel\\'s life and leadership in the three decades leading up to the independence of India in addition to establishing Patel\\'s key role in the formative years of India following independence, until his death. The Man who Saved India is a timely and much needed historical account of modern India.\\nThe Man who Saved India is a must read for every Indian as well as every person interested in learning the true history of India.\\n- Lavanya Vemsani, Professor, Shawnee State University, Vice President, Ohio Academy of History, President & Cofounder, American Academy of Indic Studies\\nEvery nation has its own narrative that is built over time, even centuries. For India and as Indians we claim ancient ancestry and are therefore, civilisational. Yet, we are a new democratic republic trying to find our place in the new tumultuous 21st century.\\nThere comes a time when we cannot look forward without revisiting our past to see if the narrative of the past was complete, accurate and fair. For decades, the narrative has been that there were essentially two leaders in India - Mahatma Gandhi and his protégé Jawahar Lal Nehru, who together had led the country to independence. Their contribution to the cause of independence was monumental and this became the widely accepted truth. Yet there was another truth, long ignored in our national narrative.\\nHindol Sengupta\\'s book, The Man Who Saved India, provides the other truth. In a meticulously and extensively researched book, the young author brings to the reader the significant role that Sardar Vallabhbhai Patel played not only in our struggle for independence but in the consolidation of the new country. Valabhbhai Patel was truly a part of the Trinity, along with Mahatma Gandhi and Jawahar Lal, that led India to freedom. He was the man who ensured that this newfound independence did not collapse in a heap of dust. It was the Sardar who dealt with the reluctant Maharajas, Nawabs and the obdurate Nizam of Hyderabad, when they dreamt of returning to their feudal opulence outside modern India. Patel liquidated the princely states without liquidating the princes. It was his commitment, diplomatic and political skills along with the force of his personality that made unified India a reality.\\nThe book begins with a description of what was once the ancestral home of this great man of India. This decrepit and rather lonely state of the house and its utter neglect is a clear indication that the prevalent narrative preferred to ignore Valabhbhai Patel\\'s contribution. Gandhi knew the value of God and religion in an India that was subjugated. Nehru knew that if India had to talk about its future then there had to be a grand past of aloofness and elitism. It was Patel who was the hard realist who knew that democracy was not about daily plebiscites but hard decisions cloaked in egalitarianism. Nehru looked at the heavens for inspiration Patel looked at the ground beneath his feet for solutions. While Nehru wrote elegant prose and Gandhi spoke to the masses, it was left to Patel to worry over mundane matters about funds and their distribution. Patel was a man of few words and there is every reason to give him credit today for many of his arguments and ideas ranging from tackling Kashmir, the future of Pakistan and how socialism without industrialization could be dangerous for the country. His warnings about Kashmir, Tibet and China went unheeded.\\nThe book is thus not only one of the finest biographies in recent times but is also a much-needed redefining of the roles played by Sardar Patel during India\\'s freedom movement and as the great unifier. The author argues that Patel was not only a pillar of strength behind some of Gandhi\\'s earlier successes to holding the country together.\\nThe author quotes Patel\\'s speech on 5 July 1947 where he warned, \"Our mutual conflicts and internecine quarrels and jealousies have in the past been the cause of our downfall and our falling victim to foreign domination a number of times. We cannot afford to fall into these errors or traps again.\" Sardar Patel was relevant then as he is today. So is Hindol Sengupta\\'s book.\\n- Vikram Sood, bestselling author and the former head of India\\'s foreign intelligence agency, the Research and Analysis Wing\\nThe Man Who Saved India is an excellent deep dive into the life and struggles of one of the tallest leaders in Indian history. Hindol Sengupta written a fascinating book full of insights on things that are rarely discussed - for instance, Patel\\'s economic ideas or his key role as the man who raised critical funds for the Congress Party. This captivating book breaks many myths and throws new light on one of the most important figures in Indian history.\\n- Vijay Govindarajan, NYT and Wall Street Journal bestselling author, Coxe Distinguished Professor at the Tuck School of Business at Dartmouth College\\nA very engaging biography of Sardar Patel, the man and his times, by one of India\\'s best young writers\\n- Sanjeev Sanyal, bestselling author and Principal Economic Advisor, Ministry of Finance, Government of India\\nThe genre of popular history and biography has been experiencing a golden age and Hindol Sengupta\\'s fluent biography of Sardar Vallabhai Patel joins a list of distinguished titles alongside Michael Axworthy\\'s Sword of Persia, Nadir Shah and Roger Crowley\\'s Constantinople, The Last Great Siege, 1453. Very welcome indeed that an eminently accessible account of the life and achievements of one of India\\'s true great sons is now available to a new generation of readers.\\n- Gautam Sen, lecturer (retd.), London School of Economics and co-author of Analysing the Global Political Economy, Princeton University Press, 2009.\\nRich with detail and illuminating insight, Hindol Sengupta\\'s The Man Who Saved India brings alive Sardar Vallabhbhai Patel\\'s indomitable spirit and tenacity in the face of constant challenges that would crush a weaker man. Few people immediately think of Patel when they think of men responsible for the shape and form of modern India. This is a great injustice, for, as Hindol explains with a wealth of anecdote and context, it was Patel who defined the very contours of the India we know today. This book is a must read.\\n- - Saradindu Mukherji, Member, Indian Council of Historical Research\\nHindol writes popular, unpopular history. Another brilliant book, this time putting the untold story of one of India\\'s greatest political leaders in front of today\\'s readers in a way which makes it accessible and unputdownable. His work of putting together Sardar Patel\\'s life story is strikingly accurate yet stunningly engrossing.\\n- Vikramjit Bannerjee, Senior Advocate, Supreme Court of India, and Advocate General of Nagaland\\n\\'This is one of those rare, great books which every Indian should read. After all, it is also about Vallabhbhai Patel, the man who created the modern state of India out of myriad fragments. Drawing upon a well-researched base of facts and writings, Sengupta emerges as a masterful storyteller who has weaved together a very coherent and absorbing story. The complex relationships among the leaders of the Indian freedom struggle are captured in all their subtle shades and colourful complexity. The end result is, for the reader, a more holistic understanding of Indian history, a much-needed filling up of certain gaps in our knowledge of the past, and a refreshingly enriched perspective on the architects of modern India\\'\\n-N.C. Suresh, UB Distinguished Professor, School of Management, State University of New York, Buffalo',\n",
              " 'A former UN worker and prominent architect, Johan van Lengen has seen firsthand the desperate need for a \"greener\" approach to housing in impoverished tropical climates. This comprehensive book clearly explains every aspect of this endeavor, including design (siting, orientation, climate consideration), materials (sisal, cactus, bamboo, earth), and implementation. The author emphasizes throughout the book what is inexpensive and sustainable. Included are sections discussing urban planning, small-scale energy production, cleaning and storing drinking water, and dealing with septic waste, and all information is applied to three distinct tropical regions: humid areas, temporate areas, and desert climates. Hundreds of explanatory drawings by van Lengen allow even novice builders to get started.',\n",
              " 'Ray Bradbury’s internationally acclaimed novel Fahrenheit 451 is a masterwork of twentieth-century literature set in a bleak, dystopian future.\\n\\nRay Bradbury’s internationally acclaimed novel Fahrenheit 451 is a masterwork of twentieth-century literature set in a bleak, dystopian future.\\n\\nGuy Montag is a fireman. In his world, where television rules and literature is on the brink of extinction, firemen start fires rather than put them out. His job is to destroy the most illegal of commodities, the printed book, along with the houses in which they are hidden.\\n\\nMontag never questions the destruction and ruin his actions produce, returning each day to his bland life and wife, Mildred, who spends all day with her television “family.” But then he meets an eccentric young neighbor, Clarisse, who introduces him to a past where people didn’t live in fear and to a present where one sees the world through the ideas in books instead of the mindless chatter of television.\\n\\nWhen Mildred attempts suicide and Clarisse suddenly disappears, Montag begins to question everything he has ever known. He starts hiding books in his home, and when his pilfering is discovered, the fireman has to run for his life.',\n",
              " \"Ten years ago, Calamity came. It was a burst in the sky that gave ordinary men and women extraordinary powers. The awed public started calling them Epics. But Epics are no friend of man. With incredible gifts came the desire to rule. And to rule man you must crush his wills.\\nNobody fights the Epics... nobody but the Reckoners. A shadowy group of ordinary humans, they spend their lives studying Epics, finding their weaknesses, and then assassinating them.\\nAnd David wants in. He wants Steelheart - the Epic who is said to be invincible. The Epic who killed David's father. For years, like the Reckoners, David's been studying, and planning - and he has something they need. Not an object, but an experience.\\nHe's seen Steelheart bleed. And he wants revenge.\",\n",
              " \"Not every door should be opened . . . With stunning locations and page-turning tension, The Paris Secret is an intense and gripping tale from bestselling author Karen Swan, the perfect escapism for fans of Santa Montefiore. Somewhere along the cobbled streets of Paris, an apartment lies thick with dust and secrets: full of priceless artworks hidden away for decades. High-flying fine art agent Flora from London, more comfortable with the tension of a million-pound auction than a cosy candlelit dinner for two, is called in to assess these suddenly discovered treasures. As an expert in her field, she must trace the history of each painting and discover who has concealed them for so long. Thrown in amongst the glamorous Vermeil family as they move between Paris and Antibes, Flora begins to discover that things aren't all that they seem, while back at home her own family is recoiling from a seismic shock. The terse and brooding Xavier Vermeil seems intent on forcing Flora out of his family's affairs – but just what is he hiding?\",\n",
              " \"When one Garfield isn't enough - there's only one thing better than a Garfield collection: three Garfield collections! Garfield goes to his happy place: to Garfield, happiness is a full tummy (followed by a long nap). fans of the fat cat, hungry for laughs, can go to their happy place when they gobble up this latest treasure of pleasure! Garfield the big cheese: head honcho Garfield takes charge in this latest collection of Comics. The fat cat is up to his usual funny business-pestering Jon, punting Odieand pounding down pasta-and it's a true labour of love! Garfield cleans his plate: when it comes to eating-and entertainment-Garfield is a natural. Every time he opens his pie hole, something fattening goes in or something funny comes out. Fans of the portly Puss can follow his latest antics in this fun-filled new collection of Comics.\",\n",
              " \"The Matarese killers will take over the world within two years...The No. 1 bestseller from 'the world's most read writer' GQ\\nThe Matarese Circle are the Corsican Brotherhood. Terrifying killers, their power could plunge the world into chaos and destruction within two years.\\nOnly two rival spies - and one mysterious woman - can stop them. Brandon Scofield, CIA, and the West's most professional assassin, and Vasili Taleniekov, former KGB, now wanted by the Politburo. They share a genius for espionage - and a life of terror and explosive violence.\\nSworn enemies, Scofield and Taleniekov have vowed to terminate each other, but now they must become allies. Because only they possess the brutal skills and ice-cold nerves necessary to destroy an international circle of killers: the Matarese...\",\n",
              " 'You Think You Know Your Partner? Think Again.\\nTwenty-seven-year-old Kiyan Roy is the reclusive author of the bestselling erotica trilogy, Handcuffs. When he appears in public for the first time to promote his books, his readers fall in love with his good looks and wit. However, one of them gets too close. Kiyan is followed and seduced by a mysterious girl who pursues him across cities, book events, hotels, luncheons and media interviews. Soon, he becomes obsessed by her and falls for her charms.\\nAs Kiyan gets sucked deeper and deeper into this dark and twisted love affair, he is forced to surrender all control. His life and career slowly begin to unravel as this girl leads him down a dangerous path. But is it already too late to rectify his mistakes? Will the path to desire end in doom? Black Suits You is a gripping, fast-paced and a clever psychosexual thriller that will keep you guessing till the end.',\n",
              " 'The Beautiful Disaster phenomenon continues in the first heart-pounding new adult romance in The Maddox Brothers series. Fiercely independent Camille \"Cami\" Camlin gladly moved on from her childhood before it was over. She has held down a job since before she could drive, and moved into her own apartment after her freshman year of college. Now tending bar at The Red Door, Cami doesn\\'t have time for much else besides work and classes, until a trip to see her boyfriend is cancelled, leaving her with her first weekend off in almost a year. Trenton Maddox was the king of Eastern State University, dating co-eds before he even graduated high school. His friends wanted to be him, and women wanted to tame him, but after a tragic accident turned his world upside down, Trenton leaves campus to come to grips with the crushing guilt. Eighteen months later, Trenton is living at home with his widower father, and works full-time at a local tattoo parlour to help with the bills. Just when he thinks his life is returning to normal, he notices Cami sitting alone at a table at The Red. As the baby sister of four rowdy brothers, Cami believes she\\'ll have no problem keeping her new friendship with Trenton Maddox strictly platonic. But when a Maddox boy falls in love, he loves forever - even if she is the only reason their already broken family could fall apart. In the first instalment of The Maddox Brothersseries, readers can experience the rush of reading Beautiful Disasterfor the first time, all over again.',\n",
              " \"Danielle Steel proves she is the world's favourite storyteller with Against All Odds, a powerful story of a mother's unconditional love. Style. Success. Secrets. Kate Madison’s stylish boutique has been a big success in New York, supporting her and her four kids since her husband’s untimely death. Now, her children have grown up and are ready to forge lives of their own. Isabelle, a dedicated attorney, falls for a client in a criminal case. She tells herself she can make a life with him – but can she? Julie, a young designer, meets a man who seems too good to be true. She gives up her job and moves to LA to be at his side, ignoring the danger signs. Justin is a struggling writer who pushes his partner for children before they’re financially or emotionally ready. And Willie, the youngest, makes a choice that shocks them all . . . For Kate – loving, supportive and outspoken – the hardest lesson will be that she can’t protect her children from their choices, but can only love them as they make them.\",\n",
              " \"Join LEGO® Batman™, LEGO Superman™, LEGO Wonder Woman™ and a whole host of LEGO DC Super Heroes characters as they team up in this exciting LEGO book for children beginning to read.\\nThe most-loved LEGO DC Super Heroes characters come together in this book to create an awesome Super Hero team! LEGO DC Super Heroes Amazing Battles features popular LEGO DC Super Heroes sets and minifigures. This fun LEGO book sparks kids' imaginations with its captivating story and engaging LEGO DC Super Heroes characters.\\nCombining DK's four-level reading scheme with the popular LEGO DC Super Heroes theme, LEGO DC Super Heroes Amazing Battles is at Level 2 and will help kids learn to read - and to love reading.\\nLEGO, the LEGO logo, the Brick and Knob configurations and the Minifigure are trademarks of the LEGO Group. ©2014 The LEGO Group. All rights reserved.\\nProduced by DK Publishing under licence from the LEGO Group.\\nCopyright © 2014 DC Comics.\\nBATMAN and all related characters and elements are trademarks of and © DC Comics.\\nWB SHIELD: ™ & © Warner Bros. Entertainment Inc.\\n(s14)\",\n",
              " 'In his riveting new book, The Art of Learning, Waitzkin tells his remarkable story of personal achievement and shares the principles of learning and performance that have propelled him to the top—twice.\\n\\nJosh Waitzkin knows what it means to be at the top of his game. A public figure since winning his first National Chess Championship at the age of nine, Waitzkin was catapulted into a media whirlwind as a teenager when his father’s book Searching for Bobby Fischer was made into a major motion picture. After dominating the scholastic chess world for ten years, Waitzkin expanded his horizons, taking on the martial art Tai Chi Chuan and ultimately earning the title of World Champion. How was he able to reach the pinnacle of two disciplines that on the surface seem so different? “I’ve come to realize that what I am best at is not Tai Chi, and it is not chess,” he says. “What I am best at is the art of learning.”\\n\\nWith a narrative that combines heart-stopping martial arts wars and tense chess face-offs with life lessons that speak to all of us, The Art of Learning takes readers through Waitzkin’s unique journey to excellence. He explains in clear detail how a well-thought-out, principled approach to learning is what separates success from failure. Waitzkin believes that achievement, even at the championship level, is a function of a lifestyle that fuels a creative, resilient growth process. Rather than focusing on climactic wins, Waitzkin reveals the inner workings of his everyday method, from systematically triggering intuitive breakthroughs, to honing techniques into states of remarkable potency, to mastering the art of performance psychology.\\n\\nThrough his own example, Waitzkin explains how to embrace defeat and make mistakes work for you. Does your opponent make you angry? Waitzkin describes how to channel emotions into creative fuel. As he explains it, obstacles are not obstacles but challenges to overcome, to spur the growth process by turning weaknesses into strengths. He illustrates the exact routines that he has used in all of his competitions, whether mental or physical, so that you too can achieve your peak performance zone in any competitive or professional circumstance.\\n\\nIn stories ranging from his early years taking on chess hustlers as a seven year old in New York City’s Washington Square Park, to dealing with the pressures of having a film made about his life, to International Chess Championships in India, Hungary, and Brazil, to gripping battles against powerhouse fighters in Taiwan in the Push Hands World Championships, The Art of Learning encapsulates an extraordinary competitor’s life lessons in a page-turning narrative.',\n",
              " \"Turgenev's timeless tale of generational collision, in a sparkling new translation\\n\\nWhen Arkady Petrovich returns home from college, his father finds his eager, naïve son changed almost beyond recognition, for the impressionable Arkady has fallen under the powerful influence of the friend he has brought home with him. A self-proclaimed nihilist, the ardent young Bazarov shocks Arkady's father with his criticisms of the landowning way of life and his determination to overthrow the traditional values of contemporary society. Vividly capturing the hopes and fears, regrets and delusions of a changing Russia around the middle of the nineteenth century, Fathers and Sons is Ivan Turgenev's masterpiece.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.\",\n",
              " \"  An eye-catching new edition of this Horrible Histories Special - with all the gruesome bits left in!\\nHORRIBLE HISTORIES SPECIAL: ENGLAND lets readers discover all the foul facts about England, including:\\nwhich monk tried to pinch the devil's nose with a pair of tongs,\\nwhy some people in the Middle Ages ate dove droppings\\nand which English king was accused of being a werewolf.\\nWith a bold look these bestselling titles aresure to be a huge hit with yet another generation of Terry Deary fans.\\nWith shiny foil cover\",\n",
              " 'Presumed Innocent meets Gone Girl in an explosive new psychological thriller from New York Times bestselling author James Grippando.\\n\\nA respected prosecuting attorney in Miami, Abe Beckham is called in to consult when the FBI discovers a woman’s body in the nearby Everglades. At first the agents think the woman is the victim of a serial killer who preys in the cane fields to the north. But despite some similarities, this murder soon proves to be different from the others. Then the cops find out that Abe knew the dead woman, a beautiful lawyer he finally admits to having dated briefly before his marriage. When Abe’s wife goes missing, he’s suddenly on the hot seat, going from prosecutor to suspect. He needs to find the killer fast—but he no longer knows whom he can trust.',\n",
              " \"A landmark publishing event of one of Japan's most famous cartoonists\\nShigeru Mizuki is the preeminent figure of Gekiga manga and one of the most famous working cartoonists in Japan today–a true living legend. Onward Towards Our Noble Deaths is his first book to be translated into English and is a semiautobiographical account of the desperate final weeks of a Japanese infantry unit at the end of WorldWar II. The soldiers are told that they must go into battle and die for the honor of their country, with certain execution facing them if they return alive. Mizuki was a soldier himself (he was severely injured and lost an arm) and uses his experiences to convey the devastating consequences and moral depravity of the war.\\nMizuki's list of accolades and achievements is long and detailed. In Japan, the life of Mizuki and his wife has been made into an extremely popular television drama that airs daily. Mizuki is the recipient of many awards, including the Best AlbumAward for his book NonNonBa (to be published in 2012 by D+Q) and the Heritage Essential Award for Onward Towards Our Noble Deaths at the Angoulême International Comics Festival, the Tezuka Osamu Cultural Prize Special Award, the Kyokujitsu Sho Decoration, the Shiju Hosho Decoration, and the KodanshaManga Award.His hometown of Sakaiminato honored him with Shigeru Mizuki Road―a street decorated with bronze statues of his Ge Ge Ge no Kitaro characters―and the Shigeru Mizuki International Cultural Center.\",\n",
              " '\"If you want to learn how to build efficient user interfaces with React, this is your book. Authors Alex Banks and Eve Porcello show you how to create UIs with this small JavaScript library that can deftly display data changes on large-scale, data-driven websites without page reloads. Along the way, youíll learn how to work with functional programming and the latest ECMAScript features.\\nDeveloped by Facebook and used by companies including Netflix, Walmart and The New York Times for large parts of their web interfaces, React is quickly growing in use. By learning how to build React components with this hands-on guide, youíll fully understand how useful React can be in your organization.\\n\\nLearn key functional programming concepts with JavaScript\\nPeek under the hood to understand how React runs in the browser\\nCreate application presentation layers by mounting and composing React components\\nUse component trees to manage data and reduce the time you spend debugging applications\\nExplore Reactís component lifecycle and use it to load data and improve UI performance\\nUse a routing solution for browser history, bookmarks and other features of single-page applications\\nLearn how to structure React applications with servers in mind\\n\"',\n",
              " 'This beautifully remastered deluxe $19.99 hardcover version of the best-selling THE BEST OF ARCHIE COMICS BOOK 2 is jam-packed with over 400 pages of the funniest and most iconic Archie stories of the past 70 years, lovingly hand-selected by Archie creators, editors, and historians from 200,000 pages of material.\\n\\nThis edition includes a full book redesign, additional decade by decade content and newly remastered art and coloring on selected stories. Designed to be enjoyed by both kids and adults together, THE BEST OF ARCHIE COMICS BOOK 2 DELUXE EDITION includes a decade by decade overview of Archie Comics in pop culture and introductions to each story by celebrities, Archie artists, writers, staff, and editors sharing why these beloved stories have become such an integral part of the American entertainment landscape.',\n",
              " \"When Potnis a captain in the Indian Army meets Pansy during the Operation\\nBlue Star he knows he has found the love of his life. Their passionate romance\\nleads them to the altar and blossoms into a beautiful child, Rihana.\\nHistory repeats itself when Advik, a wayward boy from a broken family,\\ncatapults into Rihana's heart, as smoothly as he conquers the skies as an Air\\nForce pilot. What follows is a night of unabashed love, transporting the couple to\\nheavenly bliss. Where will their unbridled love lead them? Will Rihanna remain a\\nmistress to Advik, for whom the love for his country comes first or will she be\\nable to make a place in his heart that is second to none? This heart-breaking\\nsaga of love, courage and sacrifice will leave you asking for more.\",\n",
              " 'The second collection of memoirs from the author who inspired the BBC series All Creatures Great and Small. Now settled into the sleepy Yorkshire village of Darrowby, and married to Helen the farmer’s daughter, James Herriot thinks he’s finally got himself sorted. But life as a vet in the 1930s was never going to be easy. Quite aside from his unpredictable colleagues, brothers Siegfried and Tristan Farnon, he must contend with new-fangled medical techniques, delivering calves after far too much home-made wine, and a grudge-holding dog called Magnus who never forgets. And then, with Britain on the verge of war, James faces a decision that could separate him from Darrowby – and Helen – for ever . . . Since they were first published, James Herriot’s memoirs have sold millions of copies and entranced generations of animal lovers. Charming, funny and touching, All Things Bright and Beautiful is a heart-warming story of determination, love and companionship from one of Britain’s best-loved authors. This omnibus edition comprises the majority of chapters from Let Sleeping Vets Lie and Vets in Harness.',\n",
              " \"In honour of the centennial of the birth of J.D. Salinger in 1919, Penguin reissues all four of his books in beautiful commemorative hardback editions - with artwork and text based on the very first Salinger editions published in the 1950s and 1960s.\\nThe hero-narrator of The Catcher in the Rye is an ancient child of sixteen, a native New Yorker named Holden Caulfield...\\nOne of the greatest American novels of all time, The Catcher in the Rye is a classic coming-of-age story: an elegy to teenage alienation, capturing the deeply human need for connection and the bewildering sense of loss as we leave childhood behind.\\n'A perfect novel ... it changed US culture forever' Independent\\n'It was a very pure voice he had. There was no one like him' Martin Amis\\n'He was the poet of youthful alienation before youth really knew what that was' Sunday Times\\n'His work meant a lot to me when I was a young person and his writing still sings now' Dave Eggers\",\n",
              " \"This pack contains 49 high-quality origami sheets printed in 6 different colors plus 1 bonus gold sheet.\\n\\nFolding is fun! — This affordable origami paper pack includes durable, authentic origami paper folding sheets in a variety of bright, vibrant colors that are perfect for any folder who wants to add a distinctive flair to their origami projects. The package includes folding sheets as well as an 8-page booklet with instructions so that folders can start right away.\\n\\nThis paper pack includes:\\n48 Sheets of high-quality origami paper\\n6 different bright colors\\nDouble-sided color\\n1 sheet of gold foil paper\\n6x6'' squares\\nIntroduction to basics and folding techniques\\nInstructions for several easy origami projects\",\n",
              " '\\'The best American political biography since Obama\\'s Dreams from My Father\\' Guardian\\n\\nNEW YORK TIMES BESTSELLER\\n\\nA mayor\\'s inspirational story of a Midwest city that has become nothing less than a blueprint for the future of American renewal.\\nOnce described by the Washington Post as \"the most interesting mayor you\\'ve never heard of,\" Pete Buttigieg, the thirty-seven-year-old mayor of South Bend, Indiana, has now emerged as one of America\\'s most visionary politicians. With soaring prose that celebrates a resurgent American Midwest, Shortest Way Home narrates the heroic transformation of a \"dying city\" (Newsweek) into nothing less than a shining model of urban reinvention.\\nElected at twenty-nine as the nation\\'s youngest mayor, Pete Buttigieg immediately recognized that \"great cities, and even great nations, are built through attention to the everyday.\" As Shortest Way Home recalls, the challenges were daunting?whether confronting gun violence, renaming a street in honor of Martin Luther King Jr., or attracting tech companies to a city that had appealed more to junk bond scavengers than serious investors. None of this is underscored more than Buttigieg\\'s audacious campaign to reclaim 1,000 houses, many of them abandoned, in 1,000 days and then, even as a sitting mayor, deploying to serve in Afghanistan as a Navy officer. Yet the most personal challenge still awaited Buttigieg, who came out in a South Bend Tribune editorial, just before being reelected with 78 percent of the vote, and then finding Chasten Glezman, a middle-school teacher, who would become his partner for life.\\nWhile Washington reels with scandal, Shortest Way Home, with its graceful, often humorous, language, challenges our perception of the typical American politician. In chronicling two once-unthinkable stories?that of an Afghanistan veteran who came out and found love and acceptance, all while in office, and that of a revitalized Rust Belt city no longer regarded as \"flyover country\" Buttigieg provides a new vision for America\\'s shortest way home.',\n",
              " 'The partition of India in 1947 caused one of the most harrowing human convulsions in history: over twelve million people were displaced amidst a frenzy of murder, rape and abduction on a massive scale. For decades these violent realities remained buried in silence, even though the memories of brutality never faded. Urvashi Butalia’s The Other Side of Silence was the first major work to exhume the personal trauma of the Partition. An undisputed classic, it meticulously locates the individual experiences and private pain at the heart of this cataclysmic event. Furthermore, Butalia reveals how people on the margins of history—children, women, ordinary people, the lower castes, the untouchables—were affected by this upheaval.\\n\\nIn a passionate and stimulating new introduction, Butalia examines not only recent developments in the expanding field of Partition studies but also the heart-breaking ways in which this colossal tragedy continues to impact our lives and what this means for the future of the Indian subcontinent.',\n",
              " 'In Peace of Mind, Zen Master Thich Nhat Hanh reminds us that integrating body and mind is the only way to feel truly alive in each moment.\\n\\nBringing together ancient wisdom and contemporary thinking on the subject of mindfulness, Peace of Mind is a deceptively simple book which provides a practical foundation for understanding the principles of mind/body awareness. As it introduces critical tools for sustaining authentic wellbeing, it helps us to take control of our lives, de-stress and find peace and happiness in this frantic world.',\n",
              " 'This superb translation of Death in Venice and six other stories by Thomas Mann is a tour de force, deserving to be the definitive text for English-speaking readers. These seven stories represent Mann’s early writing career and a level of literary quality Mann himself despaired of ever again matching. In these stories he began to grapple with themes that were to recur throughout his work. In Little Herr Friedemann, a character’s carefully structured way of life is suddenly threatened by an unexpected sexual passion. In Gladius Dei, puritanical intellect clashes with beauty. In Tristan, Mann presents an ironic and comic account of the tension between an artist and bourgeois society.\\n\\nAll seven of these stories are accomplished and memorable, but it is Death in Venice that truly forms the centerpiece of the collection. The themes that Mann weaves through the shorter pieces come to a climax in this stunning novella, one of the most hauntingly magnificent tales of art and self-destruction ever written.',\n",
              " 'My Autobiography (Modern Classics) is the autobiography of one of the most prominent and well known comedians worldwide, Charles Charlie Chaplin. Charlie Chaplin played the role of a comedian in many silent movies and is praised till today for the his legendary slapstick routines\\nMy Autobiography (Modern Classics) talks about the life of one of the greatest filmmakers and comedians, with an introduction by David Robinson. The book tells the reader about the events in the childhood of the Charlie Chaplin that drove him to dream to become an actor, and chronicles the various hardships he faced in reaching his goal. Having been born into a theatrical family, the world of acting was never anew to Chaplin but he faced many challenges due to poverty and the untimely death of his drunkard father, leaving behind his mother mentally unstable and unable to cope with the harsh economy. All these factors drove Chaplin to make his first official debut in acting on the Music Hall Stage. The book also talks about the worldwide fame Charlie Chaplin acquired after the success of his film The Little Tramp, and also the various harsh events that followed suit in the form of failed marriages and personal scandals which lead to his exile from Hollywood.\\nThe book was published in 2003 by Penguin UK and is available in paperback.',\n",
              " 'CCENT/CCNA ICND1 Official Certification Guide is a comprehensive self-study tool for professionals preparing for the new ICND1 exam. Complete coverage of all exam topics as posted on the exam topic blueprint ensures readers will arrive at a thorough understanding of what they need to master to succeed on the exam.',\n",
              " \"Oskar and Eli. In very different ways, they were both victims. Which is why, against the odds, they became friends. And how they came to depend on one another, for life itself. Oskar is a 12-year-old boy living with his mother on a dreary housing estate at the city's edge. He dreams about his absentee father, gets bullied at school and wets himself when he's frightened. Eli is the young girl who moves in next door. She doesn't go to school and never leaves the flat by day. She is a 200-year-old vampire, forever frozen in childhood and condemned to live on a diet of fresh blood. John Ajvide Lindqvist's novel is a unique and brilliant fusion of social novel and vampire legend, a deeply moving fable about rejection, friendship and loyalty.\",\n",
              " 'Experience learning made easy—and quickly teach yourself how to organize, analyze, and present data with Excel 2007. With Step By Step, you set the pace—building and practicing the skills you need, just when you need them!\\n\\nCreate formulas, calculate values, and analyze data\\nPresent information visually with graphics, charts, and diagrams\\nBuild PivotTable dynamic views—even easier with new data tables\\nReuse information from databases and other documents\\nShare spreadsheets for review and manage changes\\nCreate macros to automate repetitive tasks and simplify your work\\n\\n\\nYour all-in-one learning experience includes:\\n\\nFiles for building skills and practicing the book’s lessons\\nFully searchable eBook\\nBonus quick reference to the Ribbon, the new Microsoft Office interface\\nWindows Vista® Product Guide eReference—plus other resources on CD\\nA Note Regarding the CD or DVD\\n\\nThe print version of this book ships with a CD or DVD. For those customers purchasing one of the digital formats in which this book is available, we are pleased to offer the CD/DVD content as a free download via OReilly Medias Digital Distribution services. To download this content, please visit OReillys web site, search for the title of this book to find its catalog page, and click on the link below the cover image (Examples, Companion Content, or Practice Files). Note that while we provide as much of the media content as we are able via free download, we are sometimes limited by licensing restrictions. Please direct any questions or concerns to booktech@oreilly.com.',\n",
              " 'From the #1 New York Times bestselling author of It Ends With Us and November 9 comes a moving and haunting novel of family, love and the power of the truth.\\n“Not every mistake deserves a consequence. Sometimes the only thing it deserves is forgiveness.”\\nThe Voss family is anything but normal. They live in a repurposed church, newly baptized Dollar Voss. The once cancer-stricken mother lives in the basement, the father is married to the mother’s former nurse, the little half-brother isn’t allowed to do or eat anything fun and the eldest siblings are irritatingly perfect. Then, there’s Merit.\\nMerit Voss collects trophies she hasn’t earned and secrets her family forces her to keep. While browsing the local antiques shop for her next trophy, she finds Sagan. His wit and unapologetic idealism disarm and spark renewed life into her—until she discovers that he’s completely unavailable. Merit retreats deeper into herself, watching her family from the sidelines, when she learns a secret that no trophy in the world can fix.\\nFed up with the lies, Merit decides to shatter the happy family illusion that she’s never been a part of before leaving them behind for good. When her escape plan fails, Merit is forced to deal with the staggering consequences of telling the truth and losing the one boy she loves.\\nPoignant and powerful, Without Merit explores the layers of lies that tie a family together and the power of love and truth.',\n",
              " \"A Newsweek Best Graphic Novel of the Year\\n\\nThe impossible has happened: The Joker has become a ... hero?\\n\\nBatman: White Knight follows the man now known as Jack Napier as he embarks on a quest to heal the city he once terrorized. After reconciling with his long-suffering partner, Harley Quinn, he sets in motion a carefully plotted campaign to discredit the one person whom he views as Gotham City's true enemy: Batman.\\n\\nHis crusade exposes a decades-long history of corruption within the Gotham City Police Department and transforms Napier into a city councilman and civic hero. But when the sins of his past return to threaten everything that he has accomplished, the distinctions between savior and destroyer begin to break down for both The Joker and Batman alike--and with them any hope for Gotham's future.\\n\\nWriter and artist Sean Murphy delivers an extraordinary examination of comics' greatest antagonists in Batman: White Knight, exploring justice, corruption, activism and the darkest depths of mental illness. Collecting the acclaimed eight-issue miniseries, this stunning graphic novel also marks the debut of the DC Black Label imprint, which features classic DC characters in standalone stories written and illustrated by world-class authors and artists.\",\n",
              " \"In this extraordinary novel, Stingo, an inexperienced twenty-two year old Southerner, takes us back to the summer of 1947 and a boarding house in a leafy Brooklyn suburb. There he meets Nathan, a fiery Jewish intellectual; and Sophie, a beautiful and fragile Polish Catholic. Stingo is drawn into the heart of their passionate and destructive relationship as witness, confidant and supplicant. Ultimately, he arrives at the dark core of Sophie's past: her memories of pre-war Poland, the concentration camp and - the essence of her terrible secret - her choice.\",\n",
              " '\"Join the technological revolution thatís taking the financial world by storm. Mastering Bitcoin is your guide through the seemingly complex world of bitcoin, providing the knowledge you need to participate in the internet of money. Whether youíre building the next killer app, investing in a startup, or simply curious about the technology, this revised and expanded second edition provides essential detail to get you started.\\nBitcoin, the first successful decentralized digital currency, is still in its early stages and yet itís already spawned a multi-billion-dollar global economy open to anyone with the knowledge and passion to participate. Mastering Bitcoin provides the knowledge. You simply supply the passion.\\nThe second edition includes:\\n\\nA broad introduction of bitcoin and its underlying blockchainóideal for non-technical users, investors and business executives\\nAn explanation of the technical foundations of bitcoin and cryptographic currencies for developers, engineers and software and systems architects\\nDetails of the bitcoin decentralized network, peer-to-peer architecture, transaction lifecycle and security principles\\nNew developments such as Segregated Witness, Payment Channels and Lightning Network\\nA deep dive into blockchain applications, including how to combine the building blocks offered by this platform into higher-level applications\\nUser stories, analogies, examples and code snippets illustrating key technical concepts\\n\"',\n",
              " 'Hunters are a special breed, dedicated to tracking down treasures, magical beasts, and even other men. But such pursuits require a license, and less than one in a hundred thousand can pass the grueling qualification exam. Those who do pass gain access to restricted areas, amazing stores of information, and the right to call themselves...Hunters.',\n",
              " 'Data Structure Through C is a book that can be used as a guide to understand the basic principles of data structures. It adopts a novel approach, by using the programming language C to teach data structures.\\nThe book discusses concepts like arrays, algorithm analysis, strings, queues, trees and graphs. Well-designed animations related to these concepts are provided in the CD-ROM which accompanies the book. This enables the reader to get a better understanding of the complex procedures described in the book through a visual demonstration of the same.\\nData Structure Through C is a comprehensive book which can be used as a reference book by students as well as computer professionals. It is written in a clear, easy-to-understood manner and it includes several programs and examples to explain clearly the complicated concepts related to data structures. The book was published by BPB Publications in 2003 and is available in paperback.\\nKey Features:\\nThe book contains example programs that elucidate the concepts. It comes with a CD that visually demonstrates the theory presented in the book.',\n",
              " 'This iconic National Geographic photography collection of the world’s most majestic nature landscapes presents the exquisiteness of the great outdoors and showcases evocative and extraordinary images, often unseen. With vast deserts in twilight, snowcapped mountain ranges at the brink of dawn, a forest in the height of autumn colors, these indelible images will magnify the beauty, emotion, and depth that can be captured in the split second of a camera flash, taking readers on a spectacular visual journey and offering an elegant conduit to the world around them. Paired with illuminating insights from celebrated photographers, this beautiful book weaves a vibrant tapestry of images that readers will turn to again and again.',\n",
              " 'For Kids Aged 10+ (And Their Parents)\\n\\nThe code in this book runs on almost anything: Windows, Mac, Linux, even an OLPC laptop or Raspberry Pi!\\nPython is a powerful, expressive programming language that\\'s easy to learn and fun to use! But books about learning to program in Python can be kind of dull, gray, and boring, and that\\'s no fun for anyone.\\nPython for Kids brings Python to life and brings you (and your parents) into the world of programming. The ever-patient Jason R. Briggs will guide you through the basics as you experiment with unique (and often hilarious) example programs that feature ravenous monsters, secret agents, thieving ravens, and more. New terms are defined; code is colored, dissected, and explained; and quirky, full-color illustrations keep things on the lighter side.\\nChapters end with programming puzzles designed to stretch your brain and strengthen your understanding. By the end of the book you\\'ll have programmed two complete games: a clone of the famous Pong and \"Mr. Stick Man Races for the Exit\"—a platform game with jumps, animation, and much more.\\nAs you strike out on your programming adventure, you\\'ll learn how to:\\nUse fundamental data structures like lists, tuples, and maps\\nOrganize and reuse your code with functions and modules\\nUse control structures like loops and conditional statements\\nDraw shapes and patterns with Python\\'s turtle module\\nCreate games, animations, and other graphical wonders with tkinter\\nWhy should serious adults have all the fun? Python for Kids is your ticket into the amazing world of computer programming.',\n",
              " \"'You're asking me to hold your hand. And now you're turning away from me. You are saying something but I can't hear you. It's too windy. You're crying now. Now you're smiling. I'm done. I love you . . .'\\nIt's been two years since Raghu left his first love, Brahmi, on the edge of the roof one fateful night. He couldn't save her; he couldn't be with her. Having lost everything, Raghu now wants to stay hidden from the world.\\nHowever, the annoyingly persistent Advaita finds his elusiveness very attractive. And the more he ignores her, the more she's drawn to him till she bulldozes her way into an unlikely friendship.\\nWhat attracts at first, begins to grate. Advaita can't help but want to know what Raghu has left behind, what he's hiding, and who broke his heart. She wants to love him back to life, but for that she needs to know what wrecked him in the first place.\\nAfter all, the antidote to heartache is love.\",\n",
              " \"The story of the forging of India, the world's largest democracy, is a rich and\\ninspiring one. This volume, a sequel to the best-selling India's Struggle for\\nIndependence, analyses the challenges India has faced and the successes it\\nhas achieved in the light of its colonial legacy and century-long struggle for\\nfreedom. It covers the framing of the Constitution and the evolution of the\\nNehruvian political and economic agenda and basics of foreign policy; the\\nconsolidation of the nation and contentious issues like party politics in the\\nCentre and the states, the Punjab problem and anti-caste politics and\\nuntouchability. These, along with objective assessments of Jawaharlal Nehru,\\nIndira Gandhi, Jayaprakash Narayan, Lal Bahadur Shastri, Rajiv Gandhi,\\nVishwanath Pratap Singh, Atal Bihari Vajpayee and Manmohan Singh,\\nconstitute a remarkable overview of a nation on the move.\",\n",
              " 'A Children’s Bookshelf Selection: Each month our editor’s pick the best books for children and young adults by age to be a part of the children’s bookshelf. These are editorial recommendations made by our team of experts. Our monthly reading list includes a mix of bestsellers and top new releases and evergreen books that will help enhance a child’s reading life.',\n",
              " 'The city of the dead, Egypt 1353 BC\\n\\nBuried deep beneath the Saharan desert is a secret - an ancient elixir so powerful that it is rumoured to take life from the living and restore it to the dead.\\n\\nLamped USA, Present day\\n\\nOff a remote island, a mysterious ship releases a deadly poison. Minutes later everyone on the island is dead.\\n\\nResponding to a distress call, Kurt Austin and the NUMA team struggle to understand what has caused this disaster. As they investigate the incident further they uncover a far greater threat.\\n\\nWhat is the truth behind the legends?\\nNow Kurt Austin must learn the secrets of the past if he is to save millions from a horrifying death. But it is a desperate race against time. And he faces a deadly enemy, who will stop at nothing..',\n",
              " 'Alvin Toffler’s Future Shock and The Third Wave are among the most influential books of our time. Now, in Powershift, he brings to a climax the ideas set forth in his previous works to offer a stunning vision of the future that will change your life.\\n\\nIn Powershift, Toffler argues that while headlines focus on shifts of power at the global level, equally significant shifts are taking place in the everyday world we all inhabit—the world of supermarkets and hospitals, banks and business offices, television and telephones, politics and personal life. The very nature of power is changing under our eyes.\\n \\nPowershift maps the “info-wars” of tomorrow and outlines a new system of wealth creation based on individualism, innovation, and information. As old political antagonisms fade, Toffler identifies where the next, far more important world division will arise—not between East and West or North and South, but between the “fast” and the “slow.”\\n \\nIn Powershift, Alvin Toffler has formulated the deepest, most comprehensive synthesis yet written about the civilization of the twenty-first century. It is one of the most important books you will ever read.\\n \\nPraise for Powershift\\n \\n“[A] sweeping synthesis . . . by placing the accelerated changes of our current information age in the larger perspective of history, Mr. Toffler helps us to face the future with less wariness and more understanding.”—The New York Times Book Review\\n\\n“An insightful guide to a bewildering present and a frightening future . . . thought-provoking on every page.”—Newsday',\n",
              " \"Only her gift of love can heal . . .\\nAnnie Trimble lives in a solitary world that no one enters or understands. As delicate and beautiful as the tender blossoms of the Oregon spring, she is shunned by a town that doesn't understand her.\\nBut cruelty cannot destroy the love Annie holds in her heart.\\nWhen Alex Montgomery learns of the injustice sweet Annie has suffered, he vows to do whatever it takes to set it right—even if it means marrying her. He never dreams he will fall for her childlike innocence, her womanly charms, and the wondrous way she views her world as Alex becomes captive to Annie's sweet song of love.\",\n",
              " 'In an incredibly fun and accessible two-color graphic-book format, the cofounders of Honest Tea tell the engaging story of how they created and built a mission-driven business, offering a wealth of insights and advice to entrepreneurs, would-be entrepreneurs, and millions of Honest Tea drinkers about the challenges and hurdles of creating a successful business--and the importance of perseverance and creative problem-solving.\\n\\nSeth Goldman and Barry Nalebuff began Honest Tea fifteen years ago with little more than a tea leaf of an idea and a passion to offer organic, freshly brewed, lightly sweetened bottled tea. Today Honest Tea is a rapidly expanding national brand sold in more than 100,0000 grocery stores, restaurants, convenience stores and drugstores across the country. The brand has flourished as American consumers move toward healthier and greener lifestyles.',\n",
              " 'Insights and inspiration for anyone who makes art (or anything else)\\n\\nThe Ultimate BuzzFeed Books Gift Guide - Official Selection\\n\\nFrom the creative mind and heart of designer Adam J. Kurtz comes this upbeat rallying cry for creators of all stripes. Expanding on a series of popular essays, this handwritten and heartfelt book shares wisdom and empathy from one working artist to others. Perforated tear-and-share pages make it easy to display themost crucial reminders or to pass a bit of advice on to someone who needs it.\\n\\nAs wry and cheeky as it is empathic and empowering, this deceptively simple, vibrantly full-color book will be a touchstone for writers, artists, entrepreneurs, and anyone else who wants to be more creative--even when it would be easier to give up and act normal.',\n",
              " 'Mouseford Academy is the location of a movie shoot for Two Mice in the Moonlight, the latest blockbuster starring the famous Dylan Delarattand Kiki Cattail. The Thea Sisters are excited to learn about the movie business —and the film’s stars. What’s Kiki Cattail really like in the fur? And is Dylan Delarattas snobby as he seems on screen? the Thea Sisters —and the entire school —can’t wait to find out!',\n",
              " \"When P.V. Narasimha Rao became the unlikely prime minister of India in 1991, he inherited economic crisis, violent insurgencies and a nation adrift. Despite being unloved by his people, mistrusted by his party, a minority in Parliament and ruling under the shadow of 10 Janpath, Rao reinvented India, at home and abroad.\\nWith exclusive access to Rao's never-before-seen personal papers as well as over a hundred interviews, this definitive biography provides new revelations on the Indian economy, nuclear programme, foreign policy and the Babri Masjid. Meticulously researched and honestly told, this landmark political biography is a must-read for anyone interested in the man responsible for transforming India.\",\n",
              " 'Sanjana’s best friend at college is murdered. She was in love with a business tycoon named Nik Sethi, and Sanjana is certain that he killed her. In an effort to find proof, she decides to get close to him.\\nGood looking and rich, Nik falls in love with Sanjana instantly, but a month later, when he accidently discovers her real agenda, he throws her out of his life. Determined to nail him, Sanjana’s desperation exceeds all limits when she realizes that she too, like her friend, has fallen in love with a killer.\\nAfter she escapes an attack one night, Sanjana quits college and goes into hiding. Now her only ambition is to punish the killer and her only weapon is her body. In a last, desperate attempt, she uses herself as bait and pursues her best plan. There are only two options: she becomes a victim or she becomes a victor.\\nThrough the story of Sanjana and Nik, In Love With Simran explores the boundaries of the basic instincts of the young: love, sex, trust and survival.',\n",
              " \"The Assassin’s Creed series of novels expand in the background of the Assassin’s Creed Video Game. The game’s premise is a continuing struggle between two groups, a war that has been going on for centuries. The Knights Templar have ambitions that encompass the whole of humanity, seeking an ancient secret that could give them control over each and every person on the Earth.\\nThe Assassin’s Order is a secret society that was created to stop the Knights Templar from realising their dreams. The game is set in current times with the protagonist, Desmond Miles, a bartender, as the modern descendant of a long lineage of Assassins. He experiences the memories of his ancestors through a device known as the animus.\\nAssassin's Creed: Brotherhood continues the story of one of Desmond’s prominent Assassin ancestors, Ezio Auditore da Firenze, who lived in the Renaissance Italy of the early sixteenth century. The previous book, Assassin's Creed: Renaissance introduced Ezio and began his story.\\nEzio, who trains as an Assassin to seek revenge for the murder of his family, slowly realises the deep secrets buried within his family history. In the second book, Ezio comes into confrontation with one of the most powerful families in Italy, the Borgias.\\nIn Assassin's Creed: Brotherhood, the once powerful Rome lies in ruins and the people live a life of misery, with the evil shadow of the Borgias dominating them all. Ezio is their only hope for deliverance. But Cesare Borgia is a man who is powerful and more dangerous than his father, the Pope. He has high ambitions and has no intention of letting anyone get in his way.\\nPenguin UK published this edition of the book in paperback in 2010.\",\n",
              " 'NEW YORK TIMES BESTSELLER • The pioneering experts behind The Whole-Brain Child and The Yes Brain tackle the ultimate parenting challenge: discipline.\\n \\n“A lot of fascinating insights . . . an eye-opener worth reading.”—Parents\\n\\nHighlighting the fascinating link between a child’s neurological development and the way a parent reacts to misbehavior, No-Drama Discipline provides an effective, compassionate road map for dealing with tantrums, tensions, and tears—without causing a scene.\\n \\nDefining the true meaning of the “d” word (to instruct, not to shout or reprimand), the authors explain how to reach your child, redirect emotions, and turn a meltdown into an opportunity for growth. By doing so, the cycle of negative behavior (and punishment) is essentially brought to a halt, as problem solving becomes a win/win situation. Inside this sanity-saving guide you’ll discover\\n \\n• strategies that help parents identify their own discipline philosophy—and master the best methods to communicate the lessons they are trying to impart\\n• facts on child brain development—and what kind of discipline is most appropriate and constructive at all ages and stages\\n• the way to calmly and lovingly connect with a child—no matter how extreme the behavior—while still setting clear and consistent limits\\n• tips for navigating your child through a tantrum to achieve insight, empathy, and repair\\n• twenty discipline mistakes even the best parents make—and how to stay focused on the principles of whole-brain parenting and discipline techniques\\n \\nComplete with candid stories and playful illustrations that bring the authors’ suggestions to life, No-Drama Discipline shows you how to work with your child’s developing mind, peacefully resolve conflicts, and inspire happiness and strengthen resilience in everyone in the family.\\n\\nPraise for No-Drama Discipline\\n \\n“With lucid, engaging prose accompanied by cartoon illustrations, Siegel and Bryson help parents teach and communicate more effectively.”—Publishers Weekly\\n\\n“Wow! This book grabbed me from the very first page and did not let go.”—Lawrence J. Cohen, Ph.D., author of The Opposite of Worry',\n",
              " \"Wearer of many hats-philanthropist, entrepreneur, computer scientist, engineer, teacher-Sudha Murty has above all always been a storyteller extraordinaire. Winner of the R.K. Narayan Award for Literature, the Padma Shri, the Attimabbe Award from the government of Karnataka for excellence in Kannada literature, and the Raymond Crossword Lifetime Achievement Award, her repertoire includes adult non-fiction, adult fiction, children's books, travelogues and technical books. Here, There and Everywhere is a celebration of her literary journey and is her 200th title across genres and languages. Bringing together her best-loved stories from various collections alongside some new ones and a thoughtful introduction, here is a book that is, in every sense, as multifaceted as its author.\",\n",
              " \"CISSP (ISC)2 Certified Information Systems Security Professional Official Study Guide, 7th Edition has been completely updated for the latest 2015 CISSP Body of Knowledge. This bestselling Sybex study guide covers 100% of all exam objectives. You'll prepare for the exam smarter and faster with Sybex thanks to expert content, real-world examples, advice on passing each section of the exam, access to the Sybex online interactive learning environment and much more. Reinforce what you've learned with key topic exam essentials and chapter review questions.\",\n",
              " 'In 2003 poker was put on television and no-limit hold em quickly became the most popular form of poker played in casinos, public cardrooms, and on the Internet. At first, because of the newness of the game to most participants, they could be easily be beaten by players with only a moderate knowledge of sophisticated strategy. But today, this is no longer the case.Even though no-limit hold em is not, from a Game Theory perspective, a solved game, many ideas and concepts which come from this branch of mathematics now play an important role in a strong, winning no-limit hold em strategy. But it s also important for the expert player to know when to leave what is known about optimal play and switch to exploitative strategies to maximize his profit. And this brings us to No-Limit Hold em for Advanced Players, Emphasis on Tough Games by Matthew Janda.In this text, Janda spends much time discussing sophisticated strategies that should be employed against tough opposition, some of which has never been published before. He then supports his ideas with the modern advanced software programs of PokerSnowie and PioSOLVER. Topics covered include, but not limited to, Linear, Condensed, and Polarized Ranges, Raising First In As the Button, Understanding High Variance Plays, Turn Play and Overbets, Understanding Counter Strategies, Opening Frequencies Based on Stack Depth, Playing Short in Button vs Big Blind Situations, and Denying Your Opponents Equity While Realizing Ours.',\n",
              " \"Bestselling author Judith McNaught masterfully portrays a remarkable heroine, and an unforgettable passion, in this powerfully moving love story -- one of her most beloved novels of all time!\\nThe tempestuous marriage of Alexandra Lawrence, an innocent country girl, and Jordan Townsende, the rich and powerful Duke of Hawthorne, is about to face its ultimate test of tender loyalty. Swept into the endlessly fascinating world of London society, free-spirited Alexandra becomes ensnared in a tangled web of jealousy and revenge, stormy pride and overwhelming passion. But behind her husband's cold, arrogant mask, there lives a tender, vital, sensual man...the man Alexandra married. Now, she will fight for his very life...and the rapturous bond they alone can share.\",\n",
              " 'The dying Empire’s most cunning and ruthless warlord, Grand Admiral Thrawn, has taken command of the remnants of the Imperial fleet and launched a massive campaign aimed at the New Republic’s destruction. Meanwhile, Han Solo and Lando Calrissian race against time to find proof of treason inside the highest Republic Council—only to discover instead a ghostly fleet of warships that could bring doom to their friends and victory to their enemies.\\n \\nYet most dangerous of all is a new Dark Jedi, risen from the ashes of a shrouded past, consumed by bitterness, and scheming to corrupt Luke Skywalker to the dark side.',\n",
              " 'In Sackett, Louis L’Amour introduces readers to a wandering man with a desire to settle down and build a good life.\\n\\nHard circumstances have made William Tell Sackett a drifter, but now he hungers for a place he can’t name yet knows he has to find. South of the Tetons he comes upon a ghost of a trail that leads him through a keyhole pass into a lonely, alien, yet beautiful valley—a valley that holds a fortune in gold.\\n\\nThen he finds an even greater treasure: beautiful Ange Kerry, a courageous and resourceful woman. Yet the harsh ways it takes to preserve his claim and his life could be the one thing that drives Ange away forever.',\n",
              " \"Angoor (1982) is among the best-loved comedies in Hindi cinema. It is also a perfect example of Gulzar's genius as a writer complete with his impish wordplay. Through extensive interviews with some of the main actors in Angoor (Deepti Naval and Moushumi Chatterjee) and its earlier version - both adaptations of Shakespeare's Comedy of Errors - this book traces the evolution of a comic tale that continues to amuse audiences of all ages. It deftly peels the layers exploring how song, dialogue, silences and wordplay add to the actors' arsenal in creating humour that can range from rib-tickling mirth to guffaws. Sathya Saran's book reveals what lies behind the evergreen appeal of Angoor, with memories and anecdotes shared by Gulzar himself.\",\n",
              " 'An accessible and accurate translation of the Quran that offers a rigorous analysis of its theological, metaphysical, historical, and geographical teachings and backgrounds, and includes extensive study notes, special introductions by experts in the field, and is edited by a top modern Islamic scholar, respected in both the West and the Islamic world.\\nDrawn from a wide range of traditional Islamic commentaries, including Sunni and Shia sources, and from legal, theological, and mystical texts, The Study Quran conveys the enduring spiritual power of the Quran and offers a thorough scholarly understanding of this holy text.\\nBeautifully packaged with a rich, attractive two-color layout, this magnificent volume includes essays by 15 contributors, maps, useful notes and annotations in an easy-to-read two-column format, a timeline of historical events, and helpful indices. With The Study Quran, both scholars and lay readers can explore the deeper spiritual meaning of the Quran, examine the grammar of difficult sections, and explore legal and ritual teachings, ethics, theology, sacred history, and the importance of various passages in Muslim life.\\nWith an introduction by its general editor, Seyyed Hossein Nasr, here is a nearly 2,000-page, continuous discussion of the entire Quran that provides a comprehensive picture of how this sacred work has been read by Muslims for over 1,400 years.',\n",
              " \"THE INTERNATIONAL BESTSELLER 'Ruth Dugdall's novels are intelligent and gripping, with a sophisticated psychological sensibility. She is a huge talent.' Sophie Hannah 'An absolute tour de force that left me thinking for days.' Alex Marwood They came for me, just like I knew they would. Luke had been dead for just three days. Rose Wilks' life is shattered when her newborn baby Joel is admitted to intensive care. Emma Hatcher has all that Rose lacks. Beauty. A loving husband. A healthy son. Until tragedy strikes and Rose is the only suspect. Now, having spent nearly five years behind bars, Rose is just weeks away from freedom. Her probation officer Cate must decide whether Rose is remorseful for Luke's death, or whether she remains a threat to society. As Cate is drawn in, she begins to doubt her own judgement. Where is the line between love and obsession, can justice be served and, if so... by what means? New Edition includes exclusive material. A clever, sophisticated, psychological thriller, perfect for fans of Gillian Flynn, S.J. Watson, B A Paris and Sophie Hannah\",\n",
              " 'In A Champion’s Mind, the tennis great who so often exhibited visible discomfort with letting people “inside his head” finally opens up. An athletic prodigy, Pete resolved from his earliest playing days never to let anything get in the way of his love for the game. But while this determination led to tennis domination, success didn’t come without a price.\\n\\nHere for the first time Pete speaks freely about the personal trials he faced—including the death of a longtime coach and confidant—and the struggles he gutted his way through while being seemingly on top of the world. Among the book’s most riveting scenes are the devastating early loss that led Pete to make a monastic commitment to the game; fierce on-court battles with Andre Agassi; and the triumphant last match of Pete’s career at the finals of the 2002 U.S. Open.\\n\\n\"A thoroughly compelling read that really probes the hard drive of a champion...All the emotion and insight that Sampras seemes reluctant to express during his playing days come spilling forth.\" —Jon Wertheim, senior writer, Sports Illustrated',\n",
              " 'Now at a special price for one month only.\\nFor the ultimate in glamour, it has to be Tilly Bagshawe. Perfect escapism for fans of Penny Vincenzi and Jilly Cooper.\\nSasha Miller comes to Cambridge with a dream and leaves on a mission. After falling for the lies and charms of her Director of Studies ‘Theo Dexter’ she finds herself betrayed, humiliated and nursing a bundle of broken dreams. Heading to the US she is determined to rebuild her life.\\nYears later, Sasha emerges from Harvard Business School with one thing on her mind, the downfall of the now famous Professor Theo Dexter.\\nMeanwhile Theo’s long-suffering wife Theresa also finds herself betrayed and cast aside for a younger and prettier model. Unable to cope she returns to Cambridge a broken woman and tries to rebuild her life away from the scheming Theo Dexter.\\nOne night Sasha turns up at Theresa’s door, she wants revenge at any cost, will Theresa help her?\\nFrom the deepest betrayal comes a shocking alliance.\\nTwo vengeful women, one very unlucky man…',\n",
              " \"The bestselling, much-loved classic account of an English couple enjoying the fruits of French rural living - an irresistible feast of humour and heart.\\nPeter Mayle and his wife did what most of us only imagine doing when they made their long-cherished dream of a life abroad a reality: throwing caution to the wind, they bought a glorious two hundred year-old farmhouse in the Lubéron Valley and began a new life.\\nIn a year that begins with a marathon lunch and continues with a host of gastronomic delights, they also survive the unexpected and often hilarious curiosities of rural life. From mastering the local accent and enduring invasion by bumbling builders, to discovering the finer points of boules and goat-racing, all the earthy pleasures of Provençal life are conjured up in this enchanting portrait.\\n'One of the most successful travel books of all time... Mayle created a new travel genre' Guardian\\n\\nDelightful' Washington Post\\n'Engaging, funny and richly appreciative' New York Times Book Review\\n'Stylish, witty, delightfully readable' Sunday Times\\n'I really loved this book' Julia Child\",\n",
              " \"A wartime thriller in which three people - a Texan, an Englishman and an American reporter - embark on a daring adventure as they attempt to save the Ethiopian people from annihilation by Mussolini's forces. By the author of THE SEVENTH SCROLL and BIRDS OF PREY.\",\n",
              " \"'Scalpel-sharp in observation, deceptively simple\\nin construction... at its frequent best\\nYes Prime Minister exhibits the classical perfection of a\\nMozart sonata' - Richard Last in The Times\\n\\n'Its closely observed portrayal of what goes on in the\\ncorridors of power has given me hours of pure joy'\\n- Rt Hon. Margaret Thatcher MP\\n\\n'Yes Prime Minister... is not only a continuing marvel of\\nediting by Jonathan Lynn and Antony Jay but also a\\ncollector's must' - John Coldstream in the Daily Telegraph\\n\\n'Yes Prime Minister is a comedy in a class of its own'\\n- Celia Brayfield in The Times\",\n",
              " 'The waves still crashed against the rocks.\\nThe moon still bathed the sandy beach with its light.\\nAnd the piano still played on.\\nBut, amidst all this, just like that, Sally Sequeira had disappeared.\\nWith its pristine beaches and clear turquoise waters, the picturesque hamlet of Movim in Goa seems like the perfect holiday spot for detective Janardan Maity and his friend Prakash Ray. But when the father of a local teenage girl receives a letter asking for a large sum of money in exchange for his daughter, Maity and Prakash find themselves in the thick of an unlikely mystery. For, they discover, the girl has not been kidnapped at all, and is safe and sound in her house.\\nAs they begin to investigate, the duo encounter the mysterious characters who inhabit the tiny village, each hiding a secret of their own – not least the frail and shy Sally Sequeira, who keeps to herself but steps out at night to dance to the notes of a piano.\\nWhat truth does Movim hide? And how will Janardan Maity solve a crime that has not yet been committed?',\n",
              " '\"The major contemporary work on urban design . . . Splendidly presented, filled with thoughtful and brilliant intuitive insights.\" —The New Republic\\nIn a brilliant synthesis of words and pictures, Edmund N. Bacon relates historical examples to modern principles of urban planning. He vividly demonstrates how the work of great architects and planners of the past can influence subsequent development and be continued by later generations. By illuminating the historical background of urban design, Bacon also shows us the fundamental forces and considerations that determine the form of a great city. Perhaps the most significant of these are simultaneous movement systems—the paths of pedestrian and vehicular traffic, public and private transportation—that serve as the dominant organizing force, and Bacon looks at movement systems in cities such as London, Rome, and New York. He also stresses the importance of designing open space as well as architectural mass and discusses the impact of space, color, and perspective on the city-dweller. That the centers of cities should and can be pleasant places in which to live, work, and relax is illustrated by such examples as Rotterdam and Stockholm.',\n",
              " \"Out at sea fishing, Asterix and Obelix are blown off course in a storm. Luckily they land on the shores of a Roman colony - or is it? Teepees, totems, gobbling birds: it's not what they're used to. And what are the Viking explorers in their longship doing? But perhaps 50 BC is a little too early for a voyage of discovery to this strange New World...\",\n",
              " 'There’s a killer loose in Los Angeles and super-sleuth L is on the case. Along with Naomi, a former FBI agent, he helps the LA police solve the grisly crimes. In typical Death Note fashion, things get complicated. And there’s a big surprising plot twist at the end of the book.',\n",
              " \"The fourth in the family of best-selling vocabulary reference and practice books from elementary to advanced level. The perfect choice for advanced-level students wanting to build their vocabulary skills. English Vocabulary in Use: Advanced includes over 2,000 new words and expressions which are presented and practised in typical contexts appropriate to this level. The accompanying CD-ROM provides interesting and enjoyable exercises to further challenge the learner, as well as extra listening practice and help with pronunciation. The book is informed by the Cambridge International Corpus to ensure that the vocabulary selected is useful and up to date. A test book is also available for extra practice. This 'with answers' edition is ideal for self study.\",\n",
              " '<Provocative, startling, prophetic, and more relevant than ever, <the handmaid\\'s tale has become a global phenomenon. Now, in this stunning graphic novel edition of Margaret Atwood\\'s modern classic, The terrifying reality of Gilead is brought to vivid life like never before. \"everything handmaids wear is red: the colour of blood, which defines us.\" off red is a handmaiden in the Republic of Gilead, where women are prohibited from holding jobs, reading, and forming friendships. She serves in the household of the commander and his wife, and under the new social order she has only one purpose: once a month, she must lie on her back and pray that the commander makes her pregnant, because in an age of declining births, off red and the other handmaids are valued only if they are fertile. But off red remembers the years before Gilead, when she was an independent woman who had a job, a family, and a name of her own. Now, her memories and her will to survive are acts of rebellion. <The Handmaid\\'s tale and its iconic images - The Red of the Handmaid\\'s, the Blue of the wives, the looming ileadean eye - have been adapted into a film, an opera, a ballet, and multi-award-winning TV series. This ground-breaking new graphic novel edition, adapted and featuring arresting artwork By Renée fault, is destined to become a classic in its own right.',\n",
              " \"The Hunters if you seek the y will find. The treasure, for over two thousand years the legendary tomb of Alexander the Great and the riches concealed within - has evaded discovery. Now, after centuries of searching, an ancient map has come to light that could hold the key to finding the fabled vault. Only one team has the skill and the expertise to solve the mystery once and for all. The mission, It's up to The Hunters an elite group assembled to track down the world's greatest treasures to find the tomb. But on arriving in Alexandria, it quickly becomes clear that hostile forces are on their trail. And when one of the team is captured in cisterns deep below the city, what began as a treasure hunt becomes a deadly rescue mission. For there are some who will use any means possible to destroy The Hunters' efforts and now there is more at stake than they ever could have imagined. High-octane action, Brilliant characters and Classic Kuzneski.\",\n",
              " \"In this wide-ranging collection of essays, Ramachandra Guha defends the liberal centre against the dogmas of left and right, and does so with style, depth, and polemical verve. Among the subjects on which he turns a critical eye are Hindutva, the Communist left and the dynasty-obsessed Congress party. Whether writing about politics, profiling individuals or analysing social trends, Guha displays a masterly touch, confirming his standing as India's most admired historian and public intellectual.\\nKey Features:\\nWritten in a wonderfully readable style, will provoke much debate and discussion\\nCovering a wide range of subjects of contemporary relevance, such as:\\nAn essay on the threats to the Indian republic from the Right, the Left and the state itself\\nAn analysis comparing and contrasting two old men, PM Manmohan Singh and Anna Hazare\\nAn examination of the dynasty-obsessed Congress Party and its culture of sycophancy\\nHindutva hate mail which floods the Internet\",\n",
              " 'This is the only independent biography of Bruce Lee, and it is complete in terms of both the martial arts and the movies.',\n",
              " \"A brilliant and immersive, all-consuming read about one fourteen-year-old girl's heart-stopping fight for her own soul. ‘You think you’re invincible. You think you won’t ever miss. We need to put the fear on you. You need to surrender yourself to death before you ever begin, and accept your life as a state of grace, and then and only then will you be good enough.’ At 14, Turtle Alveston knows the use of every gun on her wall; That chaos is coming and only the strong will survive it; That her daddy loves her more than anything else in this world. And he’ll do whatever it takes to keep her with him. She doesn’t know why she feels so different from the other girls at school; Why the line between love and pain can be so hard to see; Why making a friend may be the bravest and most terrifying thing she has ever done And what her daddy will do when he finds out … Sometimes strength is not the same as courage. Sometimes leaving is not the only way to escape. Sometimes surviving isn't enough.\",\n",
              " 'An old widow is brutally killed in the parlour of her cottage…\\n‘Mrs McGinty’s dead!’\\n‘How did she die?’\\n‘Down on one knee, just like I!’\\nThe old children’s game now seemed rather tasteless. The real Mrs McGinty was killed by\\na crushing blow to the back of the head and her pitifully small savings were stolen.\\nSuspicion falls immediately on her lodger, hard up and out of a job. Hercule Poirot has\\nother ideas – unaware that his own life is now in great danger…',\n",
              " \"Between 1939 and 1945 India underwent extraordinary and irreversible change. Hundreds of thousands of Indians suddenly found themselves in uniform, fighting in the Middle East, North and East Africa, Europe and—something simply never imagined—against a Japanese army poised to invade eastern India. By the war’s end, the Indian Army had become the largest volunteer force in the conflict, consisting of 2. 5 million men, while many millions more had offered their industrial, agricultural and military labour.\\nIn India’s War, historian Srinath Raghavan paints a compelling picture of battles abroad and of life on the home front, arguing that World War II is crucial to explaining how and why colonial rule ended in South Asia. The war forever altered the country’s social landscape, and when the dust settled, India had emerged as a major Asian power with her feet set firmly on the path toward Independence.\\nFrom Gandhi's early support of Britain's war efforts to the crucial Burma Campaign, Raghavan’s authoritative and vivid account shows how India’s economy, politics and people were forever transformed, laying the groundwork for the emergence of modern South Asia.\",\n",
              " 'Operations Research is the discipline of applying advanced analytical methods to help make better decisions. It helps the management to achieve its goals by using scientific techniques, making the study and understanding of operations research even more important in the present day scenario. This book has been written with the objective of providing students with a comprehensive textbook on the subject. It follows a simple algorithmic approach to explain each concept, often giving different steps. This approach stems from the author’s experience in teaching undergraduate and postgraduate students of Madras University and Anna University, Chennai, over many years.\\nOne of the highlights of this book is the solved-problems approach, as each chapter in the book is substantiated by a large number of solved problems. Many of the questions that have been incorporated are from previous examination papers of various universities. In addition, each chapter has numerous exercise problems at the end and a section on short questions with answers.\\nWith its approach and coverage the book will be indispensable to the MBA/PGDM, Engineering and Mathematics students.\\nNEW IN THIS EDITION\\nA New Chapter on Non-Linear Programming has been added\\nIncludes the Stepping Stone Method to solve Problem of Degeneracy and Transportation Problem',\n",
              " \"Head First SQL is ideally meant for somebody who wants an introduction to SQL. What really makes it an interesting read is the usage of geek humour thoroughly infused in the book. Funny photo captions, amusing exercises, and wordplay and make the idea of teaching something as dry as IT content very enjoyable and blatantly entertaining.\\nSimilar to the lines of the earlier Head First series books, as a reader, one would be put to a different experience for a beginner level technical book. A lot of visuals and graphics have been used to explain topics. For example, to explain how a select statement works, the author hits you with building a dating service to illustrate the points. In order to explain the difference between sub-selects and outer joins, the author drags them out to a symbolic stage and debates over the righteousness of usage. All these elements and outlandish scenarios only make it easier for the reader to remember the information.\\nThe book is ideally meant for somebody who is beginning to learn SQL and also has a chapter explaining the benefits of using a database at the first place. The topics mostly stick to generic SQL commonalities like selects, alter tables, joins, updates, aggregate functions, deletes, ordering, where-clauses, sub-selects, etc. Constraints, views, and some elementary security alarms are touched on, but not in great depths.\\nThe appendices are really a collection of obscure topics that the author calls ‘left-overs’. Also, there is a quick section on PHP, GUI tools for database, a list of reticent words and some additional knowhow on data types. The first edition of Head First SQL was published by Shroff/O'Reilly in 2007 and is available in paperback.\\nKey Features:\\nA journey through the language to hardcore database handling, expect to discover, inquire, control, and join your data like a pro by the time you reach the end.\",\n",
              " \"IF YOU DON'T KNOW SIMON SCARROW, YOU DON'T KNOW ROME!\\n\\nA Sunday Times bestseller. Shortlisted for the Wilbur Smith Adventure Writing Prize.\\nSimon Scarrow's veteran Roman soldier heroes face a cunning and relentless enemy in BRITANNIA, the unforgettable fourteenth novel in the bestselling Eagles of the Empire series.\\nRoman Britain, AD 52. The western tribes prepare to make a stand. But can they match the discipline and courage of the legionaries?\\nWounded Centurion Macro remains behind in charge of the fort as Prefect Cato leads an invasion deep into the hills. Cato's mission: to cement Rome's triumph over the natives by crushing the Druid stronghold. But with winter drawing in, the terrain is barely passable through icy rain and snowstorms.\\nWhen Macro's patrols report that the natives in the vicinity of the garrison are thinning out, a terrible suspicion takes shape in the battle-scarred soldier's mind. Has the acting Governor, Legate Quintatus, underestimated the enemy? If there is a sophisticated and deadly plan afoot, it's Cato and his men who will pay the price...\\nIncludes maps, chart and author Q&A.\",\n",
              " '5th edition of the high-ability French course, fully supported by Kerboodle.',\n",
              " \"Welcome to the diabolical lair of Tantri the Mantri. The throne of Hujli still eludes our evil Mantri. However, in recent years, Tantri has come closer to capturing the throne than ever before. He will not let anything come in his way. Hell step into a wrestling ring, sample foreign yet dangerous delicacies and even battle falling coconuts in his struggle to be king. Surely, with so much determination, his coronation day is not too far. You wouldn't want to miss it, would you? Grab your copy today and block your dates.\",\n",
              " \"Exploring more than 100 of the world's most important literary works and the literary geniuses that created them, this book is the perfect introduction to the subject of literature and writing.\\nThe Literature Book features over 100 of the world's most celebrated books, plays, and poetry, including Latin American and African fiction, and best-selling masterpieces from the most renowned authors ever to have lived.\\nStunning images and inspirational quotes jump out from the pages, while detailed plot summaries and feature boxes bring the timeless works of literature to life and set them into their wider social and cultural context. The book also offers a deeper look into the famed fiction of Shakespeare, Oscar Wilde, and more, as in-depth literary criticism and interesting authorial biographies give each work of literature a new meaning.\\nFrom Fitzgerald's The Great Gatsby to Shelley's Frankenstein, The Literature Book is a must-have for any literature student or fan of fiction.\",\n",
              " 'The basis for the new documentary film, Mountain: A Breathtaking Voyage into the Extreme. Combining accounts of legendary mountain ascents with vivid descriptions of his own forays into wild, high landscapes, Robert McFarlane reveals how the mystery of the world’s highest places has came to grip the Western imagination—and perennially draws legions of adventurers up the most perilous slopes.\\nHis story begins three centuries ago, when mountains were feared as the forbidding abodes of dragons and other mysterious beasts. In the mid-1700s the attentions of both science and poetry sparked a passion for mountains; Jean-Jacques Rousseau and Lord Byron extolled the sublime experiences to be had on high; and by 1924 the death on Mt Everest of an Englishman named George Mallory came to symbolize the heroic ideals of his day. Macfarlane also reflects on fear, risk, and the shattering beauty of ice and snow, the competition and contemplation of the climb, and the strange alternate reality of high altitude, magically enveloping us in the allure of mountains at every level.',\n",
              " \"Following his blockbuster biography of Steve Jobs, The Innovatorsis Walter Isaacson's story of the people who created the computer and the Internet. It is destined to be the standard history of the digital revolution and a guide to how innovation really works. What talents allowed certain inventors and entrepreneurs to turn their disruptive ideas into realities? What led to their creative leaps? Why did some succeed and others fail? In his exciting saga, Isaacson begins with Ada Lovelace, Lord Byron's daughter, who pioneered computer programming in the 1840s. He then explores the fascinating personalities that created our current digital revolution, such as Vannevar Bush, Alan Turing, John von Neumann, J.C.R. Licklider, Doug Engelbart, Robert Noyce, Bill Gates, Steve Wozniak, Steve Jobs, Tim Berners-Lee and Larry Page. This is the story of how their minds worked and what made them so creative. It's also a narrative of how their ability to collaborate and master the art of teamwork made them even more creative. For an era that seeks to foster innovation, creativity and teamwork, this book shows how they actually happen.\",\n",
              " 'A full-length novel by Charles Osborne adapted from Agatha Christie’s stage play, in which a diplomat’s wife finds a body that mustn’t be discovered…\\n\\nFollowing BLACK COFFEE and THE UNEXPECTED GUEST comes the final Agatha Christie play novelisation, bringing her superb storytelling to a new legion of fans.\\n\\nClarissa, the wife of a Foreign Office diplomat, is given to daydreaming. ‘Supposing I were to come down one morning and find a dead body in the library, what should I do?’ she muses.\\n\\nClarissa has her chance to find out when she discovers a body in the drawing-room of her house in Kent. Desperate to dispose of the body before her husband comes home with an important foreign politician, Clarissa persuades her three house guests to become accessories and accomplices. It seems that the murdered man was not unknown to certain members of the house party (but which ones?), and the search begins for the murderer and the motive, while at the same time trying to persuade a police inspector that there has been no murder at all…',\n",
              " \"YOUR LIFE . . . IN 300 WORDS OR LESS\\nIt's a daunting task. Even the most seasoned professionals find business school application essays to be among the hardest pieces they ever write. With a diverse pool of talented people applying to the nation's top schools from the most successful companies and prestigious undergraduate programs in the world, a simple biography detailing accomplishments and goals isn't enough. Applicants need clear and compelling arguments that grab admissions officers and absolutely refuse to let go.\\nTo help them write the essays that get them accepted into Harvard or any of the country's other top programs, the staff of The Harbus---HBS's student newspaper---have updated and revised their collection of sixty-five actual application essays as well as their detailed analysis of them so that applicants will be able to:\\n* Avoid common pitfalls\\n* Play to their strengths\\n* Get their message across\\nWherever they are applying, the advice and tested strategies in 65 Successful Harvard Business School Application Essays give business professionals and undergraduates the insider's knowledge to market themselves most effectively and truly own the process.\",\n",
              " 'Architecture that is meant to have a sensuous connection to life calls forthinking that goes far beyond form and construction. In his texts, Peter Zumthor articulates what motivates him to design his buildings, which appeal to the visitor\\'s heart and mind in so many different ways and possess a compelling and unmistakable presence and aura. Now in its third edition, this book has been expanded to include two new essays: \"Architecture and Lanscape\" deals with the relationship between the structure and its surroundings, with the secret of the successful placement and topographical integration of architecture. In \"The Leis Houses\", Peter Zumthor describes the genesis of two wooden houses in the town of Leis in the Swiss canton of Graubunden, thus thematizing the special challenge of integrating contemporary architecture into a traditional architectural context.',\n",
              " \"The industry Bible. Programming Microsoft Visual C++, Fifth edition, is the newest edition of the book that has become the industry's most trusted text (previously published as Inside Visual C++). Newly expanded and updated for Microsoft Visual C++ 6.0, it offers even more of the detailed, comprehensive coverage that's consistently made this book the best overall explanation of the capabilities of this powerful and complex development tool. The companion CD-ROM contains valuable sample source code and sample applications developed for the book - to make Programming Microsoft Visual C++, Fifth Edition, one book you'll want to keep close at hand.\",\n",
              " \"Join the world's most famous travelling reporter in his exciting adventures as races to claim a mysterious meteorite in The Shooting Star, and uncovers a lost pirate's hoard in the two-part story The Secret of the Unicorn and Red Rackham's Treasure. The Shooting Star Things are hotting up for Tintin as a huge fireball comes hurtling towards earth! Soon he is setting sail with Captain Haddock to find the meteorite in the stormy Arctic Ocean, but a valuable metal is contained in the meteorite and Tintin's attempts to reach it are met with relentless sabotage! The Secret of the Unicorn When Tintin stumbles across a model ship at the Old Street Market, he buys it as a gift for his friend Captain Haddock. But this isn't just any old model ship . . . it's the Unicorn. Built by one of Haddock's ancestors it holds a clue to finding the treasure of a notorious pirate. Red Rackham's Treasure Determined to find the treasure of the notorious pirate Red Rackham, Tintin and Captain Haddock set sail aboard the Sirius to find the shipwreck of the Unicorn. With the help of an ingenious shark-shaped submarine, Tintin follows the clues deep down on this ocean adventure. Join the most iconic character in comics as he embarks on an extraordinary adventure spanning historical and political events, and thrilling mysteries. Still selling over 100,000 copies every year in the UK and having been adapted for the silver screen by Steven Spielberg and Peter Jackson in 2011, The Adventures of Tintin continue to charm more than 80 years after they first found their way into publication. Since then an estimated 230 million copies have been sold, proving that comic books have the same power to entertain children and adults in the 21st century as they did in the early 20th.\",\n",
              " \"As Rome's war with Carthage continues, two friends - now on opposing sides - confront each other in one of the most brutal sieges of all time. A new Hannibal novel by the Sunday Times bestselling author of The Forgotten Legion series.\\n\\n213 BC. Syracuse. Under the merciless Sicilian sun, a city is at war.\\n\\nOutside the walls, a vast Roman army waits. Yet the city’s incredible defences, designed by Archimedes, mean that Syracuse will not be taken easily.\\n\\nA veteran of the bitter war since its beginning, Quintus is ready to give his life in the service of the Republic. But dangers face him from within his own ranks as well as from the enemy - who include his former friend, the Carthaginian, Hanno.\\n\\nHanno has been sent by his general Hannibal to aid Syracuse in its fight against Rome. Pledged to bring death to all Romans, he is diverted from his mission by the discovery of Quintus’ sister Aurelia, a captive within the city.\\n\\nTwo friends on opposing sides. A woman caught between them. They are about to meet in one of the most brutal sieges of all time.\\n\\nWho will survive?\",\n",
              " \"Once upon a time, not so long ago, a monster came to the small town of Castle Rock, Maine . . . He was not a werewolf, vampire, ghoul, or unnameable creature from the enchanted forest or snow wastes; he was only a cop . . .\\nCujo is a huge Saint Bernard dog, the best friend Brett Camber has ever had. Then one day Cujo chases a rabbit into a bolt-hole. Except it isn't a rabbit warren any more. It is a cave inhabited by rabid bats.\\nAnd Cujo falls sick. Very sick. And the gentle giant who once protected the family becomes a vortex of horror inexorably drawing in all the people around him . . .\",\n",
              " 'Meet the inimitable four sisters Ina, Mina, Mynah and Mo. What mischief are they up to now?',\n",
              " \"    The Scarecrows' Wedding is a wonderfully heartwarming picture book from the creators of The Gruffalo and Stick Man.\\n  Written in Julia Donaldson’s glorious rhyme and illustrated in glowing colour by Axel Scheffler, The Scarecrows' Wedding is a fabulous love story, with drama, humour, originality – and a happy ending!\\n  Two scarecrows, Betty O’Barley and Harry O’Hay, are planning the perfect wedding.\\n  But wicked scarecrow, Reginald Rake, has other ideas and almost ruins their special day.\\n  Harry must become a hero before he and Betty can have the wedding of their dreams.\\nA wonderfully funny story from the author and illustrator of The Gruffalo, Stick Man and ZOG, which have all been made into animated films shown on BBC1\\n     \",\n",
              " '3000 Synonyms and Antonyms by Sam Phillips is a matchless collection of three thousand words along with their synonyms and antonyms. The book is incredibly practical for everyone-common learners, students, teachers, writers, authors, editors and journalists-as it assists them in their work. The main word in the book has been given in the bold type followed by its synonyms and antonyms. Nearly every synonym of the main word has been given a precise antonym.',\n",
              " 'This book serves as a complete reference for information security including IT security, data security, network security, internet security, penetration testing, cryptography and laws governing the industry.\\nThe book describes the tools and penetration testing methodologies used by ethical hackers and provides a discussion of what and who an ethical hacker is and what role he plays in protecting corporate and government data from cyber attacks.\\nIt also offers an understanding of how to effectively protect data and computer networks.\\nFinally, it presents the subject in a simplified manner so that even a beginner dealing in a security environment understands and implements information security at personal and corporate levels.',\n",
              " \"The bestselling and prize-winning study of one of the most legendary American Presidents in history, Team of Rivals by Doris Kearns Goodwin is the book that inspired Barack Obama in his presidency.\\nWhen Barack Obama was asked which book he could not live without in the White House, his answer was instant: Team of Rivals. This monumental and brilliant work has given Obama the model for his presidency, showing how Abraham Lincoln saved America by appointing his fiercest rival to key cabinet positions. As well as a thrilling piece of narrative history, it's an inspiring study of one of the greatest leaders the world has ever seen.\\n'A wonderful book . . . a remarkable study in leadership' Barack Obama\\n'A portrait of Lincoln as a virtuosic politician and managerial genius' The New York Times\\n'I have not enjoyed a history book as much for years' Robert Harris\\n\\nDoris Kearns Goodwin is the doyenne of US presidential historians, and one of the most acclaimed non-fiction authors in the world. Her works include Lyndon Johnson and the American Dream, The Fitzgeralds and the Kennedys: An American Saga, and No Ordinary Time: Franklin and Eleanor Roosevelt, for which she won the Pulitzer Prize for History in 1995.\",\n",
              " \"What makes a nation great? Is it simply economic prosperity and military strength - or something more? What is it that we as a nation require to make that last-mile journey to what all the plans, investments and projects are meant to lead up to? It is only a matter of time before India is termed economically developed. But a nation has to learn to survive in tough times too. And for that what is most important is national character, born out of the value systems that exist in our families, what schools teach students, and the culture of the nation. In Pathways to Greatness, A.P.J. Abdul Kalam shifts focus from the economic development of India by 2020 to the development of our strengths, offering key lessons that will help India withstand the forces of change. He identifies what makes a nation great and also compares the standards of living of other nations with India's. He draws on his travels and his interactions with people. He evolves unique oaths for citizens from all walks of life to ensure that a better life becomes possible for everyone. In the book he completed just a few months before he passed away in 2015, one of India's best-known icons writes how our nation can become a leader on the pathways to greatness.\",\n",
              " \"The saga that has enthralled the millions of readers of The Pillars of the Earth and World Without End now continues with Ken Follett's magnificent, gripping A Column of Fire. Christmas 1558, and young Ned Willard returns home to Kingsbridge to find his world has changed. The ancient stones of Kingsbridge Cathedral look down on a city torn by religious hatred. Europe is in turmoil as high principles clash bloodily with friendship, loyalty and love, and Ned soon finds himself on the opposite side from the girl he longs to marry, Margery Fitzgerald. Then Elizabeth Tudor becomes queen and all of Europe turns against England. The shrewd, determined young monarch sets up the country’s first secret service to give her early warning of assassination plots, rebellions and invasion plans. Elizabeth knows that alluring, headstrong Mary Queen of Scots lies in wait in Paris. Part of a brutally ambitious French family, Mary has been proclaimed the rightful ruler of England, with her own supporters scheming to get rid of the new queen. Over a turbulent half-century, the love between Ned and Margery seems doomed, as extremism sparks violence from Edinburgh to Geneva. With Elizabeth clinging precariously to her throne and her principles, protected by a small, dedicated group of resourceful spies and courageous secret agents, it becomes clear that the real enemies – then as now – are not the rival religions. The true battle pitches those who believe in tolerance and compromise against the tyrants who would impose their ideas on everyone else – no matter the cost.\",\n",
              " 'Coloring allows a child to relax and be comfortable while creating a piece of art. Children can fill in the figures and shapes on the page any way they choose, coloring helps a child practice holding a writing tool the correct way and aids in developing those tiny muscles in their hands, fingers, and wrist. This is a fun book which will directly reflect to developing the skills of your child.',\n",
              " \"America's preeminent makeup artist shares his secrets, explaining not only the basics of makeup application and technique but also how to use the fundamentals to create a wide range of different looks. 200 color photos & sketches.\",\n",
              " 'One of Sidney Sheldon’s most popular and bestselling titles, repackaged and reissued for a new generation of fans.\\nKate Blackwell is one of the richest and most powerful women in the world. She is an enigma, a woman surrounded by a thousand unanswered questions. Her father was a diamond prospector who struck it rich beyond his wildest dreams. Her mother was the daughter of a crooked Afrikaaner merchant. Her conception was itself an act of hate-filled vengeance.\\nAt the extravagent celebrations of her ninetieth birthday, there are toasts from a Supreme Court Judge and a telegram from the White House. And for Kate there are ghosts, ghosts of absent friends and of enemies. Ghosts from a life of blackmail and murder. Ghosts from an empire spawned by naked ambition…\\nSidney Sheldon is one of the most popular storytellers in the world. This is one of his best-loved novels, a compulsively readable thriller, packed with suspense, intrigue and passion. It will recruit a new generation of fans to his writing.',\n",
              " \"Every phase since the advent of the industrial revolution - from the fate of the British Empire, to the global challenges from Germany, Japan and Russia, to America's emergence as a sole superpower, to the Arab Spring, to the long-term decline of economic growth that started with Japan and has now spread to Europe, to China's meteoric economy, to Brexit and the presidency of Donald Trump - can be explained better when we appreciate the meaning of demographic change across the world.The Human Tide is the first popular history book to redress the underestimated influence of population as a crucial factor in almost all of the major global shifts and events of the last two centuries - revealing how such events are connected by the invisible mutually catalysing forces of population.\\n\\nThis highly original history offers a brilliant and simple unifying theory for our understanding the last two hundred years: the power of sheer numbers. An ambitious, original, magisterial history of modernity, it taps into prominent preoccupations of our day and will transform our perception of history for many years to come.\",\n",
              " \"TOWER LORD is the second novel in the internationally bestselling Raven's Shadow series, which began with epic fantasy blockbuster BLOOD SONG.\\n\\nTHE REALM BURNS.\\nVaelin Al Sorna is tired of war. He's fought countless battles in service to the Realm and Faith. His reward was the loss of his love, the death of his friends and a betrayal by his king. After five years in an Alpiran dungeon, he just wants to go home.\\nReva intends to welcome Vaelin back with a knife between the ribs. He destroyed her family and ruined her life. Nothing will stop her from exacting bloody vengeance - not even the threat of invasion from the greatest enemy the Realm has ever faced.\\nYet as the fires of war spread, foes become friends and truths turn to lies. To save the Realm, Reva must embrace a future she does not want - and Vaelin must revisit a past he'd rather leave buried.\\n\\nPraise for Raven's Shadow:\\n'Engrossing' - Buzzfeed\\n'Powerful' - SFFWorld\\n'Compelling' - SFX\\n\\nRaven's Shadow\\nBlood Song\\nTower Lord\\nQueen of Fire\\nRaven's Blade\\nThe Wolf's Call (coming July 2019)\\nThe Draconis Memoria\\nThe Waking Fire\\nThe Legion of Flame\\nThe Empire of Ashes\",\n",
              " 'The third volume in a series of comic cartoons starring the Calvin and Hobbes pair.\\nCalvin, cheeky, hyperactive and mischievous, and Hobbes, his cuddly toy tiger who, as far as Calvin is concerned is very much alive and kicking, are two of the most loveable and hilarious characters to grace the comic strip in years.\\nSit back and enjoy . . .',\n",
              " 'In the world that Calvin and Hobbes share, treasures can be found in the most unlikely places - from the outer regions where Spaceman Spiff travels to the rocks in the backyard.',\n",
              " '\"Your first love isn’t the first person you give your heart to—it’s the first one who breaks it\". Sad Girls is the much anticipated debut novel from international best-selling author Lang Leav. A beautifully written and emotionally charged coming of age story, where young love, dark secrets and tragedy collide. School is almost out for Audrey, but the panic attacks are just beginning. Because Audrey told a lie and now her classmate, Ana, is dead. Just as her world begins to spin out of control, Audrey meets the enigmatic Rad – the boy who could turn it all around. But will their ill-timed romance drive her closer to the edge?',\n",
              " \"Naruto is a young shinobi with an incorrigible knack for mischief. He's got a wild sense of humor, but Naruto is completely serious about his mission to be the world's greatest ninja!\\n\\nWith Naruto out of commission, Guy steps up to fight against the powerful Madara. Guy is willing to give up his life to defeat Madara, but will that be enough? And can the others save Naruto in time?\",\n",
              " 'It is April 1975, and Saigon is in chaos. At his villa, a general of the South Vietnamese army is drinking whiskey and, with the help of his trusted captain, drawing up a list of those who will be given passage aboard the last flights out of the country. The general and his compatriots start a new life in Los Angeles, unaware that one among their number, the captain, is secretly observing and reporting on the group to a higher-up in the Viet Cong. The Sympathizer is the story of this captain: a man brought up by an absent French father and a poor Vietnamese mother, a man who went to university in America, but returned to Vietnam to fight for the Communist cause. A gripping spy novel, an astute exploration of extreme politics, and a moving love story, The Sympathizer explores a life between two worlds and examines the legacy of the Vietnam War in literature, film, and the wars we fight today.',\n",
              " 'Tintin walks into a dangerous trap while trying to rescue three of his friends who are falsely arrested in South America.',\n",
              " 'In Amsterdam, an Israeli terrorist analyst is murdered. The police believe the killer is a deranged Muslim extremist, but Israeli intelligence knows better. Art-restorer, assassin and spy Gabriel Allon is dispatched to investigate, uncovering a major terrorist operation in London.\\n\\n\\nGabriel arrives too late to prevent the kidnapping of the daughter of the US ambassador. With time running out, Allon has no choice but to plunge into a desperate search, both for the woman and for those responsible, but the truth, when he finds it, is more terrible than he could expect. It will endanger his life and shake him to the core.',\n",
              " \"A half-starved young Russian man in a long black overcoat is smuggled into Hamburg at dead of night. He has an improbable amount of cash secreted in a purse round his neck. He is a devout Muslim. Or is he? He says his name is Issa.\\nAnnabel, an idealistic young German civil rights lawyer, determines to save Issa from deportation. Soon her client's survival becomes more important to her than her own career. In pursuit of Issa's mysterious past, she confronts the incongruous Tommy Brue, the sixty-year-old scion of Brue Frères, a failing British bank based in Hamburg.\\nA triangle of impossible loves is born.\\nMeanwhile, scenting a sure kill in the so-called War on Terror, the spies of three nations converge upon the innocents.\\nPoignant, compassionate, peopled with characters the reader never wants to let go, A MOST WANTED MAN is alive with humour, yet prickles with tension until the last heart-stopping page. It is also a work of deep humanity, and uncommon relevance to our times.\",\n",
              " \"A manga series that packs quite the punch!\\n\\nNothing about Saitama passes the eyeball test when it comes to superheroes, from his lifeless expression to his bald head to his unimpressive physique.  However, this average-looking guy has a not-so-average problem - he just can't seem to find an opponent strong enough to take on!  He's easily taken out a number of monsters, including a crabby creature, a malicious mosquito girl and a muscly meathead.  But his humdrum life takes a drastic turn when he meets Genos - a cyborg who wants to uncover the secret behind his strength!\",\n",
              " 'The Left Hand of God by Paul Hoffman is the gripping first instalment in a remarkable trilogy.\\n\\n\"Listen. The Sanctuary of the Redeemers on Shotover Scarp is named after a damned lie for there is no redemption that goes on there and less sanctuary.\"\\nThe Sanctuary of the Redeemers is a vast and desolate place - a place without joy or hope. Most of its occupants were taken there as boys and for years have endured the brutal regime of the Lord Redeemers whose cruelty and violence have one singular purpose - to serve in the name of the One True Faith.\\nIn one of the Sanctuary\\'s vast and twisting maze of corridors stands a boy. He is perhaps fourteen or fifteen years old - he is not sure and neither is anyone else. He has long-forgotten his real name, but now they call him Thomas Cale. He is strange and secretive, witty and charming, violent and profoundly bloody-minded. He is so used to the cruelty that he seems immune, but soon he will open the wrong door at the wrong time and witness an act so terrible that he will have to leave this place, or die.\\nHis only hope of survival is to escape across the arid Scablands to Memphis, a city the opposite of the Sanctuary in every way: breathtakingly beautiful, infinitely Godless, and deeply corrupt.\\nBut the Redeemers want Cale back at any price... not because of the secret he now knows but because of a much more terrifying secret he does not.\\nThe Left Hand of God is a must read. It is the first instalment in a gripping trilogy by Paul Hoffman. Imagine if Phillip Pullman\\'s His Dark Materials met Umberto Eco\\'s Name of the Rose. Fans of epic heroic fiction will love this series.\\nPraise for Paul Hoffman:\\n\\'This book gripped me from the first chapter and then dropped me days later, dazed and grinning to myself\\' Conn Iggulden\\n\\'Tremendous momentum\\' Daily Telegraph\\n\\'A cult classic . . .\\' Daily Express',\n",
              " \"A glorious Collector's Edition of New York Times bestselling, epic fantasy novel, Six Of Crows. Beautifully designed, with an exclusive letter from the author and six stunning full-colour character portraits. This covetable hardback with red sprayed edges is a perfect gift for fans and a perfect way to discover the unforgettable writing of Leigh Bardugo. Criminal prodigy Kaz Brekker is offered a chance at a deadly heist: break into the Ice Court - a military stronghold that has never been breached - and retrieve a hostage whose knowledge could change Grisha magic forever. To succeed would mean rich beyond his wildest dreams - but he can't pull it off alone. A convict with a thirst for revenge. A sharpshooter who can't walk away from a wager. A runaway with a privileged past. A spy known as the Wraith. A Heartrender using her magic to survive the slums. A thief with a gift for unlikely escapes. Six dangerous outcasts. One impossible heist. Together they might just be unstoppable if they don't kill each other first. 'Fast, thrilling heist fantasy, boasting a brilliant new cast of characters.' Metro 'A full-throttle adventure crackling dialogue and sumptuous description. Bardugo dives deep into this world.' - New York Times.\",\n",
              " 'Draped in the Desert Prince’s diamonds…\\n\\nTo ensure his sister’s successful marriage, Kasim, Crown Prince of Zhamair, must stop Angelique Sauveterre’s alleged affair with his future brother-in-law. But when Angelique denies any involvement, Kasim can’t resist the chance to make the feisty beauty his!\\n\\nAngelique is tempted by Kasim’s offer of a fling—always compared to her twin sister, she’s never allowed to just be herself. They couldn’t be from two more different worlds, yet Angelique blossoms under Kasim’s touch and surrenders to the desert Prince. But can he give her more than passion and precious jewels?\\n\\nBook 1 in The Sauveterre Siblings quartet',\n",
              " \"Meet Mitali, a young IT professional working in Bangalore who decides to take a plunge into the complex world of matrimonial websites to find herself the perfect life partner, only to be sucked right into a quagmire of stereotypes, insane expectations, and outrageous demands.\\nAfter dozens of excruciating interviews, heartbreaking rejections, and obnoxious feedbacks, when she finally meets a guy who is refreshingly normal, Mitali agrees to marry him. But destiny has something else in mind for her.\\nWhen that magical, elusive moment of 'click' which every girl longs to experience in her life comes to Mitali in the most unexpected of ways, it throws her heart and her life up in turmoil. Caught in the centre of a crazy whirlwind that lands her in a police station in the dead of the night, two days before she is set to get married.\\nWill Mitali be able to get out of the mess she has unwittingly created? Will she find the love her heart yearns for? Will she find that elusive ‘click’ again?\",\n",
              " 'Ghouls live among us, the same as normal people in every way except their craving for human flesh. Ken Kaneki is an ordinary college student until a violent encounter turns him into the first half human half ghoul hybrid. Trapped between two worlds, he must survive Ghoul turf wars, learn more about Ghoul society and master his new powers.\\n\\nKaneki, Nishio and Touka struggle to work together to rescue their human friend Kimi while Ghoul Investigator deaths skyrocket in Wards 9 through 12. When reinforcements are called in on both sides, the stakes are suddenly higher than ever.',\n",
              " 'This new edition of the book, is restructured to trace the advancements made and landmarks achieved in software engineering. The text not only incorporates latest and enhanced software engineering techniques and practices, but also shows how these techniques are applied into the practical software assignments. The chapters are incorporated with illustrative examples to add an analytical insight on the subject. The book is logically organised to cover expanded and revised treatment of all software process activities.\\nKEY FEATURES\\n• Large number of worked-out examples and practice problems\\n• Chapter-end exercises and solutions to selected problems to check students’ comprehension on the subject\\n• Solutions manual available for instructors who are confirmed adopters of the text\\n• PowerPoint slides available online at www.phindia.com/rajibmall to provide integrated learning to the students\\nNEW TO THE FIFTH EDITION\\n• Several rewritten sections in almost every chapter to increase readability\\n• New topics on latest developments, such as agile development using SCRUM, MC/DC testing, quality models, etc.\\n• A large number of additional multiple choice questions and review questions in all the chapters help students to understand the important concepts\\nTARGET AUDIENCE\\n• BE/B.Tech (CS and IT)\\n• BCA/MCA\\n• M.Sc. (CS)\\n• MBA',\n",
              " 'In this prelude to DC UNIVERSE: REBIRTH, the Superman and Lois Lane of the pre-New 52 DC Universe return in SUPERMAN: LOIS AND CLARK, from the creative team of Dan Jurgens and Lee Weeks!\\n \\nThey were the first couple of truth and justice, the Man of Steel and the tough-as-nails reporter who was the love of his life. Then came the Flashpoint…the Convergence…and their world was wiped from reality, replaced with the New 52 worlds that make up the Multiverse.\\n \\nNow Lois and Clark, as well as their young son, Jonathan, have been transported to an Earth much like the one they left behind, yet radically different. An Earth with familiar heroes, familiar faces, familiar names, but entirely different ages and attitudes. An Earth with its own Lois Lane, its own Clark Kent—its own Superman. An Earth where Superman is distrusted—and where two Supermen could cause a disaster.\\n \\nFor years they’ve stayed below the radar, with Clark fighting evil under the cover of darkness and Lois crusading against crime as an anonymous journalist. But now all their secrets are about to be exposed to a world far harsher than the one they left behind—and Jonathan is caught in the crossfire. Now only one question remains:\\n \\nHas the time come for the original Man of Tomorrow to reveal himself?\\n \\nCollects SUPERMAN: LOIS AND CLARK #1-8.',\n",
              " 'A gripping new standalone thriller from the master of suspense and New York Times #1 bestselling author.\\n\\n‘I very much need to be dead’\\n\\nThese are the chilling last words left by a man who had everything to live for but took his own life. In the void that remains stands his widow, FBI agent Jane Hawk, determined to do what all the grief and fury inside her demand: find the truth, no matter what.\\n\\nPeople of talent, seemingly happy and sound of mind, have recently been committing suicide in surprising numbers. A disturbing pattern is beginning to emerge. Jane is determined to give up everything to find out why.\\n\\nThose arrayed against her are devoted to protecting something important – or terrifying –enough to exterminate anyone in their way. But Jane is as clever as these enemies are cold-blooded. And she is driven by a righteous rage they can never comprehend. Because it is born of love.',\n",
              " \"When war broke out in 1914, Somerset Maugham was dispatched by the British Secret Service to Switzerland under the guise of completing a play. Multilingual, knowledgeable about many European countries and a celebrated writer, Maugham had the perfect cover, and the assignment appealed to his love of romance, and of the ridiculous. The stories collected in Ashenden are rooted in Maugham's own experiences as an agent, reflecting the ruthlessness and brutality of espionage, its intrigue and treachery, as well as its absurdity.\",\n",
              " \"Ladybird I'm Ready for Phonics activity books are a great way for children to learn and practise their phonics skills. The series of phonic readers and their accompanying activity workbooks have been carefully written to give gradual, structured practise of the synthetic phonics your child is learning at school. Level 1 includes lots of fun activities and stickers to help your child practise their letters and sounds and early reading skills, just as they do at school, and will help children prepare for the Phonics Screening Check at the end of Year 1. The series closely follows the order that a child is taught phonics in school, from initial letter sounds to key phonemes and beyond. It helps to build reading confidence through practice of these phonics building blocks, and reinforces school learning in a fun way. Other titles in the series are: Phonic activity books, Phoneme Flash cards: Ladybird I'm Ready for Phonics, Say the Sounds: Ladybird I'm Ready for Phonics, Captain Comet's Space Party! Ladybird I'm Ready for Phonics: Level 1, Nat Naps! Ladybird I'm Ready for Phonics Level 2, Top Dog: Ladybird I'm Ready for Phonics: Level 3, Fix It Vets: Ladybird I'm Ready for Phonics: Level 5, Dash is Fab! Ladybird I'm Ready for Phonics: Level 6, Big BIG Fish: Ladybird I'm Ready for Phonics: Level 7, Dig, Farmer, Dig! Ladybird I'm Ready for Phonics Level 8, Fun Fair Fun: Ladybird I'm Ready for Phonics: Level 9, Wow, Wowzer, Wow! Ladybird I'm Ready for Phonics: Level 10, Wizard Woody: Ladybird I'm Ready for Phonics: Level 11, Monster Stars: Ladybird I'm Ready for Phonics: Level 12\\nA Children’s Bookshelf Selection: Each month our editor’s pick the best books for children and young adults by age to be a part of the children’s bookshelf. These are editorial recommendations made by our team of experts. Our monthly reading list includes a mix of bestsellers and top new releases and evergreen books that will help enhance a child’s reading life.\",\n",
              " \"'Indu Sundaresan has written a fascinating novel about a fascinating time, and has brought it alive with characters that are at once human and legendary, that move with grace and panache across the brilliant stage she has reconstructed for them' - Chitra Divakaruni In this lush and romantic sequel to The Twentieth Wife, Mehrunnisa, the first woman Jahangir marries for love, is now Empress Nur Jahan. As a mark of his love, he transfers his powers of sovereignty to her. But she has a formidable rival in the imperial harem, Empress Jagat Gosisni, who has plotted against her from the moment she entered the emperor's life. Beyond the harem walls, she battles powerful ministers who aren't willing to allow a mere woman to have a say in the outside world. Defying all established norms of womanhood in seventeenth-century India, Mehrunnisa combats her rivals by forming a junta of sorts with the three men she can rely on: her father, her brother and Jahangir's son Prince Khurram. She demonstrates great strength of character and cunning to get what she wants, sometimes at great personal cost, even almost losing her daughter's love. But she never loses the love of the man who bestows this power upon her: Emperor Jahangir.\",\n",
              " 'For those who eat, live and pray cricket this book is a treasure trove. With over 500 questions, trivia and quirky facts from the ODI’s and Test cricket, this Quiz Book has all that has been done on the field. This is a must-have for both connoisseurs of the game and those who follow it as a profession.',\n",
              " \"They found him in a small town in Brazil, near the border with Paraguay. He had a new name, Danilo Silva, and his appearance had been changed by plastic surgery. The search had taken four years. They'd chased him around the world, always just missing him. It had cost their clients $3.5 million. But so far none of them had complained.\\n\\nThe man they were about to kidnap had not always been called Danilo Silva. Before he had had another life, a life which ended in a car crash in February 1992. His gravestone lay in a cemetry in Biloxi, Mississippi. His name before his death was Patrick S. Lanigan. He had been a partner at an up-and-coming law firm. He had a pretty wife, a young daughter, and a bright future. Six weeks after his death, $90 million disappeared from the law firm.\\n\\nIt was then that his partners knew he was still alive, and the long pursuit had begun...\",\n",
              " 'When Deb, an author and publisher, survives the bomb blasts at Chandni Chowk, he knows his life is nothing short of a miracle. And though he escapes with minor injuries, he is haunted by that unfortunate day.His feet take him to where the blasts took place. From the burnt remains he discovers a diary. It seems to belong to a dead man who was deeply in love with a girl. As he reads the heartbreaking narrative, he knows that this story must never be left incomplete. Thus begins Deb’s journey with his girlfriend, Avantika and his best friend, Shrey, to hand over the diary to the man’s beloved.',\n",
              " \"Sam McCready is The Deceiver, one of the Secret Intelligence Service's most unorthodox and most valued operatives, a legend in his own time. The end of the cold war has, however, strengthened the hand of the Whitehall mandarins, to whom he seems about as controllable as Genhis Khan, so Sam is to have his fate decided at a special hearing.\\n\\nAs part of the proceedings, four of Sam's key operations are reviewed: a clandestine mission into East Germany in 1985 to contact the top Russian spy General Pankratin; the second involving a KGB colonel who wants to defect - but is he genuine? An audacious Qaddafi-inspired plot to ship arms to the IRA; and the fourth when McCready presided over the aftermath of political murder and mayhem in the Caribbean.\",\n",
              " \"Peppered with heartfelt accounts and charming anecdotes, Urdu film magazines were in great favour with the public from the 1930s through the 1990s - a considerable period of seven decades. Unfortunately, as Urdu got progressively marginalised in later years, these magazines were not archived, for the most part; leading to their inevitable disappearance from popular imagination.\\nTracking down these lost publications, Yasir Abbasi followed leads - some futile, some fruitful - to obscure towns and people's homes in a last-ditch effort to save valuable records of Indian cinema. As challenging as it was to locate faded issues and original texts, he managed to uncover and translate many fabulous memoirs covering a wide gamut of our favourite old artistes at their candid best.\\nA gloom-laced piece on Meena Kumari by Nargis, a rollicking description by Raja Mehdi Ali Khan of an eventful evening with Manto (not to mention a mysterious woman and a house on fire), Jaidev writing about his chequered career, Balraj Sahni introspecting about the relevance of Hindi and Urdu in films - it's a rich mix of engrossing narratives brought back from oblivion.\",\n",
              " \"Bobby Fischer Teaches Chess is a well-presented book for beginners in chess. It contains various puzzles, along with clear instructions, can help readers learn how to play chess quickly and efficiently.\\nBobby Fischer Teaches Chess follows a flowchart-like pattern. This teaching machine was made according to the principles of programmed learning, and is filled with checkmate puzzles. At every step, the authors asks you a question. The right answer takes you to the next question, whereas the incorrect answer takes you to an explanation. Readers will find themselves in complex situations, and they will have to come up with solutions and moves on the chessboard, like Bobby Fischer used to. Eighteen of the puzzles in Bobby Fischer Teaches Chess are based on positions of Fischer's games. This guidebook will teach you how to analyse chess problems, and look for themes in order to make the right moves.\\nThe book contains a section titled A Word From Bobby. The introduction explains how to play chess. Some of the chapters in this book are Elements of Checkmate, Displacing Defenders, and Attacks on the Enemy Pawn Cover. Readers will also come across diagrams which make it easier for them to understand the concepts. The reissue edition, published by RHUS in 1982, is available in paperback.\",\n",
              " 'Sana told Om about Jenny; she said, \"You know Om, Jenny is in love with you, I noticed that earlier and last night when I probed her and she agreed to it. She also told me that she feels that you love me.\" I smiled to it. She said she is happy if Om is in love with me - hope love is not bound with the limitation of being human or a machine.\"\\n\\n\"Whatever she said, she was correct, Om. You are a human and I am a Hurob. Can there be love between us?\"\\n\\nOm smiled and said, \"If it could not be it would have not been.\"\\n\\n\"Love and life always find their way\"\\n\\n\"Emotions must be respected irrespective of their origin\"\\n\\nAt last Dr. Albert said - NOTHING IN THE UNIVERSE IS MORE FATAL THAN A BROKEN HEART - \"THE COR-CONTRITUM\"',\n",
              " \"The only security book to be chosen as a Dr. Dobbs Jolt Award Finalist since Bruce Schneier's Secrets and Lies and Applied Cryptography! Adam Shostack is responsible for security development lifecycle threat modeling at Microsoft and is one of a handful of threat modeling experts in the world. Now, he is sharing his considerable expertise into this unique book. With pages of specific actionable advice, he details how to build better security into the design of systems, software, or services from the outset. You'll explore various threat modeling approaches, find out how to test your designs against threats, and learn effective ways to address threats that have been validated at Microsoft and other top companies. Systems security managers, you'll find tools and a framework for structured thinking about what can go wrong. Software developers, you'll appreciate the jargon-free and accessible introduction to this essential skill. Security professionals, you'll learn to discern changing threats and discover the easiest ways to adopt a structured approach to threat modeling. * Provides a unique how-to for security and software developers who need to design secure products and systems and test their designs * Explains how to threat model and explores various threat modeling approaches, such as asset-centric, attacker-centric and software-centric * Provides effective approaches and techniques that have been proven at Microsoft and elsewhere * Offers actionable how-to advice not tied to any specific software, operating system, or programming language * Authored by a Microsoft professional who is one of the most prominent threat modeling experts in the world As more software is delivered on the Internet or operates on Internet-connected devices, the design of secure software is absolutely critical. Make sure you're ready with Threat Modeling: Designing for Security.\",\n",
              " 'Why does a director choose a particular script? What must they do in order to keep actors fresh and truthful through take after take of a single scene? How do you stage a shootout—involving more than one hundred extras and three colliding taxis—in the heart of New York’s diamond district? What does it take to keep the studio honchos happy? From the first rehearsal to the final screening, Making Movies is a master’s take, delivered with clarity, candor, and a wealth of anecdote.\\n\\nFor in this book, Sidney Lumet, one of our most consistently acclaimed directors, gives us both a professional memoir and a definitive guide to the art, craft, and business of the motion picture. Drawing on forty years of experience on movies that range from Long Day’s Journey into Night to Network and The Verdict—and with such stars as Katharine Hepburn, Paul Newman, Marlon Brando, and Al Pacino—Lumet explains how painstaking labor and inspired split-second decisions can result in two hours of screen magic.',\n",
              " \"For C language programmers, it is must to master the complexity of the language to deal with programming software in engineering, gaming and other fields. In order to understand each concept of the C language, it is necessary to follow a good reference book in easy-to-understand text.\\nKeeping lucidity and simplicity in mind, Yashavant P Kanetkar, the author of the book, has compiled it to be one of the most interesting C programming books for C language learners. The book begins with the basic knowledge of different concepts and further leads to advanced levels. This means the book will provide complete knowledge to the readers from basic to complex programming parts.\\nIt covers various topics that could be easily understood with the help of examples given with each programming concept. Besides, the book also features several descriptive details about console input, C preprocessor, arrays, functions, strings and pointers. Explained in comprehensive manner, the book aims to provide more brief information to all C programming beginners as well as established programmers.\\n‘Let Us C’ can help the readers to prepare not only for the theoretical exams but for the practical exams as well. There is also a separate section in the book that includes most frequently asked questions in job interviews. Hence, it can be a good reference manual for preparing job interviews as well.\\nAbout the Author\\nYashavant Kanetkar is a computer science author of Indian origin and is well known for his work on programming languages. He obtained his B.E. degree from VJTI, Mumbai and completed his M.Tech from IIT Kanpur. He was awarded the 'Microsoft Most Valuable Profession' by Microsoft. He is currently the Director of KICIT and KSET. He also speaks on various technology subjects and has written columns for publications such as Developer 2.0 and Express Computers.\",\n",
              " 'In 1980, a brilliant young American scholar, George H. Gadbois, Jr., met five judges of the Supreme Court of India. The judges gave him astonishing details: about what they actually thought of their colleagues, about the inner workings and politics of the court, their interactions with the government and the judicial appointments process, among many other things. This was only the beginning. Over the course of that decade, Gadbois visited India on two more occasions and conducted over 116 interviews with more than sixty-six judges of the Supreme Court of India (nineteen of whom held the post of chief justice of India),\\nand others such as senior lawyers, politicians, relatives of deceased judges, and court staff. During each meeting, Gadbois diligently took down handwritten notes, which he later typed up on his typewriter, recording nearly every detail of what the judges had told him, sometimes to a fault.\\nRelying on these typewritten interviews, Abhinav Chandrachud sheds light on a decade of politics, decision-making and legal culture in the Supreme Court of India. This book yields a fascinating glimpse into the secluded world of the judges of the Supreme Court in the 1980s and earlier.',\n",
              " \"The Avengers assemble again in Marvel Studios' follow-up to the record-breaking Marvel's The Avengers. Robert Downey Jr., Chris Hemsworth, Mark Ruffalo, Chris Evans, Scarlett Johansson and Jeremy Renner reprise their roles as world-saving heroes in an adventure like no other. In this new collectible volume, go around the world with the Avengers in page after page of stunning concept art, production design, visual effects, revealing set photography, and commentary from cast and crew, including writer/director Joss Whedon and producer Kevin Feige. Marvel's Avengers: Age of Ultron takes Marvel's Cinematic Universe to new heights in yet another silver-screen triumph.\",\n",
              " 'The #1 New York Times bestselling original graphic novel by the critically acclaimed author Paul Dini! DARK NIGHT: A TRUE BATMAN STORY is the harrowing and eloquent autobiographical tale of Dini’s courageous struggle to overcome a truly desperate situation. It is a Batman story like no other and one that will truly resonate with fans, with art by the incredible and talented Eduardo Risso (100 BULLETS).\\n \\nIn the 1990s, writer Paul Dini had a flourishing career penning the hugely popular Batman: The Animated Series and Tiny Toon Adventures. It was a golden era of television screenwriting, and Dini and his fellow writers were at the forefront of the glittering Hollywood scene. In one night, everything would change.\\n \\nWalking home one evening, Dini was jumped and viciously beaten. With several broken bones and a shattered face, Dini experienced an arduous recovery process, hampered by the imagined antics of the villains he was writing for television, including the Joker, Harley Quinn, and the Penguin. But despite how bleak his circumstances were, or perhaps because of it, Dini also always imagined the Batman at his side, constantly chivvying him along during his darkest moments.\\n \\nWhile most know the Caped Crusader as the all-abiding icon of justice and authority, in this surprising story, we see Batman in a new light—not as the dark avenger, but the savior who helped a discouraged man recover mentally from a brutal attack that left him unable to face the world.',\n",
              " 'The book describes the story of two strangers who develop unfading love toward each other in a very short time. They suffer because of developing love of a conventional nature in a world that refuses to understand the depth and sentiments. The story conveys how certain emotions refuse to die when treated recklessly and also the magic of the product of true love whether it is a person or an object. It describes the inevitable price one has to pay for not following ones heart.',\n",
              " 'A legal thriller from the author of THE PELICAN BRIEF, THE CLIENT and THE FIRM, about what goes on behind the scenes when a jury retires to deliberate and decide whether the accused is innocent or guilty.',\n",
              " 'The Russian novelist and moral philosopher Leo Tolstoy (1828-1910) ranks as one of the world\\'s great writers and his \"War and Peace\" has been called the greatest novel ever written. The purpose of all true creative art, he believed, is to teach. But the message in all his stories is presented with such humor that the reader hardly realizes that it is strongly didactic. The seven parts into which this book is divided include the best known Tolstoy stories. \"God Sees the Truth, but Waits\" and \"A Prisoner in the Caucasus\" which Tolstoy himself considered as his best, \"How Much Land Does a Man Need?\" depicting the greed of a peasant for land, the most brilliantly told parable, \"Ivan the Fool\" these are all contained in this volume.',\n",
              " \"Asterix and Obelix escort Getafix to the druids' annual conference in the Forest of the Carnutes. Little do they know that the Goths are lying in ambush, ready to kidnap the Druid of the Year - who of course is Getafix! But what with Gauls, Goths and Romans all at odds, it's hard to tell friend from foe... until Goths begin fighting Goths in the Asterixian Wars.\",\n",
              " \"The Webbed Wonder and the Merc with a Mouth are teaming up for their first series EVER! It's action, adventure and just a smattering of (b)romance in this episodic epic featuring the WORLD'S GREATEST SUPER HERO and the star of the WORLD'S GREATEST COMICS MAGAZINE. Talk about a REAL dynamic duo!\\n\\nCOLLECTING: SPIDER-MAN/DEADPOOL 1-5, 8\",\n",
              " 'A criminal is immersed in chemicals that disﬁgure him, driving him mad and giving birth to the Joker. As Joker sets out to break commissioner Gordon, shooting and crippling gordon’s daughter, Batman races to rescue him. This edgy adaptation by hard case crime novelist Christa Faust expands adds intricate layers to the cast and events of the graphic novel, further examining the nature of morality.',\n",
              " \"Penelope Lively's Booker Prize winning classic, Moon Tiger is a haunting story of loss and desire. Claudia Hampton - beautiful, famous, independent, dying. But she remains defiant to the last, telling her nurses that she will write a 'history of the world . . . and in the process, my own'. And it is her story from a childhood just after the First World War through the Second and beyond. But Claudia's life is entwined with others and she must allow those who knew her, loved her, the chance to speak, to put across their point of view. There is Gordon, brother and adversary; Jasper, her untrustworthy lover and father of Lisa, her cool conventional daughter; and then there is Tom, her one great love, found and lost in wartime Egypt. 'Leaves its traces in the air long after you've put it away' Anne Tyler 'A complex tapestry of great subtlety. Lively writes so well, savouring the words as she goes' Daily Telegraph 'Very clever: evocative, thought-provoking and hangs on the mind long after it is finished' Literary Review Penelope Lively is the author of many prize-winning novels and short-story collections for both adults and children. She has twice been shortlisted for the Booker Prize: once in 1977 for her first novel, The Road to Lichfield, and again in 1984 for According to Mark. She later won the 1987 Booker Prize for her highly acclaimed novel Moon Tiger. Her other books include Going Back; Judgement Day; Next to Nature, Art; Perfect Happiness; Passing On; City of the Mind; Cleopatra's Sister; Heat Wave; Beyond the Blue Mountains, a collection of short stories; Oleander, Jacaranda, a memoir of her childhood days in Egypt; Spiderweb; her autobiographical work, A House Unlocked; The Photograph; Making It Up; Consequences; Family Album, which was shortlisted for the 2009 Costa Novel Award, and How It All Began. She is a popular writer for children and has won both the Carnegie Medal and the Whitbread Award. She was appointed CBE in the 2001 New Year's Honours List, and DBE in 2012. Penelope Lively lives in London.\",\n",
              " 'In many ways, Tootsie Lama is like any eight-year-old girl in the small hilly town of Darjeeling. But in many ways, she is completely different – she lives on her own, goes to bed whenever she likes and cooks her own meals. One day Tootsie decides she would really like a delicious steaming bowl of thukpa, just like her Aama used to make it. Now all she needs is a plan.',\n",
              " 'Laila, orphaned daughter of a distinguished Muslim family, is brought up in her grandfather’s house by orthodox aunts who keep purdah. At fifteen she moves to the home of a ‘liberal’ but autocratic uncle in Lucknow. Here, during the 1930s, as the struggle for Indian independence intensifies, Laila is surrounded by relatives and university friends caught up in politics. But Laila is unable to commit herself to any cause: her own fight for independence is a struggle against the claustrophobia of traditional life, from which she can only break away when she falls in love with a man whom her family has not chosen for her. With its beautiful evocation of India, its political insight and unsentimental understanding of the human heart, Sunlight on a Broken Column, first published in 1961, is a classic of Muslim life.',\n",
              " \"With Hands-On Recommendation Systems with Python, learn the tools and techniques required in building various kinds of powerful recommendation systems (collaborative, knowledge and content based) and deploying them to the web Key Features Build industry-standard recommender systems Only familiarity with Python is required No need to wade through complicated machine learning theory to use this book Book DescriptionRecommendation systems are at the heart of almost every internet business today; from Facebook to Netflix to Amazon. Providing good recommendations, whether it's friends, movies, or groceries, goes a long way in defining user experience and enticing your customers to use your platform. This book shows you how to do just that. You will learn about the different kinds of recommenders used in the industry and see how to build them from scratch using Python. No need to wade through tons of machine learning theory-you'll get started with building and learning about recommenders as quickly as possible.. In this book, you will build an IMDB Top 250 clone, a content-based engine that works on movie metadata. You'll use collaborative filters to make use of customer behavior data, and a Hybrid Recommender that incorporates content based and collaborative filtering techniques With this book, all you need to get started with building recommendation systems is a familiarity with Python, and by the time you're fnished, you will have a great grasp of how recommenders work and be in a strong position to apply the techniques that you will learn to your own problem domains. What you will learn Get to grips with the different kinds of recommender systems Master data-wrangling techniques using the pandas library Building an IMDB Top 250 Clone Build a content based engine to recommend movies based on movie metadata Employ data-mining techniques used in building recommenders Build industry-standard collaborative filters using powerful algorithms Building Hybrid Recommenders that incorporate content based and collaborative fltering Who this book is forIf you are a Python developer and want to develop applications for social networking, news personalization or smart advertising, this is the book for you. Basic knowledge of machine learning techniques will be helpful, but not mandatory.\",\n",
              " \"Enjoy this beautiful companion book to the extensive Exploring Calvin and Hobbes exhibition at the Billy Ireland Cartoon Library. Includes an in-depth, original,and lengthy interview with Bill Watterson. Exploring Calvin and Hobbes is the catalogue for an exhibition by the same name at the Billy Ireland Cartoon Library & Museum at Ohio State University that ran in 2014. The exhibit is Bill Watterson's personal exploration of how the wonder of Calvin and Hobbes came to be. It includes original art of Calvin and Hobbes, along with Watter-son's original commentary. The show also includes art from cartoons and cartoonists that Watterson has identified as influential in the development of his art, including Peanuts, Pogo, Krazy Kat, Doonesbury, Pat Oliphant, Jim Borgman, Flash Gordon, Bloom County, and Steadman. The book also includes an extensive, original interview with Watterson by Jenny Robb, the exhibition's curator.\",\n",
              " 'THE CHINESE \"LORD OF THE RINGS\" - NOW IN ENGLISH FOR THE FIRST TIME.\\nTHE SERIES EVERY CHINESE READER HAS BEEN ENJOYING FOR DECADES - 300 MILLION COPIES SOLD.\\nChina: 1200 A.D.\\nThe Song Empire has been invaded by its warlike Jurchen neighbours from the north. Half its territory and its historic capital lie in enemy hands; the peasants toil under the burden of the annual tribute demanded by the victors. Meanwhile, on the Mongolian steppe, a disparate nation of great warriors is about to be united by a warlord whose name will endure for eternity: Genghis Khan.\\nGuo Jing, son of a murdered Song patriot, grew up with Genghis Khan\\'s army. He is humble, loyal, perhaps not altogether wise, and is fated from birth to one day confront an opponent who is the opposite of him in every way: privileged, cunning and flawlessly trained in the martial arts.\\nGuided by his faithful shifus, The Seven Heroes of the South, Guo Jing must return to China - to the Garden of the Drunken Immortals in Jiaxing - to fulfil his destiny. But in a divided land riven by war and betrayal, his courage and his loyalties will be tested at every turn.\\nTranslated from the Chinese by Anna Holmwood',\n",
              " \"India's democracy often receives extreme responses of exaggerated appreciation or enlarged criticism. It is necessary that public debates on democracy in India are based on a more informed analysis. This short introduction will help the reader to put the various debated issues in perspective and arrive at a critical appreciation of the endeavour called democracy. The book takes the reader through a tour of key issues of contestations and mobilization that have occupied the terrain of democratic politics in India. Calling India's democracy 'work in progress', this short tract draws attention to the central paradoxes of Indian democracy. While taking a long term view of democracy, the book is alive to the more contemporary challenges as well. Readers may agree or disagree, but they cannot ignore the central argument that while India's democracy wades through many paradoxes, it faces the challenge of distortion if majoritarian tendencies become pervasive and if the core feature of diversity is weakened. This book is a timely warning about the possibilities and distortions that democracy in India contains.\",\n",
              " 'An emotionally rich, moving and nuanced memoir from the Yorkshire and England wicketkeeper.\\n\\nAs a young boy of eight, Jonny Bairstow was dealt a cruel blow. His father David ‘Bluey’ Bairstow, the combative and very popular wicketkeeper and captain of Yorkshire, took his own life at the age of forty-six.\\n\\nDavid left behind Jonny, Jonny’s sister Becky and half-brother Andy and his wife Janet, who had recently been diagnosed with cancer at the time of his death. From these incredibly tough circumstances, Jonny and his family strived to find an even keel and come to terms with the loss of their father and husband.\\n\\nJonny found his way through his dedication to sport. He was a gifted and natural athlete, with potential careers ahead of him in rugby and football, but he eventually chose cricket and came to build a career that followed in his father’s footsteps, eventually reaching the pinnacle of the sport and breaking the record for most Test runs in a year by a wicketkeeper.\\n\\nWritten with multiple-award-winning writer Duncan Hamilton, this is an incredible story of triumph over adversity and a memoir with far-reaching lessons about determination and the will to overcome.',\n",
              " 'From the hit TV show How I Met Your Mother comes Barney Stinson’s words of wit, wisdom, and awesomeness, The Bro Code—the New York Times bestseller (really!) with more than a million copies in print all around the world.\\n\\nEveryone’s life is governed by an internal code of conduct. Some call it morality. Others call it religion. But Bros in the know call this Holy Grail The Bro Code.\\n\\nThe Bro Code is a living document, much like the Constitution. Except instead of outlining a government, or the Bill of Rights, or anything even resembling laws, The Bro Code provides men with all the rules they need to know in order to become a “bro” and behave properly among other bros. Historically a spoken tradition passed from one generation to the next and dating back to the American Revolution, the official code of conduct for Bros appears here in its published form for the first time ever. By upholding the tenets of this sacred and legendary document, any dude can learn to achieve Bro-dom.\\n\\nContaining approximately 150 “unspoken” rules, this code of conduct for bros can range from the simple (bros before hos) to the complex (the hot-to-crazy ratio, complete with bar graphs and charts). With helpful sidebros The Bro Code will help any ordinary guy become the best bro he can be. Let ultimate bro and coauthor Barney Stinson and his book, The Bro Code share their wisdom, lest you be caught making eye contact in a devil’s three-way (two dudes, duh).',\n",
              " \"Book 4 of the After series—newly revised and expanded, Anna Todd's After fanfiction racked up 1 billion reads online and captivated readers across the globe. Experience the internet's most talked-about book for yourself from the writer Cosmopolitan called “the biggest literary phenomenon of her generation.”\\n\\nTessa and Hardin have defied all the odds, but will their fairy tale ending be turned on its head? AFTER EVER HAPPY...Life will never be the same. #HESSA\\n\\nIt’s never been all rainbows and sunshine for Tessa and Hardin, but each new challenge they’ve faced has only made their passionate bond stronger and stronger. But when a revelation about the past shakes Hardin’s inpenetrable façade to the core—and then Tessa suffers a tragedy—will they stick together again, or be torn apart?\\n\\nAs the shocking truth about each of their families emerges, it’s clear the two lovers are not so different from each other. Tessa is no longer the sweet, simple, good girl she was when she met Hardin—any more than he is the cruel, moody boy she fell so hard for. Tessa understands all the troubling emotions brewing beneath Hardin’s exterior, and she knows she’s the only one who can calm him when he erupts. He needs her.\\n\\nBut the more layers of his past come to light, the darker he grows, and the harder he pushes Tessa—and everyone else in his life—away. Tessa’s not sure if she really can save him—not without sacrificing herself. She refuses to go down without a fight. But who is she fighting for—Hardin or herself?\",\n",
              " \"Includes the brilliant story 'The Mist', now adapted into a major Netflix series.\\nHold tight. We are going into a number of dark places, but I think I know the way. Just don't let go of my arm . . .\\nUnrivalled master of suspense Stephen King takes the unsuspecting reader on a fantastic journey through the dark shadows of our innermost fears.\\nDo the dead sing?\\nIn this bumper collection of chilling tales, we meet: a woman who has never crossed The Reach, the water dividing her from the mainland; a gramma who only wants to hug little George, even after she is dead; an innocent looking toy with sinister powers; and a primeval sea creature with an insatiable appetite.\",\n",
              " 'Vengeance is a Cage. Forgiveness is Freedom.\\n\\nIn their greed to possess the deadly Halahala, the devas and the asuras have employed every dirty trick against Vikramaditya and his Council of Nine. But the humans are still standing, bloodied but unbowed.\\n\\nWhen the wily Shukracharya discovers the secret to breaking the Council’s unity and strength, he forges an unlikely alliance with his arch-enemy, Indra, to set a deceitful plan in motion.\\n\\nAs cracks emerge between the councilors and their king, ghosts from the past threaten to ruin Vikramaditya and Kalidasa’s friendship, signaling the beginning of an eclipse that will cast a long shadow over all that Vikramaditya holds dear. And into this shadow steps Indra, bearing an old grudge – and a devastating new weapon.\\n\\nHow much longer before the Guardians of the Halahala finally fall apart?',\n",
              " 'Pilger tackles the injustices and double standards inherent in the politics of globalization and exposes the terrible truth behind the power and wealth of states and corporations\\n\\nJohn Pilger is one of the world’s renowned investigative journalists and documentary filmmakers. In this classic book, with an updated introduction, he reveals the secrets and illusions of modern imperialism. Beginning with Indonesia, he shows how General Suharto’s bloody seizure of power in the 1960s was part of a western design to impose a “global economy” on Asia. A million Indonesians died as the price for being the World Bank’s “model pupil.” In a shocking chapter on Iraq, he delineates the true nature of the West’s war against the people of that country. And he dissects, piece by piece, the propaganda of the ‘war on terror’ to expose its Orwellian truth. Finally, he looks behind the picture-postcard image of his homeland, Australia, to illuminate an enduring legacy of imperialism: the subjugation of the First Australians.\\n\\nNow with a new introduction, this remains one of the most shocking investigations of contemporary power.',\n",
              " \"How to get to master the art of persuasion―from the bestselling author of Talk Like Ted. 'An easy-to-read and practical journey through personal development, plus tips on structuring the storytelling that is still considered key to connecting with other people and, crucially, with customers and investors in business.' Financial Times (Business Book of the Month) Ideas don’t sell themselves. As the forces of globalization, automation, and artificial intelligence combine to disrupt every field, having a good idea isn’t good enough. Mastering the ancient art of persuasion is the key to standing out, getting ahead, and achieving greatness in the modern world. Communication is no longer a “soft” skill―it is the human edge that will make you unstoppable, irresistible, and irreplaceable―earning you that perfect rating, that fifth star. In Five Stars, you will learn: -The one skill billionaire Warren Buffett says will raise your value by 50 percent. -Why your job might fall into a category where 75 percent or more of your income relies on your ability to sell your idea. -How Airbnb’s founders follow a classic 3-part formula shared by successful Hollywood movies. -Why you should speak in third-grade language to persuade adult listeners. -The one brain hack Steve Jobs, Leonardo da Vinci, and Picasso used to unlock their best ideas. In Five Stars, Carmine Gallo, bestselling author of Talk Like TED, breaks down how to apply Aristotle’s formula of persuasion to inspire contemporary audiences. As the nature of work changes, and technology carries things across the globe in a moment, communication skills become more valuable―not less. Gallo interviews neuroscientists, economists, historians, billionaires, and business leaders of companies like Google, Nike, and Airbnb to show first-hand how they use their words to captivate your imagination and ignite your dreams. In the knowledge age―the information economy―you are only as valuable as your ideas. Five Stars is a book to help you bridge the gap between mediocrity and exceptionality, and gain your competitive edge in the age of automation.\",\n",
              " 'India is one of the fastest-growing economies in the world. Yet health is not a part of our ambitious development story. In fact, India’s disproportionately stingy healthcare budget makes some of the poorer nations look better in comparison. Statistics, however, speak louder than critics: we have one of the highest numbers of women dying in childbirth and under-five mortality rates. Every year nearly sixty million people get pushed below the poverty line due to the health expenditures that they incur. But there are a few bright spots too: India has eradicated polio and reversed the incidence of HIV/AIDS by an impressive margin.Drawing on her experience as the former union health secretary, K. Sujatha Rao gives us an unsparingly candid insider’s view of India’s health system. This richly detailed book favours increasing the health budget, greater use of technology and providing leadership and good governance. Rao argues that unless good health is prioritized as a national goal, India’s growth story will remain largely self-congratulatory.',\n",
              " \"25th anniversary edition A true Diamond of a novel, glinting with comedy and tragedy' Daily Mail it is 1941 and Captain Antonio Corelli, a young Italian Officer, is posted to the Greek island of Cephalonia as part of the occupying forces. At first he is ostracised by the locals but over time he proves himself to be civilised, humorous – and a consummate musician. When pelagia, the local doctor's daughter, finds her letters to her fiancé go unanswered, Antonio and pelagia draw close and the working of the eternal triangle seems inevitable. But can this fragile love survive as a war of bestial savagery gets closer and the lines are drawn between invader and defender? 'louis de bernières is in the direct line that runs through Dickens and Evelyn Waugh. He has only to look into his world, one senses, for it to rush into reality, colours and touch and taste' Evening standard.\",\n",
              " \"In this classic fairy tale, a miller's daughter has to spin straw into gold for the king. A funny little man comes to help her, but if she can't guess his name, this Rumpelstiltskin will take her first-born child! Read it yourself with Ladybird is one of Ladybird's best-selling series. For over thirty-five years it has helped young children who are learning to read develop and improve their reading skills. Each Read it yourself book is very carefully written to include many key, high-frequency words that are vital for learning to read, as well as a limited number of story words that are introduced and practised throughout. Simple sentences and frequently repeated words help to build the confidence of beginner readers and the four different levels of books support children all the way from very first reading practice through to independent, fluent reading. Each book has been carefully checked by educational consultants and can be read independently at home or used in a guided reading session at school. Further content includes comprehension puzzles, helpful notes for parents, carers and teachers, and book band information for use in schools. Rumpelstiltskin is a Level 2 Read it yourself title, ideal for children who have received some initial reading instruction and can read short, simple sentences with help.\",\n",
              " \"You can't tell by looking at me that my dad is Poseidon, God of the Sea.\\nIt's not easy being a half-blood these days. Even a simple game of dodgeball becomes a death match against an ugly gang of cannibal giants - and that was only the beginning.\\nNow Camp Half-Blood is under attack, and unless I can get my hands on the Golden Fleece, the whole camp will be invaded by monsters. Big ones . . .\\nThis full-colour graphic novel is adapted by Robert Vendetti, with art by Attila Futaki and colour by Tamàs Gàspàr.\",\n",
              " '“There’s the scarlet thread of murder running through the colourless skein of life and our duty is to unravel it and isolate it and expose every inch of it.”Sherlock Holmes Consulting Detective 221B Baker Street London.\\nThis is where begins a historical partnership between Dr. Watson—the archetypal gentleman from the Victorian era—and the eccentric, legendary sleuth, Sherlock Holmes. Join them as they gather clues, ranging from bloodstains and footprints to cigarette ash and wedding rings and arrive at unusual and surprising conclusions. This book is a collection of the four novels written by Sir Arthur Conan Doyle: A Study in Scarlet (1887), The Sign of the Four (1890), The Hound of the Baskervilles (1902) and The Valley of Fear (1915). Featuring the timeless detective Sherlock Holmes, these novels have been successfully engrossing readers for more than a century now.',\n",
              " \"Learn Tamil in a Month: An Easy Method of Learning Tamil through English without a Teacher a book which is specially designed for people who don't have the time for language classes. It is a book that helps you to learn the language Tamil through your knowledge of English.\\nIt gives you the basic knowledge of alphabets, vowels, consonants, phonetics and pronunciation right at the beginning. Within a month, the book helps become capable of speaking, reading and writing the Tamil words and sentences that are frequently used in communication.\\nAlso included is an appendix that contains the Tamil counterpart for common grammatical terms of English, letter writing and additional vocabulary. Learn Tamil in a Month: An Easy Method of Learning Tamil through English without a Teacher is a useful book for beginners who want to learn the Tamil language.\\nThe book is written in the spirit of national integrity as learning different regional languages fosters bonding and unity among the various states of the country. The new edition of this book was published by Read Well Publications in 2000. It is available as a paperback.\\nKey Features:\\nThe book helps people to learn the Tamil language without going to special language classes.\\nIt starts with alphabets, vowels and consonants, which makes it easy for the learners to grasp the intricacies of the language concerned.\\nIt includes all the important concepts with examples that are required for a person to speak, read and write the commonly used words and sentences of the language in a month.\",\n",
              " 'A very useful dictionary for all Secondary and Senior Secondary Students. This dictionary is also useful for Candidates Appearing for Competitive Examinations.',\n",
              " 'This book examines in practical detail the principles of communication theory as applied to the transmission of information, with equal emphasis given to analog and digital communication. It focuses on basic issues, relating theory to practice. Examples, sets of problems, footnotes, historical references and suggestions for further reading are included.\\nTable of Contents:\\nRepresentation of Signals and Systems\\nAmplitude Modulation\\nAngle Modulation\\nRandom Processes\\nNoise in CW Modulation\\nPulse-Analog Modulation\\nPulse-Digital Modulation\\nBaseband Data Transmission\\nBand-Pass Data Transmission\\nAppendixes\\nGlossary\\nIndex',\n",
              " '‘The great, urgent, passionate American writer of our century, who offers us a model of the kind of compassionate thinking that might yet save us from ourselves.’ George Saunders prisoner of war, optometrist, time-traveller – These are the life roles of Billy pilgrim, hero of this miraculously moving, bitter and funny story of innocence faced with apocalypse. slaughterhouse five </is one of the world’s great anti-war books. Centring on the infamous fire-bombing of Dresden in the second World War, Billy pilgrim’s Odyssey through time reflects the journey of our own fractured lives as we search for meaning in what we are afraid to know. ‘An extraordinary success. A book to read and reread. He is a true artist’ <new York Times Book review.',\n",
              " \"He is one of the most beloved athletes in history and one of the most gifted men ever to step onto a tennis court – but from early childhood Andre Agassi hated the game.\\nCoaxed to swing a racket while still in the crib, forced to hit hundreds of balls a day while still in grade school, Agassi resented the constant pressure even as he drove himself to become a prodigy, an inner conflict that would define him. Now, in his beautiful, haunting autobiography, Agassi tells the story of a life framed by such conflicts.\\nAgassi makes us feel his panic as an undersized seven-year-old in Las Vegas, practicing all day under the obsessive gaze of his violent father. We see him at thirteen, banished to a Florida tennis camp. Lonely, scared, a ninth-grade dropout, he rebels in ways that will soon make him a 1980s icon. By the time he turns pro at sixteen, his new look promises to change tennis forever, as does his lightning fast return.\\nAnd yet, despite his raw talent, he struggles early on. We feel his confusion as he loses to the world's best, his greater confusion as he starts to win. After stumbling in three Grand Slam finals, Agassi shocks the world, and himself, by capturing the 1992 Wimbledon. Overnight he becomes a fan favorite and a media target.\\nAgassi brings a near-photographic memory to every pivotal match, and every public relationship. Alongside vivid portraits of rivals, Agassi gives unstinting accounts of his brief time with Barbra Streisand and his doomed marriage to Brooke Shields. He reveals the depression that shatters his confidence, and the mistake that nearly costs him everything. Finally, he recounts his spectacular resurrection and his march to become the oldest man ever ranked number one.\\nIn clear, taut prose, Agassi evokes his loyal brother, his wise coach, his gentle trainer, all the people who help him regain his balance and find love at last with Stefanie Graf.\\nWith its breakneck tempo and raw candor, Open will be read and cherished for years. A treat for ardent fans, it will also captivate readers who know nothing about tennis. Like Agassi's game, it sets a new standard for grace, style, speed and power.\",\n",
              " 'Bestselling romance writer Revati Krishna had everything that a girl could wish for – A sparkling career, a thriving fan-base and a perfect love life. However, life is not always a bed of roses… Sometimes, there appear thorns. An unfortunate car accident turns her life upside down.\\nTo start over her life, Revati moves to a sleepy hill station where she gets entangled with a murder that happened 15 years ago.\\nOn the pretext of writing a book, she starts investigating into the matter and finds striking evidence which otherwise could never be found. But she doesn’t know that she has put her own life in danger.\\n\\nHow far will one woman go to give justice to a dead girl?\\nThe answer lies in that incident which happened 15 years ago. The incident which shall tell the world… The Other Side of Her story.',\n",
              " 'NEW YORK TIMES BESTSELLER • The pioneering experts behind The Whole-Brain Child and The Yes Brain tackle the ultimate parenting challenge: discipline.\\n \\n“A lot of fascinating insights . . . an eye-opener worth reading.”—Parents\\n\\nHighlighting the fascinating link between a child’s neurological development and the way a parent reacts to misbehavior, No-Drama Discipline provides an effective, compassionate road map for dealing with tantrums, tensions, and tears—without causing a scene.\\n \\nDefining the true meaning of the “d” word (to instruct, not to shout or reprimand), the authors explain how to reach your child, redirect emotions, and turn a meltdown into an opportunity for growth. By doing so, the cycle of negative behavior (and punishment) is essentially brought to a halt, as problem solving becomes a win/win situation. Inside this sanity-saving guide you’ll discover\\n \\n• strategies that help parents identify their own discipline philosophy—and master the best methods to communicate the lessons they are trying to impart\\n• facts on child brain development—and what kind of discipline is most appropriate and constructive at all ages and stages\\n• the way to calmly and lovingly connect with a child—no matter how extreme the behavior—while still setting clear and consistent limits\\n• tips for navigating your child through a tantrum to achieve insight, empathy, and repair\\n• twenty discipline mistakes even the best parents make—and how to stay focused on the principles of whole-brain parenting and discipline techniques\\n \\nComplete with candid stories and playful illustrations that bring the authors’ suggestions to life, No-Drama Discipline shows you how to work with your child’s developing mind, peacefully resolve conflicts, and inspire happiness and strengthen resilience in everyone in the family.\\n\\nPraise for No-Drama Discipline\\n \\n“With lucid, engaging prose accompanied by cartoon illustrations, Siegel and Bryson help parents teach and communicate more effectively.”—Publishers Weekly\\n\\n“Wow! This book grabbed me from the very first page and did not let go.”—Lawrence J. Cohen, Ph.D., author of The Opposite of Worry',\n",
              " \"From her controversial rise and fall from power at Google, to her dramatic reshaping of Yahoo's work culture, people are obsessed with, and polarised by, Marissa Mayer's every move. She is full of fascinating contradictions: a feminist who rejects feminism, a charmer in front of a crowd who can't hold eye contact in one-on-ones, and a geek who is Oscar de la Renta's best customer. Marissa Mayer and the Fight to Save Yahoo! tells her story.\\nBack in the 1990s, Yahoo was the internet. It was also a $120 billion company. But just as quickly as it became the world's most famous internet company, it crashed to earth during the dotcom bust. And yet, Yahoo is still here, with nearly a billion people visiting it each month. Marissa Mayer and the Fight to Save Yahoo! tells the fly-on-the-wall story of Yahoo's history for the first time, getting inside the board room as executives make genius calls and massive blunders.\\nDan Loeb, a tough-talking hedge fund manager, set his sights on Yahoo in 2011. He grew up idolising the corporate raiders of the 1980s, building a career being more vicious than any of them. Without Loeb's initiative, Marissa Mayer would never have been given her chance to save the company. This book tells the tale of how Dan Loeb spotted the real problem inside Yahoo - its awful board - and tore it apart, getting two CEOs fired in the process.\\nWhen Marissa Mayer first started at Yahoo in 2012, the car parks would empty every week by 4.00 p.m. on Thursday. Over the next two years she made plenty of mistakes, but she learned from them. Now Yahoo's culture is vibrant and users are coming back. In Marissa Mayer and the Fight to Save Yahoo! Nicholas Carlson also explores what may be the internet's first real turnaround.\",\n",
              " 'Jacques Lacan is now regarded as a major psychoanalytical theorist alongside Freud and Jung, although recognition has been delayed by fierce arguments over his ideas. Written by a leading Lacanian analyst, \"Introducing Lacan\" guides the reader through his innovations, including his work on paranoia, his addition of structural linguistics to Freudianism and his ideas on the infant \\'mirror phase\\'. It also traces Lacan\\'s influence in postmodern critical thinking on art, literature, philosophy and feminism. This is the ideal introduction for anyone intrigued by Lacan\\'s ideas but discouraged by the complexity of his writings.',\n",
              " 'This cult classic of gonzo journalism is the best chronicle of drug-soaked, addle-brained, rollicking good times ever committed to the printed page.  It is also the tale of a long weekend road trip that has gone down in the annals of American pop culture as one of the strangest journeys ever undertaken.\\n\\nNow  a major motion picture from Universal, directed by Terry Gilliam and starring Johnny Depp and Benicio del Toro.',\n",
              " 'Jointly published by Ratna Sagar and Scholastic Pvt. Ltd., Good Grammar is an adaption of the original version published by Blake Education Pty. Ltd., Australia. These books can be used with any English Reader and serve as workbooks or grammar practice books. The exercises are varied and well-graded. They consist of crosswords, word searches, word ladders, picture-based drills and other interesting activities. Also available Teacher’s Handbooks and web support www.ratnasagar.co.in.',\n",
              " \"Somewhere on Prithvi, a mortal survives a supernatural attack. In the dark realm of Atala, an evil goddess prepares to do the Unspeakable. And a Yakshi finds herself at the heart of an other-worldly storm. Ardra has only known life as a Yakshi, designed to seduce and kill men after drawing out their deepest, darkest secrets for her evil mistress Hera, queen of the forsaken realm of Atala. Then, on one strange blood moon night, her chosen victim, Dwai, survives, and her world spins out of control. Now Ardra must escape the wrath of Hera, who is plotting to throw the universe into chaos. To stop her, Ardra needs to find answers to questions she hasn't dared to ask before. What power does the blood moon hold? Is the sky city of Aakasha as much a myth as its inhabitants - the ethereal and seductive Gandharvas and Apsaras? Who is Dara, the mysterious monster-slayer, and what makes Dwai impervious to her powers? A heady concoction of fantasy and romance, Dark Things conjures up a unique world wrought of love and sacrifice, of shadows and secrets, of evil and those who battle it.\",\n",
              " 'Drawing on the lives of five great scientists, this “scholarly, insightful, and beautifully written book” (Martin Rees, author of From Here to Infinity) illuminates the path to scientific discovery.\\n\\nCharles Darwin, William Thomson (Lord Kelvin), Linus Pauling, Fred Hoyle, and Albert Einstein all made groundbreaking contributions to their fields—but each also stumbled badly. Darwin’s theory of natural selection shouldn’t have worked, according to the prevailing beliefs of his time. Lord Kelvin gravely miscalculated the age of the earth. Linus Pauling, the world’s premier chemist, constructed an erroneous model for DNA in his haste to beat the competition to publication. Astrophysicist Fred Hoyle dismissed the idea of a “Big Bang” origin to the universe (ironically, the caustic name he gave to this event endured long after his erroneous objections were disproven). And Albert Einstein speculated incorrectly about the forces of the universe—and that speculation opened the door to brilliant conceptual leaps. As Mario Livio luminously explains in this “thoughtful meditation on the course of science itself” (The New York Times Book Review), these five scientists expanded our knowledge of life on earth, the evolution of the earth, and the evolution of the universe, despite and because of their errors.\\n\\n“Thoughtful, well-researched, and beautifully written” (The Washington Post), Brilliant Blunders is a wonderfully insightful examination of the psychology of five fascinating scientists—and the mistakes as well as the achievements that made them famous.',\n",
              " '“In Gonick’s work, clever design and illustration make complicated ideas or insights strikingly clear.”\\n—New York Times Book Review\\nLarry Gonick, master cartoonist, former Harvard instructor, and creator of the New York Times bestselling, Harvey Award-winning Cartoon Guide series now does for calculus what he previously did for science and history: making a complex subject comprehensible, fascinating, and fun through witty text and light-hearted graphics. Gonick’s The Cartoon Guide to Calculus is a refreshingly humorous, remarkably thorough guide to general calculus that, like his earlier Cartoon Guide to Physics and Cartoon History of the Modern World, will prove a boon to students, educators, and eager learners everywhere.',\n",
              " 'Over twenty years ago, Sanjoy Hazarika’s first book on the Northeast, Strangers of the Mist, was published to immediate acclaim. Hailed as an exciting, path-breaking narrative on the region, it has been cited extensively in studies of Northeast India, used as a resource for scholars and journalists and adopted as course material in colleges.\\nTwo decades later, in his new book, armed with more stories, interviews and research and after extensive travels through the region, Hazarika explains how and where things stand in the Northeast today. He examines old and new struggles, contemporary trends and the sweeping changes that have taken place and asks whether the region and its people are still ‘different’ to the rest of India, to each other and whether they are destined to remain so. While it may not be possible to overcome lingering hatred, divisions and differences by brute force, economic might or efforts at cultural or political assimilation, there are other ways forward. These include the process of engagement of accepting, if not embracing, the ‘Idea of India’ and working on forging connections between disparate cultures to overcome the mutual suspicions that have existed for decades. Hazarika tells little-known stories, drawn from personal experience and knowledge, of the way in which insurgents operate, of the reality of border towns in the region, the pain of victims and the courage of fighters on either side of the ideological and physical conflict, in the jungles and in lands awash with rain and swamped by mist. He travels across borders and mountains, listening to tales of the people of the region and those who live in neighbouring countries like Bangladesh, Bhutan and Myanmar. He challenges the stereotype of the ‘North easterner’, critiques the categorization of the ‘Bangladeshi’, deals with issues of ‘race and discrimination’ and suggests best practices that could be used to deal with intractable issues and combatants. Critically, he tries to portray the way in which new generations are grappling with old and current issues with an eye to the future. Extensively researched and brilliantly narrated, strangers no more is arguably the most comprehensive book yet available about India’s Northeast.',\n",
              " 'Tintin investigates a mysterious sleeping illness that fells seven members of a South American expedition.',\n",
              " '“The story of one of the most prolific, independent, and iconoclastic inventors of this century . . . fascinating.”—Scientific American\\n\\nNikola Tesla (1856-1943), credited as the inspiration for radio, robots, and even radar, has been called the patron saint of modern electricity. Based on original material and previously unavailable documents, this acclaimed book is the definitive biography of the man considered by many to be the founding father of modern electrical technology. Among Tesla’s creations were the channeling of alternating current, fluorescent and neon lighting, wireless telegraphy, and the giant turbines that harnessed the power of Niagara Falls.\\n\\nThis essential biography is illustrated with sixteen pages of photographs, including the July 20, 1931, Time magazine cover for an issue celebrating the inventor’s career.\\n\\n“Wizard is a truly remarkable biography of a remarkable man. The expression ‘ahead of his time’ is used too loosely and too often today but in the case of Nikola Tesla ‘ahead of his time’ barely describes the genius of this man. Marc Seifer makes us understand not only the man but the times in which he lived.”—Nelson DeMille\\n\\n“A deep and comprehensive biography of a great engineer of early electrical science—likely to become the definitive biography. Highly recommended.”—American Association for the Advancement of Science\\n\\n“Vivid, revelatory . . . the fullest account yet of Tesla as an entrepreneur, experimental physicist and inventor.”—Publishers Weekly',\n",
              " \"To a nation fed on classical music, the advent of Rahul Dev Burman with his repertoire of Western beats was a godsend. RD revolutionized Hindi film music in the 1970s, and with his emphasis on rhythm and beats, this Pied Piper of Hindi film music had young India swinging to his tunes. At the same time, this genius proved his many detractors who criticized him for corrupting popular taste wrong by composing some of the most influential raga-based songs in Hindi cinema and showing an immense comfort with all kinds of music, including Indian folk. RD: The Man, The Music looks at the phenomenon called R.D. Burman and how he changed the way Indians perceived Hindi film music. Through anecdotes and trivia that went into the making of Pancham's music - the many innovations he introduced, like mixed rhythm patterns, piquant chords and sound mixing - and through interactions with the musicians who were part of RD's team, the authors create a fascinating portrait of a man who, through his music, continues to thrive, even fifteen years after his death.\",\n",
              " \"ARCHIE GIANT COMICS SPOTLIGHT collects 480 pages of iconic Archie tales in this one amazing volume! Follow America's favorite red-head as he navigates the pressures of the American teenager in the awkward, charming, and hilarious way you've come to know and love.\",\n",
              " \"Python is the most popular programming language for beginners because it's fun, powerful, and easy to learn. So why should your introductory Python book be long and tedious?\\n\\nPython Crash Course gets you up and running with Python, teaching you the basics quickly so that you can solve problems, make things, and do cool stuff. Each chapter explains a new programming concept and includes a set of exercises to help reinforce your new knowledge.\\n\\nBut most important of all, Python Crash Course includes three hands-on projects to put your new programming skills into practice, so it's not just syntax and theory. You'll learn how to create a simple video game, use data visualization techniques to make interactive graphs and charts, and build a simple Web application. Python Crash Course teaches you Python the fun way—it's quick, hands-on, and totally useful.\",\n",
              " 'The inside story on President trump, as only bob woodward can tell it. With authoritative reporting honed through eight presidencies from Nixon to Obama, author Bob Woodward reveals in unprecedented detail the harrowing life inside President Donald Trump’s White House and precisely how he makes decisions on major foreign and domestic policies. Woodward draws from hundreds of hours of interviews with first-hand sources, meeting notes, personal diaries, files and documents. The focus is on the explosive debates and the decision-making in the Oval Office, the Situation Room, Air Force One and the White House residence. Fear is the most intimate portrait of a sitting president ever published during the president’s first years in office.',\n",
              " 'Little Moments of Love is a sweet collection of comics about the simple, precious, silly, everyday moments that make up a relationship.\\n \\n\\nWhat began as stray doodles on scraps of paper became an internet sensation when Catana Chetwynd’s boyfriend shared her drawings online. Now, Catana Comics touches millions of readers with its sweet, relatable humor. Little Moments of Love collects just that – the little moments that are the best parts of being with the person you love.',\n",
              " \"Discover the late Ursula Le Guin's passionate and enthralling story of a young boy sent to a school of wizardry to learn the ways of magic in the opening quartet of the Earthsea story.\\n\\n'One of the literary greats' Margaret Atwood\\n\\n'The deepest and smartest of writers. Her words are always with us. Some of them are written on my soul' Neil Gaiman\\n\\nA Wizard of Earthsea * The Tombs of Atuan * The Farthest Shore * Tehanu\\n\\nGed is but a goatherd on the island of Gont when he comes by his strange powers over nature. Sent to the School of Wizards on Roke, he learns the true way of magic and proves himself a powerful magician.\\nAnd it is as the Archmage Sparrowhawk that he helps the High Priestess Tenar escape the labyrinth of darkness. But over the years, Ged witnesses true magic and the ancient ways submit to the forces of evil and death. Will he too succumb, or can he hold them back?\\n'Superb. One of literature's best-written fantasy worlds. I adored A Wizard of Earthsea, which I read and reread until my ratty old paperback copy required emergency surgery. Le Guin's words are magical. Drink this magic up. Drown in it. Dream it' David Mitchell\\n\\n'One of the greats. A literary icon' Stephen King\\n\\n'A colossus of literature, a trailblazer' China Mieville\\n\\nPreviously titled The Earthsea Quartet.\",\n",
              " 'An undercover mission beyond the Iron Curtain to recover a defected scientist goes disastrously wrong – a classic early Cold War thriller from the acclaimed master of action and suspense. Now reissued in a new cover style.\\nMichael Reynolds was going insane … slowly but inevitably insane. And the most terrible part of it was that he knew it. Since the last forced injection, there had been nothing he could do about the relentless onset of this madness. The more he struggled to ignore the symptoms, the more acutely he became aware of them, the deeper into his mind dug those fiendish chemical claws that were tearing his mind apart…',\n",
              " \"Yosuke's family has a strange tradition - once every sixty years they receive an egg from a mermaid. When the egg matures his family dutifully returns it to the sea, where the whole process is then repeated. In exchange for this favor, the mer-people bless his coastal town with bountiful catches of fish and calm seas.\\n\\nBut as a commercial development encroach on the sleepy seaside village and Yosuke's father is lured away from tradition towards modern properity, and turns the egg into a tourist trap, what will happen to the promise their family made to the mermaids generations ago?\\n\\nTropic of the Sea Satoshi Kon's first feature length manga, includes a dozen black and white art plates from his original release, along with a 5-page essay written by Kon in 1999 detailing his transition from the manga industry to the animation business.\",\n",
              " 'Will you accept the challenge of 200 new thematic word searches, perfect for whiling away your downtime, on the commute, on holiday or relaxing at home?\\nRelax and give your brain a workout. Ideal for whiling away those long commutes, travelling on holiday or relaxing at home. Can you find all of the words hidden in the thematic grids? Grab a pencil and challenge yourself.',\n",
              " \"About The Book\\nAssassin's Creed: Renaissance is the first part of the Assassin's Creed series and follows the life of Ezio Auditore da Firenze. The book starts when Ezio's family is betrayed and murdered by the ruling family of Italy and he must exact revenge to restore the honour of his family's name and eradicate corruption. For this, he must learn the art of the Assassins.\\nThe story revolves around the centuries old battle between the Assassin Order and the Knights Templar. These two groups have been fighting for supremacy over each other for centuries and Ezio is drawn into this battle. During his quest for revenge, Ezio starts to train as an assassin and in the process, learns more and more truths about his family.\\nHe then comes across the reason the two groups have been at each others throats for so long: an ancient technology that could alter the way the human mind works. He realises that there is another thing that they are after. A vault, somewhere in Italy, that contains the secrets of an ancient, technologically superior civilisation, that disappeared.\\nAll along the way, Ezio will have to use the wisdom of great minds like Da Vinci and Niccolo Machiavelli to survive. To his friends, he is the force that will bring about a positive change and to his enemies, he is a threat to the tyrannical establishment in Italy.\\nAssassin's Creed: Renaissance was published by Penguin UK in 2009 and is available in paperback.\\nKey Features\\nAssassin's Creed: Renaissance is the first part of the Assassin's Creed series. It is followed by Assassin's Creed: Brotherhood.\",\n",
              " 'December 1942: Calcutta is bombed by the Japanese air force. In the ensuing panic, one and a half million flee the almost defenseless city. The Japanese, having stormed through Malaya, Singapore and Burma, appear unstoppable—and on their way to India.\\nDavid Lockwood investigates the reactions and plans of the Congress, the British and the Indian National Army (INA), concluding that the episode revealed a good deal about plans for India after the war, the impossibility of the INA’s military solution, and that it was a part of the transition of the Indian State from the British to the Congress.',\n",
              " 'Emma Bovary is beautiful and bored, trapped in her marriage to a mediocre doctor and stifled by the banality of provincial life. An ardent reader of sentimental novels, she longs for passion and seeks escape in fantasies of high romance, in voracious spending and, eventually, in adultery. But even her affairs bring her disappointment and devastating consequences. Flaubert\\'s erotically charged and psychologically acute portrayal of Emma Bovary caused a moral outcry on its publication in 1857. It was deemed so lifelike that many women claimed they were the model for his heroine; but Flaubert insisted: \"Madame Bovary, c\\'est moi.\" One of the greatest novels of the 19th century, Flaubert’s torrid debut lives on in Geoffrey Wall’s brilliant translation.  This edition features an introduction by Wall, and a preface on Emma Bovary\\'s femininity and modernity by novelist Michèle Roberts.\\n\\nPart of Penguin’s beautiful hardcover Clothbound Classics series, designed by the award-winning Coralie Bickford-Smith, these delectable and collectible editions are bound in high-quality, tactile cloth with foil stamped into the design.',\n",
              " 'An innocent looking antique model of an old sailing ship reveals a secret that could lead Tintin and Snowy to an ancient treasure.',\n",
              " 'The definitive and first non-partisan biography of one of the most formidable political figures of the twentieth century (voted Woman of the Millennium in a BBC poll, 2000)\\nIndira Gandhi’s life, from her birth in 1917, through partition and up to her assassination in 1984, was dominated by the politics of her country. Always directly involved in India’s turbulent twentieth-century history, once she accepted the mantle of power, she became one of the world’s most powerful and significant women. This biography, the first to be written by an unpartisan, Western woman, will focus on Gandhi’s role as a female leader of men in one of the most chauvinistic, complex and politicised cultures in the world.\\nComprehensive, yet also personal, Frank’s biography will deal with power and how this often isolated woman handled it, alongside her family and her emotional life. It will be the definitive book on one of this century’s most powerful and important women.',\n",
              " 'Scale, chord, arpeggio and cadence studies in all major and minor keys presented in a convenient two-page format. Includes an in-depth 12 page explanation that leads to complete understanding of the fundamentals of major and minor scales, chords, arpeggios and cadences plus a clear explanation of scale degrees and a two-page guide to fingering the scales and arpeggios. In addition, several \"enrichment options\" are provided with exercises such as harmonizing scales, accelerating scales expanding scales and much more!\\n\\nThese excellent all-inclusive books teach scales, chords, arpeggios, and cadences at three different levels. The FIRST book (#11761) accommodates the learning pace of younger students such as those in Alfred\\'s Basic, Level 2. The BASIC book (#5754) is slightly more in-depth, presenting scales, chords, arpeggios, and cadence studies in all the major and minor keys. The COMPLETE book (#5743) features everything in the BASIC book, plus extra features like a 12-page explanation that leads to complete understanding of the fundamentals of major and minor scales, chords, arpeggios, and cadences; a clear explanation of scale degrees; and a two-page guide to fingering the scales and arpeggios',\n",
              " 'Discover for yourself the magic of \"Strength Training Anatomy\", one of the best-selling strength training books ever published! Get an intricate look at strength training from the inside out. \"Strength Training Anatomy\", with over 760,000 copies already sold, brings anatomy to life with more than 400 full-color illustrations. This detailed artwork showcases the muscles used during each exercise and delineates how these muscles interact with surrounding joints and skeletal structures. Like having an X-ray for each exercise, the information gives you a multilateral view of strength training not seen in any other resource. This updated bestseller also contains new information on common strength training injuries and preventive measures to help you exercise safely. Chapters are devoted to each major muscle group, with 115 total exercises for arms, shoulders, chest, back, legs, buttocks, and abdomen.',\n",
              " 'Created by Goodwill Exclusively to Help You\\nPrepare for Any Basic Skills Examination\\nReview What You Learned Or Forgot in High School\\nGet Extra Help with your High School Or College Programme\\nSkill Builders Helps You Acquire Practical, Essential Skills Fast, through a Series of Simple 20 - Step Programmes: Reading, Comprehension, Vocabulary and Spelling, Writing Skills, Practical Math, Reasoning Skills - All in 20 Minutes a Day.\\nWriting Skills: Success in 20 Minutes a Day\\nWriting Well Isn’t a Talent You’Re Born With. It’S a Skill You Acquire through Mastery of a Few Simple Basics. this Book Will Help You Achieve that Mastery through an Easy 20 - Step Programme. Each Step Takes Just 20 Minutes a Day.\\nWhat Makes this Writing Skills Skill Builder So Effective?\\nAn Introductory Diagnostic Test Helps You Pinpoint your Strengths and Weaknesses\\n20 Steps Cover All the VItal Writing Skills, from Capitalisation, Problem Verbs and Pronouns to the Essentials of Style.\\nYou Learn by Everyday Examples\\nExercises are in Test Format (Giving You Plenty of Practice for Test - Taking)\\nEach Step is Designed for Ease and Speed in Learning\\nPost - Test Shows You the Progress You’Ve Made\\nBonus Section - ‘Preparing for a Standardised Test’ - Tips for Scoring your Best',\n",
              " \"What is your dog really thinking?\\nHow do dogs see the world about them? How do they hear, learn and relate to their owners? Why do they suffer from stress and anxiety, and how can we help them cope? The Dog's Mind is the answer to all your questions on how the canine mind works. Engaging, entertaining and essential, this book is not only the key text of many dog behaviour diploma and degree courses, but also a joyful and insightful read for any dog owner. Combining almost 50 years of practical experience as a veterinarian with an extensive understanding of the relevant research, Dr Bruce Fogle has written the definitive book for anyone who wants to understand their dog.\",\n",
              " \"Join the world's most famous travelling reporter in his exciting adventures as he braves an ancient Incan curse in the two-part story The Seven Crystal Balls and Prisoners of the Sun, and investigates espionage in the Middle East in Land of Black Gold. The Seven Crystal Balls The world's most famous travelling reporter is faced with an ancient Incan curse, which is causing its victims to fall into a life-threatening coma. The tomb of Rascar Capac has been unearthed! But one by one, the finders fall into a terrifying coma. Can this be the curse of the Inca gods? Tintin must somehow fathom out the meaning behind his only clue: the shattered crystal ball lying beside each of the victims . . . Prisoners of the Sun When Professor Calculus is kidnapped, Tintin and a desperate Captain Haddock set off to Peru on a rescue mission, braving runaway train carriages, yellow fever and avalanches. Then they must find an ancient Inca tribe if they are to find their great friend. Land of Black Gold Boom! Doctored petrol is blowing up vehicles all around the country. Determined to find the culprits, Tintin heads for the Middle East, but he is in for a nasty shock when he encounters a familiar face in the desert. Join the most iconic character in comics as he embarks on an extraordinary adventure spanning historical and political events, and thrilling mysteries. Still selling over 100,000 copies every year in the UK and having been adapted for the silver screen by Steven Spielberg and Peter Jackson in 2011, The Adventures of Tintin continue to charm more than 80 years after they first found their way into publication. Since then an estimated 230 million copies have been sold, proving that comic books have the same power to entertain children and adults in the 21st century as they did in the early 20th.\",\n",
              " \"Herge's classic comic book creation Tintin is one of the most iconic characters in children's books. These highly collectible editions of the original 24 adventures will delight Tintin fans old and new. Perfect for lovers of graphic novels, mysteries and historical adventures. The world's most famous travelling reporter must attempt to catch an Emerald thief. When Captain Haddock meets a gipsy palm reader, he dismisses a forewarning about a beautiful lady's stolen jewels. But when the famous opera singer Bianca Castafiore suddenly descends on Marlinspike Hall, the gipsy's prediction seems to be all too real. Can Tintin catch the emerald thief? The Adventures of Tintin are among the best books for readers aged 8 and up. Herge (Georges Remi) was born in Brussels in 1907. Over the course of 54 years he completed over 20 titles in The Adventures of Tintin series, which is now considered to be one of the greatest, if not the greatest, comics series of all time. Have you collected all 24 graphic novel adventures? Tintin in the Land of the Soviets Tintin in the Congo Tintin in America Tintin: Cigars of the Pharaoh Tintin: The Blue Lotus Tintin: The Broken Ear Tintin: The Black Island Tintin: King Ottakar's Sceptre Tintin: The Crab with the Golden Claws Tintin: The Shooting Star Tintin: The Secret of the Unicorn Tintin: Red Rackham's Treasure Tintin: The Seven Crystal Balls Tintin: Prisoners of the Sun Tintin: Land of Black Gold Tintin: Destination Moon Tintin: Explorers of the Moon Tintin: The Calculus Affair Tintin: The Red Sea Sharks Tintin in Tibet Tintin: The Castafiore Emerald Tintin: Flight 714 to Sydney The Adventures of Tintin and the Picaros Tintin and Alph-Art\",\n",
              " 'The Boko haram terror group aspires to establish a caliphate in Africa and 20, 000 innocents have already paid with their lives. Rohan, a young Indian merchantman, unaware of this devilish intent, joins an oil tanker bound for Nigeria. Confronted with a brutal piracy planned by the terrorists, can Rohan survive the ensuing suicidal oceanic voyage and the hellish torture by the vicious mafia? Hundreds of miles away, the Boko haram leader is equally unaware of a sinister conspiracy taking shape against him. Can he emerge unscathed? Their lives stand treacherously intertwined amidst a convoluted mix of ambition and greed. Will they survive the African roulette?.',\n",
              " 'Towel Squeeze\\nStand up with a towel or Thera-Band in your hands behind your back, with your hands as close to each other as possible. Keep the arms straight and your palms facing backwards. Keeping your shoulders back, and your elbows straight, pull the towel/band apart for 2 seconds and hold, then release. Repeat this 5 times, 3 times a day.\\nShoulder Openers\\nStand with your upper arms glued to your sides, your elbows bent and your palms face-up. Hold on to something heavy in each hand (equal weight—maybe two water bottles). Keeping your upper arms by your sides, rotate the arms open and apart, while squeezing your shoulder blades together, then return to your starting point. This shoulder-blade squeeze is super-important. Do twenty repetitions, three times a day.\\nPelvic Curl\\nOverall goal:\\nComprehensive leg and spine warm up.\\nPostural significance:\\nStrengthens the gluteals and hamstrings. Strengthens the deep intrinsic abdominals.Stretches the hip flexors (applicable to all postures).\\nSet-up:\\nLie face up with the knees bent and feet placed flat on the mat. Heels should be a hip-width distance apart. Arms should be by the side with the palms facing down. Pelvis should be in neutral position.\\nInhale:\\nPrepare for the movement and hold.\\nExhale:\\nEngage the abdominals and curl the pelvis under by pushing the lower back into the mat. Slowly roll the hips up lifting the lower back off the mat first, then the middle back, then the upper back.\\nInhale:\\nPrepare for the movement and hold.\\nExhale:\\nSlowly roll back down through the spine, placing one vertebra at a time down into the mat. Be sure to end in neutral position.\\n\\nNotes:\\nKeep the knees in parallel position. Don’t let the head or shoulders lift off the mat.\\nRepetitions:\\n10.\\nModification:\\nFor beginners or people with lower back pain, do not lift too high and move through a comfortable range of motion.\\nAdvanced:\\nBring one leg up off the floor into a tabletop position with the knee over the hip and the lower leg parallel to the mat. Continue the exercise with one leg down and one leg up for 10 repetitions. Then switch legs with one foot on the mat, and the other leg in a tabletop position.\\nSupine Spine Twist\\nOverall goal:\\nPreparing the spine for rotation.\\nPostural significance:\\nStrengthens deep intrinsic abdominals and obliques. Strengthens hip flexors. (Applicable for all postures.)\\nSet-up:\\nLie face up with the knees in a tabletop position, with the knees directly over the hips and the lower legs parallel to the mat. Place the arms out to the sides in a T position with the palms face down.\\nInhale:\\nLower both legs to one side, keeping the knees glued together. Keep the opposite shoulder down on the mat. If your shoulder raises off the mat, then your legs have gone too far.\\nExhale:\\nEngage the abdominals and bring the knees back into the tabletop position.\\nInhale:\\nRepeat on the other side.\\nExhale:\\nEngage the abdominals and bring the knees back into the tabletop position.\\nNotes:\\nDon’t let the top leg slide down from the bottom leg.\\nRepetitions:\\n10 on each side.\\nModification:\\nFor beginners, keep the feet flat on the floor with the knees together and perform the exercise the same way.\\nAdvanced:\\nAfter lowering the legs to one side, extend the knees and straighten the legs. Hold this for a breath and then bring the legs back to the centre, and return the knees back to the tabletop position.\\nHunchback-Posture Exercises\\nSpine Stretch Forward\\nOverall goal:\\nStretching the spine, strengthening the postural muscles and stretching the hamstrings.\\nPostural significance:\\nStrengthens back extensors, strengthens deep intrinsic abdominals.\\nSet-up:\\nSit with the legs extended slightly wider than hipwidth distance. Flex the feet, bringing the toes towards the knees. Sit up tall on the sitz bones. Extend the arms out in front with the palms facing each other.\\nInhale:\\nPrepare for the movement and hold.\\nExhale:\\nLower your chin to your chest and start to roll your spine down one vertebra at a time. Stretch the arms over the legs in front of you as if your arms are sliding on a table. Pretend that you are rounding over a small beach ball in front the abdominals.\\nInhale:\\nPrepare for the movement and hold.\\nExhale:\\nEngage the abdominals and begin to roll the spine back up, one vertebra at a time, until you reach your starting position.\\nNotes:\\nMake sure the shoulders stay down and away from the ears.\\nRepetitions:\\n10.\\nModification:\\nFor tight hamstrings, bend the knees slightly or sit up on a yoga block to help straighten the legs. You can also perform this exercise up against a wall to feel the articulation of the spine against a flat surface for better results.\\nAdvanced:\\nAfter rounding forward, lengthen the spine into a flat-back, diagonal position and bring the arms up parallel to the ears. Round forward again and roll up into the starting position.\\n\\n\\nSide Bend\\nOverall goal:\\nStrengthening the shoulders and obliques.\\nPostural significance:\\nStrengthens obliques. Stretches latissimus dorsi.\\nSet-up:\\nSit facing one side with all your weight on one hip. Bend your knees slightly and place the top leg just slightly in front of the bottom leg. Place your bottom hand about six to twelve inches away from the hip, with the fingers facing away from the pelvis. Rest your top arm on your top leg.\\nInhale:\\nLift your pelvis up off the floor and raise your top arm up towards the ceiling. Create a diagonal line with your body from the top of your head to your feet. You should have a diagonal T shape of the body.\\nExhale:\\nLift your pelvis up higher and bring your top arm overhead, creating a rainbow shape. Look down at your bottom hand.\\nInhale:\\nRelease the rainbow shape and return to the diagonal line.\\nExhale:\\nBend your knees and lower back down into your starting position.\\nNotes:\\nMake sure the shoulders stay down and away from the ears, especially the bottom shoulder.\\nRepetitions:\\n10.\\nModification:\\nKeep your bottom leg bent with the knee on the floor beneath the hip. Keep the top leg straight out with the foot on the mat. This will reduce the pressure on the arms, shoulders and abs.\\nAdvanced:\\nWhen lowering, keep the legs straight and lower as much as possible without touching the mat, then repeat the exercise again without resting.',\n",
              " 'Finalist for the 2011 Pulitzer Prize in General Nonfiction: “Nicholas Carr has written a Silent Spring for the literary mind.”—Michael Agger, Slate\\n\\n“Is Google making us stupid?” When Nicholas Carr posed that question, in a celebrated Atlantic Monthly cover story, he tapped into a well of anxiety about how the Internet is changing us. He also crystallized one of the most important debates of our time: As we enjoy the Net’s bounties, are we sacrificing our ability to read and think deeply?\\n\\nNow, Carr expands his argument into the most compelling exploration of the Internet’s intellectual and cultural consequences yet published. As he describes how human thought has been shaped through the centuries by “tools of the mind”—from the alphabet to maps, to the printing press, the clock, and the computer—Carr interweaves a fascinating account of recent discoveries in neuroscience by such pioneers as Michael Merzenich and Eric Kandel. Our brains, the historical and scientific evidence reveals, change in response to our experiences. The technologies we use to find, store, and share information can literally reroute our neural pathways.\\n\\nBuilding on the insights of thinkers from Plato to McLuhan, Carr makes a convincing case that every information technology carries an intellectual ethic—a set of assumptions about the nature of knowledge and intelligence. He explains how the printed book served to focus our attention, promoting deep and creative thought. In stark contrast, the Internet encourages the rapid, distracted sampling of small bits of information from many sources. Its ethic is that of the industrialist, an ethic of speed and efficiency, of optimized production and consumption—and now the Net is remaking us in its own image. We are becoming ever more adept at scanning and skimming, but what we are losing is our capacity for concentration, contemplation, and reflection.\\n\\nPart intellectual history, part popular science, and part cultural criticism, The Shallows sparkles with memorable vignettes—Friedrich Nietzsche wrestling with a typewriter, Sigmund Freud dissecting the brains of sea creatures, Nathaniel Hawthorne contemplating the thunderous approach of a steam locomotive—even as it plumbs profound questions about the state of our modern psyche. This is a book that will forever alter the way we think about media and our minds.',\n",
              " \"From the bestselling author of Persepolis comes this humorous and enlightening look at the sex lives of Iranian women. Embroideries gathers together Marjane's tough-talking grandmother, stoic mother, glamorous and eccentric aunt and their friends and neighbours for an afternoon of tea-drinking and talk. Naturally, the subject turns to loves, sex and vagaries of men...\",\n",
              " \"Indra Nooyi is an inspiring figure. She holds out hope for all young women, especially those from developing countries. After all, her journey started in India back in the 1950s and 60s. She hailed from a middle-class family in Chennai and dared to dream big at a time when women's career choices were still mainly limited to secretary and teacher posts.\\nIndra Nooyi: A Biography chronicles the life of this corporate achiever. Indra Nooyi earned her management degree from IIM, Calcutta and was later accepted at the Yale School of Management. After completing her Master's Degree in Public and Private Management in 1980, she joined the Boston Consulting Group. She joined PepsiCo in 1994 and since then, her rise has been meteoric. She became the company's CFO and led the strategic acquisition of Tropicana and the merger with the Quaker Oats Company. These initiatives added a line of health food products to a company that has traditionally been associated with fun food.\\nThe book traces her life from the early years in Chennai, to her struggles to establish herself in the corporate world. It follows her journey from the times she moved to the US, got married and continued her steady rise till her current position as the CEO of the second largest food-and-beverage company in the world. Indra Nooyi: A Biography was published by Rajpal as a paperback in 2013.\\nKey Features:\\nThe author has used interviews, profiles and write-ups available in public forums to chronicle the life of this interesting personality.\\nIndra Nooyi: A Biography pieces together information from different sources to highlight the vital roles played by Indra Nooyi's mother and later her husband and children in her life.\",\n",
              " 'From the Number One international bestselling author of Jurassic Park comes this classic Crichton page-turner, weaving together heart-pounding thrills with cutting-edge technology.\\nIn the Nevada desert, an experiment has gone horribly wrong. A cloud of nanoparticles – micro-robots – has escaped from the laboratory. This cloud is self-sustaining and self-reproducing. It is intelligent and learns from experience. For all practical purposes, it is alive. It has been programmed as a predator. It is evolving swiftly, becoming more deadly with each passing hour. Every attempt to destroy it has failed. And we are the prey.\\nAs fresh as today’s headlines, Michael Crichton’s most compelling novel yet tells the story of a mechanical plague, and the desperate efforts of a handful of scientists to stop it. Drawing on up-to-the-minute science fact, PREY takes us into the emerging realms of nanotechnology and artificial distributed intelligence – in a story of breathtaking suspense.',\n",
              " 'An Outline History of English Literature takes a look at English literature, from the pre-Chaucerian period (500–1340 A.D.) down to the present age. It not only keeps a sequential account of various English Literature works but also of great writers such as Geoffrey Chaucer, William Shakespeare, John Milton, John Dryden, William Wordsworth, Alfred Tennyson and T.S. Eliot who were the backbone of English Literature.\\nThe biographical and historical interpretation of each work reflects the writer’s individuality along with the essence of the age and the history of the nation. It also traces the development of the English language and the evolution of different genres of English Literature.',\n",
              " \"We use our teeth every day-to munch on an apple or carrot, to hold a pen-knife or a hairpin; we put them on display when we smile or laugh. But we rarely think about the strain we subject them to, rushing to the dentist only when our teeth cry out in pain. Or, sometimes, when they spoil the symmetry of our face. Teeth may go missing while shooting for a Bollywood fighting scene. They may shrink to half their size because of too much cola intake. A person may commit suicide because of the buzzing in his ears caused by a strained muscle in the jaw. Dr Sandesh Mayekar has seen all this and worse. He has performed root canal on a two-year-old, done risky surgeries and implants on senior citizens, lightened a lady's dark gums for a beauty contest and sealed that wide gap between a cricketer's front teeth for a photo shoot. More Than a Mouthful tells the stories behind those healthy teeth that you see on TV or on the big screen. Mayekar has stayed up nights, sometimes travelled miles, to address an emergency. He has assuaged patients' fears with as much clinical care as personal concern. It is also the tale of one man's determination to master his craft and lay the foundation of Aesthetic Dentistry in India-a journey that has taken him from humble beginnings in a chawl in Mumbai to the swanky Bandstand at Bandra. Written with passion and laced with wit, More Than a Mouthful is what the doctor prescribes to rid you of that fear of the dentist.\",\n",
              " 'Ahi is an aspiring publisher and wishes to make it big someday. When her favourite author’s autobiography lands on her table – which has confessions of his heinous crimes, illegal businesses and few eminent others as his partners in crime – she doesn’t know if it’s real or someone’s trap. It could get her a big breakthrough, but little does she know that it would turn her world upside down completely.\\nHer morbid curiosity pulls her into the depth of a conspiracy. She finds herself in the centre of various mishaps and murders, as if someone wants to lead the way. Driven by her childhood friend Samim’s encouragement and watched over by the ever so charming ACP Rathore, she has to jeopardize her life to find the brutal truth of her past.\\nTouching, thrilling and deeply mysterious, Sin is the New Love is the journey of a girl who stumbles upon the truth about her origin while chasing her dream.',\n",
              " \"h2 eight self-drive cars set on a Collision course. Who lives, who dies? You decide. and Lt;/h2 'provocative, terrifying and compulsive. Another savagely clever near future thriller' Cara hunter, bestselling author of close to home /The New gripping page-turning thriller for fans of black mirror from the bestselling author of her last move and the one - soon to be a major Netflix series. when someone hacks into the systems of eight self-drive cars, their passengers are set on a fatal Collision course. The passengers are: a TV star, a pregnant young woman, a disabled war hero, an abused wife fleeing her husband, an illegal immigrant, a husband and wife - and parents of two - who are travelling in separate vehicles and a suicidal man. Now the public have to judge who should survive but are the passengers all that they first seem? You what readers are saying 'I was gripped right from the start - The plot had many twists and turns' - Melanie an absorbing, engaging read that grabs you and doesn't let go, keeping you awake long past your bedtime' - Gisele one of the most thrilling books I've read this year' - Luke 'marrs is so talented at creating these future worlds that are completely plausible and therefore utterly terrifying' - Kate 'has many twists and turns which keep you guessing to the very last page' - Greta 'my heart is still pounding just thinking about this brilliantly twisted novel. An edge of your seat read' - Sara.\",\n",
              " \"Key Words with Peter and Jane uses the most frequently met words in the English language as a starting point for learning to read successfully and confidently. The Key Words reading scheme is scientifically researched and world renowned. Book 1c continues Peter and Jane's adventures and introduces 20 new words such as 'ball', 'shop' and 'toy'. Once this book has been completed, the child moves on to book 2a. The Key Words with Peter and Jane books work because each of the key words is introduced gradually and repeated frequently. This builds confidence in children when they recognise these key words on sight (also known as the 'look and say' method of learning). Examples of key words are: the, one, two, he. There are 12 levels, each with 3 books: a, b, and c. Series a: Gradually introduces new words Series b: Provides further practise of words featured in the 'a' series Series c: Links reading with writing and phonics. All the words that have been introduced in each 'a' and 'b' book are also reinforced in the 'c' books\",\n",
              " \"The dense impenetrable jungles of north-east Sri Lanka. Hot, dank and given to unleashing unsuspecting savage violence. Inhabited by wildlife and home to a new species—the Tamil Tigers. To hunt the latter, in pursuit roam a band of hard fighting men from the Indian Special Forces. The Beckoning Isle is a powerful tale of two men of war—the young and devoted Captain Hariharan of the Indian Special Forces and the thinking battle-hardened LTTE commander Silvam. Their destinies are inextricably linked, as their paths often cross from India to Sri Lanka, hurling them each time towards a bloody confrontation. A web of enigmatic politics, raw human emotions and unflinching faith in one's cause make this book unputdownable.\",\n",
              " 'Effortless English: Learn To Speak English Like A Native A.J. HOGE, THE WORLD\\'S #1 ENGLISH TEACHER, teaches you his most powerful methods for learning to speak English fluently and confidently. Famous for training corporate and government leaders, A.J. Hoge gives you a step by step program teaching you the system that will help you master English and achieve ultimate success with English. You have studied English for years and yet you still do not speak well. When you speak English, you make grammar mistakes. Your pronunciation is not clear. Worst of all, you feel nervous and shy when you try to speak English. You read English well, but after all these years you still cannot speak well. The good news is, it\\'s not your fault. You have simply used old ineffective methods. Effortless English will teach you a completely new way to learn English faster. Effortless English will..... *Teach you how to overcome nervousness, shyness, and fear when speaking English. *Master spoken English grammar quickly and naturally *Teach you how to improve your English pronunciation and develop an American, British, or Australian accent. *Show you how to achieve a high TOEFL, IELTS, or TOEIC score. *Help you learn vocabulary 4-5 times faster. *Tell you how to feel stronger, calmer, and more powerful when speaking English. *Teach you how to understand native speakers and communicate clearly with them during real conversations. *Help you get better jobs by learning business English. *Teach you how to learn grammar without memorizing grammar rules. \"You have studied English many years, yet you still do not speak well. You read English but you feel nervous and frustrated when speaking. It is time for a change. I will teach you to speak English quickly, easily, and automatically using my Effortless English System (TM). I will teach you to speak with correct grammar and excellent pronunciation. You can achieve success now simply by changing the way you learn English.\" --A.J. Hoge',\n",
              " 'Published in celebration of the twentieth anniversary of George R. R. Martin’s landmark series, this lavishly illustrated special edition of A Game of Thrones―with gorgeous full-page illustrations in every chapter―revitalizes the fantasy masterpiece that became a cultural phenomenon.\\nIn a land where summers can last decades and winters a lifetime, trouble is brewing. The cold is returning, and in the frozen wastes to the North of Winterfell, sinister and supernatural forces are massing beyond the kingdom’s protective Wall.\\nAt the centre of the conflict lie the Starks of Winterfell, a family as harsh and unyielding as the land they were born to. Sweeping from a region of brutal cold to a distant summertime kingdom of epicurean plenty, here is a tale of lords and ladies, soldiers and sorcerers, assassins and bastards, who come together in a time of grim omens.\\nAmid plots and counterplots, tragedy and betrayal, victory and terror, the fate of the Starks, their allies, and their enemies hangs perilously in the balance, as each endeavours to win that deadliest of conflicts: the game of thrones.',\n",
              " '’A contemporary masterpiece’ guardian the first volume of the extraordinary southern reach trilogy now a major motion picture written and directed by Alex Garland (Ex Machina) and starring Natalie Portman and Oscar Isaac for thirty years, area X has remained mysterious and remote behind its intangible border an environmental disaster zone, though to all appearances an abundant wilderness. The southern reach, a secretive government agency, has sent eleven expeditions to investigate area X. One has ended in mass suicide, another in a hail of gunfire, the eleventh in a fatal cancer epidemic. Now four women embark on the twelfth expedition into the unknown.',\n",
              " 'This is the next step to a perfect physique from the million bestseller. Frederic Delavier\\'s best-selling \"Strength Training Anatomy\" has sold more than 1 million copies worldwide and now his follow-up book, \"The Strength Training Anatomy Workout, Volume II\", provides serious strength trainers and bodybuilders with the keys to creating lean muscle mass. Following on from the more basic \"Volume I\", Delavier and co-author Michael Gundill focus on the more elaborate techniques that experienced strength training enthusiasts can use to accelerate their progress. In addition to 60 exercises, 19 stretches and 9 programmes, it\\'s packed with over 1,200 full-colour photographs and 160 of Delavier\\'s trademark illustrations. The book describes in detail some of the advanced methods for jump-starting a workout programme, featuring segmented workouts that target specific muscle groups like the chest, biceps and forearms.',\n",
              " 'Join Luffy as he tries to become the king of the pirates and find the legendary treasure, One Piece!\\n\\nReads R to L (Japanese Style).\\n\\nWith Whitebeard injured, the Navy launches a counterstrike to finish the pirate rebellion once and for all. And as the three Navy Admirals stand in his way, Luffy will need one last trick if he hopes to save his brother Ace from being executed.',\n",
              " 'Can you commit the perfect crime?\\n\\nPilgrim is the codename for a man who doesn’t exist. The adopted son of a wealthy American family, he once headed up a secret espionage unit for US intelligence. Before he disappeared into anonymous retirement, he wrote the definitive book on forensic criminal investigation.\\n\\nBut that book will come back to haunt him. It will help NYPD detective Ben Bradley track him down. And it will take him to a rundown New York hotel room where the body of a woman is found facedown in a bath of acid, her features erased, her teeth missing, her fingerprints gone. It is a textbook murder – and Pilgrim wrote the book.\\n\\nWhat begins as an unusual and challenging investigation will become a terrifying race-against-time to save America from oblivion. Pilgrim will have to make a journey from a public beheading in Mecca to a deserted ruins on the Turkish coast via a Nazi death camp in Alsace and the barren wilderness of the Hindu Kush in search of the faceless man who would commit an appalling act of mass murder in the name of his God.',\n",
              " \"'He found many marvelous things...But now and then a man of real spirituality set his feet on the way that finally led him to what he had looked and hoped for.' New York Times Book Review\\n\\nThe late Paul Brunton was one of the twentieth century's greatest explorers of and writers on the spiritual traditions of the East. A Search in Secret India is the story of Paul Brunton's journey around India, living among yogis, mystics and gurus, some of whom he found convincing, others not. He finally finds the peace and tranquility which come with self-knowledge when he meets and studies with the great sage Sri Ramana Maharishi.\",\n",
              " 'Himalayan Blunder: The Angry Truth About Indias Most Crushing Military Disaster is an account of the 1962 Sino-Indian war through the narrative of Brigadier J. P. Dalvi, who fought in the war. Himalayan Blunder: The Angry Truth About Indias Most Crushing Military Disaster is Brigadier J. P. Dalvis retelling of the Sino-Indian war that took place in 1962 - a war that India lost. Dalvi fought the war as the Commander of the 7th Infantry Brigade in NEFA (North-East Frontier Agency). His account of the war is graphic and telling. He was captured by the Chinese forces and held for seven months. As a participant of the war, he was privy to all that went on at the battlefield as well as behind the scenes. Based on his firsthand experiences, he recounts the events that occurred between September 8, 1962 and October 20, 1962. As early as 1951, China silently and steadily began to work its way onto Indian soil. Even in the face of indisputable evidence, India insisted on maintaining cordial relations with the Chinese. China seemed only too happy to play along. Dalvi narrates the manner in which Indias own political leadership traitorously worked against its cause. In no uncertain terms, he holds three men responsible for Indias defeat - Jawaharlal Nehru, Krishna Menon, and General Brij Mohan Kaul. Issuing orders from Delhi, they seemed to be clueless about the situation on the battlefield. Undoubtedly, when they were rushed into battle, the Indian soldiers - underfed, ill-equipped, and unprepared as they were - never stood a chance against the powerful Chinese army. Regardless of that, the soldiers fought bravely and laid down their lives for their homeland. Dalvi claims that the apathy and the sheer ineptitude of those at the helm of Indias political affairs sacrificed hundreds of valuable lives. Brigadier Dalvis detailed narrative of the massacre of the Indian soldiers, a horror that he witnessed firsthand, is heart-rending. The book was published in 1969. Among all the books based on the subject of the 1962 Sino-Indian war, this book is considered to be one the most striking and authentic versions. Due to its sensitive subject matter and its portrayal of Indias leaders in a harshly negative light, the book was banned by the Indian Government upon its release. This particular edition is a 2010 reprint by Natraj Publishers. The book has been translated into Kannada by Ravi Belagere.',\n",
              " \"Don't miss the powerful and page-turning new novel from Clare Mackintosh - AFTER THE END is out now\\n____________________\\nDiscover the twisty, gripping I See You - a Richard & Judy Book Club bestseller and a Sunday Times number one bestseller.\\nYou do the same thing every day.\\nYou know exactly where you're going.\\nYou're not alone . . .\\nWhen Zoe Walker sees her photo in the classifieds section of a London newspaper, she is determined to find out why it's there. There's no explanation: just a grainy image, a website address and a phone number. She takes it home to her family, who are convinced it's just someone who looks like Zoe. But the next day the advert shows a photo of a different woman, and another the day after that.\\nIs it a mistake? A coincidence? Or is someone keeping track of every move they make . . .\\n\\nPraise for I See You:\\n\\n'A breathless thriller . . . It's a must-finish-at-all-costs job' Daily Mail\\n'A chilling and original story . . . kept me reading until dawn' Rachel Abbott\\n'Accomplished, addictive and thought-provoking - you'll never feel the same about taking the tube again' B A Paris\\n'A deliciously creepy tale of urban paranoia' Ruth Ware\\n'Wonderfully sinister. Had me looking over my shoulder every time I travelled on the tube' Fiona Barton\\n'Another edge-of-your-seat thriller . . . a terrifyingly plausible plot and gasp-inducing ending' Good Housekeeping\\n'I had chills the entire way through' Jenny Blackhurst\",\n",
              " \"An assassination attempt on the President of the Republic; rumours circulating of an impending coup d'etat and a dying man whispering a codename - The Leopard. Prefect of the Paris police, Marc Grelle guesses there's a connection that could tear France apart.\",\n",
              " \"  Lara Jean and her love letters are back in this utterly irresistible third book in the hit series - the first being the NETFLIX feature film, To All TheBoys I've Loved Before.\\nLara Jean is having the best senior year a girl could ever hope for.\\nShe is head over heels in love with her boyfriend, Peter; her dad's finally getting remarried to their next door neighbor, Ms. Rothschild; and Margot's coming home for the summer just in time for the wedding.\\nBut change is looming on the horizon.\\nAnd while Lara Jean is having fun and keeping busy helping plan her father's wedding, she can't ignore the big life decisions she has to make. Most pressingly, where she wants to go to collegeand what that means for her relationship with Peter.\\nShe watched her sister Margot go through these growing pains. Now Lara Jean's the one who'll be graduating high school and leaving for college and leaving her family—and possibly the boyshe loves—behind.\\nWhen your heart and your head are saying two differentthings, which one should you listen to?\\nThe third book in the bestselling series by Jenny Han, which has been made into a NETFLIX feature film\\nTo All the Boys I've Loved Before is the first book\\nP.S. I Still Love You is the second book in the trilogy.\",\n",
              " \"Cricket is not just a game, it's a religion. And every four years, new gods are created on the field. Each World Cup kicks off pitched battles and fan frenzy, with every match, every player and every run being analysed with fervour and recorded with vigour.\\nThis essential guide is a companion volume for every cricket-crazy follower who wants the facts, statistics and details about the World Cup right on hand. The Cricket Fanatic's Essential Guide is jam-packed with important and fascinating, often overlooked and sometimes long-forgotten, information on the 40 thrilling years of the World Cup, its matches, its players and its results.\\nPut together by reputed sports journalist Vimal Kumar, this reader-friendly compendium is a must-have companion for every passionate enthusiast of the game and a perfect primer for the 2015 World Cup.\\n]]]\",\n",
              " \"Beginning with the day Hobbes sprang into Calvin's tuna fish trap, the first two Calvin and Hobbes collections, Calvin and Hobbes and Something Under The Bed Is Drooling, are brought together in this treasury. Including black-and-white dailies and color Sundays, The Essential Calvin and Hobbes also features an original full-color 16-page story.\\n\\nPerhaps the most brilliant comic strip ever created, Calvin and Hobbes continues to entertain with dazzling cartooning and tremendous humor.\\nBill Watterson's Calvin and Hobbes has been a worldwide favorite since its introduction in 1985. The strip follows the richly imaginative adventures of Calvin and his trusty tiger, Hobbes. Whether a poignant look at serious family issues or a round of time-travel (with the aid of a well-labeled cardboard box), Calvin and Hobbes will astound and delight you.\\n\\nBeginning with the day Hobbes sprang into Calvin's tuna fish trap, the first two Calvin and Hobbes collections, Calvin and Hobbes and Something Under The Bed Is Drooling, are brought together in this treasury. Including black-and-white dailies and color Sundays,  The Essential Calvin and Hobbes also features an original full-color 16-page story.\",\n",
              " \"Sarah's Scribbles,  Goodreads Choice Award for 2016:  Best Graphic Novels & Comics\\n\\nSarah Andersen's hugely popular, world-famous Sarah's Scribbles comics are for those of us who boast bookstore-ready bodies and Netflix-ready hair, who are always down for all-night reading-in-bed parties and extremely exclusive after-hour one-person music festivals. \\n\\nIn addition to the most recent Sarah's Scribbles fan favorites and dozens of all-new comics, this volume contains illustrated personal essays on Sarah's real-life experiences with anxiety, career, relationships and other adulthood challenges that will remind readers of Allie Brosh's Hyperbole and a Half and Jenny Lawson's Let's Pretend This Never Happened. The same uniquely frank, real, yet humorous and uplifting tone that makes Sarah's Scribbles so relatable blooms beautifully in this new longer form.\",\n",
              " 'Veteran photographer and instructor Bryan Peterson is best known for his arresting imagery using bold, graphic color and composition. Here he explores his signature use of color in photography for the first time, showing readers his process for creating striking images that pop off the page. He addresses how to shoot in any type of light and looks at color families and how they can work together to make compelling images in commercial and art photography. He also helps readers understand exposure, flash and other stumbling blocks that beginning and experienced photographers encounter when capturing images, showing how to get the most out of any composition. with its down-to-earth voice and casual teaching style, Understanding Color in Photography is a workshop in a book, helping any photographer take their images to the next level.',\n",
              " '‘Living for many years in a cottage at 7,000 feet in the Garhwal Himalayas, I was fortunate in having a big window that opened out on the forest, so that the trees were almost within my reach. Had I jumped, I should have landed safely in the arms of an oak or chestnut.’\\nIn this delightful read, Ruskin Bond has written about his special friends—the majestic trees on the mountain slopes that he visits regularly and the birds, animals and insects that visit him in his cottage. Bond is well-known for his inclination towards all things natural and his observation of them. From the oak, chestnut, deodar and chir trees to hornbills, squirrels, langurs, mynahs and blue jays—there’s a description of them all in this collection.\\nGentle, elegiac and replete with beautiful descriptions of plants, animals and birds, this collection is for every nature enthusiast.',\n",
              " \"Madhurima Pandey is twenty-five, single and gradually coming to terms with\\nthe annoying 'you're next' nudges from family and friends. But soon they realize\\nthat chances of finding a groom for her are slim-mainly because she's not. At 93\\nkilos, she knows she isn't the ideal weight for marriage, even if her family\\nbelieves she's the ideal age.\\nDespite her reservations, the hunt begins and so does a spree of rejections until\\nHarsh comes along. Madhu cannot believe that a boy with no obvious flaws\\nhas agreed to marry her. Low self-esteem makes her suspect he's either\\nimpotent or a homosexual, but she doesn't turn down the proposal immediately.\\nA negligible period of courtship and a hurried engagement follow. But does\\nMadhurima really find her happily-ever-after? Or are there more surprises in\\nstore?\\nJovial, witty and unapologetically honest, Madhurima Pandey's story of struggle\\nand survival in the run-up to her D-Day gives you a refreshingly new take on the\\nbig fat Indian wedding.\",\n",
              " \"On the eve of a landmark general election, Ruchir Sharma offers an unrivalled portrait of how India and its democracy work, drawn from his two decades on the road chasing election campaigns across every major state, travelling the equivalent of a lap around the earth. Democracy on the Road takes readers on a rollicking ride with Ruchir and his merry band of fellow writers as they talk to farmers, shopkeepers and CEOs from Rajasthan to Tamil Nadu, and interview leaders from Narendra Modi to Rahul Gandhi.\\nNo book has traced the arc of modern India by taking readers so close to the action. Offering an intimate view inside the lives and minds of India's political giants and its people, Sharma explains how the complex forces of family, caste and community, economics and development, money and corruption, Bollywood and Godmen, have conspired to elect and topple Indian leaders since Indira Gandhi. The ultimately encouraging message of Ruchir's travels is that, while democracy is retreating in many parts of the world, it is thriving in India.\",\n",
              " \"The world's best landscape photography and photojournalism stunningly depicts the passage of a single day, from dawn's first light to the closing moments of sunset. Experience shimmering mornings and opaque nights through the eyes of National Geographic's finest photographers in this gloriously uplifting mini volume. Daybreak whispers mauve over a long ocean horizon. The morning sun twinkles in a drop of dew. The broad heat of midday radiates over a beach strewn with sweat-baked sunbathers. A slender crescent moon caresses a gnarled tree standing alone on the heath.  Full of one-of-a-kind photographs, this breathtaking collection gives readers a front-row seat to the world's wonders, from its most imposing cityscapes to its most pristine landscapes. \",\n",
              " 'An abridged edition of the world’s first novel, in a translation that is “likely to be the definitive edition . . . for many years to come” (The Wall Street Journal)\\n\\nThe inspiration behind The Metropolitan Museum of Art\\'s \"The Tale of Genji: A Japanese Classic Illuminated\" -- Now through June 16 at The Met Fifth Avenue\\n\\nA Penguin Classic\\n\\nWritten in the eleventh century, this exquisite portrait of courtly life in medieval Japan is widely celebrated as the world’s first novel—and is certainly one of its finest. Genji, the Shining Prince, is the son of an emperor. He is a passionate character whose tempestuous nature, family circumstances, love affairs, alliances, and shifting political fortunes form the core of this magnificent epic.\\n\\nRoyall Tyler’s superior translation is detailed, poetic, and superbly true to the Japanese original while allowing the modern reader to appreciate it as a contemporary treasure. In this deftly abridged edition, Tyler focuses on the early chapters, which vividly evoke Genji as a young man and leave him at his first moment of triumph. This edition also includes detailed notes, glossaries, character lists, and chronologies.',\n",
              " \"A charmingly illustrated and inspiring book, Women in Sports highlights the achievements and stories of fifty notable women athletes--from well-known figures like tennis player Billie Jean King and gymnast Simone Biles, to lesser-known athletes like skateboarding pioneer Patti McGee and Toni Stone, the first woman to play baseball in a men's professional league. Covering more than forty sports, this fascinating collection also contains infographics about notable women's teams throughout history, pay and media statistics for female athletes and muscle anatomy. Women in Sports celebrates the success of the tough, bold and fearless women who paved the way for the next generation of athletes.\",\n",
              " \"A Richard & Judy Book Club Pick\\n\\nNew York Times Bestseller\\n\\nSara has never left Sweden but at the age of 28 she decides it’s time. She cashes in her savings, packs a suitcase full of books and sets off for Broken Wheel, Iowa, a town where she knows nobody.\\n\\nSara quickly realises that Broken Wheel is in desperate need of some adventure, a dose of self-help and perhaps a little romance, too. In short, this is a town in need of a bookshop.\\n\\nWith a little help from the locals, Sara sets up Broken Wheel’s first bookstore. The shop might be a little quirky but then again, so is Sara. And as Broken Wheel’s story begins to take shape, there are some surprises in store for Sara too…\\n\\n'The perfect summer read' Stylist\",\n",
              " 'In Spark of Life, a powerful classic from the renowned author of All Quiet on the Western Front, one man’s dream of freedom inspires a valiant resistance against the Nazi war machine.\\n \\nFor ten years, 509 has been a political prisoner in a German concentration camp, persevering in the most hellish conditions. Deathly weak, he still has his wits about him and he senses that the end of the war is near. If he and the other living corpses in his barracks can hold on for liberation—or force their own—then their suffering will not have been in vain.\\n \\nNow the SS who run the camp are ratcheting up the terror. But their expectations are jaded and their defenses are down. It is possible that the courageous yet terribly weak prisoners have just enough left in them to resist. And if they die fighting, they will die on their own terms, cheating the Nazis out of their devil’s contract.\\n \\n“The world has a great writer in Erich Maria Remarque. He is a craftsman of unquestionably first rank, a man who can bend language to his will. Whether he writes of men or of inanimate nature, his touch is sensitive, firm, and sure.”—The New York Times Book Review',\n",
              " \"A novel of memory and loss, set between London of the 1930s and Shanghai between the wars. England 1930s, Christopher Banks has become the country's most celebrated detective, his cases the talk of London society. Yet one unsolved crime has always haunted him: The mysterious disappearance of his parents in old Shanghai, when he was a small boy. Moving between London and Shanghai of the inter-war years, When We Were Orphans is a remarkable story of memory, intrigue and the need to return.\",\n",
              " 'A New York Times Notable Book\\nA Time Magazine “Best Comix of the Year”\\nA San Francisco Chronicle and Los Angeles Times Best-seller\\n\\nWise, funny, and heartbreaking, Persepolis is Marjane Satrapi’s graphic memoir of growing up in Iran during the Islamic Revolution.\\n\\nIn powerful black-and-white comic strip images, Satrapi tells the story of her life in Tehran from ages six to fourteen, years that saw the overthrow of the Shah’s regime, the triumph of the Islamic Revolution, and the devastating effects of war with Iraq. The intelligent and outspoken only child of committed Marxists and the great-granddaughter of one of Iran’s last emperors, Marjane bears witness to a childhood uniquely entwined with the history of her country.\\n\\nPersepolis paints an unforgettable portrait of daily life in Iran and of the bewildering contradictions between home life and public life. Marjane’s child’s-eye view of dethroned emperors, state-sanctioned whippings, and heroes of the revolution allows us to learn as she does the history of this fascinating country and of her own extraordinary family. Intensely personal, profoundly political, and wholly original, Persepolis is at once a story of growing up and a reminder of the human cost of war and political repression. It shows how we carry on, with laughter and tears, in the face of absurdity. And, finally, it introduces us to an irresistible little girl with whom we cannot help but fall in love.',\n",
              " \"Fourth in Cassandra Clare’s internationally bestselling Mortal Instruments series about the Shadowhunters.\\nDiscover more secrets about the Shadowhunters in the fourth of 6 brand-new collectable editions of the internationally bestselling Mortal Instruments series. To love is to destroy... The Mortal War is over, and Clary Fray is home in New York, excited about all the possibilities before her. She's training to become a Shadowhunter and – most importantly of all – she can finally call Jace her boyfriend. But her happiness has come at a price... Cover art by Mila Furstova, the artist who created the album art for Coldplay’s Ghost Stories. Read all the sensational books in The Shadowhunter Chronicles: The Mortal Instruments, The Infernal Devices, Tales from the Shadowhunter Academy, The Bane Chronicles, The Dark Artifices, The Last Hours and The Shadowhunter’s Codex.\",\n",
              " '\"Everything Men Know About Women is my go to present for every birthday and anniversary, it\\'s terrific.\"\\n--Barbara Corcoran, \"Shark Tank\" star and author of Shark Tales\\n\\n\"Fully reveals the shocking truth!\"\\n--Daily News\\n\\nA landmark book completely revised and updated to reveal everything men really know about the opposite sex.\\n\\nIn a little more than 100 pages, Dr. Alan Francis and collaborator Cindy Cashman distill years of research and thousands of interviews to reveal the most comprehensive understanding of men\\'s knowledge and understanding of the opposite sex in Everything Men Know About Women.\\n\\nFiercely frank and brilliantly insightful, this book spells out everything men know about such topics as:\\nMaking friends with women\\nRomancing women\\nAchieving emotional intimacy with women\\nMaking commitments to women\\nSatisfying women in bed\\n***That\\'s right, this book is completely BLANK!  A great gag gift for your boyfriend or husband, for a birthday or anniversary, or just to give to your girlfriends when you want a laugh!',\n",
              " \"Ever since the gods of Ancient Egypt were unleashed on the modern world, Carter Kane and his sister, Sadie, have been in big trouble.\\nAs descendants of the magical House of Life, they command certain powers. But now a terrifying enemy - Apophis, the giant snake of chaos - is rising. If Carter and Sadie don't destroy him, the world will end in five days' time. And in order to battle the forces of chaos, they must revive the sun god Ra - a feat no magician has ever achieved. Because first they must search the world for the three sections of the Book of Ra, then they have to learn how to chant its spells . . .\\nCan the Kanes destroy Apophis before he swallows the sun and plunges the earth into darkness . . . forever?\",\n",
              " 'Writer/artist Frank Miller completely reinvents the legend of Batman in this saga of a near-future Gotham City gone to rot, 10 years after the Dark Knight’s retirement. Forced to take action, the Dark Knight returns in a blaze of fury, taking on a whole new generation of criminals and matching their level of violence. He is soon joined by a new Robin—a girl named Carrie Kelley, who proves to be just as invaluable as her predecessors.\\n\\nCan Batman and Robin deal with the threat posed by their deadliest enemies, after years of incarceration have turned them into perfect psychopaths? And more important, can anyone survive the coming fallout from an undeclared war between the superpowers—or the clash of what were once the world’s greatest heroes?\\n\\nHailed as a comics masterpiece, The Dark Knight Returns is Frank Miller’s (300 and Sin City) reinvention of Gotham’s legendary protector. It remains one of the most influential stories ever told in comics, with its echoes felt in all media forms of DC’s storytelling. Collects issues #1-4.',\n",
              " 'Techniques for preparing and using the Golden Elixir to achieve optimum health and spiritual vitality\\n\\n•Includes practical exercises and postures to produce regenerative effects in one’s own saliva\\n\\n•Reveals how combining saliva with the hormonal fluids released during sexual practices creates the Elixir of Immortality\\n\\nGolden Elixir is the fountain or water of life. It is the combination of saliva, hormonal fluids, and external essences that when mixed together become the Elixir of Immortality. Saliva has long been considered by Taoists as a key component for optimum health. Some Taoist texts recommend swallowing the saliva up to 1,000 times a day to promote physical healing. Thousands of years ago Taoists became aware of changes in the taste and consistency of saliva that accompanied meditative practices. They learned that by combining saliva with the hormonal fluids and essences released during sexual activities a powerful elixir is formed. Taoists believe that this Golden Elixir is not only a physical healing agent, but also is a major transformative agent in preparing for higher spiritual work.\\n\\nGolden Elixir Chi Kung contains twelve postures that develop and utilize the healing power of saliva. Ten of these involve gathering energy and forces through the body’s hair, which acts as a negative-energy filter and can also be used to store surplus positive energy. Taoists regard the hair as antennae extending out into nature and the universe. By utilizing the practices in this book, readers can develop self-healing abilities and establish a better connection to the universe as a whole.',\n",
              " \"The global economy has made what seems like an incredible comeback after the financial crisis of 2008. Yet this comeback is artificial. Central banks have propped up markets by keeping interest rates low and the supply of money free-flowing. They won't bail us out again next time. And there will be a next time - soon.\\n\\nIn The Road to Ruin, bestselling author James Rickards identifies how governments around the world are secretly preparing an alternative strategy for the next big crisis: a lockdown. Instead of printing money to reliquify markets and prop up assets, governments are preparing to close banks, shut down exchanges and order powerful asset managers not to sell. They're putting provisions in place that will allow them to do so legally. What's more, the global elite has already started making their own preparations, including hoarding cash and hard assets.\\n\\nWhen the next one comes, it will be the average investor who suffers most - unless he or she heeds Rickards' warning and prepares accordingly.\",\n",
              " 'After creating 12 different billion-dollar businesses and breaking dozens of world records, wouldn’t you think you’d done it all? Not Sir Richard Branson. Having brought the Virgin brand to all corners of the globe, he’s now reaching out to the stars as he prepares to launch commercial space travel with Virgin Galactic.\\nIn this non-stop memoir, Richard takes you inside his whirlwind life: from reinventing his companies in the midst of the financial crisis and devastating personal losses, to tackling the planet’s biggest challenges, to the joys of becoming a ‘grand-dude’ at 64.Discover the irrepressible spirit, ingenious vision and relentless drive that has made Richard the ultimate entrepreneur.\\nThe iconoclastic Virgin founder is still changing the world. Next comes outer space.',\n",
              " \"Spensa's world has been under attack for hundreds of years. An alien race called the Krell leads onslaught after onslaught from the sky in a never-ending campaign to destroy humankind. Humanity's only defense is to take to their ships and fight the enemy in the skies. Pilots have become the heroes of what's left of the human race.\\nSpensa has always dreamed of being one of them; of soaring above Earth and proving her bravery. But her fate is intertwined with her father's - a pilot who was killed years ago when he abruptly deserted his team, placing Spensa's chances of attending flight school somewhere between slim and none.\\nNo one will let Spensa forget what her father did, but she is still determined to fly. And the Krell just made that a possibility. They've doubled their fleet, making Spensa's world twice as dangerous . . . but their desperation to survive might just take her skyward . . .\\nPraise for Brandon Sanderson's #1 New York Times Bestselling Reckoners series:\\n'Another win for Sanderson . . . he's simply a brilliant writer' Patrick Rothfuss\\n'Action-packed' EW.com\\n'Compelling . . . Sanderson uses plot twists that he teases enough for readers to pick up on to distract from the more dramatic reveals he has in store' AV Club\",\n",
              " \"If you have ever tried to do a card trick and failed, you know what it is to be embarrassed. You may try to cover up by doing a more difficult trick and fail again. The way out of this dilemma, however, is not immediate, but it is reliable: a surer mastery of technique. This means the proper instruction book, and practice.\\nIn this definitive work on card technique, step-by-step instructions teach you the correct methods for the basic manipulations and the more advanced flourishes, and only then allow you to learn tricks. Offering the most foolproof methods available, Jean Hugard and Fredrick Braue explain such basic manipulations as the palm, the shuffle, the lift, the side slip, the pass, the glimpse, the jog, and the reverse. They detail various false deals, crimps, and changes and the more advanced execution needed for forces, fans, and the use of the prearranged deck. Also presented is a wide variety of tricks, including discoveries, self-working tricks, one-handed tricks, stranger cards, and such individually famous tricks as the four aces, the rising cards, and the Zingone spread. In addition, the authors include a complete compendium of shakedown sleights — to warn the card player and aid the entertainer — and a performer's guide to misdirection and patter.\\nMany of the methods explained were revealed here for the first time, while many previously known tricks are presented in improved versions. In every case the aim is simplicity of technique for the purpose of mystifying an audience, not technique for the sake of technique. An unsurpassed collection of methods and manipulations, this classic work will help any aspiring magician to achieve expert card technique.\",\n",
              " 'It is not always true that one who can write and read English confidently and effortlessly can speak it fluently too. Sometimes, people good at reading and writing English find themselves tongue-tied when it comes to expressing it verbally. Learning how to speak English fluently and flawlessly is no rocket science. It requires patience and constant endeavour for positive improvements. Spoken English, created by Alison Reid, aims at offering uncomplicated, effortless and efficient ways to learn how to speak English confidently and impressively.',\n",
              " \"this short introduction provides a clear and succinct account of the evolution of indian foreign policy over six decades since independence. it explains how the three approaches to the study of international politics decision-making, systemic/global, and national/domestic have helped in formulating and implementing india's foreign policies.\\nthe five chapters cover the ideational period, starting immediately after independence and ending with the sino-indian border war of 1962; the period between 1962 and the end of the cold war; india's greater acceptance of the importance of material capabilities following the end of the cold war; and current trends and debates in indian foreign policy, unresolved tensions, and the possible way ahead.\\nacknowledgements\\norganization and structure\\nthe palimpsest of the past\\nthe fate of non-alignment\\ncoping with the cold war's end\\nforging a new consensus?\\nappendix\\nnotes\\nbibliography\\nindex\",\n",
              " \"A stylish new edition of the acclaimed graphic novel adaptation of Alex Rider's second thrilling mission.\\nInvestigations into the “accidental” deaths of two of the world's most powerful men have revealed just one link: both had a son attending Point Blanc Academy, a school for rebellious rich kids run by the sinister Dr Grief and set high in the French Alps. Armed with a new collection of gadgets, Alex must infiltrate the academy as a pupil and establish the truth about what is really happening there.\",\n",
              " \"In February 2017, Gurmehar Kaur, a nineteen-year-old student, joined a peaceful campaign after violent clashes at a Delhi University college. As part of the campaign, Kaur's post made her the target of an onslaught of social media vitriol. Kaur, the daughter of a Kargil martyr, suddenly became a focal point of a nationalism debate. Facing a trial by social media, Kaur almost retreated into herself. But she was never brought up to be silenced. ‘Real bullets killed my father. Your hate bullets are deepening my resolve,’ she wrote then. Today, Kaur is doubly determined not to be silent. Small Acts of Freedom is her story. This is the story of three generations of strong, passionate single women in one family, women who have faced the world on their own terms. With an unusual narrative structure that crisscrosses elegantly between past and present, spanning seventy years from 1947 to 2017, Small Acts of Freedom is about courage. It’s about resilience, strength and love. From her grandmother who came to India from Lahore after Partition to the whirlwind romance between her parents, from her father’s state funeral to her harrowing experiences since her days of student activism, Gurmehar Kaur’s debut is about the fierceness of love, the power of family and the little acts that beget big revolutions.\",\n",
              " \"The Greatest Dot-to-Dot Adventure, (the 18th book in the Greatest Dot-to-Dot series) by David Kalvitis, carries on his tradition of skillfully utilizing dots to create the most amazingly complex and artistic dot-to-dot puzzles. The Greatest Dot-to-Dot Adventure - Book 2 is the second in the series of revolutionary Dot-to-Dot puzzle books created by David Kalvitis. Book 2 weaves all of the varied challenges and puzzle styles of its predecessors with a multi-layered mystery that includes new styles, hidden clues, an intriguing story line and impressive three and four page landscape spreads. The adventure continues following an imaginative young boy searching for the missing pieces of a mysterious gift. Together, visitors will navigate challenging puzzles and unearth secret clues to discover new species of dots and spectacular views. One can even challenge themselves by scaling the 'Everest of Connect-the-Dots', a breathtaking four page puzzle comprised of a staggering 2,100 dots.\",\n",
              " 'Becky\\'s life is blooming! She\\'s working at London\\'s newest fashion store The Look, house-hunting with husband Luke (her secret wish is a Shoe Room)...and she\\'s pregnant! She couldn\\'t be more overjoyed - especially since discovering that shopping cures morning sickness. Everything has got to be perfect for her baby: from the designer nursery...to the latest, coolest pram...to the celebrity, must-have obstetrician. But when the celebrity obstetrician turns out to be her husband Luke\\'s glamorous, intellectual ex-girlfriend, Becky\\'s perfect world starts to crumble. She\\'s shopping for two...but are there three in her marriage? Everybody loves Sophie Kinsella: \"I almost cried with laughter.\" (Daily Mail). \"Hilarious ...you\\'ll laugh and gasp on every page.\" (Jenny Colgan). \"Properly mood-altering ...funny, fast and farcical. I loved it.\" (Jojo Moyes). \"A superb tale. Five stars!\" (Heat).',\n",
              " 'This volume narrates the political upheavals of the inter-war period, further enriched by Netaji s reflections on the key themes Indian history and a finely etched assessment of Mahatma Gandhi s role in it.',\n",
              " 'Peter Guillam, staunch colleague and disciple of George Smiley of the British Secret Service, otherwise known as the Circus, has retired to his family farmstead on the south coast of Brittany when a letter from his old Service summons him to London. The reason? His Cold War past has come back to claim him. Intelligence operations that were once the toast of secret London are to be scrutinised by a generation with no memory of the Cold War. Somebody must be made to pay for innocent blood once spilt in the name of the greater good.\\n\\nInterweaving past with present so that each may tell its own story, John le Carré has given us a novel of superb and enduring quality.',\n",
              " 'Brilliant. Complicated. Psychopath. That’s the Four Monkey Killer or ‘4MK’. A murderer with a twisted vision and absolutely no mercy. Detective Sam Porter has hunted him for five long years, the recipient of box after box of grisly trinkets carved from the bodies of 4MK’s victims. But now Porter has learnt the killer’s twisted history and is racing to do the seemingly impossible – find 4MK’s latest victim before it’s too late.',\n",
              " 'The book ‘SPOKEN ENGLISH’ helps you solve your multifarious problems in gaining good knowledge of spoken English. Wherein grammar as well as various ways of using the language are given, as such; word uses, conversation, dialogue, situational expression, trendy expression, enquiries, questionnaire, debates, symposium, extempore, presentation, speech, group discussions, sales skills, tag questions and the alikes. All these will serve as hubs and boons to enhance your knowledge of English tremendously, enabling you to express whatsoever you like helping you in your examinations, interviews, presentations, speeches, comm-unications, discussions and meetings.\\nTable of Content \\nPart-I : Spoken English & Part-II : Grammar',\n",
              " 'The authors\\' treatment of data structures in Data Structures and Algorithms is unified by an informal notion of \"abstract data types,\" allowing readers to compare different implementations of the same concept. Algorithm design techniques are also stressed and basic algorithm analysis is covered. Most of the programs are written in Pascal.',\n",
              " \"  A heart-warming picture book story of friendship, loyalty - and kittens!\\nMe, you and the old guitar.\\nHow perfectly, perfectly happy we are.\\nMEEE-EW and the old guitar.\\nHow PURRRR-fectly happy we are!\\n  Tabby McTat is purr-fectly happy, singing along all day with Fred the busker.\\nBut when Fred gives chase to a thief, the two are separated.\\nWill they ever find each other again?\\n  'Our five-year-old gave it the thumbs up, and that's aboutthe best endorsement you can get.' - News of the World\\nA wonderfully funny story from the author and illustrator of The Gruffalo, Stick Man and ZOG, which have all been made intoanimated films shown on BBC1\",\n",
              " \"From the author of Number One bestseller The Righteous Men. How are hundreds of unexplained deaths, spanning the globe, connected to the last great secret of the Second World War?\\nTom Byrne has fallen from grace since his days as an idealistic young lawyer in New York. Now he'll work for anyone – as long as the money's right.\\nSo when the United Nations call him in to do their dirty work, he accepts the job without hesitation. A suspected suicide bomber shot by UN security staff has turned out to be a harmless old man: Tom must placate the family and limit their claims for compensation.\\nIn London, Tom meets the dead man’s alluring daughter, Rebecca, and learns that her father was not quite the innocent he seemed. He unravels details of a unique, hidden brotherhood, united in a mission that has spanned the world and caused hundreds of unexplained deaths.\\nPursued by those ready to kill to uncover the truth, Tom has to unlock a secret that has lain buried for more than 60 years – the last great secret of the Second World War.\",\n",
              " \"About The Book\\nTargeted for the IELTS candidates, the book titled The Ultimate Guide To IELTS Speaking by Parthesh Thakkar is an encyclopedic text including the speaking activities and cue cards for the aspirants to refer to. The book comprises of interview questions and essential materials for the students to try hands on for gaining confidence on their preparation. This is a practice book for the IELTS aspirants to gain mastery on the vital skills.\\nKey sections of the book revolve around subjects like IELTS speaking interview questions with answers, IELTS speaking interview questions for independent practice, overall generic questions and 2 categories of cue-cards: with answers and for independent practice. The attraction of the book remains in the speaking activities for the candidates to try on and gain real-time experience of facing the interviews. From the candidate's perspective, this book would prove to be a complete handbook with almost everything one needs to gain ideas in a vast range.\\nEasy and comprehensive language of the book makes this a comfortable read. The book has been massive success because of the content and delivery and has been available on more than 50 websites. Thakkar has been known as a master to guide students for the writing and speaking skills much needed for cracking IELTS. Primary publications released The Ultimate Guide To IELTS Speaking in 2009 in a Paperback.\",\n",
              " 'Democracy rests on a delicate balance between two principles that may be called the rule of numbers and the rule of law. To ensure that the rule of law is not overwhelmed by the weight of numbers, democracy requires institutions. Examining democracy from an institutional perspective, this book studies such institutions as the Parliament, the courts of justice, and the systems of political parties, and brings out the contradictions between the ideals of democracy-such as equality and liberty-and the actual operation of government and politics. It introduces the reader to the distinction between law and custom, and between matters of right and matters of trust.',\n",
              " 'All the big ideas, simply explained. An innovative and accessible guide to economics. Bring economics to life with The Economics Book, an essential guide to more than 100 of the big ideas in economic theory and practice covering everything from ancient theories right up to cutting-edge modern developments. From Aristotle to John Maynard Keynes and beyond, all the greatest economists and their theories are featured and the innovative graphics, step-by-step summaries and mind maps help clarify hard-to grasp concepts. The Economics Book is perfect for economic students and anyone who has an interest in how economies work.',\n",
              " 'This seminal book presents biographical essays for each of the first ninety-three judges who served on the Supreme Court from 1950 through mid-1989. These essays are based on interviews the author conducted with sixty-four of the sixty-eight judges who were alive in the 1980s and on meetings and correspondence with family members or relatives, friends and associates of the deceased judges. An attempt is made to account for why certain judges rather than others were chosen the selection criteria employed and, to the extent possible in a secretive selection environment, to identify those who selected them. It concludes with a collective portrait of these judges, paying particular attention to changes in their background characteristics—fathers’ occupation, education, pre-SCI career, caste, religion, state of birth, region over four decades. The essays also embrace their post-retirement activities.',\n",
              " 'First published over fifty years ago, A GLOSSARY OF LITERARY TERMS remains an essential text for all serious students of literature. Now fully updated to reflect the latest scholarship on recent and rapidly evolving critical theories, the eleventh edition contains a complete glossary of essential literary terms presented as a series of engaging, beautifully crafted essays that explore the terms, place them in context and suggest related entries and additional reading. This indispensable, authoritative and highly affordable reference covers terms useful in discussing literature and literary history, theory and criticism. Perfect as a core text for introductory literary theory or as a supplement to any literature course, this classic work is an invaluable reference that students can continue to use throughout their academic and professional careers.',\n",
              " 'This book is essential for everyone who wants to improve their drawing skills. Written by teacher and established artist Barrington Barber, it contains a wealth of know-how and practical advice, supported by over 300 original illustrations. Guidance is given on how to overcome common difficulties without imposing a particular style or approach. The aim throughout is to help you discover your artistic purpose while giving you the technical tools necessary if you are to produce work you will be proud to call your own.',\n",
              " 'India Wins Freedom is an enlightened account of the partition from the author, Maulana Azad’s perspective. It includes his personal experiences when India became independent, and his ideas on freedom and liberty.\\nThe book takes the form of an autobiographical narrative and goes over the happenings of the Indian Independence movement. The book traces the events that took place and ultimately led to the partition in a frank and profound manner. The book says that politics was responsible for the partition more than religion. It also states that India failed to maximise its potential when it gained independence. The book discusses political hypocrisy, and also touches upon contemporaries of the author’s, like Nehru, Gandhi, and Subhash Chandra Bose, and highlights their mind-sets during that time.\\nIndia Wins Freedom was published in 1988 by Orient BlackSwan in paperback.\\nKey Features:\\nWhen the book was first published, it was sealed for its controversial take on India’s independence movement.\\nOn being released to the public, over 30 million copies of the book were sold all over the world.',\n",
              " \"Revised and updated for its second edition with new definitions and the latest buzzwords, THE NEW PENGUIN ENGLISH DICTIONARY is the most accessible and authoritative single-volume dictionary available.\\n'In a comparison of Collins, Chambers and Penguin, the New Penguin English Dictionary comes out as the top recommendation' - TLS\\n'Refreshingly forward-looking and impressively comprehensive' - Amazon.co.uk\",\n",
              " \"After a decade designing technologies meant to address education, health, and global poverty, award-winning computer scientist Kentaro Toyama came to a difficult conclusion: Even in an age of amazing technology, social progress depends on human changes that gadgets can't deliver.Computers in Bangalore are locked away in dusty cabinets because teachers don't know what to do with them. Mobile phone apps meant to spread hygiene practices in Africa fail to improve health. Executives in Silicon Valley evangelize novel technologies at work even as they send their children to Waldorf schools that ban electronics. And four decades of incredible innovation in America have done nothing to turn the tide of rising poverty and inequality. Why then do we keep hoping that technology will solve our greatest social ills?In this incisive book, Toyama cures us of the manic rhetoric of digital utopians and reinvigorates us with a deeply people-centric view of social change. Contrasting the outlandish claims of tech zealots with stories of people like Patrick Awuah, a Microsoft millionaire who left his engineering job to open Ghana's first liberal arts university, and Tara Sreenivasa, a graduate of a remarkable South Indian school that takes impoverished children into the high-tech offices of Goldman Sachs and Mercedes-Benz, Geek Heresy is a heartwarming reminder that it's human wisdom, not machines, that move our world forward.\",\n",
              " 'What is time? The 5th-century philosopher St Augustine famously said that he knew what time was, so long as no one asked him. Is time a fourth dimension similar to space or does it flow in some sense? And if it flows, does it make sense to say how fast? Does the future exist? Is time travel possible? Why does time seem to pass in only one direction?These questions and others are among the deepest and most subtle that one can ask, but \"Introducing Time\" presents them - many for the first time - in an easily accessible, lucid and engaging manner, wittily illustrated by Ralph Edney.\\nA Children’s Bookshelf Selection: Each month our editor’s pick the best books for children and young adults by age to be a part of the children’s bookshelf. These are editorial recommendations made by our team of experts. Our monthly reading list includes a mix of bestsellers and top new releases and evergreen books that will help enhance a child’s reading life.',\n",
              " 'A mother\\'s advice to her daughter--a guide to daily living, both practical and sublime--with full-color illustrations throughout.\\nOne sleepless night while she was in her early twenties, illustrator/writer Hallie Bateman had a painful realization: her mom would die, and after she died she would be gone. The prospect was devastating, and also scary--how would she navigate the world without the person who gave her life? She thought about all the motherly advice she would miss--advice that could help her through the challenges to come, including the ordeal of losing a parent.\\nThe next day, Hallie asked her mother, writer Suzy Hopkins, to record step-by-step instructions for her to follow in the event of her mom\\'s death. The list began: \"Pour yourself a stiff glass of whiskey and make some fajitas\" and continued from there, walking Hallie through the days, months, and years of life after loss, with motherly guidance and support, addressing issues great and small--from choosing a life partner to baking a quiche. The project became a way for mother and daughter to connect with humor, openness, and gratitude. It led to this book.\\nCombining Suzy\\'s wit and heartfelt advice with Hallie\\'s quirky and colorful style, What to Do When I\\'m Gone is the illustrated instruction manual for getting through life without one\\'s mom. It\\'s also a poignant look at loss, love, and taking things one moment at a time. By turns whimsical, funny, touching, and above all pragmatic, it will leave readers laughing and teary-eyed. And it will spur conversations that enrich family members\\' understanding of one another.',\n",
              " 'A fascinating guided tour of the ways things work in a modern city\\n\\nHave you ever wondered how the water in your faucet gets there? Where your garbage goes? What the pipes under city streets do? How bananas from Ecuador get to your local market? Why radiators in apartment buildings clang? Using New York City as its point of reference, The Works takes readers down manholes and behind the scenes to explain exactly how an urban infrastructure operates. Deftly weaving text and graphics, author Kate Ascher explores the systems that manage water, traffic, sewage and garbage, subways, electricity, mail, and much more. Full of fascinating facts and anecdotes, The Works gives readers a unique glimpse at what lies behind and beneath urban life in the twenty-first century.',\n",
              " \"India's struggle for independence\\nIndia’s struggle for Independence by Bipin Chandra is your go to book for an in-depth and detailed overview on Indian independence movement . Indian freedom struggle is one of the most important parts of its history. A lot has been written and said about it, but there still remains a gap. Rarely do we get to hear accounts of the independence from the entire country and not just one region at one place. This book fits in perfectly in this gap and also provides a narration on the impact this movement had on the people. Bipin Chandra’s book is a well-documented history of India's freedom struggle against the British rule. It is one of the most accurate books which have been painstakingly written after thorough research based on legal and valid verbal and written sources. It maps the first war of independence that started with Mangal Pandey’s mutiny and witnessed the gallant effort of Sri Rani Laxmi Bai. Many of the pages of this book are dedicated to Mahatma Gandhi’s non-cooperation and the civil disobedience movements. It contains detailed description of Subash Chandra Bose’s weapon heavy tactics and his charisma. This book includes all the independence movements and fights, irrespective of their size and impact, covering India in its entirety. Although these movements varied in means and ideas, but they shared a common goal of independence. This book contains oral and written narratives from different parts of the country, making this book historically rich and diverse. The book captures the evolution of Indian independence struggle in full detail and leaves no chapter of this story untouched. This book is a good read for the students of Indian modern history and especially for students who are preparing for UPSC examination and have taken History as their subject. This book is easily available online and you can purchase it at Amazon.in now\\nAbout the author\\nBipin Chandra was one of the most respected historians of our times. Between 2004 and 2012, he was the chairman of National Book Trust. Economic and political history of India was his area of specialization. He was also one of the leading authorities on Mahatma Gandhi. The author also remained the General President of National History Congress 1985. He was a professor at JNU, New Delhi and also taught at Hindu College. Some of his other works included The Rise and Growth of Economic Nationalism.\",\n",
              " \"The psychiatrist's couch holds many secrets. Can it also hold the key to a series of brutal murders? The thrilling first novel from the internationally bestselling Master of Suspense.\\nTwo murders, one victim murdered in the street in a gruesome but apparently arbitrary attack. The other brutally tortured and left to die in agony.\\nNow it's the turn of psychoanalyst Dr Judd Stevens…\\nIn a chilling game of cat and mouse, Judd must become the hunter rather than the hunted if he wants to stay alive. Working with the mindset of a detective, he must analyse his patients, searching for a motive, clues, reasons. Could it be Teri Washburn, Hollywood starlet, thrown out of tinsel town in scandal and now addicted to sex? Could it be Harrison Burke, top business man and disturbed paranoiac? Or could it be Alexander Fallon, a crazed evangelist, convinced that God has chosen him to avenge all sin in the world?\\nIn this deadly game, there can only be one winner…If Judd is to survive he must play the game to win.\\nThis is Sidney Sheldon's first novel – a gripping, intense thriller that brought him fame as a bestselling novelist.\",\n",
              " \"FRENCH-ENGLISH\\nENGLISH-FRENCH\\n\\nEntirely revised and updated, this is the most affordable and complete French-English English-French dictionary of its kind. Now easier to read and access, with more than 50,000 definitions, it is designed expressly for the widest possible variety of interests and professions -- students, teachers, travelers, and home and office libraries.\\nYou'll find keys to pronunciation, idioms, conjugations, and more, in both languages -- all here, in the finest resource of its kind.\\n\\nFRANÇAIS-ANGLAIS\\nANGLAIS-FRANÇAIS\\n\\nAvec cette édition entièrement refondue et actualisée, découvrez le dictionnaire français-anglais anglais-français à la fois le plus complet et le plus économique de sa génération. Plus facile à consulter que jamais tout en offrant plus de 50 000 mots et expressions, il est utilisable par tous, en contexte scolaire aussi bien qu'en voyage, chez soi ou au bureau. De précieuses indications sur les difficultés des deux langues (prononciation, conjugaison, tournures idiomatiques...et bien plus encore) contribuent\",\n",
              " 'Q. Why are there almost as many jokes about death as there are about sex? \\nA. Because they both scare the pants off us.\\n\\nThomas Cathcart and Daniel Klein first made a name for themselves with the outrageously funny New York Times bestseller Plato and a Platypus Walk into a Bar.... Now they turn their attention to the Big \"D\" and share the timeless wisdom of the great philosophers, theologians, psychotherapists, and wiseguys. From angels to zombies and everything in between, Cathcart and Klein offer a fearless and irreverent history of how we approach death, why we embrace life, and whether there really is a hereafter. As hilarious as it is enlightening, Heidegger and a Hippo Walk Through Those Pearly Gates is a must-read for anyone and everyone who ever expects to die.\\n\\nAnd now, you can read Daniel Klein\\'s further musings on life and philosophy in Travels with Epicurus and Every Time I Find the Meaning of Life, They Change it.',\n",
              " \"'I read a book one day, and my whole life was changed.' So begins The New Life, Orhan Pamuk's fabulous road novel about a young student who yearns for the life promised by a dangerously magical book. He falls in love, abandons his studies, turns his back on home and family, and embarks on restless bus trips through the provinces, in pursuit of an elusive vision. This is a wondrous odyssey, laying bare the rage of an arid heartland. In coffee houses with black-and-white TV sets, on buses where passengers ride watching B-movies on flickering screens, in wrecks along the highway, in paranoid fictions with spies as punctual as watches, the magic of Pamuk's creation comes alive.\",\n",
              " \"An erotic masterpiece of twentieth century fiction - a tale of sensual obsession and bloodlust in eighteenth century Paris\\n'An astonishing tour de force both in concept and execution' Guardian\\nIn eighteenth-century France there lived a man who was one of the most gifted and abominable personages in an era that knew no lack of gifted and abominable personages. His name was Jean-Baptiste Grenouille, and if his name has been forgotten today, it is certainly not because Grenouille fell short of those more famous blackguards when it came to arrogance, misanthropy, immorality, or, more succinctly, wickedness, but because his gifts and his sole ambition were restricted to a domain that leaves no traces in history: to the fleeting realm of scent . . .\\n'A fantastic tale of murder and twisted eroticism controlled by a disgusted loathing of humanity ... Clever, stylish, absorbing and well worth reading' Literary Review\\n\\n'A meditation on the nature of death, desire and decay ... A remarkable début' Peter Ackroyd, The New York Times Book Review\\n\\n'Unlike anything else one has read. A phenomenon ... [It] will remain unique in contemporary literature' Figaro\\n\\n'An ingenious and totally absorbing fantasy' Daily Telegraph\\n\\n'Witty, stylish and ferociously absorbing' Observer\",\n",
              " 'The book begins with basic concepts such as symbols, alphabets, sets, relations, graphs, strings, and languages. It then delves into the important topics including separate chapters on finite state machine, regular expressions, grammars, pushdown stack, Turing machine, parsing techniques, Post machine, undecidability, and complexity of problems. A chapter on production systems encompasses a computational model which is different from the Turing model, called Markov and labelled Markov algorithms. At the end, the chapter on implementations provides implementation of some key concepts especially related to regular languages using C program codes.\\n\\nA highly detailed pedagogy entailing plenty of solved examples, figures, notes, flowcharts, and end-chapter exercises makes the text student-friendly and easy to understand.',\n",
              " \"In 1921, a travelling sadhu appeared by a river bund in Dhaka. He was there every day. Soon, people began to identify him as none other than the Second Kumar of Bhawal, a young zamindar who had died twelve years earlier. His wife denounced him as an impostor. His sisters welcomed him back. This resulted in one of the most extraordinary legal cases in Indian history: it held the entire country’s attention for several decades as it unwound in courts from Dhaka and Calcutta to London.\\nThis is possibly the most riveting work of history ever written in the Indian subcontinent. Ever since it first appeared, Partha Chatterjee’s 'A Princely Impostor?' (2002), a telling of the notorious 'Bhawal Sannyasi Case'—among India’s best-known legal disputes—has been recognized as world-class narrative history in a league of its own. Chatterjee has written a book as spell-binding as any great Victorian or Russian novel, a story replete with courtroom drama, sexual debauchery, family intrigue and squandered wealth.\",\n",
              " 'The gripping sequel to New York Times bestselling fantasy epic The Black Prism from Brent Weeks.\\n\\nGavin Guile is dying.\\n\\nHe\\'d thought he had five years left--now he has less than one. With fifty thousand refugees, a bastard son, and an ex-fiancée who may have learned his darkest secret, Gavin has problems on every side. All magic in the world is running wild and threatens to destroy the Seven Satrapies. Worst of all, the old gods are being reborn, and their army of color wights is unstoppable. The only salvation may be the brother whose freedom and life Gavin stole sixteen years ago.\\n\\nRead the second book in Brent Weeks\\'s blockbuster epic fantasy series that had Peter V. Brett saying, \"Brent Weeks is so good, it\\'s starting to tick me off!\"\\n\\nLightbringer\\nThe Black Prism\\nThe Blinding Knife\\nThe Broken Eye\\nThe Blood Mirror\\n\\n\\nFor more from Brent Weeks, check out:\\n\\nNight Angel\\nThe Way of Shadows\\nShadow\\'s Edge\\nBeyond the Shadows\\n\\nNight Angel: The Complete Trilogy (omnibus)\\nPerfect Shadow: A Night Angel Novella (e-only)\\nThe Way of Shadows: The Graphic Novel',\n",
              " \"With an introduction by Philipp Meyer. The wrath of God lies sleeping. It was hid a million years before men were and only men have power to wake it. Hell aint half full. Set in the anarchic world opened up by America's westward expansion, Blood Meridian by Cormac McCarthy is an epic and potent account of the barbarous violence that man visits upon man. Through the hostile landscape of the Texas-Mexico border wanders the Kid, a fourteen year-old Tennessean who is quickly swept-up in the relentless tide of blood. But the apparent chaos is not without its order: while Americans hunt Indians – collecting scalps as their bloody trophies – they too are stalked as prey. Since its first publication in 1985, Blood Meridian has been read as both a brilliant subversion of the Western novel and a blazing example of that form. Powerful and savagely beautiful, it has emerged as one of the most important works in American fiction of the last century. A truly mesmerizing classic.\",\n",
              " 'In 1998, a book was published that was surprisingly ahead of its times. It was called India 2020 and proposed that India could soon be one of the top five economies of the world. The nation had set off a series of nuclear tests and was facing worldwide sanctions. A new government had taken charge, and the economy was facing a tough time. It was not the best of times to predict that India had it in her to get on the fast track to development. The vision presented in the book would go on to inspire, directly or indirectly, many sectors of the economy to work for and achieve high growth. The book has since sold hundreds of thousands of copies. In A Manifesto for Change, its author A.P.J. Abdul Kalam, writing with co-author V. Ponraj, offers a sequel. As focused then as now on his dream of a developed India by 2020, the eleventh President of India examines what we need to get right to accomplish that essential goal: harnessing the stupendous energy of our youth to contribute to growth, a united Parliament that makes full use of its time for constructive debate and rises above petty party politics to achieve the larger national vision, and a plan of action that looks at development from the grassroots to giant strides in infrastructure and bridging the urban-rural disparity. It is time to leave behind the politics of antagonism and disruption behind, he suggests. As reward: a developed India as befits this beautiful land.',\n",
              " \"For centuries the nature and meaning of Islamic art has been misunderstood in the West, being regarded as no more than decoration. But in fact the abstract art of Islam represents the sophisticated development of a supra-naturalistic tradition, since the portrayal of human and animal forms has always been discouraged by the Prophet Muhammad, so as to avoid idolatry. Hence, among the world's great artistic traditions, Islamic art has maintained its singular integrity and inner content with the least diversion from its aim: the affirmation of unity as expressed in diversity. The Pythagorean/Platonic doctrines are easily recognizable in the body of Islamic geometric art, as the wisdom of this practice was exalted by Socrates, in Plato's Republic dialogue (527), when he specifically gave the reason for practising geometry. Its practice rekindled the inner organ (or eye of wisdom) by which alone we can see the truth. The geometrical patterns of Islamic art reveal to the eye of the sensitive onlooker the intrinsic cosmological laws affecting all Creation. The primary function of these patterns is to lead the mind from the literal and mundane world towards the underlying permanent reality. The numerous sequential drawings show how the art of Islam is inseparable from the science of mathematics. Thus, we can see clearly how an Earth-centred - 'common-sense' - view of the cosmos gives renewed signficance to the number patterns produced by the orbits of the planets, correlating the cosmos as experienced by man with the patterns created in Islamic art, and thereby throwing new light on the perennial symbolic significance of number. The mathematical tessellations inherent in space-filling patterns are revealed as an essential practical and philosophical basis for the creation of each completed work of art - whether a tile, a carpet, a wall or an entire building - and thus affirm the underlying essential unity of all things.\",\n",
              " 'Presenting, for the first time ever, the whole truth about Indian advertising and nothing but the truth (with just a pinch of salt).\\n\\nFor centuries, Indians have been asking all kinds of questions – about the meaning of life, our place in the cosmos, why we have so many gods and other such vital things. In the last hundred-odd years, marketing and advertising has given us none of those answers. What it has given us, nonetheless, is life-altering stuff.\\n\\nIt has attempted to make men Fair and Handsome. It has battled to make women 18 Again. And to both men and women it has given Tinder loving care.\\n\\nIt has made us realize that we like pizza as much as the next Italian – as long as Dominos puts keema dopyaaza on it and tempts us with, ‘Hungry kya?’\\n\\nIt has made us re-evaluate our life choices and ask thought-provoking questions like ‘Kitna deti hai?’ of our cars and ‘Kya aap Close-Up karte hain?’ of our toothpaste. In short, it has enriched our lives with quirky quips and clever (and, at times, outrageous) turns of phrase, unforgettable mascots, all-out Battles of the Brands, eye-popping insights and lump-in-the-throat moments, while feeding our addiction to controversies and virtual worlds.\\nIn this must-read book, you’ll find unbusiness-like stories from Indian advertising through the ages – and everything you didn’t want to know about the hits, the misses, the also-rans and the banned.',\n",
              " 'Ancient Egypt...\\nIn the time of pyramids and pharaohs...\\nA beautiful young princess travels to the grand city of Memphis--home to the legendary pharaoh Rhampsinitus. She intends to merely search the land for a husband, but while meeting with the local men, the princess becomes privy to a secret, sinister tale.\\nThe Treasured Thief is a story about desperation, thievery, and murder, but it is also a story about family, sacrifice, and love. As this secret tale is laid bare before her, the princess learns of three feats of impossible brotherhood, ingenuity, and boldness on the part of a man who dared thwart the will of a king.',\n",
              " \"The Beatles are the world's most enduring and biggest rock band ever! This is the story of their struggle for success. Taking us through the early days of rock 'n' roll, and their lives in Liverpool during the 1950s, we journey with them to Hamburg as they come of age and through grit, determination and masses of talent became the lads who made the sixties swing! \\n\\nThis 145 page graphic novel is part of the Campfire Graphic Novel series, which brings classics, biographies and more to graphic novel format.\",\n",
              " \"- King's recent hardbacks soared to No. 1 on the Sunday Times bestseller lists.\\n- A unique collection because each story is preceded by a page of autobiographical text describing its origin.\\n- King is the master of the short story form - he continues to publish in the New Yorker, Granta and Esquire.\\n- An excellent Christmas gift and self-purchase, King's stories have generated terrific films such as the Shawshank Redemption.\\n- The introductions will delight all his readers including those who loved his insight into the craft of writing in his non-fiction title On Writing\\n- 'This collection of short works. . . reveals King's mastery of the novella' -- Guardian\\n- 'A tense inventory of stories. . . King manages to portray a remarkable depth of character within the swiftness of a short story and manoeuvres a vast range of plots. . . There are treasures to be found in the Bazaar of Bad Dreams and those who love King. . . will find much to savour' -- Independent\\n- 'A more versatile writer than you might imagine' -- Sunday Times\\n- 'The Bazaar of Bad Dreams is the title it more than lives up to, but just as interesting as the stories themselves are their prefaces, in which he reveals what inspired each one. Who besides King would conjure a flesh-eating station wagon from a drive to see his college sweetheart?' -- Observer\\n\\n\\nA generous collection of thrilling stories - some brand new, some published in magazines, all entirely brilliant and assembled in one book for the first time - with a wonderful bonus: in addition to his introduction to the whole collection, King gives readers a fascinating introduction to each story with autobiographical comments on their origins and motivation. . . The No. 1 bestselling writer has dazzled readers with his genius as a writer of novellas and short story fiction since his first collection Night Shift was published. He describes the nature of the form in his introduction to the book: 'There's something to be said for a shorter, more intense experience. It can be invigorating, sometimes even shocking, like. . . a beautiful curio for sale laid out on a cheap blanket at a street bazaar. ' In the Bazaar of Bad Dreams there is a curio for every reader - a man who keeps reliving the same life, repeating the same mistakes over and over again, a columnist who kills people by writing their obituaries, a poignant tale about the end of the human race and a firework competition between neighbors which reaches an explosive climax. There are also intriguing connections between the stories; themes of morality, guilt, the afterlife and what we would do differently if we could see into the future or correct the mistakes of the past. Effervescent yet poignant, juxtaposing the everyday against the unexpected, these stories comprise one of King's finest gifts to his constant reader as well as to those fascinated by the autobiographical insights in his celebrated nonfiction title On Writing 'I made them especially for you', says King. 'Feel free to examine them, but please be careful. The best of them have teeth. '\",\n",
              " 'LOVE. GLAMOUR. SECRETS.\\nA stunning historical love story from the bestselling author of The Secret Wife.\\nRome, 1961. As the cameras roll on Cleopatra, the world is transfixed by the love affair emerging between Hollywood’s biggest stars: Richard Burton and Elizabeth Taylor.\\nBut on the film set, tensions are running high. The money is running out, and a media storm is brewing over the Taylor-Burton relationship. When historical advisor Diana Bailey starts work on the film, she wants nothing more than to escape from her own troubled marriage and start anew. But as the heady world of Hollywood envelops her, secrets begin to emerge in the cast and crew. Is everything as it seems? And what really hides beneath the glamour of the famous film?\\nAn enthralling story of love and passion from the bestselling author of The Secret Wife, set against the stunning backdrop of one of the most iconic Hollywood movies ever made.',\n",
              " \"The Amul campaign tells the stories of India, one hoarding at a time. In this revised and updated edition of Amul's India, the much celebrated and best-selling book on fifty years of the Amul advertising campaign, new essays and ads have been added - on topics ranging from the NaMo phenomenon to women's safety and empowerment - to captivate all those who love their daily dose of the lovable little Amul girl in polka dots. This book celebrates the Amul girl's journey through the eyes of prominent writers, public figures and the subjects of hoardings themselves, offering a potted history of the country over fifty years.\",\n",
              " \"Continuing the incredible popularity of Alfred's Basic Adult Piano Course, this new book adapts the same friendly and informative style for adults who wish to teach themselves. With the study guide pages that have been added to introduce the music, it's almost like having a piano teacher beside you as you learn the skills needed to perform popular and familiar music. There are also five bonus pieces: At Last * Have Yourself a Merry Little Christmas * Laura * Over the Rainbow * Singin' in the Rain. Included is a CD containing the piano part and an engaging arrangement for each of the 65 musical examples. 192 pages.\",\n",
              " 'Become a poet and write poetry with ease with help from this clear and simple guide in the popular 101 series.\\n\\nPoetry never goes out of style. An ancient writing form found in civilizations across the world, poetry continues to inform the way we write now, whether we realize it or not—especially in social media—with its focus on brevity and creating the greatest possible impact with the fewest words. Poetry 101 is your companion to the wonderful world of meter and rhyme, and walks you through the basics of poetry. From Shakespeare and Chaucer, to Maya Angelou and Rupi Kaur, you’ll explore the different styles and methods of writing, famous poets, and poetry movements and concepts—and even find inspiration for creating poems of your own.\\n\\nWhether you are looking to better understand the poems you read, or you want to tap into your creative side to write your own, Poetry 101 gives you everything you need!',\n",
              " \"This is an unfinished masterpiece and Tintin's last adventure.\",\n",
              " 'Ever since the Alibaba Group went public on September 19, 2014--with an initial public offering of a record-breaking $25 billion--Jack Ma, the founder and charismatic \"spiritual leader\" of the e-commerce behemoth, has been making headlines around the world. In 2014, the company\\'s online transactions totaled $248 billion--more than those of Amazon and eBay combined. The first Chinese entrepreneur to appear on the cover of Forbes, Ma is the now the second-richest man in China, with a net worth that is estimated to be north of $29 billion. Despite Ma\\'s massive influence in China and in the global tech world, his inspirational rags-to-riches story is relatively unknown to the general American public. Never Give Up: Jack Ma In His Own Words is a comprehensive guide to the inner workings of arguably the most prominent figure in the global tech world in the past 20 years--comprised entirely of Ma\\'s own thought-provoking and candid quotes. When Ma decided to start his first Internet company in 1999, few Chinese people knew what the Internet was. Ma, a former English teacher, knew nothing about coding, and his $20,000 in startup funds were not made up of investments from venture capitalists but loans from his family. He channeled his startup experience into Alibaba, a group of websites that allows businesses and people to connect in order to buy and sell products (similar to eBay and Amazon) while also collecting advertising revenue (similar to Google). By some measures, Alibaba is now the largest e-commerce site in the world. In this book, more than 200 quotes on business values, innovation, entrepreneurship, competition, management, teamwork, life, and more provide an intimate and direct look into the mind of this modern business icon and philanthropist. Many of these quotes are translated directly from the Chinese press and interviews. For readers who do not read Chinese and have no other access to these materials, this book provides invaluable insight into the mind of one of the world\\'s most successful business magnates.',\n",
              " \"Major James Brionne brought Dave Allard to trial for murder. Just before the hanging, Dave swore his brothers would take vengenance. . .Four year later the Allard boys retumed to settle the score. Only Brionne's son escaped. They murdered his wife, destroyed his home, and left Brionne nothing but the charred ruins of his past to haunt him. Seeking peace and a new life, Brionne and the boy headed west. But the Allards hadn't finished with him. He knew they'd call him for a showdown-and this time he'd be ready . . . .\",\n",
              " \"‘An amazing story, and truly inspiring. The kind of book everyone will enjoy. IT’S EVEN BETTER THAN YOU’VE HEARD.’ – Bill Gates\\n\\nSelected as a book of the year by AMAZON, THE TIMES, SUNDAY TIMES, GUARDIAN, NEW YORK TIMES, ECONOMIST, NEW STATESMAN, VOGUE, IRISH TIMES, IRISH EXAMINER and RED MAGAZINE\\nTHE MULTI-MILLION COPY BESTSELLER\\n________________________\\n\\nTara Westover and her family grew up preparing for the End of Days but, according to the government, she didn’t exist. She hadn’t been registered for a birth certificate. She had no school records because she’d never set foot in a classroom, and no medical records because her father didn’t believe in hospitals.\\n\\nAs she grew older, her father became more radical and her brother more violent. At sixteen, Tara knew she had to leave home. In doing so she discovered both the transformative power of education, and the price she had to pay for it.\\n________________________\\n\\n· From one of TIME magazine's 100 most influential people of 2019\\n· Shortlisted for the 2018 BAMB Readers' Awards\\n· Recommended as a summer read by Barack Obama, Antony Beevor, India Knight, Blake Morrison and Nina Stibbe\",\n",
              " 'A teenage murder witness is drowned in a tub of apples…\\n\\nAt a Hallowe’en party, Joyce – a hostile thirteen-year-old – boasts that she once witnessed a murder. When no-one believes her, she storms off home. But within hours her body is found, still in the house, drowned in an apple-bobbing tub.\\n\\nThat night, Hercule Poirot is called in to find the ‘evil presence’. But first he must establish whether he is looking for a murderer or a double-murderer…',\n",
              " 'Alice felt very, very tired. She followed the White Rabbit down a rabbit hole, and met a caterpillar, and the Queen of Hearts! Ladybird Readers is a graded reading series of traditional tales, popular characters, modern stories, and non-fiction, written for young learners of English as a foreign or second language. Beautifully illustrated and carefully written, the series combines the best of Ladybird content with the structured language progression that will help children develop their reading, writing, speaking, listening and critical thinking skills. The five levels of Readers and Activity Books follow the CEFR framework and include language activities that provide preparation for the Cambridge English: Young Learners (YLE) Starters, Movers and Flyers exams. Alice in Wonderland, a Level 4 Reader, is A2 in the CEFR framework and supports YLE Flyers exams. The longer text is made up of sentences with up to three clauses, more complex past and future tense structures, modal verbs and a wider variety of conjunctions.',\n",
              " \"Suewellyn Mateland remembers Mateland Castle as a fascinating, but forbidden fairy palace. Her cousin's unexpected arrival stirs life into old jealousies and new conflicts, but when tragedy strikes, Suewellyn must become mistress of Mateland.\",\n",
              " \"Jack Welch is perhaps the greatest corporate leader of the 20th century. When he first became CEO of General Electric in 1981 the company was worth $12 billion. Twenty years later it is worth a total of $280 billion. But Welch was more than just the leader of the most successful business in the world. He revolutionised GE's entire corporate culture with his distinctive, highly personal management style: the individual appreciation of each of his 500 managers, the commitment to an informal but driven work style and the encouragement of candour were all part of the Welch approach. Following John Harvey Jones's MAKING IT HAPPEN and TROUBLESHOOTER, JACK has already become the businessman's bible for the 21st century - an inspiration for a new generation of corporate players.\",\n",
              " \"Published posthumously in 1964, A Moveable Feast remains one of Ernest Hemingway's most beloved works. Since Hemingway's personal papers were released in 1979, scholars have examined and debated the changes made to the text before publication. Now this new special restored edition presents the original manuscript as the author prepared it to be published.\\n\\nFeaturing a personal foreword by Patrick Hemingway, Ernest's sole surviving son, and an introduction by the editor and grandson of the author, Seán Hemingway, this new edition also includes a number of unfinished, never-before-published Paris sketches revealing experiences that Hemingway had with his son Jack and his first wife, Hadley. Also included are irreverent portraits of other luminaries, such as F. Scott Fitzgerald and Ford Madox Ford, and insightful recollections of his own early experiments with his craft.\\n\\nSure to excite critics and readers alike, the restored edition of A Moveable Feast brilliantly evokes the exuberant mood of Paris after World War I and the unbridled creativity and enthusiasm that Hemingway himself experienced. In the world of letters it is a unique insight into a great literary generation, by one of the best American writers of the twentieth century.\",\n",
              " \"The first volume in the brilliant, bestselling Bartimaeus sequence, now adapted into a stunning graphic novel format - this is Bartimaeus as you've never seen him before!\\nNathaniel, a young magician's apprentice, has revenge on his mind. Desperate to defy his master and take on more challenging spells, he secretly summons the 5000-year-old djinni, Bartimaeus. But Bartimaeus's task is not an easy one - he must steal the powerful Amulet of Samarkand from Simon Lovelace, a master magician of unrivalled ruthlessness and ambition. Before long, Bartimaeus and Nathaniel are caught up in a terrifying flood of intrigue, rebellion and murder.\\nSet in modern-day London controlled by magicians, this brilliant adaptation of Jonathan Stroud's bestselling novel will enthral readers of all ages.\",\n",
              " \"When Jack Mercy died, he left behind a ranch worth nearly twenty million dollars. Now his three daughters - each born of a different mother, and each unknown to the others - are gathered to hear the reading of the will. But the women are shocked to learn that before any of them can inherit, they must live together on the ranch for one year. For Tess, a sophisticated city-girl who just wants to collect her cash and get back to L.A., it's a nightmare. For Lily, on the run from her abusive ex-husband, it's a refuge. And for Willa - who grew up on the ranch - it's an intrusion into her rightful home.\\nThey are sisters and strangers. Now they face a challenge: to put their bitterness aside and live like a family. To protect each other from danger - and unite against a brutal enemy who threatens to destroy them all...\",\n",
              " \"Jason Bourne is back in the forthcoming major motion picture starring Matt Damon and Alicia Vikander. Go back to where it all began for Bourne in his original adventures.\\n'Watch your back 007 - Bourne is out to get you' - Sunday Times\\nIt all starts with a cat-and-mouse chase to the death in a Baltimore funfair: the Jackal, Bourne's age-old antagonist, is back and Bourne is forced from his idyllic retirement with his wife and children to confront his enemy.\\nIn Europe and America there are men and women whose lust for power is disguised by their positions and respectability. Their aim: to gain control at the highest level, to avenge, to destroy.\\nJason Bourne has been the assassin before: now he longs for peace with his family, but the threat of the Jackal puts in jeopardy all possibility of peace...\",\n",
              " \"The first book in the hilarious bestselling WHO LET THE GODS OUT series, shortlisted for the Waterstones Children's Book Prize!\\n\\n'I totally fell in love with Elliot and the gods, and I think you're all going to love them too.' ROBIN STEVENS\\n'One of the funniest new voices in children's literature. The laughs come thick and fast' DAVID SOLOMONS\\n\\nElliot's mum is ill and his home is under threat, but a shooting star crashes to earth and changes his life forever. The star is Virgo - a young Zodiac goddess on a mission. But the pair accidentally release Thanatos, a wicked death daemon imprisoned beneath Stonehenge, and must then turn to the old Olympian gods for help. After centuries of cushy retirement on earth, are Zeus and his crew up to the task of saving the world - and solving Elliot's problems too?\\n\\nIf you loved WHO LET THE GODS OUT?, check out book 2: SIMPLY THE QUEST!\",\n",
              " 'Having served on the United Nations as Canadian Ambassador, besides a host of other diplomatic positions, David Malone will help the reader gain a hitherto unexplored perspective into whether or not India is ready to become one of the five superpowers in the world. Does The Elephant Dance?: Contemporary Indian Foreign Policy takes a detailed and close look at the pros and cons in the current foreign policy that the nation follows, while drawing from various historical instances that the writer feels is still driving its present stand on the world stage. The book will also examine internal issues that are plaguing the country from within, such as domestic politics and internal security issues, and finally internal economic factors.\\nNot only have internal issues been dealt with and examined exhaustively, but the author has also written about cross-cultural issues such as the part migration plays in its foreign policy, India’s relations with Africa, the USA, as well as the Latin Americas, apart from stressing on its role within South Asia.\\nIndia’s role as a soft power has also been examined, and the author also suggests the changes that it needs to make in its current foreign and economic policy and its stand on world issues and international relations with other leading powers.\\nDoes The Elephant Dance?: Contemporary Indian Foreign Policy was published by Oxford University Press, in 2014, and is available in paperback.\\nKey Features:\\nThe author helps the reader gain a different perspective at India, its future prospects at being a superpower.\\nIt will also allow the reader to explore and find out more about the external relations of the world’s democracy with its neighbouring countries and the implications.',\n",
              " 'Gillie Trewlove knows what a stranger’s kindness can mean, having been abandoned on a doorstep as a baby and raised by the woman who found her there. So, when suddenly faced with a soul in need at her door—or the alleyway by her tavern—Gillie doesn’t hesitate. But he’s no infant. He’s a grievously injured, distractingly handsome gentleman who doesn’t belong in Whitechapel, much less recuperating in Gillie’s bed . . .\\nBeing left at the altar is humiliating; being rescued from thugs by a woman—albeit a brave and beautiful one—is the pièce de résistance to the Duke of Thornley’s extraordinarily bad day. After nursing him back from the brink, Gillie agrees to help him comb London’s darker corners for his wayward bride. But every moment together is edged with desire and has Thorne rethinking his choice of wife. Yet Gillie knows the aristocracy would never accept a duchess born in sin. Thorne, however, is determined to prove to her that no obstacle is insurmountable when a duke loves a woman.',\n",
              " \"Raymond Chandler was America's preeminent writer of detective fiction, and this Penguin Modern Classics edition of The Big Sleep and Other Novels collects three of the best novels to feature his hard-drinking, philosophising PI, Philip Marlowe.\\nRaymond Chandler created the fast talking, trouble seeking Californian private eye Philip Marlowe for his first great novel The Big Sleep in 1939. Often imitated but never bettered, it is in Marlowe's long shadow that every fictional detective must stand - and under the influence of Raymond Chandler's addictive prose that every crime author must write. Marlowe's entanglement with the Sternwood family - and an attendant cast of colourful underworld figures - is the background to a story reflecting all the tarnished glitter of the great American Dream. The hard-boiled detective's iconic image burns just as brightly in Farewell My Lovely, on the trail of a missing nightclub crooner. And the inimitable Marlowe is able to prove that trouble really is his business in Raymond Chandler's brilliant epitaph, The Long Goodbye.\\nRaymond Chandler (1888-1959) was born in Chicago. It was during the Depression era that he seriously turned his hand to writing and his first published story appeared in the pulp magazine Black Mask in 1933, followed six years later by his first novel, The Big Sleep, adapted into Howard Hawks' classic 1946 film noir, starring Humphrey Bogart and Lauren Bacall.\\nIf you liked The Big Sleep and Other Novels, you might enjoy Dashiell Hammett's The Thin Man, also available in Penguin Modern Classics.\\n'One of the greatest crime writers, who set standards that others still try to attain'\\nSunday Times\\n'Raymond Chandler invented a new way of talking about America, and America has never looked the same to us since.'\\nPaul Auster, author of The New York Trilogy\\n'Chandler wrote like a slumming angel and invested the sun-blinded streets of Los Angelos with a romantic presence'\\nRoss Macdonald, author of The Drowning Pool\",\n",
              " 'William Goldman\\'s modern fantasy classic is a simple, exceptional story about quests--for riches, revenge, power, and, of course, true love--that\\'s thrilling and timeless. Anyone who lived through the 1980s may find it impossible--inconceivable, even--to equate The Princess Bride with anything other than the sweet, celluloid romance of Westley and Buttercup, but the film is only a fraction of the ingenious storytelling you\\'ll find in these pages. Rich in character and satire, the novel is set in 1941 and framed cleverly as an \"abridged\" retelling of a centuries-old tale set in the fabled country of Florin that\\'s home to \"Beasts of all natures and descriptions. Pain. Death. Brave men. Coward men. Strongest men. Chases. Escapes. Lies. Truths. Passions.\"',\n",
              " 'Batman vs Superman – The Greatest Battles is a compilation of five stories that feature deadly battles between the two of the biggest members of the Justice League. Superman is the Man of Steel, he is fast and invincible. Batman is The Dark Knight, the biggest crime fighter who can lay deadly traps for his enemies. As long as they are fighting against the criminal side by side they are unbeatable. But when the villains cause a rift between them, they turn into enemies and use all the tricks in their power to eliminate each other.\\nFans get a treat when the two superheros, sometimes called Gods, battle with each other. Here is when the reader finds himself in the difficult situation of rooting for one of the two equally beloved Superstars.\\nThis is a compilation of comics to give a taste of the greatness of the two big DC Stars. It is a great read for new and old fans who want to see the best of the Superman vs Batman series. This comic has been issued as a foretaste of the superheros just ahead of the impending release of the 2016 movie. The books have superior artwork and a glossy finish.\\nAbout the author:\\nGeoff Johns, Frank Miller, Jim Lee, Jeff Loeb, Brian Azzarello, Joe and Jack Kelly, Geoff Jones, Scott Snyder, Greg Capullo.\\nThese are the authors of the five stories in the comic book compilation featuring Batman and Superman. They are all acclaimed writers for DC Comics with formidable credits to their name. They have been involved in making the DC Comic stars into intriguing and interesting characters that the fans cannot get enough of.',\n",
              " \"Want to speak Japanese but don't know where to start? This book is for you! Don't waste money buying ten different books when you can learn everything you need in this one book. Don't waste money taking classes at a school when you can teach yourself. With Speak Japanese in 90 Days, all of the prep work is done for you. Each daily lesson will teach you not only what, but how to study. Speak Japanese in 90 Days is a comprehensive self study guide, and teaches everything you need to know for the JLPT N5 (Japanese Language Proficiency Test) as well as most of the grammar needed for the JLPT N4. It can also be used by intermediate students to brush up on grammar and vocabulary. The content includes: How to Study - Tips and tricks on how to study and what to study to learn and retain the language quickly. Pronunciation - An easy and accurate guide for English speakers. Every sentence is written with English pronunciation, Japanese kana, and kanji. Grammar - All essential grammar tested in the JLPT N5 and most of the grammar tested in the JLPT N4. Vocabulary - Over 1000 of the most common Japanese words and phrases. Vocabulary nuances - Detailed explanations of how to use vocabulary correctly, that you can't find in a dictionary or other text books.\",\n",
              " \"This uproarious collection includes three books in one: Garfield beefs up, Garfield gets cooking 'and Garfield eats Crow. He used to be perfect, but now he's even better! Garfield, the fat cat with the super-sized ego, is back in the spotlight, discussing his dim-witted owner jon-evenn pilfering his pants. Some cats chase mice; Garfield prefers to take legal action. When down on the farm, the city Kitty puts up with-and puts down-jonn and his barnyard brother. No wonder Garfield's often in a bad mood. But no matter what, he's always in the mood for food! The Garfield fat cat 3-Pack series collects the Garfield comic-strip compilation books in a new, full-color format. Garfield may have gone through a few changes, but one thing has stayed the same: his enormous appetite for food and fun. So enjoy some super sized laughs with the insatiable cat, because too much fun is never enough!.\",\n",
              " 'Early Readers are stepping stones from picture books to reading books. A blue Early Reader is perfect for sharing and reading together. A red Early Reader is the next step on your reading journey.\\nMiranda is on a desert island - all by herself! Luckily she has some very clever ideas...\\nA full-colour blue Early Reader edition of this classic story, written and illustrated by James Mayhew.',\n",
              " 'Learn how to program in Python while making and breaking ciphers—algorithms used to create and send secret messages! \\n\\nAfter a crash course in Python programming basics, you’ll learn to make, test, and hack programs that encrypt text with classical ciphers like the transposition cipher and Vigenère cipher. You’ll begin with simple programs for the reverse and Caesar ciphers and then work your way up to public key cryptography, the type of encryption used to secure today’s online transactions, including digital signatures, email, and Bitcoin.\\n\\nEach program includes the full code and a line-by-line explanation of how things work. By the end of the book, you’ll have learned how to code in Python and you’ll have the clever programs to prove it!\\n\\nYou’ll also learn how to:\\n\\n- Combine loops, variables, and flow control statements into real working programs\\n- Use dictionary files to instantly detect whether decrypted messages are valid English or gibberish\\n- Create test programs to make sure that your code encrypts and decrypts correctly\\n- Code (and hack!) a working example of the affine cipher, which uses modular arithmetic to encrypt a message\\n- Break ciphers with techniques such as brute-force and frequency analysis\\n\\nThere’s no better way to learn to code than to play with real programs. Cracking Codes with Python makes the learning fun!',\n",
              " 'In 1993, Sonic the Hedgehog sped his way from video games to comic books. Hundreds of isues later, \"the blue blur\" can lay claim to having the most successful comic book career of all video game heroes! Now you can enjoy his earliest comic book adventures with this special edition that reprint his first, difficult-to-find mini-series, Sonic #0 through #3. Collected together for the first time are the first appearance of Tails, Princess Sally, Antoine, Rotor, Uncle Chuck and Muttski. Put on your sneakers, grab a chilidog and see where the legend began!',\n",
              " \"The athletes of the ancient world assemble in Athens for the Olympic Games. Asterix and the Gauls enter too, but they're due for a setback. As an artificial stimulant, magic potion is banned. Can our friends win at the Games without it? And what's the special ingredient of the other potion, the one in the cauldron in the shed with the door that doesn't close properly?\",\n",
              " 'Updated to include the Champions League Final 2018 “It is the intensity of the football, of how the people live football in Liverpool, all the Liverpool fans around the world. It is not a normal club, it is a special club.” Jurgen Klopp As innovative as Arsene Wenger and as crowd-pleasing as Jose Mourinho, Jurgen Klopp is the charismatic German manager who single-handedly overthrew the accepted order in German football, taking Borussia Dortmund from nowhere to back-to-back Bundesliga titles and the Champions League final. He had long been admired in the Premiership and was finally wooed by Liverpool in the belief he could bring back the glory days to the Kop. Klopp is revered as a master tactician with his own unique playing philosophies like counter pressing and spatial geometry. He is loved by his players for his passion and man-management skills and adored by the media and fans alike for his disarming wit and charm and exciting football on the pitch. Here is the definitive story of Jurgen Klopp - the normal one - and his footballing genius.',\n",
              " \"Logic is the backbone of Western civilization, holding together its systems of philosophy, science and law. Yet despite logic's widely acknowledged importance, it remains an unbroken seal for many, due to its heavy use of jargon and mathematical symbolism.This book follows the historical development of logic, explains the symbols and methods involved and explores the philosophical issues surrounding the topic in an easy-to-follow and friendly manner. It will take you through the influence of logic on scientific method and the various sciences from physics to psychology, and will show you why computers and digital technology are just another case of logic in action.\",\n",
              " \"It is said that failure is a stepping stone to success. Gone are the days when you accepted your destiny to succumb to failure and sulked looking at the happy faces around you. How to Fail at Almost Everything and Still Win Big - Kind of The Story of My Life introduces the era of standing up, dusting the failures off your shoulders and walking ahead with your chin up and mind open. Learn how to use your weaknesses to hone your strengths and your failure to build your pathway to success by reading this book.\\nAuthor Scott Adams wittily epitomizes his life as a book of failures, that eventually turned out to be his manual for success. Consistently failing and falling and perennially getting mentally fatigued can often send a man straight up the stairway to hell. But Adams on the other hand, tells us how he exploited each fall to get back up and each pinch of pain to earn a moment of bliss.\\nHe says that it's not about planning organizing and methodically approaching your dream. It's about being dynamic, pragmatic and energetic. for those who are thirsty for motivation, this is the book to read. You might even experience a gush of zeal on reading this book. How to Fail at Almost Everything and Still Win Big - Kind of The Story of My Life was published by Penguin UK in 2013 and is available in paperback.\\nKey Features:\\nAn important aspect about the book is how humor has been used as a tool to motivate and how downfalls have been expressed as strong ladders to success.\",\n",
              " \"An award-winning short story collection from the author of the must-read DAUGHTER OF SMOKE AND BONE trilogy.\\nIn Victorian times, goblins offered sumptuous fruits in exchange for maidens' souls... and were usually successful. But what does it take to tempt today's savvy young women? Discover the answer in GOBLIN FRUIT.\\nIn SPICY LITTLE CURSES a demon and the ambassador to Hell tussle over the soul of a beautiful English girl in India. Matters become complicated when she falls in love and decides to test the curse placed upon her.\\nAnd in HATCHLING, six days before Esme's fourteenth birthday, her left eye turns from brown to blue. She little suspects what the change heralds, but her small safe life begins to unravel at once. What does the beautiful, fanged man want with her, and how is her fate connected to a mysterious race of demons?\",\n",
              " \"A young dog, Buck, grew up as the favourite pet of a loving family, in California. But his life took a sudden turn when he was sold by a rogue servant and was transported to the Far North. The cold, wild land was cruel and heartless; inhabited by greedy gold prospectors, savage wolves and even more savage Indians. Buck became a sled dog and was subjected to a life of immense hardship. He soon realised that he must return to the law of the wild and learn to survive. Buck turned to the ways of his forefathers, using the cunning, toughness and ferocity that lay dormant in him he became the strongest sled dog in Alaska. But can he overcome the urge to run free of man's rule; the urge to respond to the call of the wild?\",\n",
              " 'This leading dictionary - now in its fourth edition - offers wide-ranging and authoritative coverage of the earth sciences and related topics in over 7,500 clear and accessible entries. Coverage includes geology, planetary science, oceanography, palaeontology, mineralogy, and volcanology, as well as climatology, geochemistry, and petrology.\\n\\nThis new edition has been fully updated and 150 new entries added, with expanded coverage of geology and planetary geology terms. Over 130 line drawings accompany the definitions.\\n\\nThe Dictionary also provides recommended web links which are listed and regularly updated on a dedicated companion website. Appendices include a revised geological time scale, an updated bibliography, stratigraphic units, lunar and Martian time scales, wind-strength scales, and SI units. This dictionary is essential for students of geography, geology, and earth sciences, and for those in in related disciplines.',\n",
              " \"Everybody Writes teaches how to write in a clear, accessible, honest voice across all of your marketing assets: blog posts, web pages, marketing offers and social updates. The language of business has become more like the language of real people. Customers have elevated expectations. They don't want to hear marketing messages; they want stories and a sense of the human beings behind the brand. Everybody Writes shows readers how to create compelling content that their audience can understand and how to take advantage of the opportunity to speak directly with the customer with empathy and humanity.\",\n",
              " '\"Tom is the David Ogilvy of cartooning.\"\\n--Seth Godin, author of Purple Cow\\nFrom the birth of social media to digital advertising to personal branding, marketing has transformed in the past 15 years. Capturing these quintessential moments in marketing is Marketoonist, a popular cartoon series from veteran marketer Tom Fishburne. Your Ad Ignored Here collects nearly 200 of these hilarious and apt depictions of modern marketing life on the 15th anniversary of the series.\\nFishburne began to doodle his observations in 2002 when working in the trenches of marketing. Initially intended for co-workers, they are now read by hundreds of thousands of marketers every week. The cartoons\\' popularity stem not only from their deft reflections on latest trends, but their witty summary of the shared experiences of marketing -- handling a PR crisis, giving creative feedback to an agency, or avoiding idea killers in innovation.\\nYour Ad Ignored Here gives voice to the challenges and opportunities faced by people working in business everywhere. Readers regularly inquire if Fishburne is spying on them at work. Whether or not you work in marketing, these cartoons will make you laugh ... and think about our rapidly evolving world of work.\\n\\nTom Fishburne started drawing cartoons on the backs of business cases as a student at Harvard Business School. Fishburne\\'s cartoons have grown by word of mouth to reach hundreds of thousands of marketers every week and have been featured by The Wall Street Journal, Fast Company, and The New York Times. His cartoons have appeared on a billboard ad in Times Square, helped win a Guinness World Record, and turned up in a top-secret NSA presentation released by Edward Snowden.\\nFishburne draws (literally and figuratively) from 20 years in the marketing trenches in the US and Europe. He was Marketing VP at Method Products, Interim CMO at HotelTonight, and worked in brand management for Nestlé and General Mills. Fishburne developed web sites and digital campaigns for interactive agency iXL in the late 90s and started his marketing career selling advertising space for the first English-language magazine in Prague.\\nIn 2010, Fishburne expanded Marketoonist into a marketing agency focused on the unique medium of cartoons. Since 2010, Marketoonist has developed visual content marketing campaigns for businesses such as Google, IBM, Kronos, and LinkedIn.\\nFishburne is a frequent keynote speaker on marketing, innovation, and creativity, using cartoons, case studies, and his marketing career to tell the story visually.\\nFishburne lives and draws near San Francisco with his wife and two daughters.\\nAll of his cartoons and observations are posted at marketoonist.com.\\nAdvance Praise for Your Ad Ignored Here\\n\"If marketing kept a diary, this would be it.\"\\n--Ann Handley, Chief Content Officer of MarketingProfs\\n\"Laugh and learn at the same time. BTW, if you don\\'t laugh, you\\'re clueless, and the cartoon is about you.\"\\n--Guy Kawasaki, Chief evangelist of Canva, Mercedes-Benz brand ambassador\\n\"Tom Fishburne has a knack for marketing humor (and truth) like no other.\"\\n--Lee Odden, CEO, TopRank Marketing\\n\"Any great piece of comedy is funny because its true. Well, no one has gathered marketing truths through painfully awkward insights and hilarious delivery the way Tom has.\"\\n--Ron Tite, Author, Everyone\\'s An Artist (Or At Least They Should Be)',\n",
              " \"Explore the world of Tokyo Ghoul with these prose fiction spin-offs!\\n\\nGhouls live among us, the same as normal people in every way—except their craving for human flesh.\\n\\nKen Kaneki is an ordinary college student until a violent encounter turns him into the first half-human half-ghoul hybrid. Trapped between two worlds, he must survive Ghoul turf wars, learn more about Ghoul society and master his new powers.\\n\\nMany conflicts and incidents arise involving the clientele of the café Anteiku, where Ghouls gather. Yoshimura, the café's owner, is harboring suspicions about a certain someone. What sort of darkness will that person bring between those who hunt and those who are hunted? This book chronicles six all-new stories from the Tokyo Ghoul universe.\",\n",
              " 'Ajmer Sharif, the heart of homage for thousands of all faiths, welcomes a tidal wave of humanity for the saint known as Gharib Nawaz (Benefactor of the Poor), whose spiritual order is the most lyrical and inclusive. This book showcases the absolute necessity of Khwaja Muinuddin Chishti in our conflicted times. Today, he is a signpost of unity; a reminder of the relevance of Sufism as a conduit of harmony and justice. These pages narrate the story of the founder of the Chishti Sufi Order in South Asia—one who knew that hunger supersedes doctrines—and of his movement, which began in a mud hut over eight centuries ago.\\nMuinuddin Chishti brought Qawwali, the genre of Sufi devotional recitals and the ideal of acceptance to the world. He discouraged religious supremacy and patriarchy by example—his wife was a central leader and his only daughter became a caliph—an aspect practiced at his seat, but cast off by other shrines of his lineage.\\nAn elaborately researched oeuvre on the history, spiritual journey, mystical power and legacy of the 12th century Sufi giant—rich with accounts of Mughal devotees, monuments, rituals and over 200 unique, evocative photographs—Ajmer Sharif traces the life of the saint and reveals the living force of his shrine.',\n",
              " '\"It took me a long time and most of the world to learn what I know about love and fate and the choices we make, but the heart of it came to me in an instant, while I was chained to a wall and being tortured.\"\\nSo begins this epic, mesmerizing first novel set in the underworld of contemporary Bombay. Shantaram is narrated by Lin, an escaped convict with a false passport who flees maximum security prison in Australia for the teeming streets of a city where he can disappear.\\nAccompanied by his guide and faithful friend, Prabaker, the two enter Bombay\\'s hidden society of beggars and gangsters, prostitutes and holy men, soldiers and actors, and Indians and exiles from other countries, who seek in this remarkable place what they cannot find elsewhere.\\nAs a hunted man without a home, family, or identity, Lin searches for love and meaning while running a clinic in one of the city\\'s poorest slums, and serving his apprenticeship in the dark arts of the Bombay mafia. The search leads him to war, prison torture, murder, and a series of enigmatic and bloody betrayals. The keys to unlock the mysteries and intrigues that bind Lin are held by two people. The first is Khader Khan: mafia godfather, criminal-philosopher-saint, and mentor to Lin in the underworld of the Golden City. The second is Karla: elusive, dangerous, and beautiful, whose passions are driven by secrets that torment her and yet give her a terrible power.\\nBurning slums and five-star hotels, romantic love and prison agonies, criminal wars and Bollywood films, spiritual gurus and mujaheddin guerrillas---this huge novel has the world of human experience in its reach, and a passionate love for India at its heart. Based on the life of the author, it is by any measure the debut of an extraordinary voice in literature.',\n",
              " '‘In the first NC meeting after AAP’s creation, Arvind had said: “This party is not the property of 300 founding members but of the lakhs and crores of people in this country.” This refreshing stance shifted over time, got corrupted by power . . . till, one day, Arvind told me: I do not want intellectuals in the party, just people who say “Bharat Mata ki Jai”.’\\nAuthored by a former member of the Aam Aadmi Party’s (AAP) National Executive, AAP and Down is an in-depth account of the emergence and sudden unspooling of one of India’s most closely watched parties.\\nThe story of AAP is one of troughs and crests. After capturing the imagination of over a billion Indians and winning a landslide victory in the 2015 Delhi elections, a seemingly indestructible party began to dangerously teeter. What just happened? How did a party—born of the idealistic India Against Corruption (IAC) movement—get ravaged by in-fighting and accusations of wrongdoing? What provoked the abrupt ouster of two party veterans, Yogendra Yadav and Prashant Bhushan? What accounted for the wave of ignoble defeats across not just Punjab and Goa but also AAP’s own home, Delhi? Here is a book that reveals all—from the clashes and intrigues that beset the IAC movement to the goings-on during the closed-door meetings of AAP. But beyond chronicling events, thus far undisclosed, AAP and Down analyzes the dispositions of the leaders who had once promised a better India—from a volatile Anna Hazare to an autocratic Arvind Kejriwal—to highlight how the party’s undoing was linked to the flaws of its leading men.\\nEven while recounting the true story of a party, here is a book that presents the story of India—of how a country, plagued by scams and scandals, dared to unite under IAC and overthrow the corrupt. In this, there is a lesson for AAP—the book emphatically reminds the party that its best chance of revival lies in reinvesting faith in this nation’s citizens.',\n",
              " \"Completely revised and updated throughout, Bryan Peterson's classic guide to creativity helps photographers visualize their work, and the world, in a whole new light by developing their photographic vision.\\n\\nFully revised with all new photography, this best-selling guide takes a radical approach to creativity by explaining that it is not an inherent ability but a skill that can be learned and applied. Using inventive photos from his own stunning portfolio, author and veteran photographer Bryan Peterson deconstructs creativity for photographers. He details the basic techniques that go into not only taking a particular photo, but also provides insights on how to improve upon it--helping readers avoid the visual pitfalls and technical dead ends that can lead to dull, uninventive photographs.\\nThis revised edition features a complete section on color as a design element and all new photographs to illustrate Peterson's points. Learning to See Creatively is the definitive reference for any photographer looking for a fresh perspective on their work.\",\n",
              " \"Chloe needs a holiday. She's sick of making wedding dresses and her partner is having trouble at work. Her wealthy friend Gerard has offered the loan of his luxury villa in Spain - perfect.\\n\\nHugh is not a happy man. His immaculate wife seems more interested in the granite for the new kitchen than in him, and he works so hard to pay for it all, he barely has time to see their children. But his old schoolfriend Gerard has lent them a luxury villa in Spain - perfect.\\n\\nBoth families arrive at the villa and get a shock: Gerard has double-booked. An uneasy week of sharing begins, and tensions soon mount in the soaring heat. But there's also a secret history between the families - and as tempers fray, an old passion begins to resurface...\",\n",
              " 'Inspired by true events, this riveting narrative traces the lives of Safeena Malik, Deewan Bhat and Bilal Ahanagar, three childhood friends who grow up in an atmosphere of peace and amity in Srinagar, Kashmir, until the night of 20 January 1990 changes it all.\\n\\nWhile Deewan is forced to flee from his home, Safeena’s mother becomes ‘collateral damage’ and Bilal has to embrace a wretched life of poverty and fear. The place they called paradise becomes a battleground and their friendship struggles when fate forces them to choose sides against their will.\\n\\nTwenty years later destiny brings them to a crossroads again, when they no longer know what is right and what is wrong. While both compassion and injustice have the power to transform lives, will the three friends now choose to become sinful criminals or pacifist saints?The Tree with a Thousand Apples is a universal story of cultures, belongingness, revenge and atonement. The stylized layered format, fast-paced narration and suspenseful storytelling makes for a powerful, gripping read.',\n",
              " \"A year has passed since Eden last spoke to Tyler. She remains furious at him for his abrupt departure last summer but has done her best to move on with her life at college in Chicago. As school breaks up for the holidays, she's heading back to Santa Monica, but she's not the only one who decides to come home... Eden has no time for Tyler when he reappears. But where has Tyler been? And is she as over him as she likes to think? Did I Mention I Miss You? is the explosive finale to the bestselling DIMILY trilogy concluding Eden and Tyler's addictive love story.\",\n",
              " \"This newly revised edition of Bryan Peterson's most popular book demystifies the complex concepts of exposure in photography, allowing readers to capture the images they want.\\n\\nUnderstanding Exposure has taught generations of photographers how to shoot the images they want by demystifying the complex concepts of exposure in photography. In this newly updated edition, veteran photographer Bryan Peterson explains the fundamentals of light, aperture, and shutter speed and how they interact with and influence one another. With an emphasis on finding the right exposure even in tricky situations, Understanding Exposure shows you how to get (or lose) sharpness and contrast in images, freeze action, and take the best meter readings, while also exploring filters, flash, and light.\\n\\nWith all new images, as well as an expanded section on flash, tips for using colored gels, and advice on shooting star trails, this revised edition will clarify exposure for photographers of all levels.\",\n",
              " 'This book is presented with a paninian perspective [Panini 500 B was a grammarian and well known for his contribution to the grammar and structure of the language.] It introduces three western grammar frameworks using examples from English.',\n",
              " \"Half Boy. Half God. ALL Hero.\\nIt's not every day you find yourself in combat with a half-lion, half-human.\\n\\nBut when you're the son of a Greek God, it happens. And now my friend Annabeth is missing, a Goddess is in chains and only five half-blood heroes can join the quest to defeat the doomsday monster.\\nOh and guess what. The Oracle has predicted that not all of us will survive . . .\",\n",
              " 'The field of data mining provides techniques for automated discovery of valuable information from the accumulated data of computerized operations of enterprises. This book offers a clear and comprehensive introduction to both data mining theory and practice. It is written primarily as a textbook for the students of computer science, management, computer applications and information technology. The book ensures that the students learn the major data mining techniques even if they do not have a strong mathematical background. The techniques include data preprocessing, association rule mining, supervised classification, cluster analysis, web data mining, search engine query mining, data warehousing and OLAP. To enhance the understanding of the concepts introduced and to show how the techniques described in the book are used in practice, each chapter is followed by one or two case studies that have been published in scholarly journals. Most case studies deal with real business problems (for example, marketing, ecommerce, CRM). Studying the case studies provides the reader with a greater insight into the data mining techniques. The book also provides many examples, review questions, multiple choice questions, chaptered exercises and a good list of references and Web resources especially those which are easy to understand and useful for students. A number of class projects have also been included.\\nTable of Contents\\nIntroduction\\nData Understanding and Data Preparation\\nAssociation Rules Mining\\nClassification\\nCluster Analysis\\nWeb Data Mining\\nSearch Engines and Query Mining\\nData Warehousing\\nOnline Analytical Processing (Olap)\\nInformation Privacy and Data Mining',\n",
              " \"  Howl with laughter with the FIFTH book in the hilarious full-colour, illustrated series, Dog Man, from the creator of Captain Underpants!\\nWhen a new bunch of baddies bust up the town, Dog Man is called into action -- and this time he isn't alone.\\nWith a cute kitten and a remarkable robot by his side, our heroes must save the day by joining forces with an unlikely ally: Petey, the World's Most Evil Cat.\\nBut can the villainous Petey avoid vengeance and venture into virtue?\\nDav Pilkey's wildly popular Dog Man series appeals to readers of all ages and explores universally positive themes, including:\\nempathy,\\nkindness,\\npersistence,\\nand the importance of being true to one's self.\\nFull colour pages throughout.\",\n",
              " 'THE COLOSSAL EPIC BEGINS!\\nThis volume includes Vol. 1-5 of Attack on Titan in an extra-large 7x10.5-inch size, with 15 COLOR PAGES never published in any other book - not even in Japan!\\n\\n\\n\\nThe megahit Attack on Titan anime finally returns, streaming on April 1!For eons, humans ruled the natural world. But a century ago, everything changed when the Titans appeared. Giant, grotesque parodies of the human form, these sexless monsters consumed all but a few thousand human beings, who took refuge behind giant walls. Today, the threat of the Titans is a distant memory, and a boy named Eren yearns to explore the world beyond the wall. But what began as a childish dream will become an all-too-real nightmare when a Titan finally knocks a hole in the wall, and humanity is once again on the brink of extinction.',\n",
              " 'Damon Gavros: commanding, Greek…and father of her child!\\n\\nWhen Damon Gavros steps into Lizzie Montgomery’s workplace, their searing desire cuts through the heat of the kitchen. Instantly, she’s swept back eleven years to the one exquisite night they shared! He may be the reason she once lost everything, but the irresistible connection between them blazes hotter than ever. Only there’s one thing Damon doesn’t know about Lizzie…yet.\\n\\nDamon is sure Lizzie is hiding something and he’s determined to discover what. From London to Greece, his pursuit is relentless, until he finds out Lizzie’s secret has a name… Thea – and she’s his daughter!',\n",
              " \"Discover the world's greatest works of art with this ingenious puzzle book from the creator of the best-selling 1000 Dot-to-Dot series, Thomas Pavitte. At first, you see nothing but a baffling tangle of circles - but hidden within each puzzle is a legendary masterpiece waiting to be revealed.\\nFeaturing 20 iconic artworks from the Venus de Milo to Klimt's The Kiss, Querkles Masterpieces offers hours of creative colouring fun for artistic minds of all ages. Choose five colours, enjoy the surprising results as your unique masterpiece comes to life, and you'll be rewarded with a striking image that you can easily remove and display.\",\n",
              " \"Newbery Medal-winning author Beverly Cleary expertly depicts the trials and triumphs of growing up through a relatable heroine who isn't afraid to be exactly who she is.\\nRamona Quimby is excited to start kindergarten. No longer does she have to watch her older sister, Beezus, ride the bus to school with all the big kids. She's finally old enough to take the bus too!\\nThen she gets into trouble for pulling her classmate's boingy curls during recess. Even worse, her crush rejects her in front of everyone. Beezus says Ramona needs to quit being a pest, but how can she stop if she never was trying to be one in the first place?\",\n",
              " \"'Riveting! Bussi spins psychological suspense at its finest with this consuming tale of one child, two families, and the dark secrets that define us all. Clear your schedule; this book is worth it!' - Lisa Gardner, #1 New York Times-bestselling author of Crash & Burn and Find Her\\nOn the night of 22 December 1980, a plane crashes on the Franco-Swiss border and is engulfed in flames. 168 out of 169 passengers are killed instantly. The miraculous sole survivor is a three-month-old baby girl. Two families, one rich, the other poor, step forward to claim her, sparking an investigation that will last for almost two decades. Is she Lyse-Rose or Emilie?\\nEighteen years later, having failed to discover the truth, private detective Crédule Grand-Duc plans to take his own life, but not before placing an account of his investigation in the girl's hands. But, as he sits at his desk about to pull the trigger, he uncovers a secret that changes everything - then is killed before he can breathe a word of it to anyone . . .\",\n",
              " \"SUNDAY TIMES No.1 bestseller Ian Rankin returns with his gripping new Rebus novel.\\nRebus is back on the force, albeit with a demotion and a chip on his shoulder. A 30-year-old case is being reopened, and Rebus's team from back then is suspected of foul play. With Malcolm Fox as the investigating officer, are the past and present about to collide in a shocking and murderous fashion? And does Rebus have anything to hide? His old colleagues called themselves 'the Saints', and swore a bond on something called 'the Shadow Bible'. But times have changed and the crimes of the past may not stay hidden much longer, especially with a referendum on Scottish independence just around the corner.\\nWho are the saints and who are the sinners? And can the one ever become the other?\",\n",
              " \"What can Roger Federer teach us about the secret of longevity?\\nWhat do the All Blacks have in common with improvised jazz musicians?\\nWhat can cognitive neuroscientists tell us about what happens to the brains of sportspeople when they perform?\\nAnd why did Johan Cruyff believe that beauty was more important than winning?\\n\\nMatthew Syed, the 'Sports Journalist of the Year 2016', answers these questions and more in a fascinating, wide-ranging and provocative book about the mental game of sport.\\nHow do we become the best that we can be, as individuals, teams and as organisations? Sport, with its innate sense of drama, its competitive edge, its psychological pressures, its sense of morality and its illusive quest for perfection, provides the answers.\",\n",
              " 'Kartik fell in love with Ashima the very first time he saw her. She was everything he had ever imagined in his dream girl – his angel. As their friendship blossomed into deep love, culminating into marriage, he became her teddy, her confidant and an eternal support. But in trying to be with her, Kartik made a choice that broke his father’s heart and hopes. As Kartik and Ashima gear up to step into the next Phase of their relationship, life seems like an overload of joy and love. He is confident his love for Ashima will win over all odds, even his father. Little did he know that things were going to change drastically, forever. Why does Ashima marry someone else? Why does Kartik accept it silently? And why is life so unpredictable? This is a story of deep love that knows no bounds, relationships that break all barriers, and a promise – I am always here with you.',\n",
              " 'english literature: its history and its significance for the life of the englishspeaking world is an allencompassing study of english literature from anglosaxon times to the twentieth century. it interprets literature both biographically and historically by showing how each work reflects not just the writer’s life and thoughts but also the spirit of the age and the nation’s history. it also shows the development of the english literary canon in terms of complexity, from the earlier simple moral allegories to the complex issues addressed in twentiethcentury writings. along with works of literature, this book also analyses important events and social conditions in each period, the lives of important authors, as well as literary epochs.\\nwith helpful summaries, bibliographies and tables of chronological events, this book is the best possible guide for students to understand the vast pantheon of english literature.',\n",
              " \"‘The dream team delivers big time … Clinton’s insider secrets and Patterson’s storytelling genius make this the political thriller of the decade.’ – Lee Child\\n________________________________\\n\\nThe President is Missing. The world is in shock.\\n\\nBut the reason he’s missing is much worse than anyone can imagine.\\n\\nWith details only a President could know, and the kind of suspense only James Patterson can deliver.\\n________________________________\\n\\n‘A bullet train of a thriller. The Day of the Jackal for the twenty-first century.’ – A.J. Finn, author of The Woman in the Window\\n\\n'This book moves like Air Force One. Big and fast.' – Michael Connelly, author of the Harry Bosch series\\n\\n'The President Is Missing is a big, splashy juggernaut of a novel, combining thrills with a truly authentic look at the inner happenings in Washington. I read it in one gulp. You will too.' – Harlan Coben, #1 bestselling author of Don’t Let Go\\n\\n‘Yes, The President is Missing is fiction – it’s a thriller – but James Patterson and I have come up with three of the most frightening days in the history of the presidency. And it could really happen... These days, the seemingly impossible can happen. And it happens so fast. I believe that readers will not soon forget President Jonathan Duncan and his story.’ – Bill Clinton\\n\\n‘Needless to say, we had some great conversations about the presidency, what life in Washington is really like, and about the state of America and the rest of the world.’ – James Patterson\\n\\n‘I felt like I was right along with the characters for the shocking twists and turns you’ll never guess are coming. Patterson and Clinton have created a truly unique character in their ‘missing’ President. The President is Missing puts the listener inside the White House and inside the mind of a President grappling with extraordinary circumstances.’ – Dennis Quaid on the audiobook of The President is Missing\\n\\n'Relentless in its plotting and honest in its examination of issues that strike close to our hearts.' – Jeffery Deaver\\n\\n'A first-rate collaboration from a couple of real pros! Engrossing from page one.' – David Baldacci\\n\\n'vivid, engrossing – and authentically frightening.' – Carl Hiaasen, author of Razor Girl\\n\\n'The President is Missing is more than a thriller – it's a skeleton key that lets you inside the head of a U.S. president.' – Brad Meltzer, author of The Escape Artist\\n\\n'From the pens of two American icons comes a political thriller that rocks... a helluva story.' – Nelson DeMille, author of The Cuban Affair\\n\\n'The President Is Missing is heart-pounding, gripping, terrifying... a genuinely masterful thriller. My knuckles are still white!' – Louise Penny, #1 bestselling author of Glass Houses\\n\\n'Marry the political savvy of Bill Clinton with the craftsmanship of James Patterson and you get a fabulously entertaining thriller that is meticulous in its portrayal of Washington politics, gripping in its pacing, and harrowing in its depiction of the perils of cyberwarfare.' – Ron Chernow, #1 bestselling and Pulitzer Prize-winning author of Alexander Hamilton and Grant\\n\\n'Compelling from page one. A fantastic read!' – Mary Higgins Clark, #1 bestselling author of I’ve Got My Eyes on You\",\n",
              " \"The Bridge on the River Kwai tells the story of three POWs who endure the hell of the Japanese camps on the Burma-Siam railway - Colonel Nicholson, a man prepared to sacrifice his life but not his dignity; Major Warden, a modest hero, saboteur and deadly killer; Commander Shears, who escaped from hell but was sent back.\\n\\nOrdered by the Japanese to build a bridge, the Colonel refuses, as it is against regulations for officers to work with other ranks. The Japanese give way but, to prove a point of British superiority, construction of the bridge goes ahead - at great cost to the men under Nicholson's command.\",\n",
              " '\"The following work is devoted to an account of the characteristics of crowds. The whole of the common characteristics with which heredity endows the individuals of a race constitute the genius of the race. When, however, a certain number of these individuals are gathered together in a crowd for purposes of action, observation proves that, from the mere fact of their being assembled, there result certain new psychological characteristics, which are added to the racial characteristics and differ from them at times to a very considerable degree.\"-From the Preface to \"The Crowd: A Study of the Popular Mind\", a pivotal work in the field of group psychology which was written in 1895 by French social psychologist, Gustave Le Bon.',\n",
              " 'In his bestselling work of popular science, Sir Roger Penrose takes us on a fascinating tour through the basic principles of physics, cosmology, mathematics and philosophy to show that human thinking can never be emulated by a machine.',\n",
              " 'In this readily accessible book Bardhan examines the political and social constraints on Indian development. In the newly added epilogue Bardhan comments on the process of liberalization in the 1990 s and examines the feasibility of the exercise in the light of ground realities. This ambitious and controversial book is essential reading for students of economics, politics and the general interested reader.',\n",
              " 'A searing indictment of the suspension of democracy In June 1975, a state of Emergency was declared, where civil liberties were suspended and the press muzzled. In the dark days that followed, Coomi Kapoor, then a young journalist, personally experienced the full fury of the establishment. Meanwhile, Indira Gandhi, her son Sanjay and his coterie unleashed a reign of terror that saw forced sterilizations, brutal evictions in the thousands, and wanton imprisonment of many, including Opposition leaders. This gripping eyewitness account vividly recreates the drama, the horror, as well as the heroism of a few during those nineteen months when democracy was derailed.',\n",
              " \"Marie Kondo presents the fictional story of Chiaki, a young woman in Tokyo who struggles with a cluttered apartment, messy love life, and lack of direction. After receiving a complaint from her attractive next-door neighbor about the sad state of her balcony, Chiaki gets Kondo to take her on as a client. Through a series of entertaining and insightful lessons, Kondo helps Chiaki get her home--and life--in order. This insightful case study of the KonMari Method in action will appeal to both manga and graphic novel lovers, as well as fans of Kondo's life-changing method.\",\n",
              " '70,000 years ago, the human race almost went extinct. We survived, but no one knows how. Now the next stage of human evolution is beginning. Will we survive this time? Geneticist Kate Warner and counter-terrorism agent David Vale have prevented a fierce plague from wiping out humanity - but the struggle to survive is far from over. The Atlantis World stretches deep into space and time, harbouring an enemy greater than anyone had imagined. Now Kate and David must race through galaxies, past space stations, and into the past of a mysterious culture whose secrets could save humanity in its darkest hour. This is the blockbusting final instalment in the Origin Mysteries.',\n",
              " 'Girl in the Woods is Aspen Matis’s exhilarating true-life adventure of hiking from Mexico to Canada—a coming-of-age story, a survival story, and a triumphant story of overcoming emotional devastation. On her second night of college, Aspen was raped by a fellow student. Overprotected by her parents who discouraged her from speaking of the attack, Aspen was confused and ashamed. Dealing with a problem that has sadly become all too common on college campuses around the country, she stumbled through her first semester—a challenging time made even harder by the coldness of her college’s “conflict mediation” process. Her desperation growing, she made a bold decision: She would seek healing in the freedom of the wild, on the 2,650-mile Pacific Crest Trail leading from Mexico to Canada.\\nIn this inspiring memoir, Aspen chronicles her journey, a five-month trek that was ambitious, dangerous, and transformative. A nineteen-year-old girl alone and lost, she conquered desolate mountain passes and met rattlesnakes, bears, and fellow desert pilgrims. Exhausted after each thirty-mile day, at times on the verge of starvation, Aspen was forced to confront her numbness, coming to terms with the sexual assault and her parents’ disappointing reaction. On the trail she found her strength, and after a thousand miles of solitude, she found a man who helped her learn to love and trust again—and heal.',\n",
              " '\"Much of the data available today is unstructured and text-heavy, making it challenging for analysts to apply their usual data wrangling and visualization tools. with this practical book, youíll explore text-mining techniques with tidytext, a package that authors Julia Silge and David Robinson developed using the tidy principles behind R packages like ggraph and dplyr. Youíll learn how tidytext and other tidy tools in R can make text analysis easier and more effective.\\nThe authors demonstrate how treating text as data frames enables you to manipulate, summarize and visualize characteristics of text. Youíll also learn how to integrate natural language processing (NLP) into effective workflows. Practical code examples and data explorations will help you generate real insights from literature, news and social media.\\n\\nLearn how to apply the tidy text format to NLP\\nUse sentiment analysis to mine the emotional content of text\\nIdentify a documentís most important terms with frequency measurements\\nExplore relationships and connections between words with the ggraph and widyr packages\\nConvert back and forth between Rís tidy and non-tidy text formats\\nUse topic modeling to classify document collections into natural groups\\nExamine case studies that compare Twitter archives, dig into NASA metadata and analyze thousands of Usenet messages\\n\"',\n",
              " 'A brilliant pocket-sized handbook on Hindu mythology penned in English by an Indian author Devdutt Pattanaik. The book ‘Myth = Mithya : Decoding Hindu Mythology’ is fabulously conceptualized a story based on Shiva and Shankara. The book has also decoded the Phallic Symbol and also demonstrates the story about the man who was a woman and other queer story from Hindu lore.\\nThe story has a lucid and provocative introduction named Ancient Hindu seers knew myth as mithya. Pattanaik has explained that it would be arrogant to presume that the ancients actually assumed in \"virgin births, flying horses, talking serpents, gods with six heads and demons with eight arms\". These are symbolic presentations of the thoughts of truths that need to be conveyed. The concepts in the book are inspired by the revolutionary art, establishment of empires etc. The book is segmented into three parts decanted into the Brahma-Saraswati, Vishnu-Lakshmi and Shiva-Shakti. It features these gods or goddesses in simple and attractive prose and the reference has been taken from the Vedas and Puranas. Indeed it\\'s an amazing collection for reader. The book is available on Amazon India.\\nAbout the Author:\\nDevdutt Pattanaik is an excellent Indian author, mythologist and leadership consultant. The author has published many tantalizing story on mythology over the years. He has also composed Jaya: An Illustrated Retelling of the Mahabharata and The Pregnant King.',\n",
              " \"Picked out from the life of popular actor Emraan Hashmi, 'The Kiss Of Life' is an autobiographical account of being shaken up in life when career and family was all going on well. It begins around the time Hashmi's acting career, after a long struggle, had stabilized and was on a high, when four year old son Ayaan was taken ill.\\nThe harsh world of Bollywood is unsparing for lost opportunities and only acting skills backed by a lot of hard work had made the actor cut a successful acting career. A regular diagnostic scan on the young boy shattered Hashmi and his wife Praveen, when the doctors identified that a tennis ball sized tumour had invaded Ayaan's kidney.\\nThe book's chapters constantly shuttle between the past and the present. On one account the story records the trails and trials of Hashmi’s rise as an actor and the second on, running in parallel chapters, reveals a caring fathers love for a child battling cancer.\\nThe ups and downs at the beginning of his career, his doubts about being a successful actor, battling against the typecast of being a serial killer, presented alongside Hashmi’s constant delirium when his child would be given a chemotherapy dose, the claustrophobic hospital ward scenario's that slowly do shatter ones faith make a contrast in the storyline.\\nVivid memories of his childhood and his growth from a confused teenager to an unorthodox and successful actor is also brought to life in this book.\\nDuring the child's illness, Hashmi did much research on cancer and the book records a lot of research material on the dreaded subject, right from elaborating on the guidelines for cancer patients and their caretakers, chalking out nutritional plans to the elaborate hospital procedures that one has to undergo, the book has them all.\\nMore than the actor, the book reveals the real Emraan Hashmi. Away from all lights, camera, action scenes, he is just a caring father whose life was shattered when his son was diagnosed with cancer and who left no leaf unturned to get back his son.\\nPublished by Penguin, The Kiss Of Life, an acclaimed book has been written in clear words and is an honest and personal account of a father.\\nAbout the Authors:\\nFamous Bollywood actor, Emraan Hashmi’s first film was Murder which made him one of the most sought-after actors in Bollywood and bestowed him with the title of serial kisser. Life was shaken when he and his wife Praveen learnt about Ayaan, their four year old son, being diagnosed with cancer. Bilal Siddiqui, co-author of this book, is a 21 year old man working as a screenplay writer with Red Chillies Entertainment. Siddiqui has earlier written a novel 'The Bard of Blood', which is set in Balochistan. He paired up with Hashmi to recount an authentic journey through stardom and a doting father's traumatic experience of caring for a cancer stricken son.\",\n",
              " \"About The Book\\nJustice is a detailed piece of work on political philosophy that was originally created to accompany the author's famous course. The course was also called Justice and he conducted it at Harvard University. The book has been written to address a long list of alternative theories on the topic of justice. It is a book for various readers of all ages, focusing on moral reflection. Its purpose is to throw light on various subjects that pertain to justice, from various points of view.\\nIs lying wrong in every situation, or are there exceptions? Should personal freedom have its limits? Could it ever be justified if you have to kill someone? Could the free market be considered as a fair one? What would be the best thing for anyone to do? Questions such as these have plagued human minds over the years. The book aims to answer these questions, using philosophy and politics. He introduces his readers to a long list of controversial topics such as euthanasia, democracy, equal rights, abortions and more similar issues.\\nJeremy Bentham's utilitarianism, John Stuart Mill's refinement, Immanual Kant's Categorical Imperative, Aristotle's concept of ‘telos', John Rawl's works and several other conflicting theories have been explained by the author. He aims to give his readers a clear understanding of topics pertaining to politics and philosophy.\\nJustice: What's The Right Thing To Do? was published by Penguin, in the year 2010 and is available in paperback.\\nKey Features\\nThe book speaks extensively on a variety of philosophical and political theories, attempting to give its readers an insight into these topics.\\nIt addresses a variety of modern and controversial issues that most people might struggle to form an opinion about.\",\n",
              " \"Mister Varg is a Sandinavian Blanc novel. Scandinavian Blanc is different from Scandinavian Noir: there is nothing noir about the world of Ulf Varg, a detective in the Sensitive Crimes Department in the Swedish city of Malmo. Ulf is concerned with very odd, but not too threatening crimes - injuries to the back of the knee caused by an unknown hand, young women who allow their desperation for a boyfriend to get the better of them, and peculiar goings-on in a spa on Sweden's south coast.\\nOf course, Ulf is a Swedish detective, and Swedish detectives, by convention, lead lives beset with problems of one sort or another. For a start, there is his name: Ulf derives from the Old Norse word for wolf and Varg means wolf in modern Swedish. But his character is far from vulpine: Ulf is a sympathetic, well-educated, and likeable man, with a knowledge of and interest in Nordic art. He has a dog called Marten, the only dog in Sweden who is capable of lip-reading (but only in Swedish). Martin becomes depressed and needs treatment. Dogs in Sweden are, apparently, particularly prone to Seasonal Affective Disorder. But this is summer - and there must be something else going on.\\nUlf has a number of colleagues into whose lives we gain an insight. There is Anna, married to an anaesthetist, but very fond of Ulf; there is Erik, whose sole interest is fishing; Carl, whose father has written a book on the Danish philosopher, Kierkegaard; and there is poor Blomquist, from the uniformed branch, who goes on and on about health issues but who seems to have extraordinary luck in investigations. There is also Ulf's psychotherapist, Dr Svensson, whose observations on Ulf's life - and many other topics - enlightens - or possibly confuses.\\nMister Varg introduces us to the world of this typically Scandinavian character and his friends and colleagues. Further adventures are planned.\",\n",
              " \"'A real page-turner . . . captivating and deeply moving' Climb magazine\\nIn 2015 freeclimber Tommy Caldwell spent 19 days summiting Yosemite's vertical, 3000-foot Dawn Wall - the hardest climb in history.\\nIt was the culmination of seven years planning and a lifetime's determination.\\nHere, he recounts how he got there, the falls and set backs (being held hostage, losing his index finger, the break-up of his marriage), the summits conquered and the fears overcome. It is a story about drive, focus and how to achieve the impossible - one toehold at a time.\\n'Caldwell's story is one of the best. You get more than just a climbing adventure, you get the inside view of how a person can endure crushing setbacks and persist to fulfill a spectacular vision' Jim Collins, author of Good to Great\\n\\n'Heart-stopping, absorbing' Daily Mail\\n\\n'Captivating and unfailingly honest' Jon Krakauer\\n\\n'This isn't just a book about climbing, it's about laser sharp focus in all aspects of life' Scott Jurek, author of Eat & Run\\n\\n'Absolutely captivating, thrills, enriches' Denver Post\",\n",
              " 'The inspiring true story of Lopez Lomong, a Sudanese \"lost boy\" who achieved his dream of becoming an American citizen and Olympic athlete. He was abducted. He was beaten. And he was nearly forced to become a boy solider in his war-torn homeland, Sudan. But he escaped in the night, ran three days and was taken into a refugee camp in Kenya. He never owned a pair of shoes. He never owned a pen or paper and did schoolwork in the dust with his fingertips. His boyhood was the daily struggle of an orphan and each day he would run an eighteen-mile lap around the refugee camp just to play a game of soccer. In his wildest dreams, Lopez Lomong couldn\\'t even conceive that Nike would one day be his official sponsor, that he would graduate from college and that he would represent his new home and bear the American flag in the Summer Olympics.\\nRunning for My Life is Lopez Lomong\\'s harrowing story of loss, overcoming, triumph and redemption. It is the once-in-a-lifetime story of a Sudanese lost boy who became an American citizen and Olympic athlete. His life is a powerful picture of the fact that we can overcome, that what seems out of reach is within our grasp if we\\'d believe and if we\\'d only try.\\nA story of faith and a story that captures the best of humanity, Running for My Life will arrest the hearts and inspire the hopes of readers everywhere.',\n",
              " 'Can there ever be a happy ending for star-crossed lovers Melodrama and Histionix, whose fathers are rival chieftains of the same village? The only hope is to call in Asterix, Obelix and Getafix to sort out the feud, the intriguing of the sinister traitor Codfix, and the military might of Rome. Watch out for some interesting new magic potions...',\n",
              " 'A #1 New York Times bestseller and the first novel in a brand-new series—from bestselling author Abbi Glines—about a small Southern town filled with cute boys in pickup trucks, Friday night football games, and crazy parties that stir up some major drama.\\n\\nTo everyone who knows him, West Ashby has always been that guy: the cocky, popular, way-too-handsome-for-his-own-good football god who led Lawton High to the state championships. But while West may be Big Man on Campus on the outside, on the inside he’s battling the grief that comes with watching his father slowly die of cancer.\\n\\nTwo years ago, Maggie Carleton’s life fell apart when her father murdered her mother. And after she told the police what happened, she stopped speaking and hasn’t spoken since. Even the move to Lawton, Alabama, couldn’t draw Maggie back out. So she stayed quiet, keeping her sorrow and her fractured heart hidden away.\\n\\nAs West’s pain becomes too much to handle, he knows he needs to talk to someone about his father—so in the dark shadows of a post-game party, he opens up to the one girl who he knows won’t tell anyone else.\\n\\nWest expected that talking about his dad would bring some relief, or at least a flood of emotions he couldn’t control. But he never expected the quiet new girl to reply, to reveal a pain even deeper than his own—or for them to form a connection so strong that he couldn’t ever let her go…',\n",
              " \"'Outstanding in every way' Lee Child\\n'The page turned of the season' The Times\\n____________\\n'Do you need my help?'\\nIt was the first question he asked.\\nThey called him when they had nowhere else to turn.\\nAs a boy Evan Smoak was taken from an orphanage.\\nRaised and trained in a top secret programme, he was sent to bad places to do things the government denied ever happened.\\nThen he broke with the programme, using what he'd learned to vanish. Now he helps the desperate and deserving.\\nBut someone's on his trail.\\n\\nSomeone who knows his past and believes that the boy once known as Orphan X must die . . .\\n____________\\n\\n'Read this book. You will thank me later' David Baldacci\\n'A rival to reacher' The Independent\\n\\nIf you loved Orphan X, read the gripping follow-up The Nowhere Man and brand new sequel Out of the Dark!\",\n",
              " \"Saffron terrorism.\\nIs it a fact? Or, is this a myth? After all, do we know enough?\\nThe shocking blasts of Malegaon and Samjhauta were projected as 'saffron terrorism'. A new theory, terrorist attacks were tainted as such till a few years later, Kasab's confession offered solid proof of Pakistan's role in the 26/11 attacks. Though the police had concluded a Pakistani hand for the earlier blasts, it was saffron terrorism which prevented the perpetrators of these attacks from being brought to justice.\\nAs a theory, saffron terrorism is not just hurting Hindus sentiments but is also an obstacle to fight real terrorism sponsored by Pakistan and Islamic states. The term was coined by the erstwhile UPA government to garner minority votes and manipulate the vote bank. After all, why were the Malegaon-accused SIMI activists let off? Why did certain politicians declare not to oppose their bail? What was truly behind Aseemanand's confession? The reliability of these confessions was questionable given the police brutality that the National Investigative Agency exposed.\\nJournalist Praveen Tiwari explores saffron terrorism and reveals through exclusive interviews of senior National Investigative Agency officials, undercover agents and politicians how vote bank politics can compromise ethics and national security. Should the real masterminds behind the blasts be allowed to go scot-free? Should the manipulators of the Samjhauta Express bombings not be held accountable? Should we not investigate those who had exonerated Pakistan of its guilt? An extensive research on communal politics, the book offers indisputable evidence of the 'saffron terrorism' theory as the Great Indian Conspiracy.\",\n",
              " 'Dr Surendranath Dasgupta’s in-depth work is mainly intended to provide a holistic exposition of Indian thought, based on original texts and commentaries. Occasionally, however, the author has discussed the views of other writers in the assessment of the chronology of facts.\\nYears of dedicated study and painstaking collation of data yielded this phenomenal collection of all the strains of philosophic thought propagated by various schools and philosophers in India down the ages. Originally published in five volumes, the last being posthumous, A History of Indian Philosophy remains a seminal work for scholars and students alike.\\nThis edition presents the original work in three volumes for the first time, making it more accessible and easier to handle. Nothing of the original has been abridged or sacrificed to the book.',\n",
              " 'Natural phenomena, revolutionary inventions, scientific facts, and the most up-to-date questions are all explained in detailed text that is complemented by visually arresting graphics.',\n",
              " \"On Sunday April 27, 2003, 27-year old Aron Ralston set off for a day's hiking in the Utah canyons. Dressed in a t-shirt and shorts, Ralston, a seasoned climber, figured he'd hike for a few hours and then head off to work. 40 miles from the nearest paved road, he found himself on top of an 800-pound boulder. As he slid down and off of the boulder it shifted, trapping his right hand against the canyon wall. No one knew where he was; he had little water; he wasn't dressed correctly; and the boulder wasn't going anywhere. He remained trapped for five days in the canyon: hypothermic at night, de-hydrated and hallucinating by day. Finally, he faced the most terrible decision of his life: braking the bones in his wrist by snapping them against the boulder, he hacked through the skin, and finally succeeded in amputating his right hand and wrist. The ordeal, however, was only beginning. He still faced a 60-foot rappell to freedom, and a walk of several hours back to his car - along the way, he miraculously met a family of hikers, and with his arms tourniqued, and blood-loss almost critical, they heard above them the whir of helicopter blades; just in time, Aron was rescued and rushed to hospital. Since that day, Aron has had a remarkable recovery. He is back out on the mountains, with an artificial limb; he speaks to select groups on his ordeal and rescue; and amazingly, he is upbeat, positive, and an inspiration to all who meet him. This is the account of those five days, of the years that led up to them, and where he goes from here. It is narrative non-fiction at its most compelling.\",\n",
              " 'New Insight into IELTS offers comprehensive preparation and practice for IELTS. By exploring the test paper by paper, and looking in detail at each task type, the course gradually builds up the skills, language and test techniques students need to approach IELTS with confidence. The course contains a detailed introduction to the test and a full answer key and is equally suitable for use in the classroom or for self-study. The material is intended for use with students whose current level is around Band 6 and is suitable for both Academic and General Training candidates.',\n",
              " \"Following his blockbuster biography of Steve Jobs, The Innovatorsis Walter Isaacson's story of the people who created the computer and the Internet. It is destined to be the standard history of the digital revolution and a guide to how innovation really works. What talents allowed certain inventors and entrepreneurs to turn their disruptive ideas into realities? What led to their creative leaps? Why did some succeed and others fail? In his exciting saga, Isaacson begins with Ada Lovelace, Lord Byron's daughter, who pioneered computer programming in the 1840s. He then explores the fascinating personalities that created our current digital revolution, such as Vannevar Bush, Alan Turing, John von Neumann, J.C.R. Licklider, Doug Engelbart, Robert Noyce, Bill Gates, Steve Wozniak, Steve Jobs, Tim Berners-Lee and Larry Page. This is the story of how their minds worked and what made them so creative. It's also a narrative of how their ability to collaborate and master the art of teamwork made them even more creative. For an era that seeks to foster innovation, creativity and teamwork, this book shows how they actually happen.\",\n",
              " \"David Breashears, the first American to scale Everest twice, was a veteran of nine previous Himalayan filmmaking expeditions when he agreed to lead what became his most challenging filmmaking experience. The expedition was organized by large-format motion picture producer MacGillivray Freeman Films and was comprised of an international team of climbers. Their goal was to carry a specially modified 48-pound IMAX motion picture camera to the summit of Everest and return from the top of the world with the first footage ever shot there in this spectacular format. A stunningly illustrated portrait of life and death in a hostile, high-altitude environment where no human can survive for long, Everest invites you to join Breashears, his climbers, and his crew as they make photographic history. Author Broughton Coburn traces each step of the team's progress toward a rendezvous with history - and suddenly you're on the scene of a disaster that riveted the world's attention. Everest incorporates a first-person, on-the-scene account of the most tragic event in the mountain's history: The May 10, 1996, blizzard that claimed eight lives, including two of the world's top climbing expedition leaders. It is a chronicle of the courage and cooperation that resulted in the rescue of several men and women who were trapped on the lethal, windswept slopes. Everest is also a tale of triumph. In a struggle to overcome both the physical and emotional effects of the disaster on Everest, Breashears and his team rise to the challenge of achieving their goal - humbled by the mountain's overwhelming power, yet exhilarated by their own accomplishment.\",\n",
              " \"'His clarity, wit and determination are evident, his understand and good humour moving' New Scientist\\n\\nMy Brief History recounts Stephen Hawking’s improbable journey, from his post-war London boyhood to his years of international acclaim and celebrity. Lavishly illustrated with rarely seen photographs, this concise, witty and candid account introduces readers to a Hawking rarely glimpsed in previous books: the inquisitive schoolboy whose classmates nicknamed him ‘Einstein’; the jokester who once placed a bet with a colleague over the existence of a black hole; and the young husband and father struggling to gain a foothold in the world of academia.\\n\\nWriting with characteristic humility and humour, Hawking opens up about the challenges that confronted him following his diagnosis of motor neurone disease aged twenty-one. Tracing his development as a thinker, he explains how the prospect of an early death urged him onwards through numerous intellectual breakthroughs, and talks about the genesis of his masterpiece A Brief History of Time – one of the iconic books of the twentieth century.\\n\\nClear-eyed, intimate and wise, My Brief History opens a window for the rest of us into Hawking’s personal cosmos.\\n\\n'Read it for the personal nuggets . . . but above all, it's worth reading for its message of hope' Mail on Sunday\",\n",
              " \"The prince's scandalous wedding vow she must meet him at the altar. When Josephine rescues a drowning stranger she’s captivated until it’s revealed he’s prince Alexander, heir to the throne of Aargau now the threat of scandal means this shy Cinderella must become a royal bride. Untouched queen by royal command will his desire outweigh his duty. King Augustus is shocked when his country delivers him a courtesan. But sera’s surprising innocence and undisguised yearning for him pushes Augustus's self-control to the limits. Now he won't rest until Sera becomes his queen.\",\n",
              " \"The stamping out of difference, the quelling of diversity and the burial of argument is, in fact, most un-Indian. Anyone who seeks to end that dialogue process is ignoring Indianness and patriotism. The liberal Indian argues for the rights of the marginalized in the tradition of Gandhi for trust, mutual understanding and bridge-building. Real patriotism lies in old-fashioned ideas of accommodation, friendship and generosity; not in force, muscle flexing and dominance. Why I Am a Liberal is Sagarika Ghose's impassioned meditation on why India needs to be liberal.\",\n",
              " \"Unique among Java books, A Programmer's Guide to OCA Java SE 8 Certification, A Comprehensive Primer, Fourth Edition combines an integrated, expert guide to Java SE 8 with comprehensive review for Oracle's newest OCA certification exam. Khalid A. Mughal and Rolf W. Rasmussen have thoroughly revised this tutorial/reference/prep guide to reflect major changes in the exam, including its increased focus on analyzing code scenarios, not just individual language constructs. Mughal and Rasmussen thoroughly address each exam objective, reflecting the latest Java SE 8 features, API classes and best practices for effective development.\\n\\nFeatures: The definitive, complete tutorial and prep guide for the new Oracle Certified Associate (OCA) exam for Java SE 8: Fully revised and updated\\nThe only book to combine an integrated, up-to-date guide to Java with comprehensive OCA review\\nSupports the exam's increased focus on analyzing code scenarios, not just individual language constructs\\nCovers declarations, initialization, scoping, flow control, key APIs, concurrency, objects, collections, generics, access control and more\\nProvides valuable code examples, hands-on exercises, review questions and several full practice exams\\n\\nContents:\\n\\nChapter 1: Basics of Java Programming\\nChapter 2: Language Fundamentals\\nChapter 3: Declarations\\nChapter 4: Access Control\\nChapter 5: Operators and Expressions Chapter 6: Control Flow\\nChapter 7: Object-Oriented Programming\\nChapter 8: Fundamental Classes\\nChapter 9: Object Lifetime\\nChapter 10: The Array List <E> Class and Lambda Expressions\\nChapter 11: Date and Time\\nAppendix A: Taking the Java SE 8 Programmer I Exam. Appendix B: Exam Topics: Java SE 8 Programmer Appendix C: Annotated Answers to Review Questions\",\n",
              " \"India is ready for football’s strongest comeback. 'India’s Football Dream' is a modern day guidebook to Indian Football that extensively covers the journey Indian Football has made from inception till the biggest event till date, FIFA U17 World Cup 2017. The book while talking about inception of the sport and its growth in India is also focussed towards to modern day leagues such as Indian Super League & I-League. Football is the sport that the world plays and it has been a very popular sport in India. With rich and illustrious past in India the sport is now making a very strong come back in India. Young children across India are now taking on to Football. The book aims to take the readers on a fascinating journey of the sport of Football through a diverse India and how the sport captured attention of millions of Indians in various regions. So Hop on the fascinating journey of the sport of football.”\\nExcerpt\\nFootball has long been used as a socio-political bargaining chip due to its ability to drive the masses. The social significance of football was also seen in India during the British Raj when any and every victory by an Indian club over its British counterpart resulted in widespread euphoria. The connotations can be explained with the help of Mohun Bagan’s win over East Yorkshire Regiment in the 1911 IFA Shield. Such was the importance of that victory that it changed an entire country’s perception towards its rulers. Whereas, in the late nineteenth century, the Parsees learned and accepted the British norms of playing cricket, Bagan’s win cemented the notion in the Indian psyche that playing barefoot is the way forward when it clearly was not.\",\n",
              " '‘Mastering Modern World History’ is a book which as the name suggests, deals with elaborating on the events of the modern world. The book expands on the events that occurred in the early 20th century starting from the catastrophic events of the First World War across the modern advancements in technologies towards the end of the century. The language and content in the book has been written in an informative and detailed manner. The book features an immersive language style and follows a chronological pattern that will make the readers form a connection with the events of the book; and contrast them with the present scenarios.\\nThe 20th century was a very crucial time in the context of growth and development of civilisation; and hence it is only natural that the events that occurred in the course of the century be recounted in details and evidence abstracted from the source materials. The book features a significant number of detailed maps that are precise in nature. They provide essential information in order to promote better understanding of the geographical, social and political facets of those times. The book is immensely popular among civil service aspirants as it provides them with a detailed account of the events. The book also provides relevant information about the evolution of the modern world. The book is an informative read giving a general overview first, which follows into a much detailed in-depth description of the more complex topics. The author has been successful in conducting detailed research before writing the book which makes it accurate in nature. The presence of credible maps and diagrams further enhances the learning aspect of readers.\\nAbout the author\\nThe ‘Mastering Modern World History’ has been written by the author, Norman Lowe, whose dedication and immense research towards the development of the book which has resulted in widespread success. The book has been published by the Macmillan publishers and the latest edition of the book was published in October 2000. The book is available in a paperback format.\\nThis book is easily available online for convenient shopping. Get this book from Amazon.in today with the simple click of a button.',\n",
              " \"THE SERIAL KILLER ISN'T ON TRIAL.\\nHE'S ON THE JURY...\\n****************\\n'Books this ingenious don't come along very often.' Michael Connelly\\n'THIRTEEN is my favourite read of the year.' Sarah Pinborough\\n\\n'Outstanding.' Lee Child\\n\\n'Smart and original. This is a belter of a book.' Clare Mackintosh\\n'One of the most mercilessly compelling thrillers you will read this decade' Chris Brookmyre\\n****************\\n'To your knowledge, is there anything that would preclude you from serving on this jury?'\\nMurder wasn't the hard part. It was just the start of the game.\\nJoshua Kane has been preparing for this moment his whole life. He's done it before. But this is the big one.\\nThis is the murder trial of the century. And Kane has killed to get the best seat in the house.\\nBut there's someone on his tail. Someone who suspects that the killer isn't the man on trial.\\nKane knows time is running out - he just needs to get to the conviction without being discovered.\\n****************\\nTHIRTEEN REASONS WHY ... YOU MUST READ THIS BOOK:\\n'An oh so clever hook for an oh so clever, gripping book. THIRTEEN is courtroom drama at its finest, blended with page-turning twists and characters you can't get enough of. Steve Cavanagh is the John Grisham for a new generation. Slick, thrilling and unique, THIRTEEN is my favourite read of the year.'\\nSarah Pinborough\\n'Outstanding - an intriguing premise, a tense, gripping build-up, and a spectacular climax. This guy is the real deal. Trust me.'\\nLee Child\\n'Smart and original. This is a belter of a book.'\\nClare Mackintosh\\n'Great hook. Great plot. Great book. Thirteen is a real page turner and one you won't want to put down.'\\nSimon Kernick\\n'A brilliant, twisty, ingeniously constructed puzzle of a book. Steve Cavanagh pulls off an enviable premise with panache.'\\nRuth Ware\\n'I've been tracking Steve Cavanagh for a few years now and Thirteen is his best, a dead bang beast of a book that expertly combining his authority on the law with an absolutely great thrill ride. Books this ingenious don't come along very often.'\\nMichael Connelly\\n'Quite simply deserves to be HUGE. If you read a thriller as good this year, it's only because you've read this one twice.'\\nMark Billingham\\n'Tore through this between dusk and dawn. Absolute 5-star cracker from Steve Cavanagh, who's gotta be among top legal thriller writers out there nowadays. A powerhouse of a book that's much more than its high-concept hook.'\\nCraig Sisterson\\n'Wow! This book is friggin' awesome! Utterly immersive.'\\nEmma Kavanagh\\n'Fantastically gripping? Guilty as charged!'\\nAngela Clarke\\n'An absolute cracker.'\\nSusi Holliday\\n'Guilty of thrills, twists, and expertly manipulating the reader.'\\nMason Cross\\n'An absolute rollercoaster of a read. Thrilling.'\\nCass Green\",\n",
              " 'Vimal is tired of being a convict on the loose, of drifting from one palce to another. He wishes to settle down now and live a peaceful life. But before that, he must confront his dark and devious past.',\n",
              " \"Beginning with the day Hobbes sprang into Calvin's tuna fish trap, the first two Calvin and Hobbes collections, Calvin and Hobbes and Something Under The Bed Is Drooling, are brought together in this treasury. Including black-and-white dailies and color Sundays, The Essential Calvin and Hobbes also features an original full-color 16-page story.\\n\\nPerhaps the most brilliant comic strip ever created, Calvin and Hobbes continues to entertain with dazzling cartooning and tremendous humor.\\nBill Watterson's Calvin and Hobbes has been a worldwide favorite since its introduction in 1985. The strip follows the richly imaginative adventures of Calvin and his trusty tiger, Hobbes. Whether a poignant look at serious family issues or a round of time-travel (with the aid of a well-labeled cardboard box), Calvin and Hobbes will astound and delight you.\\n\\nBeginning with the day Hobbes sprang into Calvin's tuna fish trap, the first two Calvin and Hobbes collections, Calvin and Hobbes and Something Under The Bed Is Drooling, are brought together in this treasury. Including black-and-white dailies and color Sundays,  The Essential Calvin and Hobbes also features an original full-color 16-page story.\",\n",
              " 'If a butterfly flaps its wings in Brazil, does it cause a tornado in Texas? Chaos theory attempts to answer such baffling questions. The discovery of randomness in apparently predictable physical systems has evolved into a science that declares the universe to be far more unpredictable than we have ever imagined. Introducing Chaos explains how chaos makes its presence felt in events from the fluctuation of animal populations to the ups and downs of the stock market. It also examines the roots of chaos in modern maths and physics, and explores the relationship between chaos and complexity, the unifying theory which suggests that all complex systems evolve from a few simple rules. This is an accessible introduction to an astonishing and controversial theory.',\n",
              " '\"He was no longer Jean Valjean, but No. 24601\"\\n\\nVictor Hugo’s tale of injustice, heroism and love follows the fortunes of Jean Valjean, an escaped convict determined to put his criminal past behind him. But his attempts to become a respected member of the community are constantly put under threat: by his own conscience, when, owing to a case of mistaken identity, another man is arrested in his place; and by the relentless investigations of the dogged policeman Javert. It is not simply for himself that Valjean must stay free, however, for he has sworn to protect the baby daughter of Fantine, driven to prostitution by poverty. A compelling and compassionate view of the victims of early nineteenth-century French society, Les Misérables is a novel on an epic scale, moving inexorably from the eve of the battle of Waterloo to the July Revolution of 1830. Norman Denny’s introduction to his lively English translation discusses Hugo’s political and artistic aims in writing Les Misérables.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.',\n",
              " 'This Very Short Introduction explains what international law is, its role in international society, and how it operates. Vaughan Lowe uses terrorism, environmental change, poverty, and international violence to demonstrate the theories and practice of international law, and how the principles can be used for international co-operation.',\n",
              " \"Mightier than the Sword opens with an IRA bomb exploding during the MV Buckingham's maiden voyage across the Atlantic - but how many passengers lose their lives? When Harry Clifton visits his publisher in New York, he learns that he has been elected as the new president of English PEN, and immediately launches a campaign for the release of a fellow author, Anatoly Babakov, who's imprisoned in Siberia. Babakov's crime? Writing a book called Uncle Joe, a devastating insight into what it was like to work for Stalin. So determined is Harry to see Babakov released and the book published, that he puts his own life in danger. His wife Emma, chairman of Barrington Shipping, is facing the repercussions of the IRA attack on the Buckingham. Some board members feel she should resign, and Lady Virginia Fenwick will stop at nothing to cause Emma's downfall. Sir Giles Barrington is now a minister of the Crown, and looks set for even higher office, until an official trip to Berlin does not end as a diplomatic success. Once again, Giles's political career is thrown off balance by none other than his old adversary, Major Alex Fisher, who once again stands against him at the election. But who wins this time? In London, Harry and Emma's son, Sebastian, is quickly making a name for himself at Farthing's Bank in London, and has proposed to the beautiful young American, Samantha. But the despicable Adrian Sloane, a man interested only in his own advancement and the ruin of Sebastian, will stop at nothing to remove his rival. Jeffrey Archer's compelling Clifton Chronicles continue in this, his most accomplished novel to date. With all the trademark twists and turns that have made him one of the world's most popular authors, the spellbinding story of the Clifton and the Barrington families continues.\",\n",
              " 'The second edition of this book provides a comprehensive introduction to a consortium of technologies underlying soft computing, an evolving branch of computational intelligence, which in recent years, has turned synonymous to it. The constituent technologies discussed comprise neural network (NN), fuzzy system (FS), evolutionary algorithm (EA) and a number of hybrid systems, which include classes such as neuro-fuzzy, evolutionary-fuzzy and neuro-evolutionary systems. The hybridization of the technologies is demonstrated on architectures such as fuzzy backpropagation network (NN-FS hybrid), genetic algorithm-based backpropagation network (NN-EA hybrid), simplified fuzzy ARTMAP (NN-FS hybrid), fuzzy associative memory (NN-FS hybrid), fuzzy logic controlled genetic algorithm (EA-FS hybrid) and evolutionary extreme learning machine (NN-EA hybrid)\\nEvery architecture has been discussed in detail through illustrative examples and applications. The algorithms have been presented in pseudo-code with a step-by-step illustration of the same in problems. The applications, demonstrative of the potential of the architectures, have been chosen from diverse disciplines of science and engineering.\\nThis book, with a wealth of information that is clearly presented and illustrated by many examples and applications, is designed for use as a text for the courses in soft computing at both the senior undergraduate and first-year postgraduate levels of computer science and engineering. It should also be of interest to researchers and technologists desirous of applying soft computing technologies to their respective fields of work.\\n\\nNew to the Second Edition\\n\\n• New chapters on Extreme Learning Machine, Type-2 Fuzzy Sets, Evolution Strategies, Differential Evolution and Evolutionary Extreme Learning Machine.\\n• Revised chapters on Introduction to Artificial Intelligence Systems, Fuzzy Set Theory and Integration of Neural Networks, Fuzzy Set Theories and Evolutionary Algorithms.',\n",
              " \"THE SUNDAY TIMES BESTSELLER\\n\\nWINNER OF THE COSTA FIRST NOVEL AWARD\\nWINNER OF THE BOOKS ARE MY BAG NOVEL AWARD\\nSHORTLISTED FOR THE SPECSAVERS NATIONAL BOOK AWARDS\\nSHORTLISTED FOR THE BRITISH BOOK AWARDS DEBUT OF THE YEAR\\nLONGLISTED FOR THE THEAKSTON OLD PECULIER CRIME NOVEL OF THE YEAR\\n\\nGosford Park meets Groundhog Day by way of Agatha Christie and Black Mirror - the most inventive story you'll read this year\\n\\nTonight, Evelyn Hardcastle will be killed ... Again\\nIt is meant to be a celebration but it ends in tragedy. As fireworks explode overhead, Evelyn Hardcastle, the young and beautiful daughter of the house, is killed.\\nBut Evelyn will not die just once. Until Aiden - one of the guests summoned to Blackheath for the party - can solve her murder, the day will repeat itself, over and over again. Every time ending with the fateful pistol shot.\\nThe only way to break this cycle is to identify the killer. But each time the day begins again, Aiden wakes in the body of a different guest. And someone is determined to prevent him ever escaping Blackheath...\\nSELECTED AS A BOOK OF THE YEAR BY THE GUARDIAN, I PAPER, FINANCIAL TIMES AND DAILY TELEGRAPH\",\n",
              " '\"\\nAbout the Book:\\nThe book Control Systems—GATE, PSUs and ES Examination has been designed after much consultation with the students preparing for these competitive examinations. A must buy for students preparing for GATE, PSUs and ES examinations, the book will be a good resource for students of BE/BTech programmes in the electronics engineering, electrical engineering, electrical and electronics engineering, and instrumentation engineering branches too. It will also be useful for the undergraduate students of sciences.\\nKey Features :\\nInterweaving of numerous solved examples to provide a 360 degree grip on concepts.\\nA variety of MCQs such as matching type, assertion-reason, and linked type questions.\\nPrecisely segregated and accurately solved \\';original problems\\' from previous examinations.\\nTable of Content :\\nFundamentals of Control Systems\\nMathematical Foundation\\nTransfer Function, Block Diagram and Signal Flow Graph\\nMathematical Modelling of Physical Systems\\nControl System Components\\nTime Domain Analysis of Control System\\nStability and Sensitivity\\nRoot Locus\\nFrequency Domain Analysis\\nState Space Analysis of Control Systems\\nFrequency Domain Design of Control System\\nDigital Control Systems\\nNon-Linear Control Systems\\n\"',\n",
              " \"Matt Murdock's life is changed dramatically after he is blinded by radioactive material while saving the life of an old man and develops super-senses.\",\n",
              " \"When Elfrise Swanston meets Stephen Smith she is attracted to his handsome face, gentle bearing and the sense of mystery which surrounds him. Although distressed to find that the mystery consists only in the humbleness of his origins, she remains true to their youthful vows. But societal pressures, and the advent of the superior Henry Knight, eventually displace her affections. Knight, however, proves to be an uncompromising moralist who, obsessed with fears about Elfride's sexual past, destroys her happiness. Writing of the struggle between classes and sexes, Hardy drew heavily on his own relationships, and in the introduction, Pamela Dalziel discovers fascinating parallels between Hardy's life and his art.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.\",\n",
              " 'Data Structures Using C++ is designed to serve as a textbook for undergraduate engineering students of computer science and information technology as well as postgraduate students of computer applications. The book aims to provide a comprehensive coverage of all the topics related to data structures.\\n\\nThe book begins with a discussion on the fundamentals of data structures and algorithms, and moves on to the concepts of linear data structures, stacks, recursion, queues, and searching and sorting. All the elements of data structures, such as linked lists, trees, graphs, hashing, heaps, and indexing, are covered in separate chapters in detail. The chapter on files explains file management and organization using C++ and the chapter on the standard template library provides detailed coverage of entities such as containers and iterators. A chapter on algorithm analysis and design is provided towards the end that discusses the various algorithmic strategies required to solve a problem effectively and efficiently.\\n\\nWritten in a simple manner with strong pedagogy including numerous multiple choice and review questions, the book also provides programming problems at the end of every chapter.',\n",
              " \"A true-life sporting memoir of one of the best batsman in the game who stunned the cricket world when he prematurely ended his own England career. Trescothick’s brave and soul-baring account of his mental frailties opens the way to a better understanding of the unique pressures experienced by modern-day professional sportsmen.\\nAt 29, Marcus Trescothick was widely regarded as one of the batting greats. With more than 5,000 Test runs to his name and a 2005 Ashes hero, some were predicting this gentle West Country cricket nut might even surpass Graham Gooch's record to become England's highest ever Test run scorer.\\nBut the next time Trescothick hit the headlines it was for reasons no one but a handful of close friends and colleagues could have foreseen.\\nOn Saturday 25 February 2006, four days before leading England into the first Test against India in place of the injured captain Vaughan, Trescothick was out for 32 in the second innings of the final warm-up match. As he walked from the field he fought to calm the emotional storm that was raging inside him, at least to hide it from prying eyes. In the dressing room he broke down in tears, overwhelmed by a blur of anguish, uncertainty and sadness he had been keeping at bay for longer than he knew.\\nWithin hours England's best batsman was on the next flight home. His departure was kept secret until after close of play when coach Duncan Fletcher told the stunned media his acting captain had quit the tour for 'personal, family reasons.'\\nUntil now, the full, extraordinary story of what happened that day and why, of what preceded his breakdown has never been told. He reveals for the first time that he almost flew home from the 2004 tour to South Africa – of what caused it and of what followed – his comeback to the England side and a second crushing breakdown nine months later that left him unable to continue the 2006-07 Ashes tour down under.\\nComing Back to Me replaces the myths and rumours with the truth as Trescothick talks with engaging openness and enthusiasm about his rise to the top of international cricket; and describes with equal frankness his tortured descent into private despair.\",\n",
              " \"Jason Bourne is back in the forthcoming major motion picture starring Matt Damon and Alicia Vikander. Go back to where it all began for Bourne in his first adventure - The Bourne Identity\\n'Watch your back 007 - Bourne is out to get you' - Sunday Times\\n\\nHe was dragged from the sea, his body riddled with bullets. There are a few clues: a frame of microfilm surgically implanted beneath the skin of his hip; evidence that plastic surgery has altered his face; strange things he says in his delirium, which could be code words. And a number on the film negative that leads to a bank account in Zurich, four million dollars and a name for the amnesiac: Jason Bourne.\\nNow he is running for his life. A man with an unknown past and an uncertain future, the target of assassins and at the heart of a deadly puzzle. He's fighting for survival and no one can help him - except the one woman who once wanted to escape him ...\",\n",
              " 'From silk-scarf, table, parlor and mental magic, to the kind of magic-that uses paper, rope, money and cards, here are 37 of the most astounding tricks ever. You\\'ll find that their secrets are subtle, with an emphasis on presentation, not sleight of hand. Over 90 illustrations will help you perfect your \"mysterious\" powers as you master \\'The Amazing License Plate Prediction\\', \\'The Great Cut and Restored Rope Trick\\', \\'The Impossible Coin Vanish\\', \\'The Famous Four Aces Trick\\' and other seemingly supernatural phenomena. With these astounding displays added to your magical repertoire, you\\'ll earn a reputation as a wizard of wonders and an illustrious illusionist.',\n",
              " \"This book provides a fairly comprehensive account of the evolution of India's foreign policy from 1947 to the present day. It is organized primarily in the form of India's relations with its neighbours and with key states in the global order. All the chapters in this volume utilize the level of analysis approach, a well-established conceptual scheme in the study of international politics in organizing the substantive cases. They provide crisp and lucid accounts of its developments in various parts of the world. The book is significant because there are no other viable edited volumes on the evolution of Indian foreign policy. Each chapter follows a common conceptual framework using the level of analysis approach. This framework looks at the evolution of India's foreign policy from the standpoints of systemic, national, and decision-making perspectives. In the introductory chapter, the editor carefully spells out the intellectual antecedents of the level of analysis framework in str\",\n",
              " \"Barcelona are the greatest soccer team in the world, the greatest for a generation and possibly the greatest of all time. This is the inside story of how the team came to redefine how the game is played, told by the journalist closer to it than any other. This edition contains a new epilogue reflecting on the departure of Pep Guardiola and Spain's victory at Euro 2012. It is of huge interest to anyone who loves the way this team plays soccer which is anyone who loves the game.\",\n",
              " 'Python Programming is designed as a textbook to fulfil the requirements of the first-level course in Python programming. It is suited for undergraduate degree students of computer science engineering, information technology as well as computer applications. This book will enable students to apply the Python programming concepts in solving real-world problems.\\n\\nThe book begins with an introduction to computers, problem solving approaches, programming languages, object oriented programming and Python programming. Separate chapters dealing with the important constructs of Python language such as control statements, functions, strings, files, data structures, classes and objects, inheritance, operator overloading and exceptions are provided in the book.\\n\\nFeatures\\n• Simple and lucid treatment of concepts supported with illustrations for easy understanding.\\n• Numerous programming examples along with their outputs to help students master the art of writing efficient Python programs.\\n• Notes and programming tips to highlight the important concepts and help readers avoid common programming errors.\\n• Strong chapter-end pedagogy including plenty of objective-type questions, review questions, programming and debugging exercises to facilitate revision and practice of concepts learnt.\\n• 7 Annexures and 5 appendices covering types of operating systems, differences between Python 2.x and 3.x, installing Python, debugging and testing, iterators, generators, getters, setters, Turtle graphics, plotting graphs, multi-threading, GUI and Web Programming provided to supplement the text.\\n• Case studies on creating calculator, calendar, hash files, compressing strings and files, tower of Hanoi, image processing, shuffling a deck of cards and mail merge demonstrate the application of various concepts.\\n• Point-wise summary and glossary of key terms to aid quick recapitulation of concepts.\\n\\nOnline Resources\\nFor Faculty\\n• Chapter-wise PPTs\\n• Solutions Manual\\n\\nFor Students\\n• Lab exercises\\n• Test generator\\n• Projects\\n• Model question papers\\n• Solutions to find the output and error questions\\n• Extra reading material\\n\\nTable of contents\\n1. Introduction to Computers and Problem Solving Strategies\\n2. Introduction to Object Oriented Programming\\n3. Basics of Python Programming\\n4. Decision Control Statements\\n5. Functions\\n6. Python Strings Revisited\\n7. File Handling\\n8. Data Structures\\n9. Classes and Objects\\n10. Inheritance and Polymorphism\\n11. Operator Overloading\\n12. Error and Exception Handling.',\n",
              " '‘ Why should I not publish my diary? I have often seen reminiscences of people I have never even heard of and I fail to see—because I do not happen to be a “Somebody” —why my diary should not be interesting. ’\\n\\nThe spoof diary of a lower-middle-class London clerk, The Diary of a Nobody was first serialized in the legendary magazine of humour and satire, Punch, in 1888-89. It was published as a book in 1892 and has never been out of print since. This comic masterpiece—which details the doings of the ridiculously pompous and accident-prone Charles Pooter, his wife Carrie and their troublesome son Lupin—has been a source of delight to generations of readers and inspired many celebrated writers, from J.B. Priestley and Evelyn Waugh to Helen Fielding and Sue Townsend.',\n",
              " 'The book, 71 Famous Scientists is an addition to the exclusive ‘71 Series’, which includes a number of books, such as 71 Science Experiments, 71+10 New Science Projects, 71 + 10 New Science Projects Junior, 71+10 New Science Activities, 71+10 Magic Tricks for Children, etc. published by V&S Publishers and widely appreciated by our esteemed readers.\\nIt contains 71 world-renowned Scientists from across the globe, their brief life histories, contributions to the Scientific World including the books, journals and magazines that they have published, Awards and Honours received by them and any significant happenings that have changed the course of our lives. The book includes prominent names like, Albert Einstein, Alessandro Volta, Alexander Fleming, Alexander Graham Bell, Alfred Nobel, Avogadro, Anders Celsius, Andre Marie Ampere, Antonie van Leeuwenhoek and many such notable personalities.\\nThe book has been written especially for the school students of the age group, 10-18 years, but can be read by readers of all ages, who love Science and its amazing and fascinating World of outstanding Inventions and Discoveries that have transformed the human society and our existence!\\nSo Dear Readers, grab the book at the earliest for it will educate and interest one and all!',\n",
              " \"Most of us know that we're addicted to texting, Instagram, Facebook and Twitter not because we're stupid or shallow, but because they provide real value in the form of connection, community, affirmation and information. But these tools can also disrupt our ability to focus on meaningful work and live fully in the present. In digital minimalism, Cal Newport outlines a practical philosophy and plan for a mindful, intentional use of technology that maximises its benefits while minimizing its drain on our attention, focus and time. Demonstrating how to implement a 30-day digital detox, this book will help you identify which uses of technology are actually helping you reach your goals, and which are holding you back.\",\n",
              " \"Survive anything life throws at you with The Survival Handbook.\\nServed in a handy mess tin, The Survival Book is the ultimate visual guide to camping, wilderness, and outdoor survival skills. Written by Colin Towell, an ex-SAS Combat Survival Instructor, The Survival Handbook is bursting with survival tips, manual skills, camping essentials, and advice on how to survive whatever the great outdoors throws at you.\\nLearn how to read a map, how to light a fire, and how to build a raft and be prepared for every outdoor situation. Catch your own dinner and cook it in the mess tin, revel in inspirational real-life survival stories, or serve The Survival Handbook up as a truly unique gift.\\nFrom survival basics such as finding water and catching fish, to extreme situations including being adrift at sea or lost in the jungle, The Survival Handbook will steer you through life's toughest adventures in the world's harshest climates.\\nPrevious edition ISBN 9781405393560\",\n",
              " \"This pack contains 96 high-quality origami sheets printed with 8 different natural-looking and attractive Japanese washi prints.\\n\\nThe handmade look of this paper was selected to enhance the creative work of modular origami artists and paper crafters. The pack contains prints unique to this pack, which resemble sheets of real Japanese washi paper—at a fraction of the cost!\\n\\nThere's enough paper here to assemble amazing modular origami sculptures, distribute to students for a class project, or put to a multitude of other creative uses.\\n\\nThis origami paper pack includes:\\n96 sheets of high-quality origami paper\\n8 colorful Japanese-style designs and patterns\\nSmall 6x6 inch squares\\nInstructions for 7 easy origami projects\",\n",
              " 'Every enterprise application creates data, whether itís log messages, metrics, user activity, outgoing messages, or something else. And how to move all of this data becomes nearly as important as the data itself. If youíre an application architect, developer, or production engineer new to Apache Kafka, this practical guide shows you how to use this open source streaming platform to handle real-time data feeds.\\nEngineers from Confluent and LinkedIn who are responsible for developing Kafka explain how to deploy production Kafka clusters, write reliable event-driven microservices and build scalable stream-processing applications with this platform. Through detailed examples, youíll learn Kafkaís design principles, reliability guarantees, key APIs and architecture details, including the replication protocol, the controller and the storage layer.\\n\\nUnderstand publish-subscribe messaging and how it fits in the big data ecosystem.\\nExplore Kafka producers and consumers for writing and reading messages\\nUnderstand Kafka patterns and use-case requirements to ensure reliable data delivery\\nGet best practices for building data pipelines and applications with Kafka\\nManage Kafka in production and learn to perform monitoring, tuning and maintenance tasks\\nLearn the most critical metrics among Kafkaís operational measurements\\nExplore how Kafkaís stream delivery capabilities make it a perfect source for stream processing systems',\n",
              " 'Friedrich Nietzsche was one of the most revolutionary thinkers in Western philosophy. Here he sets out his subversive views in a series of aphorisms on subjects ranging from art to arrogance, boredom to passion, science to vanity, rejecting conventional notions of morality to celebrate the individual’s ‘will to power’. Throughout history, some books have changed the world. They have transformed the way we see ourselves – and each other. They have inspired debate, dissent, war and revolution. They have enlightened, outraged, provoked and comforted. They have enriched lives – and destroyed them. Now Penguin brings you the works of the great thinkers, pioneers, radicals and visionaries whose ideas shook civilization and helped make us who we are.',\n",
              " \"More than 300 million people in the world speak English and the rest, it sometimes seems, try to...'\\n\\nOnly Bill Bryson could make a book about the English language so entertaining. With his boundless enthusiasm and restless eye for the absurd, this is his astonishing tour of English. From its mongrel origins to its status as the world's most-spoken tongue; its apparent simplicity to its deceptive complexity; its vibrant swearing to its uncertain spelling and pronunciation, Bryson covers all this as well as the many curious eccentricities that make it as maddening to learn as it is flexible to use.\\n\\nBill Bryson's classic Mother Tongue is a highly readable and hilarious tale of how English came to be the world's language.\",\n",
              " \"Unlike math and science, writing has very few absolutes. To that end, we're faced with making choices and many times those choices are word choices. Since I began teaching technical and business writing in industry years ago, one lesson never grew old an exercise on when to write the right word. Many writers confuse complement when they mean compliment, compose when they mean comprise, or assume when they mean presume. And hundreds of other mistaken word identities also exist. This Quick reference aims to filter this confusion of words. Remember, writing is much easier to comprehend when the noise around it is eliminated.\\nThough not inclusive of all word pair mistakes, this book speaks to the common word choices that challenge us daily. This book consists of more than 900 entries and the meanings of more than 2,000 individual words or phrases. Much of the content is based on word problems I've witnessed during my 25 years of business writing as well as the misuse commonly found in the online and print media. If you're looking for the basic homonyms such as to, too and two, or here and hear, you won't find them here. But you will find the basic pronoun possessives (their, they're, there, who's and whose and you're and your) because they still give us problems. I've tried to eliminate obvious synonyms (inedible vs. uneatable, simultaneous vs. concurrently, rare vs. scarce, etc.) and include just word pairs where the word distinction is still important yet challenging.\\nNote that a few entries may seem trivial and others may seem a bit obscure or obsolete. But overall you'll find it a good mix of problem words (among vs. between, flounder vs. founder, blatant vs. flagrant), Also included are a few notes on phrases that often lead to problem writing (between you and I, one of the only and others). For each entry in this reference, the difference between the words is explained and in most cases, a short and simple sentence example showing the correct usage is provided. The word definitions provided are intended strictly as guides and not dictionary definitions. They should help you differentiate between the words. Current dictionaries, popular writing books and reputable style guides were among the many' resources consulted for this book. The word list is arranged alphabetically according to the first word in the set. Always think about the words you use. Accuracy in word choice is a key to effective communication. In your daily writing, try to make sure you apply the right word for the right meaning. By doing so, its effect can affect your writing in a positive way.\",\n",
              " 'THE FIRST PSY/CHANGELING NOVEL from the New York Times bestselling author of Shards of Hope, Shield of Winter, and Heart of Obsidian...\\nThe book that Christine Feehan called \"a must-read for all of my fans.\"\\nIn a world that denies emotions, where the ruling Psy punish any sign of desire, Sascha Duncan must conceal the feelings that brand her as flawed. To reveal them would be to sentence herself to the horror of “rehabilitation”—the complete psychic erasure of everything she ever was…Both human and animal, Lucas Hunter is a Changeling hungry for the very sensations the Psy disdain. After centuries of uneasy coexistence, these two races are now on the verge of war over the brutal murders of several Changeling women. Lucas is determined to find the Psy killer who butchered his packmate, and Sascha is his ticket into their closely guarded society. But he soon discovers that this ice-cold Psy is very capable of passion—and that the animal in him is fascinated by her. Caught between their conflicting worlds, Lucas and Sascha must remain bound to their identities—or sacrifice everything for a taste of darkest temptation…',\n",
              " '\"Best book ever for marketing your business. And not just for online marketing. This book goes into so much detail and is perfect for any business. How to tell stories to make sales. How to be the EXPERT in your Niche.\" - J. Paterson, Amazon Customer \"Jam packed with step by step advice to create a sales webinar, generate more leads, and close high ticket coaching clients. \" - JulzB, Amazon Customer \"This book is incredible for Marketers, Sales people, Entrepreneurs and Online Influencers.\" - Luca Nunez, Amazon Customer \"I lost $$thousands more running ads trying to figure out what I was missing... until I came across this book.\" - Nicole, Amazon Customer Find Your Message, Build A Tribe, And Change The World... Founder of Click Funnels, Russell Brunson shares how he\\'s sold close to $400 million by building a mass movement of entrepreneurs who paid to hear his message. It doesn\\'t matter what message, product, or service you are selling online, if you don\\'t build a mass movement of people who will pay to hear your mes- sage, it\\'s unlikely you will achieve success. Expert Secrets Will Help You Too Find your voice and give you the confidence to become a leader... Build a mass movement of people whose lives you can affect... Make this calling a career, where people will pay you for your advice... Your message has the ability to change someone\\'s life. The impact that the right message can have on someone at the right time in their life is immeasurable. It could help to save marriages, repair families, change someone\\'s health, grow a company or more... But only if you know how to get it into the hands of the people whose lives you have been called to change. Expert Secrets will put your message into the hands of people who need it. Russell Brunson is a serial entrepreneur who started his first online company while he was wrestling at Boise State University. Within a year of graduating he had sold over a million dollars worth of his own products and services from his basement! For over 10 years now Russell has been starting and scaling companies online. He owns a software company, a supplement company, a coaching company, and is one of the top super affiliates in the world. Expert Secrets was created to help entrepreneurs around the world to start, promote and grow their companies online.',\n",
              " 'Evil Rakshasa Kalanemi is back on Earth, this time appearing as Kansa, the tyrant king of Mathura. To vanquish him and his horde of evil monsters, Lord Vishnu comes to Earth in his eighth avatar - Krishna, the defender of dharma. Since his birth, Krishna valiantly fights evil monsters, showing courage and valour. But as he grows up and becomes a councillor of the race of Yadavas, he observes that the real struggle in this age is not with magical monsters but with evil kings and warriors. Accompany this wise and courageous hero on his journey on Earth as he conquers evil to put mankind back on the path of righteousness.',\n",
              " 'Basic awareness about computers is a must if we wish to lead a successful personal and professional life. In view of the upcoming Bank PO and Clerk examinations like IBPS PO and Clerk, SBI PO and Clerk, LIC AAO, Railway Recruitment Exams, etc. Arihant has come up with a revised edition of the highly popular Computer Awareness which is equally useful for IBPS, SBI (Bank PO and Clerk) and other banking and recruitment entrances. The book has been revised keeping in mind the recent pattern of banking examinations.\\nThe book on computer awareness has been divided into 14 Chapters namely Introduction to Computer, Computer Architecture, Input and Output Devices, Computer Hardware, Computer Memory, Data Representation, Computer Software, Operating System (OS), Programming Concepts, Microsoft Windows, Microsoft Office, Database Concepts, Data Communication and Networking, Internet and Its Services and Computer Security, each containing complete description of essential topics for better understanding. Important Points have been covered in the boxes given in between the text. Practice Questions have been given after each chapter. Also detailed synopsis along with number of previous years’ solved questions has been given in the book. Some facts called Tit-Bits related to the appropriate topic are included with each chapter. The necessary study material well supported by definitions, examples, figures, tables, flow charts, etc. has been provided in the book. Computing Glossary has been given at the end of all the chapters to help students understand the meaning of various computing terms along with abbreviations used in the book. The book also contains five Practice Sets with their objective answers for self analysis and thorough practice.',\n",
              " \"Inquisitor Glokta, a crippled and increasingly bitter relic of the last war, former fencing champion turned torturer extraordinaire, is trapped in a twisted and broken body - not that he allows it to distract him from his daily routine of torturing smugglers.\\nNobleman, dashing officer and would-be fencing champion Captain Jezal dan Luthar is living a life of ease by cheating his friends at cards. Vain, shallow, selfish and self-obsessed, the biggest blot on his horizon is having to get out of bed in the morning to train with obsessive and boring old men.\\nAnd Logen Ninefingers, an infamous warrior with a bloody past, is about to wake up in a hole in the snow with plans to settle a blood feud with Bethod, the new King of the Northmen, once and for all - ideally by running away from it. But as he's discovering, old habits die really, really hard indeed...\\n...especially when Bayaz gets involved. A bald old man with a terrible temper and a pathetic assistant, he could be the First of the Magi, he could be a spectacular fraud, but whatever he is, he's about to make the lives of Glotka, Jezal and Logen a whole lot more difficult...\",\n",
              " \"Correlation does not imply causation.' This mantra was invoked by scientists for decades in order to avoid taking positions as to whether one thing caused another, such as smoking and cancer and carbon dioxide and global warming. But today, that taboo is dead. The causal revolution, sparked by world-renowned computer scientist Judea Pearl and his colleagues, has cut through a century of confusion and placed cause and effect on a firm scientific basis. Now, Pearl and Science journalist Dana Mackenzie explain causal thinking to General readers for the first time, showing how it allows us to explore the world that is and the worlds that could have been. It is the essence of human and artificial intelligence. And just as pearl's discoveries have enabled machines to think better, the book of why explains how we can think better.\",\n",
              " \"The demon-king Ravana, born of a union between the holiest of mortals and a demon princess, has risen from an obscure beginning at a hermitage to conquer not just hell but heaven too. No less than a god to his own people, he is the sheer embodiment of evil to his enemies. This arrogant demon brooks no hindrance to snatching his heart's desire, and his terror seems unstoppable to gods and humans alike. But he makes a mistake when he abducts the wife of Lord Rama, the exiled divine ruler of Ayodhya.\\nRavana is a story of a demon, who dared to challenge the gods, and almost got away with it. Ravana's tale is one that will incite awe and fear simultaneously. Whose side was this enigma on, good or evil? The obvious answer seems to be but one: his own. Or was he really? This graphic novel seeks to explore that question, and others.\",\n",
              " \"It is 1999 and Russia is on the edge of total implosion. Social and moral order has collapsed and what small semblance of control there is, is being imposed by mafia-like criminal gangs. While public opinion in the West is largely indifferent, the political analysts are less sanguine - Russian meltdown will make the disintegration of the Balkans look like the collapse of a cup-cake. Out of the chaos, however, a single charismatic voice is starting to be heard - that of Igor Komarov, a visionary patriot who claims he can restore Russia's greatness and bring prosperity to the masses. He even woos Western political leaders with a rather more realistic analysis of the way forward for Russia. Komarov is set to win the next election when a document is smuggled into the British Embassy in Moscow. It's called The Black Manifesto and it appears to show Komarov's secret agenda - his political blueprint is really Mein Kampf, the rebirth of Russia will be as a New Third Reich with Komarov as Fuhrer. But can the document be authenticated? And what can the Western Alliance's most secret Trilateral Commission do about it if it is? They need to find another voice the masses will listen to and obey rather than Komarov - an icon they can cleave to and trust. Once, not that long ago, he was called the Tsar.\\n\\nAnd so develops a thrilling and increasingly frightening adventure - Jason Monk, ex-CIA, who used to run agents into the Soviet Union, is recruited and slips back into Russia, into the desperate Moscow world of poverty, luxury, gangsters and prostitutes and underneath it all, the titanic power struggle to ensure the outcome of the forthcoming elections.\",\n",
              " 'Having risen to the top of the game more than a decade ago, Lionel Messi has matured into the perfect team player, as likely to provide a superlative assist as a masterful match-winner.\\n\\nAs the star of an exceptional Barcelona team, he has won more silverware than most football clubs.\\n\\nMessi is Luca Caioli’s classic portrait of a footballing icon, now fully updated to include all the action from 2016/17.\\n\\nFeaturing exclusive testimony from those who know him best, including coaches, teammates and even Messi himself, it offers an unrivalled behind-the-scenes look at the career of a sporting giant.',\n",
              " \"Destiny and darkness collide in this romantic, sweeping new fantasy series from New York Times bestselling author Sophie Jordan.\\nSeventeen years ago, an eclipse cloaked the kingdom of Relhok in perpetual darkness. In the chaos, an evil chancellor murdered the king and queen and seized their throne. Luna, Relhok's lost princess, has been hiding in a tower ever since. Luna's survival depends on the world believing she is dead.\\nBut that doesn't stop Luna from wanting more. When she meets Fowler, a mysterious archer braving the woods outside her tower, Luna is drawn to him despite the risk. When the tower is attacked, Luna and Fowler escape together. But this world of darkness is more treacherous than Luna ever realized.\\nWith every threat stacked against them, Luna and Fowler find solace in each other. But with secrets still unspoken between them, falling in love might be their most dangerous journey yet.\",\n",
              " \"You may be wondering if business analysis is the right career choice, debating if you have what it takes to be successful as a business analyst, or looking for tips to maximize your business analysis opportunities. With the average salary for a business analyst in the United States reaching above $90,000 per year, more talented, experienced professionals are pursuing business analysis careers than ever before. But the path is not clear cut. No degree will guarantee you will start in a business analyst role. What's more, few junior-level business analyst jobs exist. Yet every year professionals with experience in other occupations move directly into mid-level and even senior-level business analyst roles. My promise to you is that this book will help you find your best path forward into a business analyst career. More than that, you will know exactly what to do next to expand your business analysis opportunities.\",\n",
              " 'Explore the emotional sensations of the many facets of love and affection that bring people together with one of the twentieth century’s greatest spiritual teachers.\\nOne of the most important life events is falling in love, yet we never learn about it in school. Societies and religions force us into models and thought-forms that are often in opposition to an organic model of love, which is instead institutionalized by marriage, religious affiliations, and nationalism. This results in love that is, for most people, a painful challenge in one form or another throughout life.\\nIn these modern days, where the focus shifts more and more to realizing one’s individual potential, Osho’s The Power of Love: What Does It Take for Love to Last a Lifetime? helps us to direct our search for love by widening our view―showing us that love has many manifestations and is not limited to the “other”. One manifestation of love is meditation, a life-changing experience that allows the flowering of real love within oneself and toward others.\\nOsho challenges readers to examine and break free of the conditioned belief systems and prejudices that limit their capacity to enjoy life in all its richness. He has been described by the Sunday Times of London as one of the “1000 Makers of the 20th Century” and by Sunday Mid-Day (India) as one of the ten people―along with Gandhi, Nehru, and Buddha―who have changed the destiny of India. Since his death in 1990, the influence of his teachings continues to expand, reaching seekers of all ages in virtually every country of the world.',\n",
              " \"Mrs Funnybones unravelled\\nAlmost everyone has a similar family story in India and this is what Mrs Funnybones or rather Twinkle Khanna pokes fun at. The husband who’s the “man of the house”, the crazy mothers in law, friends and of course the constant jibes at her weight make it an intriguing read. Everything comes together to leave the reader cackling away in laughter.\\nDwell deeper because this book will certainly make you think\\nThe tag line of the book, 'She’s just like and a lot like me” coupled with the celebrity author might make one wonder, how can one compare a normal Indian household life with the glamorous life of someone like Twinkle Khanna. But that is the beauty of this book. It is not entirely an autobiography of the author, nor is it the typical tell all that celebrities so often write. This is a book of a woman who is a mother of two, an ordinary version of the star and someone who finds humour in the everyday situations that women in India face every day. That is what makes the book relatable even though it has been written by someone who’s the daughter of celebrities and married to one. Through her own anecdotes and by drawing parallels between a star’s life and a normal life, she has managed to write a book which is funny yet at the same time it forces you to think and comment on the situation. Mrs Funnybones is a comical take on the life of modern Indian women.\\nMrs Funnybones Herself: About the Author\\nTwinkle Khanna was born on 29 December 1973, to established actors, Dimple Kapadia and Rajesh Khanna. She Won her first Filmfare Award with her first movie in 1995 and played her last lead role in 2001. Since then she has moved on to be an interior designer, a columnist, producer and an author as well. She is the wife of famous Indian actor Akshay Kumar\",\n",
              " 'People like to keep certain distances between themselves and other people or thigns. And this invisible bubble of space that constitutes each person\\'s \"territory\" is one of the key dimensions of modern society. Edward T. Hall, author of The Silent Language, introduced the science of proxemics to demonstrate how man\\'s use of space can affect personal and business reltions, cross-cultural interactions, architecture, city planning, and urban renewal.\\n\\n \"One of the few extraordinary books about mankind\\'s future which should be read by every thoughtful person.\" —Chicago Tribune\\n\\n\"This is a book of impressive genius, replete with unusually sharp observations.\" —Richard J. Neutra, Landscape Architecture',\n",
              " \"Y: The Last Man, winner of three Eisner Awards and one of the most critically acclaimed, best-selling comic books series of the last decade, is that rare example of a page-turner that is at once humorous, socially relevant and endlessly surprising.\\n\\nWritten by Brian K. Vaughan (Lost, Pride of Baghdad, Ex Machina) and with art by Pia Guerra, this is the saga of Yorick Brown—the only human survivor of a planet-wide plague that instantly kills every mammal possessing a Y chromosome. Accompanied by a mysterious government agent, a brilliant young geneticist and his pet monkey, Ampersand, Yorick travels the world in search of his lost love and the answer to why he's the last man on earth.\\n\\nCollects Y: The Last Man #1-10.\",\n",
              " 'Jack Ryan risks all in the Soviet Union as the superpowers clash over Star Wars in Tom Clancy’s acclaimed fourth international No 1 bestseller – now reissued with a new cover.\\nThe superpower arms negotiations appear to be making progress. But a US spy satellite reveals that the Soviets are building a massive laser-defence system controlled from an other-worldly array of pillars and domes in the Soviet hills at Dushanbe near the border of war-torn Afghanistan.\\nThe Americans need more information. The man to give it is Colonel Mikhail Filitov of the Soviet Union, codename Cardinal, America’s highest-placed agent in the Kremlin. But Filitov’s cover is about to be betrayed to the KGB, and CIA adviser Jack Ryan must rescue Filitov and bring him to safety…',\n",
              " \"See your tennis game as you never have before. See what it takes to improve consistency and performance on the court. Tennis Anatomy will show you how to ace the competition by increasing strength, speed, and agility for more powerful serves and more accurate shots. Tennis Anatomy includes more than 72 of the most effective exercises, each with step-by-step descriptions and full-color anatomical illustrations highlighting muscles in action.Tennis Anatomy goes beyond exercises by placing you on the baseline, at the net, and on the service line. Illustrations of the active muscles for forehands, backhands, volleys, and serves show you how each exercise is fundamentally linked to tennis performance.You'll also learn how exercises can be modified to target specific areas, improve your skills, and minimize common tennis injuries. Best of all, you'll learn how to put it all together to develop a training program based on your individual needs and goals.Whether you're a serve and volleyer, baseliner, or all-court player, Tennis Anatomy will ensure that you step onto the court ready to dominate any opponent.\",\n",
              " \"Target 3 Billion: Innovative Solutions Towards Sustainable Development talks about the 3 billion people across the globe who live in villages and are often deprived of basic resources. The authors explain how the global model of development has failed to eradicate poverty.\\nIndia's 750 million people living in villages constitutes the world's largest rural population. Target 3 Billion: Innovative Solutions Towards Sustainable Development integrates the challenges and opportunities of the present human civilization. It elaborates on providing Urban Amenities in Rural Areas (PURA), a sustainable and environment friendly system that will uplift the rural masses. Instead of relying on government subsidies, Dr. Kalam says that entrepreneurship with community participation can empower villagers. PURA is a blend of people, technology, entrepreneurial spirit, traditions and skills. Readers will come across many examples throughout the book which demonstrate how PURA can change the lives of millions. Some of these include Seed Club at Chitrakoot Pura and Warana Cooperative Sugar Factory.\\nTarget 3 Billion: Innovative Solutions Towards Sustainable Development explains how governments can use this system for the benefit of the rural masses. Some of the chapters in this book include The Other Half of Mankind, Effecting A Social Transformation and Agriculture and PURA. The authors have posed the question, what can I do to empower 3 billion? The answers have been provided from the different perspectives of citizens, students and senior citizens. This book was published by Penguin India in 2011, is available in paperback.\",\n",
              " '‘The General English’ book is most useful book for any aspirants. General English is asked in possibly every competitive entrance test like PTE Academic. It is the most vital part of the exam. You need to score a very good number for cracking all the competitive examination and having sufficient grip in English language is added advantage which will provide you the self-reliance to communicate with ease in this fast modernizing world.\\nThe current book has several chapters which includes Tenses, Modals, Clause Analysis, Articles, Noun, Pronoun, Adjective, Adverb, Prepositions, Phrasal Verbs, Idioms & Phrases, Spotting Errors, etc. In addition, you can get elaborate lessons on Patronyms & Homonyms, One Word Substitutions, Synonyms & Antonyms, Cloze Test, etc. The author has also explained Sequence of Sentences, Objective Comprehension and Narration, Synthesis, Transformation, Punctuation, Spelling Rules and Contractions.\\nThe book will help you to analyze your preparation level and by this way you can rate yourself through this book. As the book provides you the comprehensive overview of English, it is a treasure for any person who is keen to improve their knowledge skill in English. The book is published by Pearson Publisher which is popular among the readers. You can get this on Amazon India in reasonable price.\\nAbout the author\\nA-K-Thakur is renowned author who has penned this book every effectively.',\n",
              " 'A new series of prose novels, straight from the worldwide Naruto franchise. Naruto’s allies and enemies take center stage in these fast-paced adventures, which each volume focusing on a particular clanmate, ally, team...or villain.\\n\\nA year has passed since the Fourth Great Ninja War, and Kakashi’s appointment as Hokage looms. But first he heads to the Land of Waves for a dangerous mission rescuing hostages from a top-secret airship. There he confronts a ninja whose heart is frozen by tragedy.\\n\\nHaving lost both his friend’s eye and his greatest abilities, can Kakashi protect anyone from his coldhearted foe? What is the true meaning of the Will of Fire gained in the distant heavens? Kakashi finds these answers and more as he enters a new ninja era.',\n",
              " \"From the author of That’s the Way We Met! and Few Things Left Unsaid, this true modern love story will sweep you off the feet. ‘It Started With a Friend Request’ is a heart-warming story that will pull all the right strings of your emotions.\\nLove silently steps in our lives and put us amidst whirlwind rising emotions. Single, young and conservative Akash prefers girls, who are smart and sensible than those in miniskirts. One lucky day, he meets Aleesha, a free-spirited girl and a Mass Media student in local discotheque. Aleesha being the only child of her parents is a spoiled brat. The meeting ends up with getting to know each other's interests and exchanging their BlackBerry pins. They start chatting regularly and slowly fall in love. As the duo plans to take their relationship ahead, fate that has already been writing a different plot, starts stirring things up. Will Akash and Aleesha give in to fate or build their own? How far will the couple go to save their relationship? See love from a completely different angle with It Started with a Friend Request.\\nAbout the author:\\nSudeep Nagarkar holds credits for five bestselling novels, including ‘You’re The Password to My Life’ (2014), ‘Sorry, You’re Not My Type’ (2014), ‘That’s the Way We Met’ (2012) and ‘Few Things Left Unsaid’ (2011). He has bagged a prestigious position the Forbes list of influential celebrities and also received Youth Achievers’ Award 2013. His passion for writing is not limited to novels, as he also contributes his works to television industry.\",\n",
              " 'Two men are responsible for the death of Hector Cross’ wife and only one is left alive: Johnny Congo – psychopath, extortionist, murderer, and the bane of Cross’s life. He caught him before and let him go. Now, Hector wants him dead. So does the US government. Congo is locked up on Death Row in the most secure prison in the free world, counting down the days until his execution. He’s got two weeks. He wants out. He’s escaped before and knows he can again, and with whizz kid D’Shonn Brown enlisted, he might just have a chance. Cross, still licking his wounds from his last bruising encounter with Congo, is back and ready for work. In the middle of the rough Atlantic stands oil supertanker Bannock A.Terrorist activity in the area has triggered panic and there’s only one person they can trust to protect her. What is promised as a cakewalk turns out to be much more, a mission that will test Cross to his emotional and physical limits. But a life spent in the SAS and private security has left Cross hard-wired for pain and as he is thrown into the bull pit once more, he will not stop until he has snared his prey.',\n",
              " \"‘My story, without the spin.’\\nFrom the start of his glittering career in 1992, to his official retirement from all formats of the game in 2013, Shane Warne has long desired to tell his incredible story without compromise. No Spin is that very story. It will offer a compelling intimate voice, true insight and a pitch-side seat to one of cricket’s finest eras, making this one of the ultimate must-have sports autobiographies.\\nShane is not only one of the greatest living cricket legends: he is as close as the game has had since Botham to a maverick genius on the field and a true rebel spirit off it, who always gives audiences what they want. Despite being the talismanic thorn in England’s side for nearly two decades of regular Ashes defeats, he was also much loved in the UK where he played cricket for Hampshire. He’s also a much-admired figure in India and South Africa.\\nAlongside his mesmerising genius as a bowler, Shane has often been a controversial figure and in this book he's talk with brutal honesty about some of the most challenging times in his life as a player. Honest, thoughtful, fearless and loved by millions, Shane is always his own man and this book is a testament to his brilliant career.\",\n",
              " \"Materialism is the mantra of the modern generation, whose motto is to 'eat, drink\\n\\nand be merry'. This philosophy gives rise to 'greed, lust and addiction' which are\\n\\nvices within us. As against this, spiritualism believes in having ' virtues, values and\\n\\nmorals' to live a contented, stress-free and purposeful life.\\n\\nThis is the journey of an investigative journalist, thrillingly revealing mysteries\\n\\nof the corrupt material world. Believing in following the virtuous, righteous and\\n\\nspiritual path in life, how far will he succeed in a society dominated by corrupt\\n\\npoliticians, unscrupulous greedy businessmen, puppet media, insensitive police,\\n\\nand even a biased judiciary?\\n\\nWill he survive in the midst of the powerful lobbies who have scant regard for\\n\\nhuman life? Or will he be crushed like a beetle under a booted foot, as everyone\\n\\npredicts? Or will he be able to stand up just with the help of a handful of yogis of\\n\\nthe Himalayan ashrams and their spiritual followers?\\n\\nA novel with lots of twists, turns, conflicts, romance, emotions, drama, suspense,\\n\\nthrill and action, promising a mesmerising reading experience...\",\n",
              " 'Completely revised and expanded brand-new edition of Volume I, the first book in the Whiskey poetry trilogy!\\n\\nWhiskey, Words, and a Shovel, Vol. 1, is about reclaiming your power on the path to a healthy relationship. It is a testament to choosing to love yourself, even if it means heartbreak.\\n \\n\\nOriginally released in 2015, this re-rerelease packs the same punch as the first version, but makes an even greater connection with the soul of the reader. Each piece has been re-seen and revamped to reflect the author’s continuing journey with his partner, Samantha King, without whom this book would not exist. Samantha is the muse, the “she” the writer speaks of; she is every woman who has felt like she wasn’t good enough, and every woman who struggles to find love.  \\n ',\n",
              " 'Elite trainer Mark Lauren has been at the front lines of preparing US Special Operations soldiers for action, getting them lean and strong in record time. Now, he shares the secrets to his simple, yet amazingly effective regimen to get you into the best shape of your life.\\n\\n- Rapid results with minimum time commitment – work out for only 30-minutes a day, four times a week\\n- No gym or equipment required – simple bodyweight resistance exercises you can do anywhere\\n- Build muscle and burn fat – get more effective results than weightlifting and aerobics\\n- Suitable for men, women and all abilities – choose your level from Basic, 1st Class, Master Class and Chief Class\\n- Safe and effective – develop balance, stability and prevent injuries\\n\\nWith 125 clear exercises to work every muscle in your body, motivation techniques and nutritional advice, Mark Lauren’s method will get you the body you want simply by using the body you have.',\n",
              " 'INSTANT #1 NEW YORK TIMES BESTSELLER\\n\\nSuperstar comedian and Hollywood box office star Kevin Hart turns his immense talent to the written word by writing some words. Some of those words include: the, a, for, above, and even even. Put them together and you have the funniest, most heartfelt, and most inspirational memoir on survival, success, and the importance of believing in yourself since Old Yeller.\\n\\nThe question you’re probably asking yourself right now is: What does Kevin Hart have that a book also has?\\n\\nAccording to the three people who have seen Kevin Hart and a book in the same room, the answer is clear:\\n\\nA book is compact. Kevin Hart is compact.\\n\\nA book has a spine that holds it together. Kevin Hart has a spine that holds him together.\\n\\nA book has a beginning. Kevin Hart’s life uniquely qualifies him to write this book by also having a beginning.\\n\\nIt begins in North Philadelphia. He was born an accident, unwanted by his parents. His father was a drug addict who was in and out of jail. His brother was a crack dealer and petty thief. And his mother was overwhelmingly strict, beating him with belts, frying pans, and his own toys.\\n\\nThe odds, in short, were stacked against our young hero, just like the odds that are stacked against the release of a new book in this era of social media (where Hart has a following of over 100 million, by the way).\\n\\nBut Kevin Hart, like Ernest Hemingway, JK Rowling, and Chocolate Droppa before him, was able to defy the odds and turn it around. In his literary debut, he takes the reader on a journey through what his life was, what it is today, and how he’s overcome each challenge to become the man he is today.\\n\\nAnd that man happens to be the biggest comedian in the world, with tours that sell out football stadiums and films that have collectively grossed over $3.5 billion.\\n\\nHe achieved this not just through hard work, determination, and talent: It was through his unique way of looking at the world. Because just like a book has chapters, Hart sees life as a collection of chapters that each person gets to write for himself or herself.\\n\\n“Not only do you get to choose how you interpret each chapter, but your interpretation writes the next chapter,” he says. “So why not choose the interpretation that serves your life the best?”',\n",
              " 'Join monkey br>D fluffy and his swashbuckling crew in their search for the ultimate treasure, one piece! As a child, monkey br>D luffy dreamed of becoming king of the pirates. But his life changed when he accidentally gained the power to stretch like rubber…at the cost of never being able to swim again! Years, later, luffy sets off in search of the “one piece,” said to be the greatest treasure in the world... Sanji and the other chefs rush to complete their cake in order to stop the rampaging big mom. Meanwhile, the battle between fluffy and the unbeatable katakuri reaches its exciting conclusion!.',\n",
              " 'A healthy young man dies in his sleep, despite the ringing of eight separate alarm clocks…\\n\\nGerry Wade had proved himself to be a champion sleeper; so the other house guests decided to play a practical joke on him. Eight alarm clocks were set to go off, one after the other, starting at 6.30 a.m. But when morning arrived, one clock was missing and the prank had backfired with tragic consequences.\\n\\nFor Jimmy Thesiger in particular, the words ‘Seven Dials’ were to take on a new and chilling significance…',\n",
              " 'Hanuman stands for extraordinary physical strength, courage and humility. Above all, he is known for his devotion to Rama. So, it was little wonder that Rama entrusted Hanuman with the task of carrying his message to Sita in Lanka. Again, it was Hanuman who brought him the life-giving Sanjeevani to revive his beloved brother Lakshmana who had been grievously injured on the battlefield. Even today, Hanuman is regarded as the epitome of unfaltering devotion and unparalleled strength.',\n",
              " 'Virat studies in a renowned boarding school in Shimla. But behind the façade of a happy teenager is a disturbed child trying to fight the pain of his mother’s death and his father’s ignorance. Neither love nor friends seem to be of any help. That’s when he finds a picture in a library book, which changes his life forever.\\nIn trying to find the truth behind the girl in the picture, he stumbles upon dirty secrets and a scandal that shocks him.\\nMeanwhile in Delhi, a CID officer and Police Inspector Mathur are trying to unravel the mystery behind the gruesome murders of two influential families that seem to be connected to the mysterious girl in the picture.\\nWill Virat be able to find the mysterious girl or lose himself on the way?\\nDiscover the chilling reality of life, friendship, love and deceit behind the curtain of White Smoke.',\n",
              " 'Mandalas are spiritual and ritual diagrams representing the various interpretations of our universe. These powerful diagrams can help you embark on a spiritual journey of self-discovery by allowing you to take a break from a hectic life style. Relax and get rid of your anxieties by adding myriad colours to these extraordinary designs.',\n",
              " \"Let the magic of J.K. Rowling's classic series take you back to Hogwarts School of Witchcraft and Wizardry. Issued to mark the 20th anniversary of first publication of Harry Potter and the Chamber of Secrets, these irresistible House Editions celebrate the noble character of the four Hogwarts houses. Featuring gorgeous house-themed cover art and interior line illustrations by Kate Greenaway Medal winner Levi Pinfold, each book will also have vibrant sprayed edges in the house livery. Entertaining bonus features exclusive to each house accompany the novel. All seven books in the series will be issued in these highly collectable House Editions.\\nA must-have for anyone who has ever imagined sitting under the Sorting Hat in the Great Hall at Hogwarts waiting to hear the words, 'Better be GRYFFINDOR!'\\nYou'll always find a home at Hogwarts!\",\n",
              " \"SHORTLISTED FOR THE ORWELL PRIZE FOR POLITICAL WRITING 2019\\n'A near miracle' Ha-Joon Chang, author of 23 Things They Don't Tell You About Capitalism\\n\\nIn The Growth Delusion, author and prize-winning journalist David Pilling explores how economists and their cult of growth have hijacked our policy-making and infiltrated our thinking about what makes societies work. Our policies are geared relentlessly towards increasing our standard measure of growth, Gross Domestic Product. By this yardstick we have never been wealthier or happier. So why doesn't it feel that way? Why are we living in such fractured times, with global populism on the rise and wealth inequality as stark as ever?\\nIn a book that is simultaneously trenchant, thought-provoking and entertaining, Pilling argues that we need to measure our successes and failures using different criteria. While for economic growth, heroin consumption and prostitution are worth more than volunteer work or public services, in a rational world we would learn how to value what makes economies better, not just what makes them bigger. So much of what is important to our wellbeing, from clean air to safe streets and from steady jobs to sound minds, lies outside the purview of our standard measure of success. We prioritise growth maximisation without stopping to think about the costs.\\nIn prose that cuts through the complex language so often wielded by a priesthood of economists, Pilling argues that our steadfast loyalty to growth is informing misguided policies - and contributing to a rising mistrust of experts that is shaking the very foundations of our democracy.\",\n",
              " 'At last! The classic screenwriting workbook—now completely revised and updated—from the celebrated lecturer, teacher, and bestselling author, Syd Field: “the most sought-after screenwriting teacher in the world”* No one knows more about screenwriting than Syd Field—and now the ultimate Hollywood insider shares his secrets and expertise, completely updating his bestselling workbook for a new generation of screenwriters. Filled with new material—including fresh insights and anecdotes from the author and analyses of films from Pulp Fiction to Brokeback Mountain—The Screenwriter’s Workbook is your very own hands-on workshop, the book that allows you to participate in the processes that have made Syd Field’s workshops invaluable to beginners and working professionals alike. Follow this workbook through to the finish, and you’ll end up with a complete and salable script! Learn how to:• Define the idea on which your script will be built• Create the model—the paradigm—that professionals use• Bring your characters to life• Write dialogue like a pro• Structure your screenplay for success from the crucial first pages to the final actHere are systematic instructions, easy-to-follow exercises, a clear explanation of screenwriting basics, and expert advice at every turn—all the moment-to-moment, line-by-line help you need to transform your initial idea into a professional screenplay that’s earmarked for success.The Perfect Companion Volume to Syd Field’s Revised and Updated Edition of Screenplay: The Foundations of Screenwriting*Hollywood Reporter',\n",
              " 'Nervous flyer Emma is sitting on a turbulent plane. She really thinks that this could be her last moment. So, naturally enough, she starts telling the man sitting next to her - quite a dishy American, but she\\'s too frightened to notice - all her secrets. How she scans the backs of intellectual books and pretends she\\'s read them. How she\\'s not sure if she has a G-spot, and whether her boyfriend could find it anyway. How she feels like a fraud at work - everyone uses the word \\'operational\\' all the time but she hasn\\'t a clue what it means. How she once threw a troublesome client file in the bin. If ever there was a bare soul, it\\'s hers.\\n\\nShe survives the flight, of course, and the next morning the famous founding boss of the whole mega corporation she works for is coming for a look at the UK branch. As he walks around, Emma looks up and realises...\\n\\nIt\\'s the man from the plane.\\n\\nWhat will he do with her secrets? He knows them all - but she doesn\\'t know a single one of his. Or... does she?\\n\\nEverybody loves Sophie Kinsella:\\n\"I almost cried with laughter\" Daily Mail\\n\"Hilarious . . . you\\'ll laugh and gasp on every page\" Jenny Colgan\\n\"Properly mood-altering . . . funny, fast and farcical. I loved it\" Jojo Moyes\\n\"A superb tale. Five stars!\" Heat',\n",
              " 'HBO’s hit series A GAME OF THRONES is based on George R. R. Martin’s internationally bestselling series A SONG OF ICE AND FIRE, the greatest fantasy epic of the modern age. A FEAST FOR CROWS is the fourth volume in the series.\\nThe Lannisters are in power on the Iron Throne.\\nThe war in the Seven Kingdoms has burned itself out, but in its bitter aftermath new conflicts spark to life. The Martells of Dorne and the Starks of Winterfell seek vengeance for their dead. Euron Crow’s Eye, as black a pirate as ever raised a sail, returns from the smoking ruins of Valyria to claim the Iron Isles.\\nFrom the icy north, where Others threaten the Wall, apprentice Maester Samwell Tarly brings a mysterious babe in arms to the Citadel. As plots, intrigue and battle threaten to engulf Westeros, victory will go to the men and women possessed of the coldest steel and the coldest hearts.',\n",
              " 'A Time To Kill is available in paperback. The book has 672 pages. The language of the book is English, and the author of the book is John Grisham.\\nA ten-year old African American girl is brutally raped and beaten by two white racist drunken men. Inspite of committing such a heinous crime, the perpetrators remain unrepentant. This horrendous act unleashes mayhem in the town of Clayton, Mississippi where the story is set.\\nCarl Lee, the father of the wronged girl, is driven into a mad rage and decides to take the law in his hands in order to bring justice to his daughter. Armed with an assault rifle, he sets out to avenge the gruesome crime committed on his daughter and kills the culprits. He is arrested and is to be tried for capital murder.\\nThis is a moving tale about a young defence lawyer, Jake Brigance, who decides to defend the black Vietnam war hero. The people of the town are divided in their opinion about the the crime committed by Carl. The book takes you through a journey of the lawyer’s struggle to save his client. Also at stake is the young attorney’s life. The novel was published in the year 1992 by RHUK.\\nKey Features:\\nThe book sensitively explores the relationship between black and whites. It is a legal suspense thriller and keeps you on an edge at all times.',\n",
              " 'Make sure to check out the other installments in this unparalleled collection of historical information on The Legend of Zelda franchise with the New York Times best selling The Legend of Zelda: Hyrule Historia and The Legend of Zelda: Encyclopedia. Also look for The Legend of Zelda: Breath of the Wild — Creating a Champion for an indepth look at the art, lore, and making of the best selling video game! \\n\\n\\nThe Legend of Zelda™: Art and Artifacts contains over four hundred pages of fully realized illustrations from the entire thirty-year history of The Legend of Zelda™ including artwork from the upcoming The Legend of Zelda™: Breath of the Wild! Every masterwork is printed on high-quality paper in an oversized format so you can immerse yourself in the fine details of each piece. This book includes rare promotional pieces, game box art, instruction booklet illustrations, official character illustrations, sprite art, interviews with the artists, and much, much more! The Legend of Zelda™: Art and Artifacts collects many of your favorite masterpieces from the storied franchise, as well as rare and never before seen content, presented in one handsome hardcover.\\n\\nSelect artwork from the entirety of the franchise!\\n \\nA nostalgic look at the past! \\n \\nAn exciting look at the future!\\n \\nInterviews with some of the artists behind The Legend of Zelda™ series!',\n",
              " 'A Number One New York Times bestseller, Humans of New York began in the summer of 2010, when photographer Brandon Stanton set out on an ambitious project: to single-handedly create a photographic census of New York City. Armed with his camera, he began crisscrossing the city, covering thousands of miles on foot, all in his attempt to capture ordinary New Yorkers in the most extraordinary of moments. The result of these efforts was Humans of New York, a vibrant blog in which he featured his photos alongside quotes and anecdotes. The blog has steadily grown, now boasting nearly a million devoted followers. Humans of New York is the book inspired by the blog. With four hundred colour photos, including exclusive portraits and all-new stories, and a distinctive vellum jacket it is a stunning collection of images that will appeal not just to those who have been drawn in by the outsized personalities of New York, but to anyone interested in the breathtaking scope of humanity it displays. Heartfelt and moving, Humans of New York is a celebration of individuality and a tribute to the spirit of a city.',\n",
              " '50 Greatest Horror Stories is a selection of some of the best horror fiction from all over the world, bringing together writings by great masters of the genre, carefully picked for their timeless quality. It features some of the most powerful tales that have attained the status of classics, such as ‘The Monkey’s Paw’, ‘The Signal Man’, ‘The Cobweb’, and many more.\\nThe oldest and strongest emotion of mankind is fear, the catechism of the unknown. Be it the supernatural, the gothic or the terror of the unexpected, this selection covers it all and is bound to send a chill down the spine.\\nA collector’s item, this anthology is as good as a haunting!',\n",
              " 'This stunning volume was the gift book of the year when it first published, and the images that grace its pages remain iconic. From the famous Afghan girl whose haunting green eyes stare out from the book’s cover, and her poignant story that captured the world’s interest, to award-winning photography culled from the Society’s vast archives, The Photographs offers readers an inside look at National Geographic and a sharp-eyed view of the world. The book showcases the skill and imagination of such notable Geographic photographers as David Doubilet, William Albert Allard, Sam Abell, Jim Stanfield, Jodi Cobb, Jim Brandenburg, David Alan Harvey, and many more. They share their techniques, as well as personal and colorful anecdotes about individual images and their adventures in the field—sometimes humorous, sometimes terrifying, always vividly compelling. Author Leah Bendavid-Val writes about the photographers’ achievements from technical, journalistic, and artistic perspectives.\\n\\nFive chapters cover core National Geographic themes—wildlife on land and water; cultures in the United States and around the world; and science, from astronomy to archaeology to the human senses. The photographs in each chapter capture rare moments in nature and the lives of animals, along with defining events in the lives of people everywhere. This exquisite collection is as elegant as it is timeless.',\n",
              " 'An ambitious intern. A perfectionist executive. And a whole lot of name calling. Discover the story that garnered more than two million reads online.\\n\\nWhip-smart, hardworking, and on her way to an MBA, Chloe Mills has only one problem: her boss, Bennett Ryan. He’s exacting, blunt, inconsiderate—and completely irresistible. A Beautiful Bastard.\\n\\nBennett has returned to Chicago from France to take a vital role in his family’s massive media business. He never expected that the assistant who’d been helping him from abroad was the gorgeous, innocently provocative—completely infuriating—creature he now has to see every day. Despite the rumors, he’s never been one for a workplace hookup. But Chloe’s so tempting he’s willing to bend the rules—or outright smash them—if it means he can have her. All over the office\\n\\nAs their appetites for one another increase to a breaking point, Bennett and Chloe must decide exactly what they’re willing to lose in order to win each other.\\n\\nOriginally only available online as The Office by tby789—and garnering over 2 million reads on fanfiction sites—Beautiful Bastard has been extensively updated for re-release.',\n",
              " 'The Hindu Temple is a comprehensive exposé on the evolution of a simple and sacred place of worship to a monument, which over the centuries became the pivot of Indian religiosity, culture and aesthetic significance; the superordinate ideological focus of society and polity. Tracing its origin to the pre-Christian era, its sanctity and the unwavering reverence accorded to it even today, is manifested in the elaborate rituals performed within its precincts by devotees, and the festivals celebrated by them. The book brings into special prominence the relevance of the temple in the social, economic and political integration of the subcontinent. Authoritatively written and impressively illustrated with exceptional and rare photographs, it elucidates in minute and graphic detail the multifaceted sculptural and architectural wonder that is the Hindu temple.',\n",
              " \"A hands-on guide with a minimalist and flexible approach that enables quick learning and rapid delivery of cloud-ready enterprise applications with Angular 6 Key Features Explore tools and techniques to push your web app to the next level Master Angular app design and architectural considerations Learn continuous integration and deploy your app on a highly available cloud infrastructure in AWS Book DescriptionAngular 6 for Enterprise-Ready Web Applications follows a hands-on and minimalist approach demonstrating how to design and architect high quality apps. The first part of the book is about mastering the Angular platform using foundational technologies. You will use the Kanban method to focus on value delivery, communicate design ideas with mock-up tools and build great looking apps with Angular Material. You will become comfortable using CLI tools, understand reactive programming with RxJS, and deploy to the cloud using Docker. The second part of the book will introduce you to the router-first architecture, a seven-step approach to designing and developing mid-to-large line-of-business applications, along with popular recipes. You will learn how to design a solid authentication and authorization experience; explore unit testing, early integration with backend APIs using Swagger and continuous integration using CircleCI. In the concluding chapters, you will provision a highly available cloud infrastructure on AWS and then use Google Analytics to capture user behavior. By the end of this book, you will be familiar with the scope of web development using Angular, Swagger, and Docker, learning patterns and practices to be successful as an individual developer on the web or as a team in the Enterprise. What you will learn Create full-stack web applications using Angular and RESTful APIs Master Angular fundamentals, RxJS, CLI tools, unit testing, GitHub, and Docker Design and architect responsive, secure and scalable apps to deploy on AWS Adopt a minimalist, value-first approach to delivering your app with Kanban Get introduced to automated testing with continuous integration on CircleCI Optimize Nginx and Node.js web servers with load testing tools Who this book is forThis book is for developers who want to confidently deliver high-quality and production-grade Angular apps from design to deployment. We assume that you have prior experience in writing a RESTful API with the tech stack of your choice; if you don't, you can still gain a lot of benefit from this book, which focuses on the entire scope of frontend development, from design to deployment!\",\n",
              " 'After setting their YouTube videos in viral motion, Dan and Phil share their extraordinary world with the readers in this book. Dan Howell, who constantly claims that he is not on fire (danisnotonfire) and Phil Lester, also known as Amazing Phil, are two popular YouTubers who have come together to share their bizarre world with fans through this book.\\nThese colourful characters from UK are two of the most amusing comedians on the internet and coincidentally they are friends in real life. Their twisted take on random casual events leaves their viewers laughing out loud which explains their 11-million and more viewer community. Through the book, these young comedians share with their fans some exclusive behind-the-scene facts and also some secrets, which they might grow up to regret someday.\\nPhil discusses why he was such a weird kid while sharing some pages from his secret diary. Dan discovers why he has been such a fail so far. With their twisted advices on how to react in some common awkward situations and how to draw perfect cat whiskers, the reader also gets the inside story of when they met One Direction. Apart from all the madness and comical parts that make this book, there are also a lot of unseen colour photographs of the two touring around the world, their drawings and more.\\nAbout the author:\\nDan Howell and Phil Lester: Two young YouTube vloggers and entertainers from the United Kingdom, Dan Howell and Phil Lester’s quirky comic videos about their lives and observations on the world have earned them the status of great comedy maestros with both of them sharing around 11-million viewers in their community.',\n",
              " 'Colleen Hoover, the New York Times bestselling author of Maybe Someday, brilliantly brings to life the story of the hilarious and charismatic Warren in this new novella.\\n\\nWhen Warren has the opportunity to live with a female roommate, he instantly agrees. It could be an exciting change.\\n\\nOr maybe not.\\n\\nEspecially when that roommate is the cold and seemingly calculating Bridgette. Tensions run high and tempers flare as the two can hardly stand to be in the same room together. But Warren has a theory about Bridgette: anyone who can hate with that much passion should also have the capability to love with that much passion. And he wants to be the one to test this theory.\\n\\nWill Bridgette find it in herself to warm her heart to Warren and finally learn to love?\\n\\nMaybe.\\n\\nMaybe not.',\n",
              " \"Who really is A.R. Rahman? We know the music. But do we know the man? For the first time, a nation's pride--winner of National Film awards, Oscars, Grammys and hearts--opens up about his philosophies: hope, perseverance, positivity and love. From his early days as a composer of advertisement jingles to his first big break in feature films, from his keenness to integrate new technology with good old-fashioned music scores to the founding of his music school, from his resounding entry on to the international stage to his directorial debut, from his philanthropy to his inner life, Notes of a Dream captures Rahman's extraordinary success story with all the rhythm and melody, the highs and lows, of a terrific soundtrack by the man himself.\\nFeaturing intimate interviews with the soft-spoken virtuoso, as well as insights and anecdotes from key people in his life, this balanced, uplifting and affectionate book is the definitive biography of A.R. Rahman--the man behind the music and the music that made the man.\",\n",
              " 'Selected pieces for Trinity College London piano exams for 2018-2020, expertly graded and edited by leading music educators, with informative teaching notes for all pieces (including the alternative repertoire not in the book). Includes a CD of recordings of all pieces for the grade (including alternative repertoire).\\nPlease note that the book does not contain the sheet music for the alternative repertoire.',\n",
              " 'NEW YORK TIMES BESTSELLER\\n\\n“A bright new voice in the fantasy genre” (George R. R. Martin), acclaimed author Scott Lynch continues to astound and entertain with his thrillingly inventive, wickedly funny, suspense-filled adventures featuring con artist extraordinaire Locke Lamora. And The Republic of Thieves is his most captivating novel yet.\\n \\nWith what should have been the greatest heist of their career gone spectacularly sour, Locke and his trusted partner, Jean, have barely escaped with their lives. Or at least Jean has. But Locke is slowly succumbing to a deadly poison that no alchemist or physiker can cure. Yet just as the end is near, a mysterious Bondsmage offers Locke an opportunity that will either save him or finish him off once and for all.\\n \\nMagi political elections are imminent, and the factions are in need of a pawn. If Locke agrees to play the role, sorcery will be used to purge the venom from his body—though the process will be so excruciating he may well wish for death. Locke is opposed, but two factors cause his will to crumble: Jean’s imploring—and the Bondsmage’s mention of a woman from Locke’s past: Sabetha. She is the love of his life, his equal in skill and wit, and now, his greatest rival.\\n \\nLocke was smitten with Sabetha from his first glimpse of her as a young fellow orphan and thief-in-training. But after a tumultuous courtship, Sabetha broke away. Now they will reunite in yet another clash of wills. For faced with his one and only match in both love and trickery, Locke must choose whether to fight Sabetha—or to woo her. It is a decision on which both their lives may depend.\\n\\nPRAISE FOR SCOTT LYNCH\\n\\nThe Republic of Thieves\\n \\n“Fast paced, fun, and impossible to put down . . . Locke and company remain among the most engaging protagonists in fantasy.”—Publishers Weekly (starred review)\\n \\n“The Republic of Thieves has all the colorful action, witty repartee, and devious scheming that fans of the series have come to expect.”—Wired\\n \\n“A fantasy world unique among its peers . . . If you’re looking for a great new fantasy series this is one you won’t want to miss. . . . In a word: AWESOME!”—SF Revu\\n\\nRed Seas Under Red Skies\\n \\n“Lynch hasn’t merely imagined a far-off world, he’s created it, put it all down on paper—the smells, the sounds, the people, the feel of the place. The novel is a virtuoso performance, and sf/fantasy fans will gobble it up.”—Booklist (starred review)\\n\\n“Red Seas Under Red Skies firmly proves that Scott Lynch isn’t a one-hit wonder. . . . It’ll only be a matter of time before Scott Lynch is mentioned in the same breath as George R. R. Martin and Steven Erikson.”—Fantasy Book Critic\\n \\n“Grand, grandiose, grandiloquent . . . No critic is likely to fault Lynch in his overflowing qualities of inventiveness, audacious draftsmanship, and sympathetic characterization.”—Locus\\n\\nThe Lies of Locke Lamora\\n \\n“Right now, in the full flush of a second reading, I think The Lies of Locke Lamora is probably in my top ten favorite books ever. Maybe my top five. If you haven’t read it, you should. If you have read it, you should probably read it again.”—Patrick Rothfuss, New York Times bestselling author of The Name of the Wind\\n\\n\\nFrom the Hardcover edition.',\n",
              " 'A revised edition of the classic drawing book that has sold more than 1.7 million copies in the United States alone. Translated into more than seventeen languages, Drawing on the Right Side of the Brain is the world\\'s most widely used drawing instruction book. Whether you are drawing as a professional artist, as an artist in training, or as a hobby, this book will give you greater confidence in your ability and deepen your artistic perception, as well as foster a new appreciation of the world around you. This revised/updated fourth edition includes: a new preface and introduction; crucial updates based on recent research on the brain\\'s plasticity and the enormous value of learning new skills/ utilizing the right hemisphere of the brain; new focus on how the ability to draw on the strengths of the right hemisphere can serve as an antidote to the increasing left brain emphasis in American life; the worship of all that is linear, analytic, digital, etc; an informative section that addresses recent research linking early childhood \"scribbling\" to later language development and the importance of parental encouragement of this activity and new reproductions of master drawings throughout. A life changing book, this fully revised and updated edition of Drawing on the Right Side of the Brain is destined to inspire generations of readers to come.',\n",
              " \"Using the Oxford reading Tree Floppy's phonics sound and letters programme and synthetic phonics, the Oxford phonics spelling dictionary helps children become proficient readers and spellers. With 4000 words, ordered by sounds and spellings and linked to the alphabetic code chart, it makes preparing for the phonics screening check simple and fun.\",\n",
              " \"Penelope Lively's Booker Prize winning classic, Moon Tiger is a haunting story of loss and desire. Claudia Hampton - beautiful, famous, independent, dying. But she remains defiant to the last, telling her nurses that she will write a 'history of the world . . . and in the process, my own'. And it is her story from a childhood just after the First World War through the Second and beyond. But Claudia's life is entwined with others and she must allow those who knew her, loved her, the chance to speak, to put across their point of view. There is Gordon, brother and adversary; Jasper, her untrustworthy lover and father of Lisa, her cool conventional daughter; and then there is Tom, her one great love, found and lost in wartime Egypt. 'Leaves its traces in the air long after you've put it away' Anne Tyler 'A complex tapestry of great subtlety. Lively writes so well, savouring the words as she goes' Daily Telegraph 'Very clever: evocative, thought-provoking and hangs on the mind long after it is finished' Literary Review Penelope Lively is the author of many prize-winning novels and short-story collections for both adults and children. She has twice been shortlisted for the Booker Prize: once in 1977 for her first novel, The Road to Lichfield, and again in 1984 for According to Mark. She later won the 1987 Booker Prize for her highly acclaimed novel Moon Tiger. Her other books include Going Back; Judgement Day; Next to Nature, Art; Perfect Happiness; Passing On; City of the Mind; Cleopatra's Sister; Heat Wave; Beyond the Blue Mountains, a collection of short stories; Oleander, Jacaranda, a memoir of her childhood days in Egypt; Spiderweb; her autobiographical work, A House Unlocked; The Photograph; Making It Up; Consequences; Family Album, which was shortlisted for the 2009 Costa Novel Award, and How It All Began. She is a popular writer for children and has won both the Carnegie Medal and the Whitbread Award. She was appointed CBE in the 2001 New Year's Honours List, and DBE in 2012. Penelope Lively lives in London.\",\n",
              " 'The first volume of Samaithu Paar was published in 1951. More than just a cookery book, it was intended to serve as a manual for daily use. Over the years, those who did not find time to learn cooking in the traditional way from their mothers have used the three volumes of Samaithu Paar to set up homes and manage kitchens all over the world.\\nThe Best of Samaithu Paar brings together a hundred most-loved recipes chosen from the three-volume original. Maintaining the simplicity of language, easy-to-follow directions and adherence to the smallest details, the recipes have been suitably revised and adapted using universal measures of cups and spoons and modern utensils and appliances in place of the more traditional ones.\\nRecipes range from the basic idli, dosai, sambar and rasam to their many variations that are not so familiar to all Indians. The book also includes specialities like Moar Kuzhambu, Mysore Rasam, Pongal, Murukku and Jangiri, as well as pachadis and pickles.\\nA must-have for all those who enjoy traditional Indian cuisine.',\n",
              " \"Master the moves of krav maga―the international self-defense and physical fitness sensation\\nIncreasingly popular around the world, krav maga is the renowned hand-to-hand Martial Arts defense fighting designed by the Israeli military forces. Swift, powerful, and simple, it is an effective method for fending off any kind of attacker―and it is also an amazing workout.\\nRegardless of size, strength, and fitness level, anyone can master the essentials of krav maga―and reap the rewards of increased safety, confidence, and conditioning. With moves you can learn in as little as five minutes, or train and practice for long-term success, Krav Maga covers all the below:\\n*What is Krav Maga all about\\n*Instruction on how to protect your body's vulnerable target and learn weapons defense combat\\n*Use an opponent's momentum to fuel your counterattack\\n*Escape all kinds of grips and holds\\n*Combine training punches, kicks, and other moves into a powerful conditioning workout---to lose weight, increase core strength, and improve muscle tone\\n*Use specially designed drills and a 12-week training program to become a kravist---a smart and prepared fighter\\nWritten by one of America's foremost krav maga experts, this exciting new guide opens the door to an empowering and important set of techniques that you or anyone can master.\",\n",
              " \"Sometimes, it seems like you can reach out and touch the past...\\n\\nAn old man wearing a brown robe is found wandering disoriented in the Arizona desert. He is miles from any human habitation and has no memory of how he got to be there, or who he is. The only clue to his identity is the plan of a medieval monastery in his pocket.\\n\\nIn France, Professor Edward Johnston and his students are studying the ruins of a medieval town. Suspicious of the knowledge of the site shown by their mysterious financier, he returns to the US to investigate. But in his absence, the students make a disturbing discovery in the ruins: the long-decayed remains of Johnston's glasses - and a message in modern English.\\n\\nThe implications are staggering. The consequences are earth-shaking. And the distant past isn't so distant any more.\\n______________________\\n\\nIncreasingly considered an underappreciated classic that stands proudly alongside his more famous works like Jurassic Park and Westworld, Timeline confirms Michael Crichton as the king of the high-concept thriller, and a master storyteller to boot.\",\n",
              " 'Rahul Pandita was fourteen years old when he was forced to leave his home in Srinagar along with his family. They were Kashmiri Pandits- the Hindu minority within a Muslim-majority Kashmir that was by 1990 becoming increasingly agitated with the cries of ‘Azaadi’ from India.\\nOur Moon Has Blood Clots is the story of Kashmir, in which hundreds of thousands of Kashmiri Pandits were tortured, killed and forced to leave their homes by Islamist militants and to spend the rest of their lives in exile in their own country. Rahul Pandita has written a deeply personal, powerful and unforgettable story of history, home and loss.',\n",
              " 'Sparks fly immediately when Rukmani—fierce and assertive in the best and worst possible ways—meets the gentle Ayaan in the magical city Paris. Meanwhile, back in India, her reticent sister, Mrinalini struggles to cope with the void of a loveless marriage and an early pregnancy.\\nTides Don’t Cross follows these extremely interesting characters as their lives cross in surprising ways. Mrinalini, Ayaan and Rukmani wade through choppy tides, unaware of their common destiny. Deeply touching, this is an unforgettable story of thwarted desires, of love and its loss, of losing and finding oneself, and of falling and learning to rise.\\nYoung and talented author Simar Malhotra has created yet another piercing and riveting read.',\n",
              " 'Build confidence in French grammar with hundreds of activities to embed the grammar knowledge necessary for exam success.\\n\\nSuitable for all exam boards and abilities, this French A-level Grammar Workbook will help students to:\\n- Extend their learning beyond the classroom by supplementing key resources such as course textbooks\\n- Develop their understanding with clear explanations of grammatical rules and exceptions\\n- Improve confidence with 190 exercises which build in complexity for each grammar point\\n- Strengthen translation skills by putting grammar into practice with 82 translation exercises\\n- Make the most of opportunities for self-directed learning and assessment with answers to activities supplied online',\n",
              " \"To Las Vegas... and beyond!\\n\\nBecky Brandon (née Bloomwood) is on a major rescue mission! Hollywood was full of surprises and now she's on a road trip to Las Vegas to help her friends and family.\\n\\nShe's determined to get to the bottom of why her dad has mysteriously disappeared, help her best friend Suze and even bond with Alicia Bitch Long-legs (maybe).\\n\\nAs Becky discovers just how much her friends and family need help, she comes up with her biggest, boldest, most brilliant plan yet! So can she save the day just when they need her most?\\n\\nBecky is setting out to make things right in this laugh-out-loud, feel-good conclusion to her American adventure that began with Shopaholic to the Stars.\",\n",
              " \"A growing threat from China leaves US President Jack Ryan with only a few desperate options in this continuation of the internationally bestselling Tom Clancy series.\\nAs the world shakes from seemingly disparate outbursts of terrorism, Jack Ryan prepares for a crucial negotiation with President Zhou's increasingly hostile China.\\nBut when a routine traffic stop on an isolated road in rural Texas leads to the discovery of a stolen USB stick, it becomes clear there is a dark connection between the attacks.\\nWith tensions rising, a US spy ship in the South China Sea gets caught up in a violent storm - a storm which threatens to push it into the path of the Chinese navy, with potentially disastrous consequences.\\nAs the international summit approaches, and the mastermind behind the violence remains at large, Jack Ryan has no choice but to face Zhou. Little does he realise he's walking straight into the jaws of the dragon...\\nPraise for Tom Clancy\\n'He constantly taps the current world situation for its imminent dangers and spins them into an engrossing tale'\\nNew York Times\\n'Heart-stopping action ... entertaining and eminently topical'\\nWashington Post\\n'Exhilarating. No other novelist is giving so full a picture of modern conflict'\\nSunday Times\\n'A brilliantly constructed thriller that packs a punch like Semtex'\\nDaily Mail\\n'A virtuoso display of page-turning talent'\\nSunday Express\",\n",
              " 'The Kabuliwala sells his wares in the streets of Calcutta, thinking of his little daughter who awaits him in faraway Afghanistan, an elderly stranger charms a group of unruly schoolboys who try to harass him. The lady of wishes passes by just as a father wishes he was his son and his son wishes he was the father-these stories-both commonplace and wildly imaginative are told with charming simplicity by the Nobel Prize Laureate in Literature. This book is the perfect introduction for younger readers to the magical world of Rabindranath Tagore.',\n",
              " \"IF YOU ONLY READ ONE BOOK THIS YEAR, MAKE IT THE GIRL BEFORE...\\n\\n'An outstanding debut that is more than a match for Paula Hawkins's The Girl on the Train' Sunday Times\\n\\nTHE SUNDAY TIMES BESTSELLER\\nTHE SUNDAY TIMES THRILLER OF THE MONTH\\nTHE SIMON MAYO RADIO 2 BOOK CLUB PICK\\nTHE NEW YORK TIMES BESTSELLER\\n\\n'DAZZLING' - Lee Child\\n'ADDICTIVE' - Daily Express\\n'DEVASTATING' - Daily Mail\\n'INGENIOUS' - The New York Times\\n'COMPULSIVE' - Glamour Magazine\\n'ELEGANT' - Peter James\\n'SEXY' - Mail on Sunday\\n'ENTHRALLING' - Woman and Home\\n'ORIGINAL' - The Times\\n'RIVETING' - Lisa Gardner\\n'CREEPY' - Heat\\n'SATISFYING' - Reader's Digest\\n'SUPERIOR' - The Bookseller\\n'This is going to be the buzziest book of 2017 . . . This year The Girl Before will be that book' InStyle\\n**********\\nEnter the world of One Folgate Street and discover perfection . . . but can you pay the price?\\n\\nJane stumbles on the rental opportunity of a lifetime: the chance to live in a beautiful ultra-minimalist house designed by an enigmatic architect, on condition she abides by a long list of exacting rules. After moving in, she discovers that a previous tenant, Emma, met a mysterious death there - and starts to wonder if her own story will be a re-run of the girl before.\\nAs twist after twist catches the reader off guard, Emma's past and Jane's present become inexorably entwined in this tense, page-turning portrayal of psychological obsession.\\n**********\\nFollowing in the footsteps of Gone Girl and The Girl on the Train, The Girl Before is being brought to the big screen by Academy Award-winning director Ron Howard.\",\n",
              " 'Political violence, or terrorism, by State as well as by non- State actors has a long history in India. The allegation that sections of and individual Indian Muslims indulged in terrorism surfaced for the first time with the ascent of the Hindutva forces in mid-1990s and became state policy with the BJP s coming to power at the Centre. With even secular media joining the role as stenographers of security agencies, this became an accepted fact so much so that common Indians and even many Muslims started believing in this false propaganda. This book, by a former senior police officer, with a distinguished career that included unearthing the Telgi scam, peeps behind the propaganda screen, using material mostly in the public domain as well as his long police experience. It comes out with some startling facts and analysis, the first of its kind, to expose the real actors behind the so-called Islamic terrorism in India whose greatest feat was to murder the Maharashtra ATS chief Hemant Karkare who dared to expose these forces and paid with his life for his courage and commitment to truth. While unearthing the conspiracy behind the murder of Karkare, this book takes a hard look at some of the major incidents attributed to Islamic terrorism in India and finds them baseless.',\n",
              " \"What bird can lift the heaviest weight?\\nWhat is worse than finding a worm in an apple?\\nWhat animal doesn't play fair?\\nThese and about 600 other howlers and teeth-gritters will be found in this entertaining collection. You'll find rib-tickling riddles on all sorts of subjects, from animals, plants, fruits and people, to love and courtship, and eating and drinking.\\nYou'll have loads of fun baffling friends and relatives with this nearly inexhaustible treasury of favorite riddles that have puzzled people for generations. Parents and teachers will remember the fun times they had when they endlessly recited these humorous stumpers in their own younger days. Ready to tickle the funny bones and challenge the wits of riddle lovers of all ages, these brain-teasers, puns, and puzzles are perfect for enjoying by yourself or to liven up parties, family gatherings and other group occasions.\\nAnswers: The crane. Finding half a worm. The Cheetah.\",\n",
              " \"In Letters to a Young Gymnast, Nadia Comaneci tells how she found the inner strength to become a world-class athlete at such a young age. Now a woman of tremendous poise and self-assurance, she offers unique insights into the mind of a top competitor. From how to live after you've realized your dream to the necessity of “a spirit forged with mettle,” Comaneci's thoughts on athleticism and sacrifice are eye-opening.\",\n",
              " \"The international bestseller, translated from the German by Simon Pare.\\nOn a beautifully restored barge on the Seine, Jean Perdu runs a bookshop; or rather a 'literary apothecary', for this bookseller possesses a rare gift for sensing which books will soothe the troubled souls of his customers.\\nThe only person he is unable to cure, it seems, is himself. He has nursed a broken heart ever since the night, twenty-one years ago, when the love of his life fled Paris, leaving behind a handwritten letter that he has never dared read. His memories and his love have been gathering dust - until now. The arrival of an enigmatic new neighbour in his eccentric apartment building on Rue Montagnard inspires Jean to unlock his heart, unmoor the floating bookshop and set off for Provence, in search of the past and his beloved.\",\n",
              " 'DARKSEID VS. THE ANTI-MONITOR!\\n \\nThe Justice League first came together years ago to stop the Darkseid and his parademon army from invading our Earth. Now Darkseid will once again make the planet a warzone, as Earth becomes the frontline in his battle with the Anti-Monitor, one of the most powerfully destructive creatures ever created. \\n \\nWonder Woman, Superman, Batman and the rest of the Justice League are working with Mister Miracle to stop the coming bloodshed, but when two unstoppable forces of evil go to war, even the world’s greatest heroes might not be enough to save the world! \\n \\nCollects JUSTICE LEAGUE #40-44 and DC SNEAK PEEK: JUSTICE LEAGUE #1.',\n",
              " 'Mr. Wolf\\nMr. Shark\\nMr. Snake\\nMr. Piranha\\nThey are Bad Guys, everybody knows that. They’re scary and dangerous and well . . . just BAD. But these guys want to be HEROES. And they’re going to prove it by doing good deeds . . . whether you want them to or not.\\nBuckle up for the funniest, naughtiest and coolest book you’ll ever read – it’s time to meet the BAD GUYS.',\n",
              " 'Describing the roots of the Mumbai Mafia and its genesis into what it has become today, the book revolves around the life of Dawood Ibrahim. Some of the other gangsters that share the spotlight with Dawood in the book are Chhota Rajan, Karim Lala, Abu Salem, Haji Mastan and Varadarajan Mudaliar. The stories of each of these hooligans have been detailed along with the list of their illegal actions.\\nTalking about the Mumbai mafia’s history through the last six decades, the book takes a deep plunge into the life of the most famous goon in the nation, Dawood. His story from his days in Dongri, Mumbai to his rise to becoming the international terrorist that is known as today has been described. Dawood’s craving for power, his unaltered focus and his astute mind and tactics have been discussed along with the descriptions of his first robbery, his youth and his love affairs.\\nPolitical connections and international links have been scrutinized in this book along with some unknown facts, unreported crimes, events and stories that have not been covered before. The book was also adapted into a Bollywood movie and is available in paperback.\\nAbout the author:\\nA well-known Indian author and crime reporter, S. Hussain Zaidi has worked for esteemed newspapers across India, including Mumbai Mirror, Mid-Day, Asian Age and Indian Express. Apart from this book, he has also authored ‘Headley and I’ and ‘Black Friday: The True Story of the Bombay Bomb Blasts’ among others.',\n",
              " \"This book contains nine pieces from ABRSM's Grade 1 Piano Syllabus for 2019 & 2020, three pieces chosen from each of Lists A, B and C. The pieces have been carefully selected to offer an attractive and varied range of styles, creating a collection that provides an excellent source of repertoire to suit every performer. The book also contains helpful footnotes and, for those preparing for exams, useful syllabus information. Recordings of all 18 pieces on the Grade 1 syllabus are available. These can be purchased as part of the Piano Exam Pieces with CD package or as audio downloads (see www.abrsmdownloads.org for more details). ABRSM also offers a range of apps to support musical learning, available from www.abrsm.org/apps.\",\n",
              " \"'Utterly fascinating' Daisy Goodwin, Sunday Times Benjamin Franklin took daily naked air baths and Toulouse-Lautrec painted in brothels. Edith Sitwell worked in bed, and George Gershwin composed at the piano in pyjamas. Freud worked sixteen hours a day, but Gertrude Stein could never write for more than thirty minutes, and F. Scott Fitzgerald wrote in gin-fuelled bursts - he believed alcohol was essential to his creative process. From Marx to Murakami and Beethoven to Bacon, Daily Rituals by Mason Currey presents the working routines of more than a hundred and sixty of the greatest philosophers, writers, composers and artists ever to have lived. Whether by amphetamines or alcohol, headstand or boxing, these people made time and got to work. Featuring photographs of writers and artists at work, and filled with fascinating insights on the mechanics of genius and entertaining stories of the personalities behind it, Daily Rituals is irresistibly addictive, and utterly inspiring.\",\n",
              " 'A volume of plays from the founding architect of twentieth-century drama, including his most popular and controversial work\\n\\nA Penguin Classic\\n\\nPirandello is brilliantly innovatory in his forms and themes, and in the combined energy, imagination and visual colours of his theatre. This volume of plays, translated from the Italian by Mark Musa, opens with Six Characters in Search of an Author, in which six characters invade the stage and demand to be included in the play. The tragedy Henry IV dramatizes the lucid madness of a man who may be King. In So It Is (If You Think So), the townspeople exercise a morbid curiosity attempting to discover “the truth” about the Ponza family. Each of these plays can lay claim to being Pirandello’s masterpiece, and in exploring the nature of human personality, each one stretches the resources of drama to their limits.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,800 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.',\n",
              " 'New Insight into IELTS offers comprehensive preparation and practice for IELTS. By exploring the test paper by paper, and looking in detail at each task type, the course gradually builds up the skills, language and test techniques students need to approach IELTS with confidence. The course contains a detailed introduction to the test and a full answer key and is equally suitable for use in the classroom or for self-study. The material is intended for use with students whose current level is around Band 6 and is suitable for both Academic and General Training candidates.',\n",
              " \"A legendary language learning course undoubtedly! Rapidex English Speaking Course is the only speaking course of its kind whose features can be described in superlative degree only. It has already benefited millions of readers across the country. Experts have unanimously declared it as the best self-study course. Perhaps that is the reason that for over 15 years it has constantly been on the top of the best sellers list in the market.\\nWhat is the secret of its great popularity? Maybe, a no-nonsense down-to-earth approach and a very scientific methodology that takes into its account the special requirements of Indian learners. In any case it is not just an another crash course that begins with a bang but ends with a whimper. Rather its contents and subject matters have been specially designed to help a common man achieve maximum command on the language in a very short time. The most important part of this self-study course is confidence building measures because it has been found from the long experience that biggest problem in the path of fluently speaking a foreign language is hesitation, that arises out of lack of confidence. Many people, who have good command over the language and are well are of its grammar and composition, can't speak the language fluently. Why? The answer is simple. While speaking they try to construct a sentence in their mother language and do the mental translation before uttering a single line. This creates not only confusion but utter chaos in their mind. As a result they feel great hesitation to speak simple sentences. In some cases, however, the problem is more complex, as they lack the basic knowledge of the English language. Rapidex English Speaking Course takes care of both the cases.\\nSo, what are you waiting for? Go ahead and discover the secret of fluency in English speaking. Explore and develop your hidden potential and face the world with a new-found confidence and smartness. Only the sky should be your limit. No matter which language you know or speak, be it Hindi, Marathi, Gujarati, Tamil, Telugu, Assamese, Oriya, Kannada, Punjabi, Bengali, Nepali or Urdu, with the help of this book English will no more be a foreign language for you. A must for every Indian home.\",\n",
              " \"Evan Waller is a monster. He has built a fortune from his willingness to buy and sell anything . . . and anyone. In search of new opportunities, Waller has just begun a new business venture: one that could lead to millions of deaths all over the globe. On Waller's trail is Shaw, the mysterious operative from The Whole Truth, who must prevent Waller from closing his latest deal. Shaw's one chance to bring him down will come in the most unlikely of places: a serene, bucolic village in Provence. But Waller's depravity and ruthlessness go deeper than Shaw knows. And now, there is someone else pursuing Waller in Provence – Reggie Campion, an agent for a secret vigilante group headquartered in a musty old English estate – and she has an agenda of her own. Hunting the same man and unaware of each other's mission, Shaw and Reggie will be caught in a deadly duel of nerve and wits. Deliver Us From Evil is Hitchcockian in its intimate build-up of suspense and filled with the remarkable characters, breathtaking plot turns, and blockbuster finale that are David Baldacci's hallmarks.\",\n",
              " \"Discover the world's best teams and biggest competitions with the ultimate visual guide to football, fully updated to include the results of the 2018 World Cup.\\nAre you a keen player, a lifelong supporter, or simply an armchair football manager? The Football Book has something for everyone, packed with stats and facts. From the history of football to the most recent tournaments, The Football Book reveals the story behind the game.\\nProfiling more than 65 of the world's best sporting nations, including the winners of the 2018 World Cup, France, the biggest names in club football and looking at techniques from free-kicks to block tackling, this is the ultimate guide to the beautiful game.\\nPrevious ISBN: 9780241317631.\",\n",
              " 'IPL Cricket And Commerce: An Inside Story aims to give the reader an insight into the Indian Premier League, how it originated, and the processes that go on within it.\\nThe book chronicles how the IPL was incepted. It explains how it originated and the politics that were involved in the background. The book also attempts to explain how it became a success and to trace its path to becoming one of the most watched televised sports tournaments. The book sheds light on the character of Lalit Modi and shows how his unique mix of luck and ability to make intelligent decisions helped him to pull off the creation and execution of the Indian Premier League.\\nThe influence of the IPL on the institution of cricket in India and the after effects have also been covered in this book. The lives of Shane Warne and Lalit Modi, and how the IPL turned their situation around completely is explained in the book, as is the overnight transformation of the lives of small-time Indian cricketers who got their big break when they were given the chance to prove themselves by playing in the IPL. Interviews with some of these young players have also been included. The book describes the controversies surrounding the IPL as well, including the entire bidding process. The manner in which national and international players are bid on in a grand auction is explained in detail.\\nIPL Cricket And Commerce: An Inside Story was published by Roli Books Pvt. Ltd in 2009 in paperback.',\n",
              " 'In 1988, the World Health Organization launched what was intended to be a twelve-year campaign to wipe out the polio virus and end the disease for all time. Seventeen years after that deadline, and several billion dollars over budget, the polio campaign continues to grind on, vaccinating millions of children and hoping that each new year might see an end to the disease. A surprisingly resilient polio virus, an unexpectedly weak vaccine, uninterested governments and public indifference in those countries still afflicted by the disease, added to the vagaries of global politics, have meant that success remains elusive.\\nHow did an innocuous campaign to rid the world of a crippling disease become a hostage in geopolitical wars? Why do parents refuse to vaccinate their children against polio? And why have poorly paid healthworkers, trudging from door to door delivering drops of polio vaccine, been assassinated?\\nDrawing on detailed interviews with key players and reporting from the frontlines of the war against this potentially deadly disease, Thomas Abraham records the story of one of the world’s most ambitious health campaigns, and draws lessons for the future.',\n",
              " 'South African born Elon Musk is the renowned entrepreneur and innovator behind PayPal, SpaceX, Tesla, and SolarCity. Musk wants to save our planet; he wants to send citizens into space, to form a colony on Mars; he wants to make money while doing these things; and he wants us all to know about it. He is the real-life inspiration for the Iron Man series of films starring Robert Downey Junior.\\n\\nThe personal tale of Musk’s life comes with all the trappings one associates with a great, drama-filled story. He was a freakishly bright kid who was bullied brutally at school, and abused by his father. In the midst of these rough conditions, and the violence of apartheid South Africa, Musk still thrived academically and attended the University of Pennsylvania, where he paid his own way through school by turning his house into a club and throwing massive parties.\\n\\nHe started a pair of huge dot-com successes, including PayPal, which eBay acquired for $1.5 billion in 2002. Musk was forced out as CEO and so began his lost years in which he decided to go it alone and baffled friends by investing his fortune in rockets and electric cars. Meanwhile Musk’s marriage disintegrated as his technological obsessions took over his life ...\\n\\nElon Musk is the Steve Jobs of the present and the future, and for the past twelve months, he has been shadowed by tech reporter, Ashlee Vance. Elon Musk: How the Billionaire CEO of Spacex and Tesla is Shaping our Future is an important, exciting and intelligent account of the real-life Iron Man.',\n",
              " \"'Deliriously clever' GUARDIAN\\n-----\\nNow a major BBC drama: The Strike series\\nWhen a mysterious package is delivered to Robin Ellacott, she is horrified to discover that it contains a woman's severed leg.\\nHer boss, private detective Cormoran Strike, is less surprised but no less alarmed. There are four people from his past who he thinks could be responsible - and Strike knows that any one of them is capable of sustained and unspeakable brutality.\\nWith the police focusing on the one suspect Strike is increasingly sure is not the perpetrator, he and Robin take matters into their own hands, and delve into the dark and twisted worlds of the other three men. But as more horrendous acts occur, time is running out for the two of them...\\nA fiendishly clever mystery with unexpected twists around every corner, Career of Evil is also a gripping story of a man and a woman at a crossroads in their personal and professional lives. You will not be able to put this book down.\\n*** The latest book in the thrilling Strike series, LETHAL WHITE is out now! ***\\n\\n-----\\nPRAISE FOR THE STRIKE SERIES:\\n'One of the most unique and compelling detectives I've come across in years' MARK BILLINGHAM\\n'The work of a master storyteller' DAILY TELEGRAPH\\n'Unputdownable. . . Irresistible' SUNDAY TIMES\\n'Will keep you up all night' OBSERVER\\n'A thoroughly enjoyable classic' PETER JAMES, SUNDAY EXPRESS\",\n",
              " \"Black Mirror is hands down the most relevant program of our time, if for no other reason than how often it can make you wonder if we’re all living in an episode of it.’ – New York Times\\nWhat becomes of humanity when it’s fed into the jaws of a hungry new digital machine? Discover the world of Black Mirror in this immersive, illustrated, oral history.\\nThis first official book logs the entire Black Mirror journey, from its origins in creator Charlie Brooker’s mind to its current status as one of the biggest cult TV shows to emerge from the UK. Alongside a collection of astonishing behind-the-scenes imagery and ephemera, Brooker and producer Annabel Jones will detail the creative genesis, inspiration and thought process behind each film for the first time, while key actors, directors and other creative talents relive their own involvement.\\n‘Brooker continues to solidify himself as one of the most creative writers in the medium. Even when the unfair creep of expectations rears up, Black Mirror and Brooker deliver.’ – The Hollywood Reporter\\n‘Black Mirror: the future is already here and it's terrifying’ - Telegraph.\",\n",
              " \"The fascinating life story of professional cricketer Kevin Pietersen, MBE, from his childhood in South Africa to his recent experiences as one of the leading lights in the world of international cricket.\\nKevin was dropped from the England squad in February of this year, seemingly calling time on an international career that began nearly ten years earlier. The decision puzzled many observers - although the England team had failed miserably in the Ashes tour of 2013-14, Kevin was the tourists' leading run scorer across the series, and he remains the country's highest run scorer of all time across all formats of the game.\\nThis autumn Kevin will reveal all in his autobiography, telling the stories behind the many other highs and lows of his incredible career. Giving readers the full story of his life, from his childhood in South Africa to his recent experiences as one of the leading lights in the world of international cricket, this will be an autobiography that entertains and fascinates readers in equal measure.\",\n",
              " \"'I'll stop doing it as soon as I understand what I'm doing.' Somewhere between a historical account and work of philosophy, Socrates' Defence details the final plea of Plato's beloved mentor. Introducing Little Black Classics: 80 books for Penguin's 80th birthday. Little Black Classics celebrate the huge range and diversity of Penguin Classics, with books from around the world and across many centuries. They take us from a balloon ride over Victorian London to a garden of blossom in Japan, from Tierra del Fuego to 16th-century California and the Russian steppe. Here are stories lyrical and savage; poems epic and intimate; essays satirical and inspirational; and ideas that have shaped the lives of millions. Plato (474 BC-347 BC). Plato's works available in Penguin Classics are Republic, The Last Days of Socrates, The Laws, Phaedrus, Protagoras and Meno, Timaeus and Critias, Theaetetus, Early Socratic Dialogues, The Symposium and Gorgias.\",\n",
              " 'This sparkling story of transatlantic manners follows the fortunes of playboy Jimmy Crocker in England and America. When Jimmy falls for a girl in London and vows to reform himself as a result, the quest for love leads him to his Aunt Nesta’s house in New York, where his escapades involve impersonating himself and attempting to kidnap Nesta’s odious son Ogden – with the boy demanding a cut of the ransom money. A full flush of minor characters – pretentious poets, butlers, boxers, put-upon husbands and Wall Street businessmen – make the comedy crackle as only Wodehouse knew how.',\n",
              " \"About the Book. An action packed ride through the history of the motorcycle featuring over 1,000 of the latest and greatest motorbikes in the world from the 1910s right up to the superbikes of today. All about the men, machines and landmark technology behind the most iconic bikes from ACE to Zündapp, chronicled in stunning visual detail. The Motorbike Book is all about the men, machines and landmark technology behind the most iconic bikes from ACE to Zundapp. Featuring over 1,000 of the latest and greatest motorbikes in the world this is an amazing visual guide tracing their evolution from the 1910s right up to the superbikes of today. All about the grit and the glory, The Motorbike Book takes a truly international view from Italy's Ducati to Japan's Suzuki and tells you just how famous marques like Honda and Harley-Davidson became household names. Plus, go on a virtual tour inside the engines of some of these legendary bikes to see just what powers their performance. If you're born to be wild then The Motorbike Book is the ultimate ride for you.\",\n",
              " 'The Remains Of The Day: Booker Prize Winner 1989 is a love story told from the first person perspective of Stevens, a butler. Stevens recalls the course of his life in the form of diary entries while the actions of the present progress in the book. Most of what Stevens talks about revolves around his relationship with Miss Kenton, the housekeeper, on a professional and personal level.\\nThe novel begins with Stevens reading a letter from Miss Kenton, the former housekeeper in the house of Lord Darlington. Miss Kenton details her married life in her letter, and Stevens begins to believe that she is hinting at the fact that she is unhappy in her marriage. Stevens is currently employed by Mr Farraday, a wealthy American who lends Stevens his car to go on a trip. Stevens decides to take the car and revisit Lord Darlington in the hopes of being re-employed by him. As he travels, he reflects on his loyalty to Darlington, his relationship with his late father, and the implications of dignity. As he continues to ponder, it slowly becomes obvious that Stevens and Kenton were in love with each other. Though their conversations when they worked together were frequent, they always skirted the line between professional and personal, ultimately causing them to fail to admit their true feelings to each other.\\nWhat will happen to Stevens when he reaches Lord Darlington’s? Will he and Miss Kenton find a second chance at love? The Remains Of The Day: Booker Prize Winner 1989 was published in 2010 by Faber in paperback.\\nKey Features:\\nThe book won the Man Booker Prize for Fiction in 1989.\\nIt was adapted into a Hollywood film that was nominated for eight Academy Awards.',\n",
              " \"Named a Best Book of Summer by Cosmopolitan * InStyle * Redbook * Us Weekly * PopSugar * Buzzfeed * Bustle * Brit+Co * Parade\\n\\n“No one does life and love better.” –InStyle\\n“Earth-shaking…you will flip for this epic love story.” –Cosmopolitan\\n“Reid's heartwrenching tale asks if it's possible to have multiple soul mates.” —Us Weekly\\n\\nFrom the author of Maybe in Another Life—named a People Magazine pick—comes a breathtaking new love story about a woman unexpectedly forced to choose between the husband she has long thought dead and the fiancé who has finally brought her back to life.\\n\\nIn her twenties, Emma Blair marries her high school sweetheart, Jesse. They build a life for themselves, far away from the expectations of their parents and the people of their hometown in Massachusetts. They travel the world together, living life to the fullest and seizing every opportunity for adventure.\\n\\nOn their first wedding anniversary, Jesse is on a helicopter over the Pacific when it goes missing. Just like that, Jesse is gone forever.\\n\\nEmma quits her job and moves home in an effort to put her life back together. Years later, now in her thirties, Emma runs into an old friend, Sam, and finds herself falling in love again. When Emma and Sam get engaged, it feels like Emma’s second chance at happiness.\\n\\nThat is, until Jesse is found. He’s alive, and he’s been trying all these years to come home to her. With a husband and a fiancé, Emma has to now figure out who she is and what she wants, while trying to protect the ones she loves.\\n\\nWho is her one true love? What does it mean to love truly?\\n\\nEmma knows she has to listen to her heart. She’s just not sure what it’s saying.\",\n",
              " 'From Donald Trump to Viktor Orban, populists are on the rise across the globe. But what exactly is populism? Should everyone who criticizes Wall Street or Washington be called a populist? What precisely is the difference between right-wing and left-wing populism? Who are \"the people\" anyway and who can speak in their name? These questions have never been more pressing.\\n\\nIn this provocative book, Jan-Werner Müller argues that at populism\\'s core is a rejection of pluralism. Populists will always claim that they and they alone represent the people and their true interests. Proposing a number of concrete strategies for how liberal democrats should best deal with populists, Müller shows how to counter their claims to speak exclusively for \"the silent majority\".',\n",
              " 'Dangerous Minds will delve into the complex and intricate lives of some of the most talked-about terrorists of the country. Dr Jalees Ansari, a doctor from Malegaon involved in eighty blasts, including some on railway tracks, was supposed to be a quiet, peace-loving medical professional. Fahmida Ansari, a housewife and mother of two from the Jogeshwari slums of north-west Mumbai, physically planted the bombs herself in a bus and taxis and returned home as if nothing had happened. What drove them to such violent designs? What were their compulsions? Can a human being be so ruthless and heartless and why?\\n\\nThe book will explore the lives, early beginnings, careers and sudden transformations of such persons into merchants of death.',\n",
              " 'Our civilization runs on software. Yet the art of creating it continues to be a dark mystery, even to the experts. To find out why it’s so hard to bend computers to our will, Scott Rosenberg spent three years following a team of maverick software developers—led by Lotus 1-2-3 creator Mitch Kapor—designing a novel personal information manager meant to challenge market leader Microsoft Outlook. Their story takes us through a maze of abrupt\\ndead ends and exhilarating breakthroughs as they wrestle not only with the abstraction of code, but with the unpredictability of human behavior— especially their own.',\n",
              " \"Who can have dumped a baby boy in a basket outside Asterix's hut? Child-care is something new to Asterix and Obelix, not to mention Crismus Cactus, Prefect of Gaul, masquerading as a Gaulish nursemaid. Why are the Romans after the baby - and just what plot is the treacherous Brutus hatching against Julius Caesar? Find out the answers inside.\",\n",
              " 'New editions of this popular series are fully in line with the National Curriculum, providing parents with reassurance whilst supporting their child’s learning at home.\\nEach activity is designed to give children a real sense of achievement.\\n\\n• Help boost confidence and develop good learning habits for life.\\n• Colourful, motivating activities make learning fun.\\n• Helpful tips and answers are included so that parents can support their child’s learning.\\nA Children’s Bookshelf Selection: Each month our editor’s pick the best books for children and young adults by age to be a part of the children’s bookshelf. These are editorial recommendations made by our team of experts. Our monthly reading list includes a mix of bestsellers and top new releases and evergreen books that will help enhance a child’s reading life.',\n",
              " '‘Illuminated by finely turned phrases and vivid insights’ - Richard Williams, Guardian Sports Books of the Year. Thierry Henry – gifted, charismatic and a genuinely world-class footballer – has passed into Arsenal legend as the hero of a team that finally ended Manchester United’s dominance. But as he approached the autumn of his career, Thierry’s crown began to slip – from the infamous ‘Hand of Gaul’ incident to a dismal World Cup 2010 campaign. Suddenly, a player who Arsene Wenger once dubbed ‘the greatest striker ever’, a man who had spent his career at the very top of the game, began to learn how lonely such a position could be. Drawing from numerous interviews and impeccable sources, as well as his own observations over the course of Henry’s entire career, award-winning author Philippe Auclair has produced the most complete portrait of the Arsenal hero ever to be written. Clear-eyed, lyrical and passionately argued, Thierry Henry: Lonely at the Top is as raw, shocking and thought-provoking as it is celebratory of Henry’s outstanding flair and talent.',\n",
              " \"The third full-color artbook from the hit series Naruto!\\n\\nExperience Masashi Kishimoto's artwork in all of its colorful glory in this collection of images from the conclusion of the best-selling Naruto manga! Naruto, Sasuke, Sakura, Kakashi and all your favorite characters appear in nearly a hundred pages of gorgeous full-color images. The book also features commentary from creator Masashi Kishimoto, a beautiful double-sided poster and a sticker sheet!\",\n",
              " \"Collins Primary Literacy Pupil Book 3 features fiction from Roald Dahl and Humphrey Carpenter, poetry from Michael Rosen, and exciting non-fiction on garden birds, Divali and more. Pupil Book 3 covers a wide variety of text types and topics, and provides engaging activities to help you deliver the objectives of the renewed Framework.\\n• Three clear levels of differentiation throughout ensure you provide the right work for every child\\n• Stimulating activities, role plays and group work including comprehension, speaking and listening, and writing will get your pupils thinking, talking and writing\\n• Lots of ideas for planning and writing stories, articles and poems\\n• Top children's authors will grab your pupils' attention and ensure they love reading\",\n",
              " \"One little ancient British village still holds out against the Roman invaders. Asterix and Obelix are invited to help. They must face fog, rain, warm beer and boiled boar with mint sauce, but they soon have Governor Encyclopaedicus Britannicus's Romans declining and falling. Until a wild race for a barrel of magic potion lands them in the drink. It's not quite cricket - how about a nice cup of hot water, though? Or even the first ever tea-party?\",\n",
              " \"Fantastically Great Women Who Made History is the eagerly anticipated follow up to Kate Pankhurst's hugely successful, Fantastically Great Women Who Changed the World, number one best-selling children's non-fiction title in the UK market in 2017. This beautiful title looks at the stories, accomplishments and adventures of many more brilliant women from throughout history. The perfect gift for the Fantastically Great Women in your life.\\nTravel through the Underground Railroad with the brave and courageous Harriet Tubman, turn the pages of the hauntingly brilliant Frankenstein with the incredibly talented Mary Shelley and prepare yourself for an unforgettable journey through history with these and many other remarkable women.\\nOverflowing with vibrant and beautiful illustrations, and wonderfully engaging text, Fantastically Great Women Who Made History is a celebration of just some of the inspirational women who put their mark on the world we live in. Written by the incredibly talented Kate Pankhurst, prepare to be inspired.\\nList of women featured: Elizabeth Blackwell, Qiu Jin, Boudicca, Flora Drummond, Pocahontas, Noor Inayat Khan, Harriet Tubman, Valentina Tereshkova, Ada Lovelace, Sayyida al-Hurra, Hatshepsut, Josephine Baker, Mary Wollstonecraft, Mary Shelley\",\n",
              " 'When Lalita Shastri saw her husband’s body, it did not appear he had been dead only a few hours. His face was dark bluish and swollen. The body was bloated and it bore strange cut marks. The sheets, pillows and the clothes were all soaked in blood. As the family members raised doubts, suddenly sandal paste was smeared on Lal Bahadur Shastri’s face. And yet, the controversy whether or not India’s second prime minister’s death was really due to a heart attack, couldn’t be contained. Allegations of the KGB’s, the CIA’s or an insider’s hand in the death of Lal Bahadur Shastri emerged in time\\nIn this first-ever comprehensive study of the enduring Shastri death mystery, Anuj Dhar puts together a disturbing narrative going against the official version. Dhar’s bestselling book \"India’s biggest cover-up\" inspired declassification of the Subhas Chandra Bose files and hit web series \"Bose: Dead/Alive',\n",
              " 'Born into a conservative family in a provincial town in Haryana, Kalpana Chawla dreamt of the stars. And through sheer hard work, indomitable intelligence and immense faith in herself, she became the first PBI - Indian woman to travel to space, and even more remarkably, to travel twice. In this well-researched biography, journalist Anil Padmanabhan talks to people who knew her― family and friends at Karnal, and colleagues at NASA―to produce a moving portrait of a woman whose life was a shining affirmation that if you have a dream, no matter how hard it is, you can achieve it.',\n",
              " \"From the brilliant storyteller who gave us Exodus, QB VII, The Haj, and Mitla Pass\\n\\nIt was a time of crisis, a time of tragedy—and a time of transcendent courage and determination.  Leon Uris's blazing novel is set in the midst of the ghetto uprising that defied Nazi tyranny, as the Jews of Warsaw boldly met Wehrmacht tanks with homemade weapons and bare fists. Here, painted on a canvas as broad as its subject matter, is the compelling of one of the most heroic struggles of modern times.\\n\\n“Not only authentic as history . . . it is convincing as fiction. . . . The story of a sacrifice that had real meaning and will forever be remembered . . . A fine and important novel!”—The New York Times\",\n",
              " 'Bill is a dedicated young lawyer working in New York. He leaves everything he trained for to follow his dream to become a minister in rural Wyoming. Jenny, his fashion stylist wife, leaves the milieu and life she loves to join him. The certainty they share is that their destinies are linked forever.\\n\\nFast forward thirty-eight years. Robert is a hard-working independent book publisher in Manhattan, looking for one big hit novel to publish. Lillibet is a young Amish woman, living as though in the seventeenth century, caring for her widowed father and three young brothers on their family farm. In secret at night, by candlelight, she has written the novel that burns within her. When it falls into Robert’s hands, he falls in love first with the book, and then with the woman he has never met.\\n\\nIn the hands of bestselling author Danielle Steel, these two remarkable relationships come together in unexpected and surprising ways, as lovers are lost, and find each other again. If it is true that real love lasts forever and lovers cannot lose each other, then Until the End of Time will not only comfort and fascinate us, as destiny does her dance, but it will give us hope as well. Love and fate are powerful, irresistible forces, as Steel proves to us here, in a book about courage, change, risk, and hope…and love that never dies.',\n",
              " \"On the day Yamini accidentally finds herself behind the door of her parents' bedroom, she witnesses the monster that her father is and her life is changed irrevocably. Scared, friendless and alone, she revolts in solitude. It is when her father sends her to a convent boarding school in the serene hills of Nainital does she find her true friend. Lavanya.\\n\\nBut Lavanya, a chirpy girl, has her share of problems from a broken family. In each other, the two young girls find their faithful confidantes and cultivate a deep friendship. Both girls have secrets of their own - painful past that refuses to abandon their thoughts.\\n\\nThey take their separate ways, Yamini in her quest to live with dignity and independence and Lavanya striving to find an honest relationship. But life has its own twists. Will the two friends meet again? Can friendships stand the test of time? Can scars of childhood be erased and forgotten?\\n\\nA story of courage To Live Once Again.\",\n",
              " 'Can true love bring someone back from the dead?Akshara is left devastated by her mother’s death and spends most of her time in solitude at the local park. One day, as she is sobbing uncontrollably, a young man named Harry approaches her. They become friends and Harry recounts to her a story about the miraculous reunion of a young woman and her dead boyfriend to help ease some of her pain. The story makes Akshara hopeful that she can perhaps see her dead mother again. But she soon realizes that Harry isn’t what he seems to be. Even the characters in his story seem dubious, almost unreal. So what is he hiding? And why? Is there any truth to his story at all?In this darkly suspenseful romance mystery, Akshara is left facing a truth that will make her doubt not just Harry but herself as well . . .',\n",
              " 'A Corner of a Foreign Field seamlessly interweaves biography with history, the lives of famous or forgotten cricketers with wider processes of social change. C. K. Nayudu and Sachin Tendulkar naturally figure in this book but so, too, in unexpected ways, do B. R. Ambedkar, Mahatma Gandhi and M. A. Jinnah. The Indian careers of those great British cricketers, Lord Harris and D. R. Jardine, provide a window into the operations of Empire. The remarkable life of India’s first great slow bowler, Palwankar Baloo, provides an arresting new perspective on the struggle against caste discrimination. Later chapters explore the competition between Hindu and Muslim cricketers in colonial India and the destructive passions now provoked when India plays Pakistan.\\nFor this new edition, Ramachandra Guha has added a fresh introduction as well as a long new chapter, bringing the story up to date to cover, among other things, the advent of the Indian Premier League and the Indian team’s victory in the World Cup of 2011, these linked to social and economic transformations in contemporary India.\\nA pioneering work, essential for anyone interested in either of those vast themes, cricket and India, a Corner of a Foreign Field is also a beautifully written meditation on the ramifications of sport in society at large.',\n",
              " '\"Puzzled by past tenses? Confused by comparatives? This comprehensive, beautifully presented guide to English grammar makes even the trickiest grammar rules incredibly easy to understand.\\nThe English for Everyone Grammar Guide is a detailed visual reference that uses attractive illustrations, step-by-step graphics, and crystal-clear explanations to help you learn English grammar. Ideal for English language learners at all levels, the Grammar Guide covers basic, intermediate, and advanced English grammar in one easy-to-navigate book. It includes detailed information on tenses, verbs, clauses, comparatives and superlatives, and adverbs, as well as easily confused phrases, English conversation starters, and question words.\\nThis Grammar Guide is part of DK\\'s innovative English for Everyone series, an exciting and comprehensive self-study course for adults learning English as a foreign language. Using the Grammar Guide alongside the English for Everyone course and practice books will help you improve your understanding of English grammar, build up your confidence, and become more fluent. Whether you want to improve your grammar for work, study, travel, or exams such as TOEFL and IELTS, the English for Everyone Grammar Guide offers you a simple way to learn, understand, and remember the most important English grammar constructions.\\n\"',\n",
              " 'Bookworms rejoice! These charming comics capture exactly what it feels like to be head-over-heels for hardcovers. And paperbacks! And ebooks! And bookstores! And libraries!\\n\\nBook Love is a gift book of comics tailor-made for tea-sipping, spine-sniffing, book-hoarding bibliophiles. Debbie Tung’s comics are humorous and instantly recognizable—making readers laugh while precisely conveying the thoughts and habits of book nerds. Book Love is the ideal gift to let a book lover know they’re understood and appreciated.  ',\n",
              " 'A three-level English grammar practice series for the classroom or self-study with clear explanations and lots of extra practice. Each level has an interactive Practice-Boost CD-ROM.',\n",
              " 'Just for Once Allow Your Heart to Rule Your Prudent common Sense. Tempting advice dare Eugenie take it? A country upbringing has taught her to be practical, not to cherish romantic dreams about tall, handsome strangers! But a chance encounter one misty day inspiring changes all that. Eminent surgeon Aderik Rijnmter Salis is a very special man.He makes Eugenieos life seem brighter and full of exciting possibilities! But with the gorgeous Saphira at his side, could Aderikos feelings for Eugenie ever be more than strictly professional?',\n",
              " \"Do you sometimes feel tired, lethargic and spiritless? How can Ayurveda help in a simple, practisable manner?\\nTime is scarce and precious in today's world, and we seek solutions that are quick. While allopathic medicine tends to focus on the management of disease, the ancient study of dinacharya provides us with holistic knowledge of preventing disease and eliminating its root cause.\\nTaking us through a day in the life of Ayurveda living, Dr Bhaswati Bhattacharya illustrates the core principles of Ayurveda and shows us how to incorporate these in our routine. She explains the logic behind the changes she recommends and how they benefit us. Informative and accessible, Everyday Ayurveda is the perfect lifestyle guide designed to maximize health, longevity and happiness the natural way.\\nPraise for Everyday Ayurveda\\n'Dr Bhaswati has written a book that will help resurrect the knowledge I grew up on, using scientific logic for modern scientists, and slokas for ancient scientists and observers. Her writing will appeal to the intelligent seeker dedicated to achieving a good life using conscious self-care, attention to healthy habits and respect for the wisdom of the ancients.' – Shashi Tharoor, member of parliament\\n'Bhaswati combines her talents as a healer with passion for sharing truly healing medicine. She has written from the song in her heart that celebrates light, sound and connection with nature.' – Pandits Rajan and Sajan Misra, Padma Bhushan recipients and classical Hindustani vocalists of the Banaras Gharana\\n'Bhaswati is uniquely able to diagnose like good medical doctors of the past, watching the patient rather than the test result. With a person-centered approach, this book is a tribute to Ayurveda and explains why we should pay more attention to the signals our bodies give us.' – Ashok H. Advani, founder publisher, The Business India Group\\n'Bhaswati has preserved the Sanskrit from which dinacharya emanates. Through her unending curiosity and dedication as a well-trained physician, scientist and professor, she has brought basic concepts of Ayurveda to light. She has toiled and delved deeply with engaging clarity, a fine mind and an extraordinarily connected soul. She teaches Ayurveda authentically because it is in her heart.' – Dr Vd. Chandrabhushan Jha, former dean, faculty of Ayurveda, Banaras Hindu University and professor emeritus of Rasa Shastra\\n'Ancient Ayurvedic rishis developed dinacharya, a way of maintaining normal body rhythms and staying healthy. Assisted by logic and personal accounts, Dr Bhaswati brings dinacharya alive in this book and reinforces its importance and necessity, especially in busy lives.' – Vd. Partap Chauhan, founder of Jiva Ayurveda\\n'Dr Bhaswati unlocks the age-old tenets of Ayurveda contained in cryptic Sanskrit verses. Her strong roots in tradition and willingness to branch out to the modern world make this book precious, like any true vidya.' – Dr. PR Krishnakumar, Padma Shree recipient and managing director, Arya Vaidya Pharmacy (Coimbatore)\\n'As a fellow Fulbright Scholar, I have witnessed Dr Bhaswati devote her life to researching and preserving Ayurveda, and bringing it back to the hands and hearts of people.' – Gautam Gandhi, former head of new business development, Google India\",\n",
              " 'Former Army Ranger Tucker Wayne and his war dog Kane are thrust into a global conspiracy in this second Sigma Force spinoff adventure from New York Times bestselling authors James Rollins and Grant Blackwood\\n\\n“Nobody and I mean nobody does this stuff better than Rollins.”—Lee Child\\n\\nTucker Wayne’s past and present collide when a former army colleague comes to him for help. She’s on the run from brutal assassins hunting her and her son. To keep them safe, Tucker must discover who killed a brilliant young idealist—a crime that leads back to the most powerful figures in the U.S. government.\\n\\nFrom the haunted swamplands of the deep South to the beachheads of a savage civil war in Trinidad, Tucker and his beloved war dog, Kane, must work together to discover the truth behind a mystery that dates back to World War\\nII, involving the genius of a young code breaker, Alan Turing . . .\\n\\nThey will be forced to break the law, expose national secrets and risk everything to stop a madman determined to control the future of modern warfare for his own diabolical ends. But can Tucker and Kane withstand a force so indomitable that it threatens our future?',\n",
              " \"This is a semi-autobiographical coming-of-age story from the perspective of a typical middle-class Indian teenager, in love with the words around and scared of the world around him.\\n\\nIt covers his incredible journey of transformation from an under-confident nerd to an over-confident rule-breaker, under the influence of a new set of friends.\\n\\nThat, of course, has its own backlashes.\\n\\nThe book covers his journey to, and back from, rock bottom .\\n\\nThis story, infused with the nuances of our adolescence, takes you down the forgotten memory lanes on a hilarious yet emotional joyride making you laugh a lot and maybe, cry a little.\\n\\nIt's a story of teenage rebellion and its repercussions, of friendship and its meaning, and of love and its magic. It's a teenager's diary that will make you say - Oh! The days that were!\",\n",
              " \"Unleash the power of computer vision with Python to carry out image processing and computer vision techniques About This Book * Learn how to build a full-fledged image processing application using free tools and libraries * Perform basic to advanced image and video stream processing with OpenCV's Python APIs * Understand and optimize various features of OpenCV with the help of easy-to-grasp examples Who This Book Is For This book is for Python developers who want to perform image processing. It's ideal for those who want to explore the field of computer vision, and design and develop computer vision applications using Python. The reader is expected to have basic knowledge of Python. What You Will Learn * Working with open source libraries such Pillow, Scikit-image, and OpenCV * Writing programs such as edge detection, color processing, image feature extraction, and more * Implementing feature detection algorithms like LBP and ORB * Tracking objects using an external camera or a video file * Optical Character Recognition using Machine Learning. * Understanding Convolutional Neural Networks to learn patterns in images * Leveraging Cloud Infrastructure to provide Computer Vision as a Service In Detail This book is a thorough guide for developers who want to get started with building computer vision applications using Python 3. The book is divided into five sections: The Fundamentals of Image Processing, Applied Computer Vision, Making Applications Smarter,Extending your Capabilities using OpenCV, and Getting Hands on. Throughout this book, three image processing libraries Pillow, Scikit-Image, and OpenCV will be used to implement different computer vision algorithms. The book aims to equip readers to build Computer Vision applications that are capable of working in real-world scenarios effectively. Some of the applications that we will look at in the book are Optical Character Recognition, Object Tracking and building a Computer Vision as a Service platform that works over the internet. Style and approach Each stage of the book elaborates on various concepts and algorithms in image processing/computer vision using Python. This step-by-step guide can be used both as a tutorial and as a reference.\",\n",
              " \"'Outstanding in every way' Lee Child\\n'The page turned of the season' The Times\\n____________\\n'Do you need my help?'\\nIt was the first question he asked.\\nThey called him when they had nowhere else to turn.\\nAs a boy Evan Smoak was taken from an orphanage.\\nRaised and trained in a top secret programme, he was sent to bad places to do things the government denied ever happened.\\nThen he broke with the programme, using what he'd learned to vanish. Now he helps the desperate and deserving.\\nBut someone's on his trail.\\n\\nSomeone who knows his past and believes that the boy once known as Orphan X must die . . .\\n____________\\n\\n'Read this book. You will thank me later' David Baldacci\\n'A rival to reacher' The Independent\\n\\nIf you loved Orphan X, read the gripping follow-up The Nowhere Man and brand new sequel Out of the Dark!\",\n",
              " 'There’s never been a better time to discover the novels behind the blockbuster Starz original series Outlander. Blending rich historical fiction with riveting adventure and a truly epic love story, here are the first four books of Diana Gabaldon’s New York Times bestselling saga that introduced the world to the brilliant Claire Randall and valiant Highlander Jamie Fraser:\\n \\nOUTLANDER\\nDRAGONFLY IN AMBER\\nVOYAGER\\nDRUMS OF AUTUMN\\n \\nScottish Highlands, 1945. Claire Randall, a former British combat nurse, is just back from the war and reunited with her husband on a second honeymoon when she walks through a standing stone in one of the ancient circles that dot the British Isles. Suddenly she is a Sassenach—an “outlander”—in a Scotland torn by war and raiding clans in the year of Our Lord . . . 1743. Claire is catapulted into the intrigues of a world that threatens her life, and may shatter her heart. Marooned amid danger, passion, and violence, her only chance of safety lies in Jamie Fraser, a gallant young Scots warrior. What begins in compulsion becomes urgent need, and Claire finds herself torn between two very different men, in two irreconcilable lives.\\n \\nPraise for Diana Gabaldon’s Outlander novels\\n \\n“Marvelous and fantastic adventures, romance, sex . . . perfect escape reading.”—San Francisco Chronicle, on Outlander\\n \\n“History comes deliciously alive on the page.”—New York Daily News, on Outlander\\n \\n“Gabaldon is a born storyteller. . . . The pages practically turn themselves.”—The Arizona Republic, on Dragonfly in Amber\\n \\n“Triumphant . . . Her use of historical detail and a truly adult love story confirm Gabaldon as a superior writer.”—Publishers Weekly, on Voyager\\n \\n“Unforgettable characters . . . richly embroidered with historical detail.”—The Cincinnati Post, on Drums of Autumn',\n",
              " \"The second book in the Gabriel's Inferno series, a wildly romantic tale of forbidden passion. This has the obsessive yearning of Twilight crossed with the eroticism of the Fifty Shades series Two lovers bound by their darkest desires. Professor Gabriel Emerson has embarked on a passionate but clandestine love affair with his former student, Julia Mitchell and is teaching her the sensual delights of the body and the raptures of sex. But their happiness is threatened by conspiring students, academic politics and jealous ex-lovers. Can their love survive against all odds and will Gabriel fight to keep Julia?.\",\n",
              " \"The sleepy town of Heartsdale, Georgia, is jolted into panic when Sara Linton, paediatrician and medical examiner, finds Sibyl Adams dead in the local diner. As well as being viciously raped, Sibyl has been cut: two deep knife wounds form a lethal cross over her stomach. But it's only once Sara starts to perform the post-mortem that the full extent of the killer's brutality becomes clear.\\n\\nPolice chief Jeffrey Tolliver - Sara's ex-husband - is in charge of the investigation, and when a second victim is found, crucified, only a few days later, both Jeffrey and Sara have to face the fact that Sibyl's murder wasn't a one-off attack. What they're dealing with is a seasoned sexual predator. A violent serial killer...\",\n",
              " \"On a bright morning in Rome, a terrible explosion rips a hole in the Israeli embassy. Moments later, four gunmen cut down survivors as they stagger from the burning building. Gabriel Allon is hastily recalled to Israel and drawn once more into the heart of the secret service he'd hoped to leave behind. For the blast has led to a disturbing revelation: a dossier that strips away Allon's secrets and lays bare his history. A dossier that had fallen into terrorist hands . . .\",\n",
              " \"In Danielle Steel’s powerful novel, four San Francisco trauma doctors – the best and brightest in their field – confront exciting and exacting new challenges, both personally and professionally, when given a rare opportunity. Bill Browning heads the trauma unit at San Francisco’s busiest emergency room. With his ex-wife and daughters in London, he immerses himself in his work and lives for the little time he can spend with his children. A rising star at her teaching hospital, Stephanie Lawrence has two young sons, a frustrated stay-at-home husband, and not enough time for any of them. Harvard-educated Wendy Jones is a dedicated trauma doctor, trapped in a dead-end relationship with a married cardiac surgeon. And Tom Wylie’s popularity with women rivals the superb medical skills he employs at his medical centre, but he refuses to let anyone get too close, determined to remain unattached forever. These exceptional doctors are chosen for an honour and a unique project: to work with their counterparts in Paris in a mass-casualty training programme. As professionals they will gain invaluable knowledge, but as ordinary men and women they will find that the City of Light opens up incredible new possibilities, exhilarating, enticing and frightening. When an unspeakable act of mass violence galvanizes them into action, their temporary life in Paris becomes a stark turning point: a time to make harder choices than they have ever faced before – with consequences that will last a lifetime. Turning Point is a highly-charged, emotional tale about how suddenly life can change for all of us, and that we might find what we're looking for in the most unlikely of places . . .\",\n",
              " 'THE HUGE INTERNATIONAL BESTSELLER\\n\\nA former FBI hostage negotiator offers a new, field-tested approach to negotiating – effective in any situation.\\n\\n‘Riveting’ Adam Grant\\n‘Stupendous’ The Week\\n‘Brilliant’ Guardian\\n____________________________\\n\\nAfter a stint policing the rough streets of Kansas City, Missouri, Chris Voss joined the FBI, where his career as a kidnapping negotiator brought him face-to-face with bank robbers, gang leaders and terrorists. Never Split the Difference takes you inside his world of high-stakes negotiations, revealing the nine key principles that helped Voss and his colleagues succeed when it mattered the most – when people’s lives were at stake.\\n\\nRooted in the real-life experiences of an intelligence professional at the top of his game, Never Split the Difference will give you the competitive edge in any discussion.\\n\\n\\nPRAISE FOR NEVER SPLIT THE DIFFERENCE\\n\\n‘My pick for book of the year.’ Forbes\\n\\n‘Who better to learn [negotiation] from than Chris Voss, whose skills have saved lives and averted disaster?’ Daily Mail\\n\\n‘Filled with insights that apply to everyday negotiations.’ Business Insider\\n\\n‘It’s rare that a book is so gripping and entertaining while still being actionable and applicable.’ Inc.\\n\\n‘A business book you won’t be able to put down.’ Fortune',\n",
              " \"The most advanced and celebrated mind of the 20th century, without a doubt, is attributed to albert Einstein. This interesting book allows us to explore his beliefs, philosophical ideas and opinions on many subjects. Subjects include politics, religion, education, the meaning of life, Jewish issues, the world economy, peace and pacifism. Einstein believed in the possibility of a peaceful world and in the high mission of science to serve human well-being. As we near the end of a century in which science has come to seem more and more remote from human values, Einstein's perspective is indispensable.\",\n",
              " '\"You\\'ll learn everything there is to learn about drawing animals.\" — Collectors\\' Corner\\nThis thoughtful and incisive guidebook, written by a former animator for Walt Disney Studios, will help artists at many skill levels improve their ability to draw a wide variety of animal forms both realistically and as caricatures.\\nYou\\'ll learn why the author considers construction, action analysis, and caricature all-important for a clear understanding of animal anatomy and movement. You\\'ll also benefit from Mr. Hultgren\\'s expert\\nadvice and tips on catching the essential movement and character of animals and avoiding the stiff, wooden poses that are the frequent and unfortunate result of much sketching of animals from life. Throughout, the emphasis is on construction drawings (there are over 700 line illustrations and halftones) rather than on text. This means the student is able to view the development process of the drawing by example rather than theory or description.\\nThe book begins with introductory chapters on the special techniques of drawing animals, the use of line, establishing mood and feeling, conveying action, and brush techniques. Mr. Hultgren then turns to individual animal forms — horses, deer, cats, cows and bulls, giraffes, camels, gorillas, pigs, and many more. His instruction on animal caricature will be especially valuable to the legions of artists avidly interested in the subject.\\nThe Art of Animal Drawing belongs in the library of any artist — student, amateur, or professional — who is interested in drawing animals.',\n",
              " 'I was Top Gear\\'s script editor for 13 years and all 22 series. I basically used to check spelling and think of stupid gags about The Stig. I also got to hang around with Jeremy Clarkson, Richard Hammond and James May. It didn\\'t feel like something you should get paid for.\\n\\nFrom the disastrous pilot show of 2002 to the sudden and unexpected ending in 2015, working on Top Gear was quite a rollercoaster ride. We crossed continents, we made space ships, we bobbed across the world\\'s busiest shipping lane in a pick-up truck. We also got chased by an angry mob, repeatedly sparked fury in newspapers, and almost killed one of our presenters.\\n\\nI realised that I had quite a few stories to tell from behind the scenes on the show. I remembered whose daft idea it was to get a dog. I recalled the willfully stupid way in which we decorated our horrible office. I had a sudden flashback to the time a Bolivian drug lord threatened to kill us. I decided I should write down some of these stories. So I have. I hope you like them.\\n\\nAnd now, a quote from James May: \\'Richard Porter has asked me to \"write a quote\" for his new book about the ancient history of Top Gear. But this is a ridiculous request. How can one \"write a quote\"? Surely, by definition, a quote must be extracted from a greater body of writing, for the purpose of illustrating or supporting a point in an unrelated work. I cannot \"write a quote\" any more than I could \"film an out-take\".\\n\\n\\'Porter, like Athens, has lost his marbles.\\'',\n",
              " 'Czech: An Essential Grammar is a practical reference guide to the core structures and features of modern Czech. Presenting a fresh and accessible description of the language, this engaging grammar uses clear, jargon-free explanations and sets out the complexities of Czech in short, readable sections.\\nSuitable for either independent study or for students in schools, colleges, universities and adult classes of all types, key features include:\\n* focus on the morphology and syntax of the language\\n* clear explanations of grammatical terms\\n* full use of authentic examples\\n* detailed contents list and index for easy access to information.\\nWith an emphasis on the Czech that native speakers use today, Czech: An Essential Grammar will help students to read, speak and write the language with greater confidence.',\n",
              " \"INSPIRATION FOR THE THRILLING AMAZON PRIME SERIES JACK RYAN . . .\\nA truly brilliant thriller by Tom Clancy, The Bear and the Dragon is a Jack Ryan novel.\\nNewly elected in his own right, Jack Ryan has found that being President has gotten no easier: domestic pitfalls await him at every turn; the Asian economy is going down the tubes; and now, in Moscow, someone may have tried to take out the chairman of the SVR - the former KGB - with a rocket-propelled grenade. Things are unstable enough in Russia without high-level assassination, but even more disturbing may be the identities of the potential assassins. Were they political enemies, the Russian Mafia, disaffected former KGB? Or, Ryan wonders, is something far more dangerous at work here?\\nRyan is right to wonder. For even while Russian investigators pursue the case, and some of his most trusted eyes and ears, including antiterrorism specialist John Clark, head to Moscow, forces in China are moving forward with a plan of truly audacious proportions. Tired of what they view as the presumption of the West, eager to fulfill their destiny, they are taking matters into their own hands. If they succeed, the world as we know it will never look the same. If they fail. . . the consequences may be unspeakable.\\nThe Bear and the Dragon is Tom Clancy at his best - and there is none better. This is the third Jack Ryan thriller, following Without Remorse and Rainbow Six.\\nPraise for Tom Clancy:\\n'A brilliantly constructed thriller' Daily Mail\\n'Truly riveting, a dazzling read' Sunday Express\",\n",
              " \"The Spirit of 'C' familiarises you with the computer programming language, C. The book can help you effectively learn the language. There are more than one hundred programs written in C included in the book. The Spirit of 'C' is written in a simple and lucid manner, explaining the essentials of this language in detail. The book does not require readers to have mathematical or programming experience. It covers illustrated programs and includes a plethora of questions, with answers, to help readers recap what they have studied. C language functions in a similar way as the modern high level languages like Pascal, Cobol, Basic and Fortran.\\nThe Spirit of 'C' is a useful guide for students and beginners. Even teachers can benefit from this book. Jaico Publishing House published the first edition in 1998. It is available in paperback format.\",\n",
              " \"This book is a citizen's introduction to the law, the legal system, and a wide range of contemporary social and political issues in India. Written by experts, but concise and easy-to-read, it shows how • the law impacts everyday life and society • the focus of law is not merely punishment of wrongdoers but also protection of the weak • the law is an instrument of social justice • the constitution relates to other laws • security concerns are interconnected with human rights This volume invites readers to explore the Indian legal system in its totality and introduces them to all key aspects of the law • the basic structure doctrine • the criminal justice system • the concept of religious personal laws • anti-terror laws • cyber laws • law of contract • labour and employment laws • environmental law • gender justice Written especially for students of the recently restructured BA Programme of the University of Delhi and designed as a text for its Legal Literacy course, this book will also be of immense use to students in the early stages of courses in political science, law, sociology of law, gender studies, as well as to curious and concerned general readers.\",\n",
              " 'HBO’s hit series A GAME OF THRONES is based on George R R Martin’s internationally bestselling series A SONG OF ICE AND FIRE, the greatest fantasy epic of the modern age.\\nA STORM OF SWORDS: BLOOD AND GOLD is the SECOND part of the third volume in the series.\\n‘There is no better distraction than these magic tales’ Guardian\\nThe Starks are scattered.\\nRobb Stark may be King in the North, but it will be a bloody struggle for him to hold his crown. And while his youngest sister has escaped the clutches of the depraved Lannisters, Sansa Stark remains their captive.\\nMeanwhile, across the ocean, Daenerys Stormborn, the last heir of the Dragon King, approaches Westeros with vengeance in her heart.\\nA STORM OF SWORDS, PART TWO: BLOOD AND GOLD, the second half of Book Three of A Song of Ice and Fire, continues the greatest fantasy epic of the modern age.',\n",
              " \"In the enchanted woods of County Mayo, a young woman must fight the one thing she fears the most - her own heart. Meara Quinn is in trouble. Fiercely independent and scarred by a tough childhood, she is convinced that love is for other people. She is certainly not going to fall for Connor O'Dwyer - her best friend's brother. He may be drop dead gorgeous, with a good heart and a wicked smile, but he's never taken his relationships seriously. Safer for them to stay friends, share the odd pint - nothing more. And loving Connor would be a dangerous business. With his sister Branna and his American cousin Iona, Connor has inherited a dark gift, passed down through generations. The cousins use their powers for good, but they are being hunted by evil. An evil that is determined to destroy them - and everyone they care for.\",\n",
              " \"'Access inside the changing room and behind the scenes that any journalist or writer would kill for - Perarnau's insights are astonishing' - Graham Hunter 'Write about everything you see. Be as critical as you like' - Pep Guardiola to Marti Perarnau, summer 2013 Marti Perarnau was given total access to Bayern Munich during season 2013-14. This book represents the first time in the modern era that a writer has got this close to one of the elite teams of world football. At the invitation of Pep Guardiola, he shadowed the Catalan, his staff and his superstar players during training and on matchdays. Bayern smashed domestic records on their way to the double, but were humiliated by Real Madrid in the Champions League semi-final. Perarnau was with them every step of the way. Perarnau is with Guardiola as he is courted by the world's greatest clubs during his sabbatical in New York. We hear Guardiola explain in detail the radical tactical moves which transform Bayern's season and reprogramme the players who will win the World Cup with Germany.Perarnau talks exclusively and in fascinating detail with players such as Arjen Robben, Manuel Neuer, Philipp Lahm, Thiago Alcantara and Bastian Schweinsteiger. Pep Confidential is much more than the story of a season - it is also a lasting portrait of one of the greatest coaches in sport. Marti Perarnau, former Spanish Olympiad turned journalist, is a renowned football analyst. He writes for a number of Spanish newspapers and runs the hugely popular Perarnau Magazine blog. He lives in Barcelona.\",\n",
              " \"Leave 21st century London and go back to Ice Age Europe. Follow Ayla, a Cro-Magnon child who loses her parents in an earthquake and is adopted by a tribe of Neanderthal, the Clan. See how the Clan's wary suspicion is gradually transformed into acceptance of this girl, so different from them, under the guidance of its medicine woman Iza and its wise holy man Creb. Immerse yourself in a world dictated by the demands of survival in a hostile environment, and be swept away in an epic tale of love, identity and struggle.\",\n",
              " \"Richard Feynman was the most brilliant and influential physicist of our time. Architect of quantum theories, enfant terrible of the atomic bomb project, caustic inquisitor on the space shuttle commission, ebulent bongo-player and storyteller - Feynman played a bewildering assortment of roles in the science of the post-war era.\\nA brilliant interweaving of Richard Feynman's colourful life and a detailed and accessible account of his theories and experiments.\",\n",
              " 'Renowned typographer and poet Robert Bringhurst brings clarity to the art of typography with this masterful style guide. Combining the practical, theoretical, and historical, this edition is completely updated, with a thorough revision and updating of the longest chapter, \"Prowling the Specimen Books,\" and many other small but important updates based on things that are continually changing in the field.',\n",
              " 'Acclaimed as one of the most exciting books in the history of American letters, this modern epic became an instant bestseller upon publication in 1974, transforming a generation and continuing to inspire millions. A narration of a summer motorcycle trip undertaken by a father and his son, the book becomes a personal and philosophical odyssey into fundamental questions of how to live. Resonant with the confusions of existence, Zen and the Art of Motorcycle Maintenance is a touching and transcendent book of life.',\n",
              " \"A collection of stories that make you cry and they make you laugh but above all they make you hope and want to believe that all will be well! Love blooms in the wreckage of violence. It blooms even as hearts turn strangers and brothers enemies. It blooms even as rivers of blood flow, passions are inflamed, and hatred stalks the land. Love knows no boundaries. No caste. No religion. Love can strike the heart of the village belle or the doughty lad with the look from under dark kohl lined eyes, the accidental touch of a hand adorned in simple glass bangles. And... Darvish, Beeran, Meiral and Chaitanya are helpless as its arrow pierces their hearts. All they understand is love. All the world around them understands is hate. Set against the backdrop of a time when death trains pulled into stations between Lahore and Delhi, East of Love West of Desire' is a collection of short stories about love that sears and consumes, elevates and devastates. They are stories of fathers and daughters, mothers and sons, men and women who come together, and yet are symbols of possibilities unexplored because the world around them is exploding, stripping them of their dignity and humanity. These stories are about the darkest chapter of modern Indian history, as the sun set on the British Raj. But they are also timeless tales about growing up, love, longing, faith, differences...\",\n",
              " 'The first official Minecraft novel by bestselling author, Max Brooks A Sunday Times Bestseller A New York Times Bestseller be immersed in the Minecraft universe for the first time in a thrilling new adventure like no other! Minecraft: the Island will tell the story of a new hero stranded in the world of Minecraft, who must survive the harsh, unfamiliar environment and unravel the secrets of the island. Washed up on a beach, the lone castaway looks around the shore. Where am I? Who am I? And why is everything made of blocks? But there isn’t much time to soak up the sun. It’s getting dark and there’s a strange new world to explore! the top priority is finding food. The next is not becoming food. Because there are others out there on the island like the horde of zombies that appear after night falls. Crafting a way out of this mess is a challenge like no other. Who could build a home while running from exploding creepers, armed skeletons and an unstoppable tide of hot lava? Especially with no help except for a few makeshift tools and sage advice from an unlikely friend: a cow. In this world, the rules don’t always make sense, but courage and creativity go a long way. There are forests to explore, hidden underground tunnels to loot and undead mobs to defeat. Only then will the secrets of the island be revealed. Minecraft: the Crash, by Tracey Baptiste, is the second official Minecraft novel. It will be released in the UK on July 12.',\n",
              " \"In a glittering palace on a sun-drenched coast, secrets run deep and passions run hot \\nThe Playboy Prince \\nWhen it comes to women, Prince Bennett has always enjoyed a challenge. So after meeting the quiet and beautiful Lady Hannah Rothchild, the dashing prince cannot rest until he breaks through her careful reserve. Love has always been a game to Bennett, but with this elusive, mysterious woman he discovers his heart is on the line, and he's playing for keeps… \\nCordina's Crown Jewel \\nOn the run from the palace, Her Royal Highness Camilla de Cordina wants to be just plain Camilla MacGee, even if it's only for a few precious weeks. Working in rural Vermont for the devastatingly handsome and utterly cantankerous archaeologist Delaney Caine is the perfect refuge. But Camilla's irritation with the man soon turns into fascination, then desire, and soon, the royal runaway knows she'll have to confess her secret…\",\n",
              " 'The basis for the new documentary film, Mountain: A Breathtaking Voyage into the Extreme. Combining accounts of legendary mountain ascents with vivid descriptions of his own forays into wild, high landscapes, Robert McFarlane reveals how the mystery of the world’s highest places has came to grip the Western imagination—and perennially draws legions of adventurers up the most perilous slopes.\\nHis story begins three centuries ago, when mountains were feared as the forbidding abodes of dragons and other mysterious beasts. In the mid-1700s the attentions of both science and poetry sparked a passion for mountains; Jean-Jacques Rousseau and Lord Byron extolled the sublime experiences to be had on high; and by 1924 the death on Mt Everest of an Englishman named George Mallory came to symbolize the heroic ideals of his day. Macfarlane also reflects on fear, risk, and the shattering beauty of ice and snow, the competition and contemplation of the climb, and the strange alternate reality of high altitude, magically enveloping us in the allure of mountains at every level.',\n",
              " \"On the night of 10-11 May 1996, eight climbers perished in what remains the worst disaster in Everest's history. Following the tragedy, numerous accounts were published, with Jon Krakauer's Into Thin Air becoming an international bestseller. But has the whole story been told?\\n\\nA Day to Die For reveals the full, startling facts that led to the tragedy. Graham Ratcliffe, the first British climber to reach the summit of Mount Everest twice, was a first-hand witness, having spent the night on Everest's South Col at 26,000 ft, sheltering from the deadly storm. For years, he has shouldered a burden of guilt, feeling that he and his teammates could have saved lives that fateful night. His quest for answers has led to discoveries so important to an understanding of the disaster that he now questions why these facts were not made public sooner.\\n\\nHistory is dotted with high-profile disasters that both horrify and capture the attention of the public, but very rarely is our view of them revised to such devastating effect.\",\n",
              " \"Oxford Learner's Pocket English Dictionary: Student Book (Advanced) is highly recommended by the readers. This dictionary has the strongest vocabulary from years. People who want to improve their vocabulary can purchase this dictionary from Amazon India. Proper details and explanations are provided in the dictionary for each word.\\nThose who want a command over English Language, this pocket dictionary is perfect for them. End number of words are written in the dictionary and along with them the dictionary has a huge collection of phrases and idioms. Besides, the dictionary comes with a CD ROM which can be used to know the pronunciation of the words. Inside the dictionary readers will also find the common mistakes that users do. These mistakes are highlighted in the dictionary. Filled with commonly used abbreviations, irregular verbs and geographical names, this dictionary is helpful for quick and easy reference.\\nAbout the author:\\nOxford Learner's Pocket English Dictionary: Student Book (Advanced) was published by Oxford University Press as student's edition in 2008. It is available in paperback.\",\n",
              " \"Featuring show-stopping imagery and thrilling behind-the-scenes tales, National Geographic 125 Years captures the heart of National Geographic's fascinating history, from its earliest days as a scientific club to its growth into one of the world's largest geographic organizations. The book reveals how much we've come to know about our fascinating world through the pages and unforgettable imagery of National Geographic, and taps key voices from the forefront of ocean and space exploration, climate science, archaeology, mountaineering, and many other disciplines to peer with us over the horizon and see where we are heading in the future.\",\n",
              " 'From one of the pioneers of the scientific study of happiness, an indispensable guide to living your best life\\n\\nWhat makes a good life? Is it money? An important job? Leisure time? Mihaly Csikszentmihalyi believes our obsessive focus on such measures has led us astray. Work fills our days with anxiety and pressure, so that during our free time, we tend to live in boredom, watching TV or absorbed by our phones.\\n\\nWhat are we missing? To answer this question, Csikszentmihalyi studied thousands of people, and he found the key. People are happiest when they challenge themselves with tasks that demand a high degree of skill and commitment, and which are undertaken for their own sake. Instead of watching television, play the piano. Take a routine chore and figure out how to do it better, faster, more efficiently. In short, learn the hidden power of complete engagement, a psychological state the author calls flow. Though they appear simple, the lessons in Finding Flow are life-changing.',\n",
              " \"For Milli Bajwa, life is at a stand-still. Grounded in the Chandigarh airport where she works day after day, she watches flights leaving for destinations she knows she'll never visit. Loveless and luckless, she would rather bury her nose in a book than face her grim reality. And then, on a whim, she swipes right on a new dating app, and finds the man of her dreams - someone who can sweep her off her feet and teach her how to fly. But the mysterious and charming Karan Singhania has secrets of his own, and a heart damaged in more ways than one. This is the story of two people about to find out how far they're willing to go for the promise of true love.\",\n",
              " 'Henry Wilt, tied to a daft job and a domineering wife, has just been passed over for promotion yet again. Ahead of him at the Polytechnic stretch years of trying to thump literature into the heads of plasterers, joiners, butchers and the like. And things are no better at home where his massive wife, Eva, is given to boundless and unpredictable fits of enthusiasm - for transcendental meditation, yoga or the trampoline.\\n\\nBut if Wilt can do nothing about his job, he realises he can do something about his wife - and as each day passes, his fantasies grow more murderous and more real.',\n",
              " 'The book is a popular guide to help improve ones vocabulary in a span of just one month. Norman Lewis, who is one of the best known English grammarians, in the book lays out time tested but simple ways for improving ones communication skills.\\nFor those who often misuse, mispronounce or lack basic vocabulary knowledge, this book is a good companion. Having a history of being in print for the last 40 years, this vocabulary guide has been relied upon by millions of learners for enhancing their reading, writing and speaking skills in mere 30 days.\\nA learner needs to use the book daily for 15 minutes for one month and results about improvements in vocabulary and usage of English language would begin to show up. Once the step-by-step instructions for proper usage of English language is over, the book also has 30 challenging tests for self-evaluation purposes.\\nThe workbook will guides out about proper usage of words and tenses in order to form correct sentences while writing or speaking. When using this vocabulary guide, one can even learn about the etymology of word, correct usage of verbs, adjectives, prepositions and articles, as well as learn odd/new words for personalized growth in English language.\\n30 Days to a More Powerful Vocabulary will even help a learning in remembering difficult words which can then be used on a daily basis for better academic performance, communication or for expressing ones viewpoint. From beginners to public speaking in English, s whoever seeks to command the usage of English language, this standard book is a great companion for improving vocabulary, language and communication skills.\\nAbout the Author:\\nNorman Lewis was a leading grammarian who worked a lifetime to help people improve their English language skills. The author, lexicographer and etymologist was a professor at New York University and did teach at City College, New York as well. His best selling 30 Days to a More Powerful Vocabulary is still referred to by people who want to improve their English Grammar.',\n",
              " \"They told David it was impossible - that even the Reckoners had never killed a High Epic. Yet, Steelheart - invincible, immortal, unconquerable - is dead. And he died by David's hand.\\nEliminating Steelheart was supposed to make life more simple. Instead, it only made David realize he has questions. Big ones. And there's no one in Newcago who can give him the answers he needs.\\nBabylon Restored, the old borough of Manhattan, has possibilities, though. Ruled by the mysterious High Epic, Regalia, David is sure Babylon Restored will lead him to what he needs to find. And while entering another city oppressed by a High Epic despot is a gamble, David's willing to risk it. Because killing Steelheart left a hole in David's heart. A hole where his thirst for vengeance once lived. Somehow, he filled that hole with another Epic - Firefight. And he's willing to go on a quest darker, and more dangerous even, than the fight against Steelheart to find her, and to get his answers.\",\n",
              " \"Rafah, a town at the southernmost tip of the Gaza Strip, is a squalid place. Raw concrete buildings front rubbish-strewn alleys. The narrow streets are crowded with young children and unemployed men. Situated on the border with Egypt, swaths of Rafah have been reduced to rubble. Rafah is today and has always been a notorious flashpoint in this most bitter of conflicts.\\n\\nBuried deep in the archives is one bloody incident, in 1956, that left 111 Palestinian refugees dead, shot by Israeli soldiers. Seemingly a footnote to a long history of killing, that day in Rafah - coldblooded massacre or dreadful mistake - reveals the competing truths that have come to define an intractable war. In a quest to get to the heart of what happened, Joe Sacco arrives in Gaza and, immersing himself in daily life, uncovers Rafah, past and present. Spanning fifty years, moving fluidly between one war and the next, alive with the voices of fugitives and schoolchildren, widows and sheikhs, Footnotes in Gaza captures the essence of a tragedy.\\n\\nAs in Palestine and Safe Area Goražde, Joe Sacco's unique visual journalism has rendered a contested landscape in brilliant, meticulous detail. Footnotes in Gaza, his most ambitious work to date, transforms a critical conflict of our age into intimate and immediate experience.\",\n",
              " 'Becky Brandon (nee bloomwood) is in Hollywood! She\\'s hanging out with celebs Or at least she will be, once her husband Luke introduces her to his new A-list client sage Seymour. Becky sets her heart on a new career – she\\'s going to be a stylist to the stars! And when a chance encounter thrusts her into the limelight, she grabs her opportunity. But in between choosing clutch bags and chasing celebrities, Becky loses touch with her family and her best friend Caught up in the whirlwind of tinseltown, has Becky gone too far this time? \"I almost cried with laughter\" \"properly mood-altering Funny, fast and farcical. I loved it\" \"a superb tale. Five stars!\" heat.',\n",
              " 'Up-to-date resources providing full coverage of Cambridge IGCSE® First Language English (0500 and 0522) for first examination in 2015. This updated, write-in Workbook can be used for independent learning, for homework tasks or revision. It contains text extracts from around the world with linked exercises for students to practise the skills they need for the Cambridge IGCSE. Exercises are grouped into 12 diverse units on cross-curricula topics which are not linked to the Coursebook themes, so students remain engaged in the reading material. The Workbook has been completely updated in line with the new syllabus. It is particularly suitable for students who need additional support with language and grammar. A microsite provides free online resources to support the course.',\n",
              " \"Drawing activities, art instruction, and advice for artists and non-artists alike.\\n     \\nUrban sketching--the process of drawing on the go as a regular practice--is a hot trend in the drawing world. It's also a practical necessity for creatively minded people in a busy world.  In this aspirational guide, self-taught French artist France Belleville-Van Stone emboldens readers to craft a ritual of their own and devote more time to art, even if it's just 10 minutes a day. She offers motivation to move beyond the comfort zone, as well as instruction on turning rough sketches into finished work. \\n\\nBelleville Van-Stone learned how to draw through her own daily practice and knows first-hand how hard it is to find time to incorporate creativity into a busy life. She encourages and teaches us how to do it with advice and guidance such as:\\n\\n·         An A-to-Z list of daily sketch prompts, from airports to bananas, faces to hands, meetings and workplaces\\n·         Tips on what drawing supplies you can and should have--and how to carry them around\\n·         Sections on accepting mistakes, drawing with limited resources, and redefining completion\\n·         Plusses and minuses of going digital, including apps, styluses, and brushes\\n\\nFor those of us who dream of drawing in the minutes between school and work, bathtime and bedtime, and waking and walking out the door, the practical advice in Sketch! is a revelation. By sharing her own creative process, Belleville-Van Stone Sketch inspires artists both established and aspiring to rethink their daily practice, sketch for the pure joy of it, and document their lives and the world around them.\",\n",
              " 'The Gandhi Quiz encapsulates the life and work of Mahatma Gandhi in a multiple-choice quiz format. This handy book will not only test the reader on the extent and depth of their knowledge and understanding of Gandhi, but also encourage them to learn more about the Father of the Nation.',\n",
              " 'The first edition of Network Security received critical acclaim for its lucid and witty explanations of the inner workings of network security protocols. Honored by Network Magazine as one of the top 10 most useful networking books, it is now fully updated for the latest standards and technologies.\\nIn the second edition, the authors draw on their considerable experience to illuminate all facets of information security, from the basics to advanced cryptography and authentication; secure Web and email services and emerging security standards. The authors go far beyond documenting standards and technology: They contrast competing schemes, explain strengths and weaknesses and identify the crucial errors most likely to compromise secure systems.\\nThe highlights of the book’s extensive new coverage include Advanced Encryption Standard (AES), IPsec, SSL, X.509 and related PKI standards,\\nand Web security.\\n\\nTable of Contents\\nChapter 1 Introduction\\nChapter 2 Introduction to Cryptography\\nChapter 3 Secret Key Cryptography\\nChapter 4 Modes of Operation\\nChapter 5 Hashes and Message Digests\\nChapter 6 Public Key Algorithms\\nChapter 7 Overview of Authentication Systems\\nChapter 8 Authentication of People\\nChapter 9 Security Handshake Pitfalls\\nChapter 10 Strong Password Protocols\\nChapter 11 Kerberos V4\\nChapter 12 Kerberos V5\\nChapter 13 PKI (Public Key Infrastructure)\\nChapter 14 Real-time Communication Security\\nChapter 15 IPSEC: AH And ESP\\nChapter 16 IPsec: IKE\\nChapter 17 Electronic Mail Security\\nChapter 18 PEM and S/MIME\\nChapter 19 PGP (Pretty Good Privacy)\\nChapter 20 Firewalls\\nChapter 21 More Security Systems\\nChapter 22 Folklore\\nChapter 23 Number Theory (online)\\nChapter 24 Math with AES and Elliptic Curves (online)\\nChapter 25 SSL/TLS (online)\\nChapter 26 Web Issues (online)',\n",
              " \"With an Introduction and Notes by Keith Wren, University of Kent at Canterbury. The story of Edmund Dantes, self-styled Count of Monte Cristo, is told with consummate skill. The victim of a miscarriage of justice, Dantes is fired by a desire for retribution and empowered by a stroke of providence. In his campaign of vengeance, he becomes an anonymous agent of fate. The sensational narrative of intrigue, betrayal, escape, and triumphant revenge moves at a cracking pace. Dumas' novel presents a powerful conflict between good and evil embodied in an epic saga of rich diversity that is complicated by the hero's ultimate discomfort with the hubristic implication of his own actions. Our edition is based on the most popular and enduring translation first published by Chapman and Hall in 1846. The name of the translator was never revealed.\",\n",
              " 'This popular four-book series has been revised and updated, while preserving the features which have made it so successful. These include: Clear, simple explanations of key points of English grammar, using only essential technical terms. A large number of graded exercises lively illustrations enhancing understanding of the text. Book 1 is suitable for use with beginners and the series takes students upto intermediate level.',\n",
              " \"Quirk's best-selling series of owner's and instruction manuals have covered everything from sex, pregnancy, and planning a wedding to babies, toddlers, and teenagers. With The Newlywed's Instruction Manual, we're guiding readers through yet another of life's milestones the first year of marriage.\\n\\nHere are all the topics that a newly married couple can expect to encounter during their first year as Mr. and Mrs. from finances and home-buying to quarreling in-laws and the inevitable question of children. Illustrated in the trademark techy style that has made this series an international success, The Newlywed's Instruction Manual is the perfect gift for any couple embarking on a new life together.\",\n",
              " \"An ancient mystery becomes an all-too-real modern threat for Dirk Pitt and his colleagues, in an extraordinary adventure novel in one of suspense fiction's most beloved series. The murder of a team of U.N. Scientists while investigating mysterious deaths in El Salvador. A deadly Collision in the waterways off Detroit. An attack from tomb raiders on an archaeological site along the Nile. Is there a link between these violent events? The answer may lie with the tale of an Egyptian princess forced to flee the armies of her father three thousand years ago. From the desert sands of Egypt, to the Rocky Isles of Ireland, to the Deepwater lochs of Scotland, only Dirk Pitt can unravel the secrets of an ancient enigma that could change the very future of mankind.\",\n",
              " \"The new Kurt Austin adventure in the NUMA Files series from UK No. 1 bestseller Clive Cussler.\\nA global threat\\nThe world's sea levels are rising at an alarming rate, too quickly to be caused by glacier melt. A risk so big it sends Kurt Austin, Zoe Zavala and the NUMA team rocketing around the world in search for answers.\\nA desperate mission\\nTheir hunt takes them from the shark-filled waters of Asia to the high-tech streets of Tokyo to a forbidden secret island, but it's in the East China Sea that a mysterious underwater mining operation is discovered.\\nA devastating endgame\\nKurt uncovers a plot more dangerous than they could have imagined: a plan to upset the Pacific balance of power, threatening the lives of millions. It falls to the NUMA team to risk everything to stop it and save the world from the coming catastrophe.\\n\\nNew Feature Information\\n0\",\n",
              " \"'Excellent . . . an in-depth excavation of the murky and mysterious world of football business. Smith's candid and often shocking book reveals the true workings of football business that take into account things few of us even could even imagine . . . The Deal answers some of those questions and leaves you wanting more. It is an educational tool that most fans could do with researching' Joe Short, Express\\nFootball analysis has grown at the same exponential rate as the sport's popularity and yet one of its most intrinsic elements remains tantalisingly opaque: the role of 'agent'. The Deal is a unique and fascinating perspective into the business of sports management through the eyes of 'Mr Football', 'super-agent', Jon Smith. 800,000 watch their professional football team play each week and TV pulls in audiences of around 600 million. Despite these phenomenal figures, the complex money-making scene behind sport is one of its biggest mysteries. The Deal will be an unprecedented insight into this world, showing what goes on as players and big money change hands.\\nThe Deal is also the story of one of the shrewdest and most successful businessmen of our time. Documented through Jon's personal rollercoaster of high-flying success to near bankruptcy, the book's over-arching narrative will offer an inspiring personal journey as well as insider knowledge of brokering deals at a high level and under extreme pressure.\\nThe Deal will appeal strongly to buyers of business books as well as a significant number of sports fans interested to know what goes on in the back room of their favourite sport.\",\n",
              " 'The Malabar coast is full of dangers: greedy tradesmen, fearless pirates and men full of vengeance. But for a Courtney, the greatest danger might just be his own family…\\n\\nFrancis Courtney flees the comfort of his Devonshire estate when his stepfather’s gambling debts leave him penniless and at risk. He sails to South Africa with revenge and fortune on his mind: his uncle Tom Courtney killed his father and Francis intends to avenge his death and make his fortune in the process. However, upon his arrival in Cape Town, he uncovers a truth that leaves him overwhelmed and disoriented.\\nChristopher Courtney sets out to make his own way in the world, giving up his privileged position as the son of the Governor of Bombay. The perils and betrayals on his journey carve a fierce warrior out of him, but they also harden his soul and lead him to greater violence and treachery. As the lives of these two Courtney men intertwine, the sins of the fathers will forever alter the lives of a younger generation.\\n\\nThe Tiger’s Prey takes readers on an epic journey from the southernmost point of Africa, through the perilous waters of the Arabian Sea, to the lush Indian coastline. It is an incredible and breathless tale of intrigue and family betrayal from one of the world’s greatest storytellers.',\n",
              " '\"I\\'m Sebastian, Lord St. Vincent. I can\\'t be celibate. Everyone knows that.\"\\nDesperate to escape her scheming relatives, Evangeline Jenner has sought the help of the most infamous scoundrel in London.\\nA marriage of convenience is the only solution.\\nNo one would have ever paired the shy, stammering wallflower with the sinfully handsome viscount. It quickly becomes clear, however, that Evie is a woman of hidden strength—and Sebastian desires her more than any woman he\\'s ever known.\\nDetermined to win her husband\\'s elusive heart, Evie dares to strike a bargain with the devil: If Sebastian can stay celibate for three months, she will allow him into her bed.\\nWhen Evie is threatened by a vengeful enemy from the past, Sebastian vows to do whatever it takes to protect his wife . . . even at the expense of his own life.\\nTogether they will defy their perilous fate, for the sake of all-consuming love.',\n",
              " 'Have you got the Guts? Kentaro Miura\\'s Berserk has outraged, horrified, and delighted manga and anime fanatics since 1989, creating an international legion of hardcore devotees and inspiring a plethora of TV series, feature films, and video games. And now the badass champion of adult fantasy manga is presented in an oversized 7\" x 10\" deluxe hardcover edition, nearly 700 pages amassing the first three Berserk volumes, with following volumes to come to serve up the entire series in handsome bookshelf collections. No Guts, no glory!',\n",
              " 'FROM THE #1 BESTSELLING AUTHOR\\n‘Deadly conspiracies, bone-crunching action and a tormented hero with a heart . . . packs a real punch’ Andy McDermott\\nAN ANCIENT SWORD. A LONG BURIED SECRET.\\nONE MAN WILL UNCOVER THE MYSTERY.\\nIt’s Christmas, and on a trip back to the UK to try to sort out his stormy personal life, ex-SAS soldier Ben Hope runs into two old friends. Simeon and Michaela were once his fellow students at Oxford – now they are the Reverend and Mrs Arundel.\\nBen senses that Simeon is deeply troubled and frightened, but before the truth can emerge about his secretive research project concerning a mysterious, ancient ‘sacred sword’, both he and Michael are wiped out in a devastating road crash. Convinced that his friends’ deaths were no accident, Ben is propelled on a global quest to unlock the enigma of the sacred sword. At every step, he is pursued by ruthless agents of the sinister organisation that will stop at nothing to acquire it.\\nIn the course of his investigations, Ben will discover an incredible secret, not just about the history of Christianity but about his own past.\\nThe Ben Hope series is a must-read for fans of Dan Brown, Lee Child and Mark Dawson. Join the millions of readers who get breathless with anticipation when the countdown to a new Ben Hope thriller begins…\\nWhilst the Ben Hope thrillers can be read in any order, this is the seventh book in the series.',\n",
              " \"The thriller that opened a new chapter in the sex wars ...\\n\\nThomas Sanders' world collapses in just 24 hours - he is passed over for promotion, his new woman boss comes on to him during a drink after work, then, the next morning, he learns that she has accused him of sexually harassing her. She demands his transfer, thereby threatening to cut him off from the millions he would have made when his high-tech company was floated on the stock market.\\nWhat follow next made Disclosure the most talked about novel of the decade.\",\n",
              " 'Comprehensively surveying the field of labor economics, this market-leading textbook showcases both current and classic research. The authors develop the modern theory of labor market behavior, summarize empirical evidence that supports or contradicts each hypothesis, and illustrate the usefulness of various theories for public policy analysis. In addition to the policyexamples woven throughout the narrative, the text offers two or more boxed examples per chapter that illustrate the application of theory in a nontraditional, business, historical, or cross-cultural context. The Seventh Edition provides updated coverage and updated references to the professional literature throughout, as well as many new boxed policy examples and new end-of-chapter numerical problems. An all-new companion Web Site rounds out the teaching and learning resources of the supplements program.',\n",
              " 'The life story of the world’s greatest motorcycle rider — five-time-winner of the World Championships and one of the superstars of the sporting world — his post race antics and cheeky personality have won him as many fans as his on-track prowess.',\n",
              " \"In the 1980s, an unheralded Hindi movie, made on a budget of less than Rs 7 lakh, went from a quiet showing at the box office to developing a reputation as India's definitive black comedy. Some of the country's finest theatre and film talents - all at key stages in their careers - participated in its creation, but the journey was anything but smooth. Among other things, it involved bumping off disco killers and talking gorillas, finding air-conditioned rooms for dead rats, persuading a respected actor to stop sulking and eat his meals, and resisting the temptation to introduce logic into a madcap script. In the end, it was worth it. Kundan Shah's Jaane Bhi Do Yaaro is now a byword for the sort of absurdist, satirical humours that Hindi cinema just hasn't seen enough of. This is the story of how it came to be despite incredible odds - and what it might have been. Jai Arjun Singh's take on the making of the film and its cult following is as entertaining as the film itself.\",\n",
              " 'A guide to nourishing the body through bone marrow rejuvenation exercises\\n\\n• Presents exercises to “regrow” bone marrow, revive the internal organs, and prevent osteoporosis\\n\\n• Explains the use of bone breathing and bone compression, “hitting” to detoxify the body, and sexual energy massage and chi weight lifting to enhance the life force within\\n\\nMost Westerners believe that a daily physical exercise program helps slow the aging process. Yet those whose bodies appear most physically fit on the outside often enjoy only the same life span as the average nonathletic person. It is the internal organs and glands that nourish every function of the body, and it is the bone marrow that nourishes and rejuvenates the organs and glands through the production of blood. By focusing only on the muscles without cultivating the internal organs, bones, and blood, the Western fitness regimen can ultimately exhaust the internal system.\\n\\nIn Bone Marrow Nei Kung Master Mantak Chia reveals the ancient mental and physical Taoist techniques used to “regrow” bone marrow, strengthen the bones, and rejuvenate the organs and glands. An advanced practice of Iron Shirt Chi Kung, Bone Marrow Nei Kung was developed as a way to attain the “steel body” coveted in the fields of Chinese medicine and martial arts. This method of absorbing energy into the bones revives the bone marrow and reverses the effects of aging through the techniques of bone breathing, bone compression, and sexual energy massage, which stimulates the hormonal production that helps prevent osteoporosis. Also included is extensive information on chi weight lifting and the practice of “hitting” to detoxify the body.',\n",
              " 'Complaints about the state of medical care are increasing in today’s India; whether it’s unnecessary investigations, botched operations or expensive, sometimes even harmful, medication. But while the unease is widespread, few outside the profession understand the extent to which the medical system is being distorted. Dr Arun Gadre and Dr Abhay Shukla have gathered evidence from seventy-eight practising doctors, in both the private and public medical sectors, to expose the ways in which vulnerable patients are exploited by a system that promotes unscrupulous medical practices.\\nAt a time when the medical sector is growing rapidly, especially in urban areas, with the proliferation of multi-specialty hospitals and the adoption of ever-more sophisticated technologies, rational and ethical medical care is becoming increasingly rare. Honest doctors feel under siege, professional bodies meant to regulate the medical sector fail to do so, and the influence of the powerful pharmaceutical industry becomes even more pervasive.\\nDrawing on the frank and courageous statements of these seventy-eight doctors dismayed at the state of their profession, Dissenting Diagnosis lays bare the corruption afflicting the medical sector in India and sets out solutions for a healthier future.',\n",
              " 'Named a Best Book of the Year by the Economist, Wall Street Journal & Vox\\n\\n‘The father of virtual reality’ (Sunday Times) explains why virtual reality presents the ultimate test for humanity.\\n\\n‘Essential reading, not just for VR-watchers but for anyone interested in how society came to be how it is, and what it might yet become’ Economist\\n\\nWelcome to a mind-expanding, life-enhancing, world-changing adventure.\\n\\nVirtual reality has long been one of the dominant clichés of science fiction. Now virtual reality is a reality: from the startling beauty of lifelike video games to the place where war veterans overcome PTSD, surgeries are trialled, and aircraft and cities are designed. VR is, in fact now, the most effective device ever invented for researching what a human being actually is – and how we think and feel.\\n\\nMore than thirty years ago, legendary computer scientist, visionary and artist Jaron Lanier pioneered its invention. Here he blends scientific investigation, philosophical thought experiment and his memoir of a life lived at the centre of digital innovation to explain what VR really is: the science of comprehensive illusion; the extension of the intimate magic of earliest childhood into adulthood; a hint of what life would be like without any limits.\\n\\nWe are standing on the threshold of an entirely new realm of human creativity, expression, communication and experience, and as we use VR to test our relationship with reality, it may test us in return.\\n\\n‘Vivid and absolutely extraordinary’ Evening Standard',\n",
              " 'The authoritative preparation guide to help you conquer the IELTS\\nThe International English Language Testing System (IELTS) is a paper-based test that consists of four modules―Listening, Reading, Writing, and Speaking. Question types include multiple choice, sentence completion, short answer, classification, matching, labeling, and diagram/chart interpretation. Created by ESL experts at Cambridge University in the UK, the IELTS English proficiency exam is used worldwide for admission to English-speaking colleges and universities.\\nWe’ve put all of our proven expertise intoMcGraw-Hill Education: IELTS to make sureyou’re ready for this crucial test. Whether you’retaking the exam to qualify for entry to an Englishlanguageuniversity, for immigration, or foremployment in a government agency or multinationalcorporation that uses English, this bookgives you essential skill-building techniques andstrategies you need to achieve your desired score.You’ll get five practice tests, skill-building drills,test-taking strategies, and all the facts aboutthe current exam. With McGraw-Hill Education:IELTS, we’ll guide you step by step through yourpreparation program―and give you the tools youneed to succeed.\\nInside you’ll find\\n5 practice tests (4 Academic Tests, 1 General Training Test)\\nSkill-building drills with hundreds of practice questions\\nStrategies to help you master every question type\\nReview of all subjects featured on the exam\\nCD-ROM with 70-minutes of audio recordings for listening exercises and practice tests\\nAbout the Author\\nMonica Sorrenson has been an IELTS examinersince 2001 in China, Australia, Syria, Ukraine, and Oman.She has also taught in the United Kingdom, New Zealand,Indonesia, Sudan, and Russia. She has qualifications fromthe University of New South Wales, Sydney.',\n",
              " 'The ultimate guide for artists in every medium. Every artist needs to know how to create the illusion of depth and distance. Packed with specific tips and instruction for several different mediums including acrylic, oil, watercolor and graphite Teaches essential perspective techniques through easy step-by-step demonstrations and exercises Starts simple and moves on to complex scenes and challenges such as stairways, curves, and non-rectangular objects Most books that cover perspective thoroughly are difficult to navigate. This book provides comprehensive and friendly instruction within a user-friendly North Light design',\n",
              " \"A heartwarming and hilarious romantic debut about falling in love from afar.\\nIt's 1999 and for the staff of one newspaper office, the internet is still a novelty. By day, two young women, Beth and Jennifer, spend their hours emailing each other, discussing in hilarious detail every aspect of their lives, from love troubles to family dramas. And by night, Lincoln, a shy, lonely IT guy spends his hours reading every exchange.\\nAt first their emails offer a welcome diversion, but as Lincoln unwittingly becomes drawn into their lives, the more he reads, the more he finds himself falling for one of them. By the time Lincoln realizes just how head-over-heels he really is, it's way too late to introduce himself. What would he say to her? 'Hi, I'm the guy who reads your e-mails - and also, I think I love you'.\\nAfter a series of close encounters, Lincoln decides it's time to muster the courage to follow his heart, and find out whether there really is such a thing as love before first-sight.\",\n",
              " 'Take tiny steps to enter the big world of data science through this interesting guide About This Book * Learn the fundamentals of machine learning and build your own intelligent applications * Master the art of building your own machine learning systems with this example-based practical guide * Work with important classification and regression algorithms and other machine learning techniques Who This Book Is For This book is for anyone interested in entering the data science stream with machine learning. Basic familiarity with Python is assumed. What You Will Learn * Exploit the power of Python to handle data extraction, manipulation, and exploration techniques * Use Python to visualize data spread across multiple dimensions and extract useful features * Dive deep into the world of analytics to predict situations correctly * Implement machine learning classification and regression algorithms from scratch in Python * Be amazed to see the algorithms in action * Evaluate the performance of a machine learning model and optimize it * Solve interesting real-world problems using machine learning and Python as the journey unfolds In Detail Data science and machine learning are some of the top buzzwords in the technical world today. A resurging interest in machine learning is due to the same factors that have made data mining and Bayesian analysis more popular than ever. This book is your entry point to machine learning. This book starts with an introduction to machine learning and the Python language and shows you how to complete the setup. Moving ahead, you will learn all the important concepts such as, exploratory data analysis, data preprocessing, feature extraction, data visualization and clustering, classification, regression and model performance evaluation. With the help of various projects included, you will find it intriguing to acquire the mechanics of several important machine learning algorithms - they are no more obscure as they thought. Also, you will be guided step by step to build your own models from scratch. Toward the end, you will gather a broad picture of the machine learning ecosystem and best practices of applying machine learning techniques. Through this book, you will learn to tackle data-driven problems and implement your solutions with the powerful yet simple language, Python. Interesting and easy-to-follow examples, to name some, news topic classification, spam email detection, online ad click-through prediction, stock prices forecast, will keep you glued till you reach your goal. Style and approach This book is an enticing journey that starts from the very basics and gradually picks up pace as the story unfolds. Each concept is first succinctly defined in the larger context of things, followed by a detailed explanation of their application. Every concept is explained with the help of a project that solves a real-world problem, and involves hands-on work-giving you a deep insight into the world of machine learning. With simple yet rich language-Python-you will understand and be able to implement the examples with ease.',\n",
              " 'The defining, behind-the-scenes chronicle of one of the most extraordinary, beloved, and dominant pop cultural entities in America’s history -- Marvel Comics – and the outsized personalities who made Marvel including Martin Goodman, Stan Lee, and Jack Kirby.  \\n“Sean Howe’s history of Marvel makes a compulsively readable, riotous and heartbreaking version of my favorite story, that of how a bunch of weirdoes changed the world…That it’s all true is just frosting on the cake.”\\n—Jonathan Lethem\\nFor the first time, Marvel Comics tells the stories of the men who made Marvel: Martin Goodman, the self-made publisher who forayed into comics after a get-rich-quick tip in 1939, Stan Lee, the energetic editor who would shepherd the company through thick and thin for decades and Jack Kirby, the WWII veteran who would co-create Captain America in 1940 and, twenty years later, developed with Lee the bulk of the company’s marquee characters in a three-year frenzy. Incorporating more than one hundred original interviews with those who worked behind the scenes at Marvel over a seventy-year-span, Marvel Comics packs anecdotes and analysis into a gripping narrative of how a small group of people on the cusp of failure created one of the most enduring pop cultural forces in contemporary America.',\n",
              " \"Some imaginary friends never go away...\\n\\nTen years after starting Project Mayhem, he lives a mundane life. A kid, a wife. Pills to keep his destiny at bay. But it won't last long, the wife has seen to that. He's back where he started, but this go-round he's got more at stake than his own life. The time has arrived... Rize or Die.\",\n",
              " 'Alex Rider is back in this brand new, explosive mission in the number one bestselling series.\\nIn this brand new, explosive adventure in the number one bestselling series, Alex Rider is trying to get his life back on track after\\nthe traumatic events of his last mission. But even Alex can’t fight the past … especially when it holds a deadly secret.',\n",
              " \"The original edition of the most trusted writer's guide to American English, this is the book that generations of writers have relied upon for timeless advice on grammar, diction, syntax, sentence construction, and other writing essentials. In brief and concise terms, author William Strunk, Jr., identifies the principal requirements of proper American English style and concentrates on the most often violated rules of composition.\\nOriginally published in 1918, this authoritative and engagingly written manual retains its immediacy and relevance. Strunk begins with the basic rules of usage, offering accessible explanations of correct punctuation and grammar. His advice on the principles of composition cites concrete examples of how to maintain clarity by eliminating redundancy and unnecessary embellishment. In addition, Strunk provides valuable pointers on avoiding common errors in the usage of words and expressions, and in spelling. The greatest book of its kind, this volume is a must for any student or writer.\",\n",
              " \"We have two choices. We can be pessimistic, give up and help ensure that the worst will happen. Or we can be optimistic, grasp the opportunities that surely exist and maybe help make the world a better place. Not much of a choice.\\n\\nAnother essential work from a peerless political thinker, exploring the rise of neoliberalism, the refugee crisis in Europe, the Black Lives Matter movement, the dysfunctional US electoral system and the prospects and challenges of building a movement for radical change. Including four up-to-the-minute interviews on the 2016 American election campaign and global resistance to Trump, this Penguin Special is a concise introduction to Chomsky's ideas and his take on the state of the world today.\",\n",
              " 'In a major event in chess publishing, two German endgame experts have produced a masterly one-volume encyclopaedia that covers all major endgames.\\n\\nThis is the first truly modern one-volume endgame encyclopaedia. It makes full use of endgame tablebases and analytical engines that access these tablebases; where previous authors could only make educated guesses, Müller and Lamprecht have often been able to state the definitive truth, or get much closer to it.\\n\\nNew time-controls involve competitive games being played to a finish in one session, so it is especially important that chess-players understand the key endgame principles. This book provides comprehensive assistance for any players wishing to study the endgame. In addition to a feast of detailed analysis, the authors emphasize the practical side of endgame play, describing rules of thumb, principles, and thinking methods.\\n\\nFundamental Chess Endings is both the ideal endgame reference work, and a book that can profitably and enjoyably be read from start to finish.',\n",
              " '2016 AD. When footage of a team of archaeologists bursting into flames at the ancient site of Mohenjodaro goes viral, the world is horrified and shaken. While authorities suspect it to be an incendiary terrorist attack, Nadia Osbourne determines to find her archaeologist sister, Layla, convinced that she has survived. Her frantic search takes her to the ruins and forces her to confront her own demons – her inexplicable dreams about a woman named Jaya.\\n3800 BC. The city of Meluhha is on the brink of a revolution and Iaf and his coterie of corrupt priests will do anything to maintain their power. Jaya is the only one who can read the Bloodstone, the heart of the Goddess Shakari and divine the future. But with her daughter under Iaf’s control, will Jaya be able to prevent what is to come?\\nInspired by the legends surrounding the lost Indus Valley city, the Curse of Mohenjodaro is a gripping thriller about a powerful relic, a sinister cult and family secrets that haunt generations.',\n",
              " \"Introduction and Notes by Dr David Rogers, Kingston University. 'There he lay looking as if youth had been half-renewed, for the white hair and moustache were changed to dark iron-grey, the cheeks were fuller, and the white skin seemed ruby-red underneath; the mouth was redder than ever, for on the lips were gouts of fresh blood, which trickled from the corners of the mouth and ran over the chin and neck. Even the deep, burning eyes seemed set amongst the swollen flesh, for the lids and pouches underneath were bloated. It seemed as if the whole awful creature were simply gorged with blood; he lay like a filthy leech, exhausted with his repletion.' Thus Bram Stoker, one of the greatest exponents of the supernatural narrative, describes the demonic subject of his chilling masterpiece Dracula, a truly iconic and unsettling tale of vampirism.\",\n",
              " 'Concurrency can be notoriously difficult to get right, but fortunately, the Go open source programming language makes working with concurrency tractable and even easy. If youíre a developer familiar with Go, this practical book demonstrates best practices and patterns to help you incorporate concurrency into your systems.\\nAuthor Katherine Cox-Buday takes you step-by-step through the process. Youíll understand how Go chooses to model concurrency, what issues arise from this model and how you can compose primitives within this model to solve problems. Learn the skills and tooling you need to confidently write and implement concurrent systems of any size.\\n\\nUnderstand how Go addresses fundamental problems that make concurrency difficult to do correctly\\nLearn the key differences between concurrency and parallelism\\nDig into the syntax of Goís memory synchronization primitives\\nForm patterns with these primitives to write maintainable concurrent code\\nCompose patterns into a series of practices that enable you to write large, distributed systems that scale\\nLearn the sophistication behind goroutines and how Goís runtime stitches everything together',\n",
              " 'More and more data-driven companies are looking to adopt stream processing and streaming analytics. with this concise ebook, youíll learn best practices for designing a reliable architecture that supports this emerging big-data paradigm.\\nAuthors Ted Dunning and Ellen Friedman (Real World Hadoop) help you explore some of the best technologies to handle stream processing and analytics, with a focus on the upstream queuing or message-passing layer. To illustrate the effectiveness of these technologies, this book also includes specific use cases.\\nIdeal for developers and non-technical people alike, this book describes:\\n\\nKey elements in good design for streaming analytics, focusing on the essential characteristics of the messaging layer\\nNew messaging technologies, including Apache Kafka and MapR Streams, with links to sample code\\nTechnology choices for streaming analytics: Apache Spark Streaming, Apache Flink, Apache Storm and Apache Apex\\nHow stream-based architectures are helpful to support microservices\\nSpecific use cases such as fraud detection and geo-distributed data streams',\n",
              " \"The Lost Symbol (Robert Langdon) is written by Dan Brown. The story is about a man named Robert Langdon, who thinks that he had been invited to give a lecture in the Capitol Building, Washington DC. Later he realised that the invitation is a gruesome invitation into an ancient world of hidden wisdom. When Langdon's mentor, Peter Solomon - prominent mason and philanthropist - is kidnapped, Langdon realizes that his only hope of saving his friend's life is to accept this mysterious summons.\\nIt takes him on a breathless chase through Washington's dark history. All that was familiar is changed into a shadowy, mythical world in which Masonic secrets and never-before-seen revelations seem to be leading him to a single impossible and inconceivable truth. What had happened to Peter Solomon? Was Robert Langdon able to release him from the kidnappers? The book is an exciting read; thriller and entertaining at the same time. Purchase the book from Amazon India at reasonable rates and get the answers of all your questions. The book will be delivered within 2-4 working days.\\nAbout the author:\\nDan Brown the author of The Lost Symbol (Robert Langdon) had also written various other books like Digital Fortress, Angels & Demons and The Da Vinci Code, which has sold more than 80 million copies worldwide, making it one of the bestselling novels of all time. The book has also been adapted into a motion picture. Named one of the World's 100 Most Influential People by TIME Magazine, he has appeared in the pages of Newsweek, Forbes, People, GQ, The New Yorker and others. His novels are published in 51 languages around the world. He is a graduate of Amherst College and Phillips Exeter Academy, where he also taught English. Digital Fortress was inspired by his experiences at the academy. He lives in New England with his wife.\",\n",
              " 'Rex is a big dinosaur. He sees some small dinosaurs. They are playing. \"Can I play, too?\" asks Rex. Ladybird Readers is a graded reading series of traditional tales, popular characters, modern stories, and non-fiction, written for young learners of English as a foreign or second language. Beautifully illustrated and carefully written, the series combines the best of Ladybird content with the structured language progression that will help children develop their reading, writing, speaking, listening and critical thinking skills. The five levels of Readers and Activity Books follow the CEFR framework and include language activities that provide preparation for the Cambridge English: Young Learners (YLE) Starters, Movers and Flyers exams. Rex the Dinosaur, a Level 1 Activity Book, is Pre-A1 in the CEFR framework and supports YLE Starters exams. The activities encourage children to practice short sentences containing a maximum of two clauses, using the present tense and some simple adjectives.',\n",
              " 'One thousand years ago, India was at the height of its power, influencing the world with its ideas and trade. Now, ten centuries later, India’s recent economic performance is once again attracting world attention as the country re-awakens not just as an economy but as a civilization.\\nIn The Indian Renaissance: India’s Rise after a Thousand Years of Decline, Sanjeev Sanyal looks at the processes that led to ten centuries of decline. He also examines the powerful economic and social forces that are working together to transform India beyond recognition. These range from demographic shifts to rising literacy levels and, the most important revolution, the opening of minds and changed attitude towards innovation and risk—fundamental, if India is to take advantage of the twenty-first century.',\n",
              " \"Herge's classic comic book creation Tintin is one of the most iconic characters in children's books. These highly collectible editions of the original 24 adventures will delight Tintin fans old and new. Perfect for lovers of graphic novels, mysteries and historical adventures. The world's most famous travelling reporter journeys to South America on a mission to save Professor Calculus. When Professor Calculus is kidnapped, Tintin and a desperate Captain Haddock set off to Peru on a rescue mission, braving runaway train carriages, yellow fever and avalanches. Then they must find an ancient Inca tribe if they are to find their great friend. The Adventures of Tintin are among the best books for readers aged 8 and up. Herge (Georges Remi) was born in Brussels in 1907. Over the course of 54 years he completed over 20 titles in The Adventures of Tintin series, which is now considered to be one of the greatest, if not the greatest, comics series of all time. Have you collected all 24 graphic novel adventures? Tintin in the Land of the Soviets Tintin in the Congo Tintin in America Tintin: Cigars of the Pharaoh Tintin: The Blue Lotus Tintin: The Broken Ear Tintin: The Black Island Tintin: King Ottakar's Sceptre Tintin: The Crab with the Golden Claws Tintin: The Shooting Star Tintin: The Secret of the Unicorn Tintin: Red Rackham's Treasure Tintin: The Seven Crystal Balls Tintin: Prisoners of the Sun Tintin: Land of Black Gold Tintin: Destination Moon Tintin: Explorers of the Moon Tintin: The Calculus Affair Tintin: The Red Sea Sharks Tintin in Tibet Tintin: The Castafiore Emerald Tintin: Flight 714 to Sydney The Adventures of Tintin and the Picaros Tintin and Alph-Art\",\n",
              " 'Take Control of Your Comics-Making Destiny\\n\\nCreating your own comic is easier than ever before. With advances in technology, the increased connectivity of social media, and the ever-increasing popularity of the comics medium, successful\\nDIY comics publishing is within your reach. With The Complete Guide to Self-Publishing Comics, creators/instructors Comfort Love and Adam Withers provide a step-by-step breakdown of the comics-making process, perfect for any aspiring comics creator. This unprecedented, in-depth coverage gives you expert analysis on each step—writing, drawing, coloring, lettering, publishing,\\nand marketing. Along the way, luminaries in the fields of comics, manga, and webcomics—like\\nMark Waid, Adam Warren, Scott Kurtz, and Jill Thompson—lend a hand, providing “Pro Tips” on essential topics for achieving your comics-making dreams. With the insights and expertise contained within these pages, you’ll have everything you need and no excuses left: It’s time to make your comics!',\n",
              " \"A fascinating discussion on sex, gender, and human instincts, as relevant today as ever\\n\\nIn the course of a lively drinking party, a group of Athenian intellectuals exchange views on eros, or desire. From their conversation emerges a series of subtle reflections on gender roles, sex in society and the sublimation of basic human instincts. The discussion culminates in a radical challenge to conventional views by Plato's mentor, Socrates, who advocates transcendence through spiritual love. The Symposium is a deft interweaving of different viewpoints and ideas about the nature of love—as a response to beauty, a cosmic force, a motive for social action and as a means of ethical education.\\n\\nFor more than seventy years, Penguin has been the leading publisher of classic literature in the English-speaking world. With more than 1,700 titles, Penguin Classics represents a global bookshelf of the best works throughout history and across genres and disciplines. Readers trust the series to provide authoritative texts enhanced by introductions and notes by distinguished scholars and contemporary authors, as well as up-to-date translations by award-winning translators.\",\n",
              " 'American favourite cat is back with this new edition of three-in-one comic strips—containing: Garfield rounds out (#16), Garfield chews the fat (#17)and Garfield goes to waist (#together for the first time in color, super sized and value-priced!.',\n",
              " \"Robinson Crusoe on Mars\\n\\nA survival story for the 21st century and the international bestseller behind the major film from Ridley Scott starring Matt Damon and Jessica Chastain.\\n\\nI’m stranded on Mars.\\n\\nI have no way to communicate with Earth.\\n\\nI’m in a Habitat designed to last 31 days.\\n\\nIf the Oxygenator breaks down, I’ll suffocate. If the Water Reclaimer breaks down, I’ll die of thirst. If the Hab breaches, I’ll just kind of explode. If none of those things happen, I’ll eventually run out of food and starve to death.\\n\\nSo yeah. I’m screwed.\\n\\nAndy Weir's second novel Artemis is now available\",\n",
              " 'No one has ever recorded the voice of Radha – one of the most powerful characters in all of chronicled history. As one half of the indivisible entity ‘RadhaKrishna’ she resonates through millions of lives every day. And yet, her songs remain unsung; her stories stay untold. Until now. This book, entitled an eponymous ‘Radha’; is a collection of 101 poems written in the voice of Radha. This book is Radha speaking to her beloved Krishna in poetry, across the ages, in an endless conversation that ebbs and flows and heaves and rests like the rhythm of a vast ocean. Sometimes she is a young girl, sometimes she is an old woman, sometimes she is petulant in her love for her Krishna, sometimes she is unflinching in her courage in letting go of her beloved Krishna. This book is an imagined conversation between 2 of the most evocative characters of all time. Her Krishna is as present in the writing as Radha is, although he never speaks a word. This is not a love story. This is the love story.',\n",
              " \"A dark and powerful goddess hunts for three jewels: fallen stars that will give her endless power. To save the world, six friends have joined forces to stop her. Now, on the beautiful island of Capri, their battle continues... Mermaid Annika is proud to have been chosen for such an important quest. But now that her identity has been revealed, her time is running out. She knows that soon she must return to her people. But, she also knows that she is in love with Sawyer King - the brave and loyal adventurer with secrets of his own. As Annika, Sawyer and their four friends hunt for the mysterious 'star of water', the goddess Nerezza sends a terrifying foe to destroy them. It seems there is no place for love and desire at such a dark time. And so Sawyer tries to protect Annika by pushing her away. But true love cannot be denied. And in a battle between the dark and the light - it might be the only thing that can save them from a terrible fate.\",\n",
              " 'No. 1 international bestseller and Swedish crime sensation Camilla Lackberg’s new psychological thriller featuring Detective Patrik Hedström and Erica Falck – irresistible for fans of Stieg Larsson and Jo Nesbo.\\nA missing child\\nWhen a four-year-old girl disappears in the woods just outside Fjällbacka, the community is horror-struck. Thirty years ago, a young girl went missing from the exact same spot, and was later discovered, murdered.\\nA murder\\nBack then, two teenage girls were found guilty of the killing. Could it really be a coincidence that one of the girls – now a world-famous actress – has just returned to Fjällbacka? Detective Patrik Hedström starts investigating, with his wife, bestselling crime writer Erica Falck, by his side.\\nA community torn apart\\nBut as Patrik and Erica dig deeper, the truth becomes ever murkier, because it seems that everyone in the tight-knit community is hiding something. And soon, the residents must confront the fact that they could be harbouring a murderer in their midst…',\n",
              " \"THE NO.1 SUNDAY TIMES AND NEW YORK TIMES HARDBACK BESTSELLER\\nFall under the spell of Diana and Matthew once more in the stunning climax to their epic tale, following A DISCOVERY OF WITCHES and SHADOW OF NIGHT.\\nA world of witches, daemons and vampires. A manuscript which holds the secrets of their past and the key to their future. Diana and Matthew - the forbidden love at the heart of it.\\nAfter travelling through time in SHADOW OF NIGHT, the second book in Deborah Harkness's enchant\\xading series, historian and witch Diana Bishop and vampire scientist Matthew Clairmont return to the present to face new crises and old enemies. At Matthew's ancestral home in France they reunite with their families - with one heart-breaking exception. But the real threat to their future is yet to be revealed, and when it is, the search for the elusive manuscript Ashmole 782 and its missing pages takes on a terrifying urgency. Using ancient knowl\\xadedge and modern science, from the palaces of Venice and beyond, Diana and Matthew will finally learn what the witches discovered so many centuries ago.\",\n",
              " \"Whether you're a student or professional, a jazz player, rocker, or salsa enthusiast, performing today's music can be challenging. Developed by the author during a lifetime spent as an arranger and performer, this unique, well-planned book, trains musicians to understand and perform the most complex rhythms at sight. It contains well over 1,000 examples of rhythmic figures common in jazz, rock, Latin, blues, funk and other styles. The CD demonstrates rhythms from throughout the book performed in a variety of feels.\",\n",
              " 'Anitha Devasia\\'s translation is alive to the nuances...without eroding the narrative smoothness. [There is] helpful additional material in the form of appendices...Susie Tharu, in the succinct foreword put[s] the reader \"on the scent of the momentous issues that are at stake\". -- The Hindu \\'The most charming part of the novel is the encounter between Indulekha and Surinamboothiri. Anitha Devasia has ...succeeded in keeping the spirit of the original intact.\\' -- Deccan Herald \\'The novel is a blend of West and East...The heroine is reminiscent of Austen\\'s heroines...\\' -- The Telegraph Indulekha (1889) is widely held to be the first Malayalam novel. Often called an \\'accidental\\' and \\'flawed\\' work, as its core lies a love story. Written by a Nair, Indulekha achieves certain social goals: firstly, it creates a novel much like those of English authors read by Chandumenon, and secondly, it illustrates Nair society at that time. This novel will appeal to ge neral readers interested in Indian writings in translation. Students of literature, history and culture, political and legal theory, and gender studies, will also find it useful.',\n",
              " 'While other books give you tips on how to \\x93write funny,\\x94 this book offers a paradigm shift in understanding the mechanics and art of comedy, and the proven, practical tools that help writers translate that understanding into successful, commercial scripts. The Hidden Tools of Comedy unlocks the unique secrets and techniques of writing comedy. Kaplan deconstructs sequences in popular films and TV that work and don\\x92t work, and explains what tools were used (or should have been used).',\n",
              " \"When it came to hunting, she was a master shot. As a dress designer, few could match. An ingenious architect, she innovated the use of marble in her parents' mausoleum on the banks of the Yamuna, which inspired her stepson's Taj Mahal. And she was both celebrated and reviled for her political acumen and diplomatic skill, which rivaled those of her female counterparts in Europe and beyond.\\nIn 1611, thirty-four-year-old Nur Jahan, daughter of a Persian noble and widow of a subversive official, became the twentieth and most cherished wife of Emperor Jahangir. While other wives were secluded behind walls, Nur ruled the vast Mughal Empire alongside her husband, and governed in his stead when his health failed and his attention wandered from matters of state. An astute politician and a devoted partner, Nur led troops into battle to free Jahangir when he was imprisioned by one of his officers. She signed and issued imperial orders, and coins of the realm bore her name.\\nAcclaimed historian Ruby Lal uncovers the rich life and world of Nur Jahan, rescuing this dazzling figure from patriarchal and orientalist cliches of romance and intrigue, while giving a new insight into the lives of the women and the girls during the Mughal Empire, even where scholars claim there are no sources. Nur's confident assertion of authority and talent is revelatory. In Empress, she finally receives her due in a deeply researched and evocative biography that awakens us to a fascinating history.\",\n",
              " 'Cambridge International AS and A Level Computer Science offers a complete set of resources to accompany the 9608 syllabus. This revision guide helps students to prepare and practice skills for the Cambridge AS and A Level Computer Science examination. It contains clear explanations and key information to support learners, with additional practice questions to help students feel confident and reinforce their understanding of key concepts.',\n",
              " 'A personal and powerful essay from Chimamanda Ngozi Adichie, the bestselling author of Americanah and Half of a Yellow Sun.\\n‘I would like to ask that we begin to dream about and plan for a different world. A fairer world. A world of happier men and happier women who are truer to themselves. And this is how to start: we must raise our daughters differently. We must also raise our sons differently…’\\nWhat does “feminism” mean today?\\nIn this personal, eloquently argued essay – adapted from her much-admired Tedx talk of the same name – Chimamanda Ngozi Adichie offers readers a unique definition of feminism for the twenty-first century, one rooted in inclusion and awareness. Drawing extensively on her own experiences and her deep understanding of the often masked realities of sexual politics, here is one remarkable author’s exploration of what it means to be a woman now – an of-the-moment rallying cry for why we should all be feminists.',\n",
              " \"Follow the Avengers of the Marvel Cinematic Universe in their adventures leading up to this summer's blockbuster event, Marvel's Avengers: Infinity War! Plus, tales from the long and villainous history of the Mad Titan, Thanos!\\n\\nCOLLECTING: MARVEL'S AVENGERS: INFINITY WAR PRELUDE 1-2, INFINITY 1, THANOS ANNUAL 1\",\n",
              " \"THE DRAWING OF THE THREE is the second volume in Stephen King's epic Dark Tower series. The Dark Tower is now a major motion picture starring Matthew McConaughey and Idris Elba.\\nIn the second novel of Stephen King's bestselling fantasy masterpiece, Roland of Gilead, the Last Gunslinger, encounters three mysterious doors which open - for Roland, and Roland alone - to different times in our world. He must draw the three who should accompany him on the road.\\nIn 1980s New York, Roland joins forces with defiant Eddie Dean. The second door leads to the 1960s and conflicted civil rights activist Odetta Holmes. The final door reveals Jack Mort, a deadly serial killer, in the 1970s. Mort is stalking Jake, the young boy Roland met in Mid-World.\\nAs titanic forces gather, a savage struggle between underworld evil and otherworldly enemies threatens to bring an end to Roland's journey toward the Dark Tower...\\nJOIN THE QUEST FOR THE DARK TOWER...\\nTHE DARK TOWER SERIES:\\nTHE DARK TOWER I: THE GUNSLINGER\\nTHE DARK TOWER II: THE DRAWING OF THE THREE\\nTHE DARK TOWER III: THE WASTE LANDS\\nTHE DARK TOWER IV: WIZARD AND GLASS\\nTHE DARK TOWER V: WOLVES OF THE CALLA\\nTHE DARK TOWER VI: SONG OF SUSANNAH\\nTHE DARK TOWER VII: THE DARK TOWER\\nTHE WIND THROUGH THE KEYHOLE: A DARK TOWER NOVEL\",\n",
              " '2011 Reprint of 1938 Edition. Full facsimile of the original edition, not reproduced with Optical Recognition Software. Ueland published two books during her life. The first was \"If You Want to Write,\" first published in 1938. In this book, she shares her philosophies on writing and life in general. She stresses the idea that \"Everyone is talented, original, and has something important to say.\" Drawing heavily on the work and influence of William Blake, she suggests that writers should \"Try to discover your true, honest, un-theoretical self.\" She sums up her book with 12 points to keep in mind while writing. Carl Sandburg called \"If You Want to Write\" the best book ever written on how to write.',\n",
              " \"The Second Oxford India Illustrated Corbett is another collection of Corbett's best-loved writings complemented by gripping illustrations. Whether lovingly sketching life in a Himalayan village as in 'The Queen of the Village' and 'Kunwar Singh', or describing the dense Indian jungles and teeming wildlife of his days in Jungle Lore , or telling the story, tinged with regret, of the hunting of the beautiful Pipal Pani Tiger, the stories reflect Corbett's involvement with India, her people and her flora and fauna. With 'The Talla Des Man-eater' and 'The Man-eating Leopard of Rudraprayag' we enter vintage Corbett territory-tightly paced and nail-biting accounts of hunting man-eaters in the hills of Kumaon and Garhwal. The entire collection, with its rich visuals and riveting tales, will appeal to young readers, and Corbett admirers alike. However, some stories have been chosen particularly for our younger readers. 'Robin' is a poignant story of Corbett's favourite hunting dog; 'Sultana' is an appealing tale of an Indian Robin Hood; and 'The Muktesar Man-eater' is a touching account of Corbett's satisfaction at 'having made a small portion of the earth safe for a brave little girl to walk on'.\",\n",
              " 'Ogilvy On Advertising (Vintage) is a book on business management that covers the aspects of advertising.\\nThis book is a comprehensive guide on all the facets of advertising. It begins with an introduction with a chapter called Overture. It then goes on to explain the core concepts of the field like how to get clients and how to come up with sellable advertising. It then provides details on the advertising job markets and how to get these jobs. It also contains an open letter to a client in search of an advertising agency and information on how to run and manage an advertising agency.\\nThe author imparts to the reader his experience in making TV commercials and advertising corporations in this book, as well as his belief that print advertising is in great need of a renaissance. It explains the 18 miracles of research and the manner in which to advertise foreign travel. It explains that direct mail is the secret weapon of an advertising executive, and contains the secrets of business-to-business advertising and the principles of marketing.\\nIn this book are the four fundamental principles of advertising mantra: maintaining professional discipline, performing intense research, being creatively brilliant, and producing results for clients. The book also explains the good causes that advertising can be used for, and how to use it for this. It teaches readers how to compete with Procter & Gamble, and in the last chapter, explains the author’s personal opinion on the flaws in advertising. Ogilvy On Advertising (Vintage) was published by RHUS in 1985. This 1st vintage books edition is available in paperback.\\nKey Features:\\nThe book contains 233 photos that illustrate the content and make it more entertaining.',\n",
              " \"During a hot-air balloon flight, the Thea Sisters discover that the turtles have vanished from Turtle Island! If that wasn't alarming enough, dangerous drums of a strange oil are spotted off the shore of the island-a disaster waiting to happen! The Thea Sisters are surprised that none other than Vissa De Vissen wants to help them save the island and the turtles! All seems lost until Nicky and Paulina spot a couple of turtles entering a mysterious underground cave! There are mysteries within mysteries, but the Thea Sisters are determined to solve them all and save the day.\",\n",
              " \"A Jeeves and Wooster novel\\n\\nGussie Fink-Nottle's knowledge of the common newt is unparalleled. Drop him in a pond of newts and his behaviour will be exemplary, but introduce him to a girl and watch him turn pink, yammer, and suddenly stampede for great open spaces. Even with Madeline Bassett, who feels that the stars are God's daisy chain, his tongue is tied in reef-knots. And his chum Tuppy Glossop isn't getting on much better with Madeline's delectable friend Angela.\\n\\nWith so many broken hearts lying about him, Bertie Wooster can't sit idly by. The happiness of a pal - two pals, in fact - is at stake. But somehow Bertie's best-laid plans land everyone in the soup, and so it's just as well that Jeeves is ever at hand to apply his bulging brains to the problems of young love. Along with The Code of the Woosters, Right Ho, Jeeves is considered by many his finest comic novel - and perhaps the finest in the English language.\",\n",
              " \"The No. 1 Sunday Times bestseller in both hardback and paperback about everyone's favourite dog.\",\n",
              " \"A nonfiction guide to the time periods featured in the Assassin's Creed games - the Crusades, Italian Renaissance, Colonial Americas, French Revolution, Imperial China and Victorian England. Illustrated with screenshots from the games as well as historical artifacts.\",\n",
              " '\"June, 1942: I hope I will be able to confide everything to you, as I have never been able to confide in anyone, and I hope you will be a great source of comfort and support.\" in Amsterdam, in the summer of 1942, The Nazis forced teenager Anne Frank and her family into hiding. For over two years, they, another family and a German dentist lived in a \\'secret annexe\\', fearing discovery. All that time, Anne kept a diary. An intimate record of tension and struggle, adolescence and confinement, anger and heartbreak, this is the definitive edition of the Diary of Anne Frank.',\n",
              " '*Winner of the Irish Popular Fiction Book of the Year*\\nA thoughtful, captivating and ultimately uplifting novel from this uniquely talented author\\nThe year that changed my life. For Jasmine, losing her job felt like losing everything.\\nThe year I found home. With a life built around her career and her beloved sister Heather, suddenly her world becomes the house and garden she has hardly seen and the neighbours she has yet to meet.\\nThe year I met you. But being fired is just the beginning for Jasmine. In the year that unfolds she learns more about herself than she could ever imagine – and more about other people than she ever dreamed. Sometimes friendship is found in the most unexpected of places.',\n",
              " \"See how SAP HANA has changed ABAP! Whether you are a newbie or an old-hat, you can learn to design simple and advanced SAP HANA applications with ABAP by using this comprehensive guide. Learn to enable code pushdown, use new Open SQL enhancements and CD5 views, and integrate native SAP HANA objects. Use detailed programming examples to develop database procedures and optimize your applications. You'll be programming for SAP HANA in no time.\",\n",
              " \"The beautiful, poignant and utterly compelling Number One bestseller from the internationally bestselling author of Me Before You, After You and Still Me\\nOne eternal optimist . . .\\nJess Thomas, with two jobs and two kids and never enough money, wears flip flops in the hope of spring. And when life knocks her down she does her best to bounce right back. But no one told her it's okay to ask for help.\\nPlus one lost stranger . . .\\nEd Nicholls is the good guy gone bad. He had it all, then one stupid mistake cost him everything. Now he'll do anything to make it right.\\nEquals a chance encounter . . .\\nEd doesn't want to save anyone and Jess doesn't want saving, but could Jess and Ed add up to something better together?\\n\\nPraise for The One Plus One:\\n\\n'A beautifully written love story I relished' Daily Mail\\n\\n'A heartbreaking, laugh-out-loud, roller coaster' Sunday Express\\n'An uplifting, charming, life-affirming tale that you won't want to put down' Heat\\n'Raw, funny, real and sad, this is storytelling at its best' Marie Claire\",\n",
              " \"For artists looking to grow beyond the studio, En Plein Air: Watercolor is the definitive resource for experiencing the rewards and invigoration of watercolor painting on location.\\n\\nEn Plein Air: Watercolor introduces beginning artists and fine-art enthusiasts to the core concepts of painting on location. This guide will have you fully prepared for a day of painting outdoors. Inside, you'll find a comprehensive list of what to pack to maximize your time, learn how to choose a subject and interpret it for a composition, discover how to deal with the challenges of shadows and shifting natural light, and hone your watercolor techniques.\\n\\nThe collection of step-by-step projects will help you gain new artistic skills as you create unique, dynamic artwork outside the studio. Perfect for artists looking to break outside the norm, En Plein Air: Watercolor is your complete guide to experiencing the unique rewards of painting on location with the invigorating medium of watercolor.\",\n",
              " \"Francis Urquhart's eventful career as Prime Minister comes to a spectacular end in the final volume in the Francis Urquhart trilogy - now with brand new material. He schemed his way to power in House of Cards and had a memorable battle of wills with the new king in 'To Play the King'. Now Francis Urquhart is about to take his place in the record books as the longest-serving Prime Minister this century. Yet it seems the public is tiring of him at last and the movement to force him from power is growing. But Urquhart is not yet ready to be driven from office. If the public demand new blood, that is precisely what he will give them. Francis Urquhart goes out in a blaze of glory in this final volume in the irresistible story of the most memorable politician of the decade.\",\n",
              " 'This first book from Chicago author Chris Ware is a pleasantly-decorated view at a lonely and emotionally-impaired \"everyman\" (Jimmy Corrigan: The Smartest Kid on Earth), who is provided, at age 36, the opportunity to meet his father for the first time. An improvisatory romance which gingerly deports itself between 1890\\'s Chicago and 1980\\'s small town Michigan, the reader is helped along by thousands of colored illustrations and diagrams, which, when read rapidly in sequence, provide a convincing illusion of life and movement. The bulk of the work is supported by fold-out instructions, an index, paper cut-outs, and a brief apology, all of which concrete to form a rich portrait of a man stunted by a paralyzing fear of being disliked.',\n",
              " \"Discover fascinating facts about Marvel comics' friendly neighborhood hero, Spider-Man, and read in-depth profiles of over 200 Marvel super heroes and villains from the amazing Spider-Man comics.\\nDK's Spider-Man Character Encyclopedia is created in full collaboration with Marvel Comics and packed with original comic book art. This scintillating guide to Spider-Man's world also offers interesting info written by Spider-Man expert, Daniel Wallace, from how each character interacts with Spider-Man to the powers each character possesses. Listed in alphabetical order, each character's profile is crammed with facts, stats, annotations, and exciting original artwork. Plus, get the inside scoop on the Spidey suit and the Webhead's alter-ego, Peter Parker.\\nAre your spider-senses tingling!? Then be sure to catch the Spider-Man Character Encyclopedia in your web.\\n© 2014 MARVEL\",\n",
              " 'Even as Kapil Dev lifts the 1983 World Cup, thousands of miles away, in the dense jungles of Mizoram, a secret mission stands compromised. At a terrible cost. Forced to engage in fierce combat with a group of insurgents, Captain Akhil Mehra loses his right hand. Leaving the army, he arrives at Carlington tea estate, owned by the dignified and wealthy Rai Bahadur, hoping to start a new chapter. His troubles, though, are just beginning. Here he meets Norden, the Rai Bahadur’s reticent and faithful assistant, the beautiful Indrani and wheelchair-bound Ipsita, the Rai Bahadur’s free-spirited daughters, whose cloistered lives are turned-upside down upon his arrival.',\n",
              " 'When it comes to recruiting, motivating and creating great teams, Patty McCord says most companies have it all wrong. McCord helped create the unique and high-performing culture at Netflix, where she was chief talent officer. In her new book, Powerful: Building a Culture of Freedom and Responsibility, she shares what she learned there and elsewhere in Silicon Valley.\\nMcCord advocates practicing radical honesty in the workplace, saying good-bye to employees who don’t fit the company’s emerging needs and motivating with challenging work, not promises, perks and bonus plans. McCord argues that the old standbys of corporate HR annual performance reviews, retention plans, employee empowerment and engagement programs often end up being a colossal waste of time and resources. Her road-tested advice, offered with humour and irreverence, provides readers a different path for creating a culture of high performance and profitability.\\nPowerful will change how you think about work and the way a business should be run.',\n",
              " \"The tiny village of Moon fleet nestles on the English coast and every one of its inhabitants lives off the sea in one way or another. When local young man John Trenchard accidentally stumbles upon treasure stashed in the local crypt, he unknowingly enters the murky world of the smuggling trade and the local secret of Colonel John Mohune's treasure. Trenchard is soon forced to flee England with a price on his head, leaving behind his beloved Grace and the life he hoped for. But the adventures, trials and tribulations that befall him on his personal journey back to Moon fleet and ultimately redemption are written with such intensity and hope, as well as love for the history and landscape of Dorset, that the story never loses pace or power on its epic journey. 'Moon fleet' is a classic adventure story to be read again and again.\",\n",
              " 'In The 1000 Dot-to-Dot Book: Cityscapes, Thomas Pavitte takes his incredible 1000 Dot-to-Dot series to the urban jungle.\\nTackle the iconic skylines and landmarks of Paris, New York, London, Venice and many more besides, and experience the cities of the world in a whole new way as each one comes to life beneath your pen.',\n",
              " \"Basic Approach Renowned for its comprehensive coverage, exceptional illustrations and clear instructions, Patternmaking for Fashion Design offers detailed yet easy-to-understand explanations of the essence of patternmaking. Hinging on a recurring theme that all designs are based on one or more of the three major patternmaking and design principles-dart manipulation, added fullness and contouring-it provides students with all the relevant information necessary to create design patterns with accuracy regardless of their complexity. ome innovative, new information in this book include: Updated with modern, cutting-edge sketches and designs. Ribbing added to the knit section of Chapter 27. More materials on children's wear and swim wear. Knock offs. Fitting corrections Advanced design projects A practical introduction to patternmaking Complete coverage of the three steps of design patterns: dart manipulation, added fullness and contouring. Chapter projects\",\n",
              " 'Imagine a reality where the world’s most powerful super-being does not grow up in Smallville, Kansas—or even America, for that matter…\\n\\nSuperman: Red Son is a vivid tale of Cold War paranoia, that reveals how the ship carrying the infant who would later be known as Superman lands in the midst of the 1950s Soviet Union.  Raised on a collective, the infant grows up and becomes a symbol to the Soviet people, and the world changes drastically from what we know - bringing Superman into conflict with Batman, Lex Luthor and many others.\\n\\nThe acclaimed story by writer Mark Millar and artists Dave Johnson & Kilian Plunkett is collected here, featuring an extensive sketch section by Johnson, Plunkett and Alex Ross. Collects Superman: Red Son #1-3.',\n",
              " \"'I am Dying to Live for You, ' is a very touching story of a boy who experiences the pangs and passion of love at first sight for a classmate and how he spends the rest of his life loving her. Every deep emotion and feeling of ecstasy a lover feels as a high school student is poignantly engraved in the pages of the book. Readers can feel the magical spell a girl may cast on a boy absolutely besotted with her charm and grace.\\n\\nThe book gives you every atom of detail about teenage love at school, a bit of conceit and deception, fun-filled days spent together, the painful separation and the eager anticipation of the reunion, and finally how much lovers mean to one another. All in all, it's an unforgettable story of undying love.\\n\\nEvery reader will surely find some nostalgic connect with this saga of young first love.\",\n",
              " 'A fog of mystery surrounds the Rashtriya Swayamsevak Sangh—or RSS—the largest cadre-based organisation in the world. Veteran journalist and author of the bestseller Narendra Modi: The Man, The Times, Nilanjan Mukhopadhyay lays bare its fascinating, unique and perhaps startling world. He also chronicles the personal and political journeys of the most important men (and a woman) of the Hindu Right-wing, digging up little-known but revealing facts about them.\\nKESHAV BALIRAM HEDGEWAR: The founder of the RSS, and its first sarsanghchalak, was called ‘Cocaine’ as a young revolutionary, and transported subversive literature and arms for a group back home in Nagpur.\\nVINAYAK DAMODAR SAVARKAR: This leading light of the Hindu Right had once invited the vegetarian Mahatma Gandhi to dinner and told him that unless one consumed animal protein, one would not be able to challenge the might of the British.\\nMADHAV SADASHIV GOLWALKAR aka ‘GURUJI’: The iconic ‘hermit-ideologue’, whose appointment as sarsanghchalak was challenged by many in the RSS, had once warned a protesting colleague, ‘I will throw him out of (the) RSS like a stone in rice...’\\nSYAMA PRASAD MOOKERJEE: A brilliant academic-statesman who became part of Nehru’s Cabinet, Mookerjee had several differences with the prime minister. He once asked Nehru: ‘Are Kashmiris Indians first and Kashmiris next, or are they Kashmiris first and Indian next, or are they Kashmiris first, second and third, and not Indians at all?’\\nBALASAHEB DEORAS: This towering pracharak had a strong dislike for religious rituals, and referred to himself as a ‘Communist’ within the RSS—‘it is highly debatable if he believed in God, or if in any way needed Him.’\\nDEENDAYAL UPADHYAYA: The man who propounded the ‘philosophy’ of Integral Humanism was opposed to the partition of India and recommended that, ‘If we want unity, we must adopt the yardstick of Indian nationalism, which is Hindu nationalism, and Indian culture, which is Hindu culture.’\\nThese and other leaders, including Vijaya Raje Scindia, Atal Bihari Vajpayee, Lal Krishna Advani, Ashok Singhal and Bal Thackeray, are all reckoned with in The RSS: Icons of the Indian Right. Through individual stories of the organisation’s tallest leaders, a bigger picture emerges: in spite of a three-time ban on the RSS in a multicultural and secular India—and despite the RSS’s insistence that it has no truck with electoral politics—the group is, and will be, the hand that rocks the BJP’s cradle.',\n",
              " \"‘It’s been a long time since I picked up a book so impossible to put down. ifurious hours/made me forget dinner, ignore incoming calls and stay up reading into the small hours. It’s a work of literary and legal detection as gripping as a thriller. but it’s also a meditation on motive and mystery, the curious workings of history, hope and ambition, justice and the darkest matters of life and death. Casey cep’s investigation into an infamous southern murder trial and Harper Lee’s quest to write about it is a beautiful, sobering and sometimes chilling triumph.’ Helen MacDonald, author of h is for Hawk ‘A triumph on every level./one of the losses to literature is that Harper Lee never found a way to tell a Gothic true-crime story she’d spent years researching. Casey cep has excavated this mesmerising story and tells it with grace and insight and a fierce fidelity to the truth.’ David Grann, author of killers of the flower moon The stunning story of an Alabama serial killer and the true-crime book that Harper Lee worked on obsessively in the years after to kill a Mockingbird reverend Willie Maxwell was a rural preacher accused of murdering five of his family members for Insurance money in the 1970s. With the help of a savvy lawyer, he escaped justice for years until a relative shot him dead at the funeral of his last victim. Despite hundreds of witnesses, Maxwell’s murderer was acquitted – thanks to the same attorney who had previously defended the Reverend. As Alabama is consumed by these gripping events, it’s not long until news of the case reaches Alabama's – and American – most famous writer. Intrigued by the story, Harper Lee makes a journey back to her home state to witness the reverend’s killer face trial. Harper had the idea of writing her own in cold blood, The true-crime classic she had helped her friend Truman Capote research. Lee spent a year in town reporting on the Maxwell case and many more years trying to finish the book she called the Reverend. now Casey cep brings this story to life, from the shocking murders to the courtroom drama to the racial politics of the deep South. At the same time, she offers a deeply moving portrait of one of the country most beloved writers and her struggle with fame, success and the mystery of artistic creativity. This is the story Harper Lee wanted to write. This is the story of why she couldn't.\",\n",
              " \"A NEW YORK TIMES BESTSELLER\\nSelected by Emma Watson as an Our Shared Shelf Book Club Pick\\n'I loved it' Kate Tempest\\n'Astounding' Roxane Gay\\n'A sledgehammer' New York Times\\nHeart Berries is a powerful, poetic memoir of a woman's coming of age on an Indian Reservation in the Pacific Northwest. Having survived a profoundly dysfunctional upbringing only to find herself hospitalised and facing a dual diagnosis of post-traumatic stress disorder and bipolar II disorder, Terese Marie Mailhot is given a notebook and begins to write her way out of trauma.\\nThe triumphant result is Heart Berries, a memorial for Mailhot's mother, a social worker and activist who had a thing for prisoners; a story of reconciliation with her father - an abusive drunk and a brilliant artist - who was murdered under mysterious circumstances; and an elegy on how difficult it is to love someone while dragging the long shadows of shame.\\nMemory isn't exact, but melded to imagination. In Heart Berries, Mailhot discovers her own true voice, seizes control of her story, and, in so doing, re-establishes her connection to her family, to her people, and to her place in the world.\",\n",
              " \"Ora, a middle-aged Israeli mother, is about to celebrate her son Ofer's release from army service when he returns to the front for a major offensive. Instead of waiting at home for the 'notifiers' who could arrive at any moment to tell her of her son's fate, she sets off for a hike in Galilee, leaving no forwarding address. If a mother is not there to receive the news, a son cannot die, can he?\\n\\nRecently estranged from her husband, Ora drags along an unlikely companion: their former best friend and her former lover Avram, the man who in fact turns out to be her son's biological father. As they sleep out in the hills, ford rivers and cross valleys, Ora recounts, step by step and word by word, the story of her son's birth, life and possible death, in one mother's magical, passionate and heartbreaking attempt to keep her son safe from harm.\",\n",
              " 'New Book by Best-Selling Author Jamie Chan. Learn Java Programming Fast with a unique Hands-On Project. Book 4 of the Learn Coding Fast Series.\\n\\nHave you always wanted to learn computer programming but are afraid it\\'ll be too difficult for you? Or perhaps you know other programming languages but are interested in learning the Java language fast?\\nThis book is for you. You no longer have to waste your time and money trying to learn Java from boring books that are 600 pages long, expensive online courses or complicated Java tutorials that just leave you more confused and frustrated.\\nWhat this book offers...\\n\\nJava for Beginners\\nComplex concepts are broken down into simple steps to ensure that you can easily master the Java language even if you have never coded before.\\nCarefully Chosen Java Examples\\nExamples are carefully chosen to illustrate all concepts. In addition, the output for all examples are provided immediately so you do not have to wait till you have access to your computer to test the examples.\\nCareful selection of topics (Covers Java 8)\\nTopics are carefully selected to give you a broad exposure to Java, while not overwhelming you with information overload. These topics include object-oriented programming concepts, error handling techniques, file handling techniques and more. In addition, new features in Java (such as lambda expressions and default methods etc) are also covered so that you are always up to date with the latest advancement in the Java language.\\nLearn The Java Programming Language Fast\\nConcepts are presented in a \"to-the-point\" style to cater to the busy individual. You no longer have to endure boring and lengthy Java textbooks that simply puts you to sleep. With this book, you can learn Java fast and start coding immediately.\\nHow is this book different...\\n\\nThe best way to learn Java is by doing. This book includes a unique project at the end of the book that requires the application of all the concepts taught previously. Working through the project will not only give you an immense sense of achievement, it\\x92ll also help you retain the knowledge and master the language.\\nAre you ready to dip your toes into the exciting world of Java coding? This book is for you. Click the \"Add to Cart\" button and download it now.\\nWhat you\\'ll learn:\\n\\nIntroduction to Java - What is Java?- What software do you need to code Java programs?- How to install and run JDK and Netbeans?\\nData types and Operators - What are the eight primitive types in Java?- What are arrays and lists?- How to format Java strings- What is a primitive type vs reference type?- What are the common Java operators?\\nObject Oriented Programming - What is object oriented programming?- How to write your own classes- What are fields, methods and constructors?- What is encapsulation, inheritance and polymorphism?- What is an abstract class and interface?\\nControlling the Flow of a Program - What are condition statements?- How to use control flow statements in Java- How to handle errors and exceptions- How to throw your own exception\\nand Others... - How to accept user inputs and display outputs- What is a generic?- What are lambda expressions and functional interface?- How to work with external files...and so much more....\\n\\nFinally, you\\'ll be guided through a hands-on project that requires the application of all the topics covered.\\nClick the BUY button at the top of this page now to start learning Java. Learn it fast and learn it well.',\n",
              " \"Saffron terrorism.\\nIs it a fact? Or, is this a myth? After all, do we know enough?\\nThe shocking blasts of Malegaon and Samjhauta were projected as 'saffron terrorism'. A new theory, terrorist attacks were tainted as such till a few years later, Kasab's confession offered solid proof of Pakistan's role in the 26/11 attacks. Though the police had concluded a Pakistani hand for the earlier blasts, it was saffron terrorism which prevented the perpetrators of these attacks from being brought to justice.\\nAs a theory, saffron terrorism is not just hurting Hindus sentiments but is also an obstacle to fight real terrorism sponsored by Pakistan and Islamic states. The term was coined by the erstwhile UPA government to garner minority votes and manipulate the vote bank. After all, why were the Malegaon-accused SIMI activists let off? Why did certain politicians declare not to oppose their bail? What was truly behind Aseemanand's confession? The reliability of these confessions was questionable given the police brutality that the National Investigative Agency exposed.\\nJournalist Praveen Tiwari explores saffron terrorism and reveals through exclusive interviews of senior National Investigative Agency officials, undercover agents and politicians how vote bank politics can compromise ethics and national security. Should the real masterminds behind the blasts be allowed to go scot-free? Should the manipulators of the Samjhauta Express bombings not be held accountable? Should we not investigate those who had exonerated Pakistan of its guilt? An extensive research on communal politics, the book offers indisputable evidence of the 'saffron terrorism' theory as the Great Indian Conspiracy.\",\n",
              " 'WINNER OF THE PULITZER PRIZE\\n \\nThe beloved, award-winning The Amazing Adventures of Kavalier & Clay, a Michael Chabon masterwork, is the American epic of two boy geniuses named Joe Kavalier and Sammy Clay. Now with special bonus material by Michael Chabon.\\n \\nA “towering, swash-buckling thrill of a book” (Newsweek), hailed as Chabon’s “magnum opus” (The New York Review of Books), The Amazing Adventures of Kavalier & Clay is a triumph of originality, imagination, and storytelling, an exuberant, irresistible novel that begins in New York City in 1939. A young escape artist and budding magician named Joe Kavalier arrives on the doorstep of his cousin, Sammy Clay. While the long shadow of Hitler falls across Europe, America is happily in thrall to the Golden Age of comic books, and in a distant corner of Brooklyn, Sammy is looking for a way to cash in on the craze. He finds the ideal partner in the aloof, artistically gifted Joe, and together they embark on an adventure that takes them deep into the heart of Manhattan, and the heart of old-fashioned American ambition. From the shared fears, dreams, and desires of two teenage boys, they spin comic book tales of the heroic, fascist-fighting Escapist and the beautiful, mysterious Luna Moth, otherworldly mistress of the night. Climbing from the streets of Brooklyn to the top of the Empire State Building, Joe and Sammy carve out lives, and careers, as vivid as cyan and magenta ink. Spanning continents and eras, this superb book by one of America’s finest writers remains one of the defining novels of our modern American age.\\n \\nNEW YORK TIMES BESTSELLER\\n \\nFinalist for the PEN/Faulkner Award, National Book Critics Circle Award, and Los Angeles Times Book Prize\\n \\nWinner of the Bay Area Book Reviewers Award and the New York Society Library Book Award\\n \\nNamed one of the 10 Best Books of the Decade by Entertainment Weekly',\n",
              " '\"Quaint, meditative and sometimes dreamy, blankets will take you straight back to your first kiss.\" --The Guardian\\n\\nBlankets is the story of a young man coming of age and finding the confidence to express his creative voice. Craig Thompson\\'s poignant graphic memoir plays out against the backdrop of a Midwestern winterscape: finely-hewn linework draws together a portrait of small town life, a rigorously fundamentalist Christian childhood, and a lonely, emotionally mixed-up adolescence.\\nUnder an engulfing blanket of snow, Craig and Raina fall in love at winter church camp, revealing to one another their struggles with faith and their dreams of escape. Over time though, their personal demons resurface and their relationship falls apart. It\\'s a universal story, and Thompson\\'s vibrant brushstrokes and unique page designs make the familiar heartbreaking all over again.\\nThis groundbreaking graphic novel, winner of two Eisner and three Harvey Awards, is an eloquent portrait of adolescent yearning; first love (and first heartache); faith in crisis; and the process of moving beyond all of that. Beautifully rendered in pen and ink, Thompson has created a love story that lasts.',\n",
              " \"About The Book\\nMakers of Modern India is a detailed source for information about the country's political traditions. The republic of India had a very tumultuous beginning and the author shows you how 19 political activists were instrumental in the evolution of this country. The author goes beyond a description of the people by including extracts of the speeches they have written. Each phase of the freedom movement and the following years of independent India are shown through the written works produced by these 19 individuals.\\nIn Makers of Modern India you will see caste, religion, colonialism, the economy language, gender, nationalism, democracy and secularism in a historical context. The book is a treat for those who are curious about the formation of the multifarious collection of people, ideas and religions in India.\\nThe author shows you how the lack of unison in the opinions of the makers of India complemented each other and resulted in the finished product called India. Makers of Modern India was published on 7 November 2012 and is available in paperback.\\nKey Features\\nThis book gives you an informative look at the 19 individuals who played a role in the formation of the India you live in today.\\nSome of the issues discussed in the 568-page book like, democracy and religion, are relevant even in the India of the 21st century.\",\n",
              " 'Ahi is an aspiring publisher and wishes to make it big someday. When her favourite author’s autobiography lands on her table – which has confessions of his heinous crimes, illegal businesses and few eminent others as his partners in crime – she doesn’t know if it’s real or someone’s trap. It could get her a big breakthrough, but little does she know that it would turn her world upside down completely.\\nHer morbid curiosity pulls her into the depth of a conspiracy. She finds herself in the centre of various mishaps and murders, as if someone wants to lead the way. Driven by her childhood friend Samim’s encouragement and watched over by the ever so charming ACP Rathore, she has to jeopardize her life to find the brutal truth of her past.\\nTouching, thrilling and deeply mysterious, Sin is the New Love is the journey of a girl who stumbles upon the truth about her origin while chasing her dream.',\n",
              " \"Michael Palin, the No. 1 bestselling author, explores an exotic country now a global superpower.\\nBrazil is one of the four new global super powers with its vast natural resources and burgeoning industries. Half a continent in size and a potent mix of races, religions and cultures, of unexplored wildernesses and bustling modern cities, it is also one of the few countries Michael Palin has never fully travelled. With the next Olympics to be held in Rio in 2016 and the World Cup in Brazil in 2014, international attention will be on Brazil as never before.\\nMichael Palin's timely book and series take a closer look at a remarkable new force on the world scene. From the Venezuelan border and the forests of the Lost World where he encounters the Yanomami tribe and their ongoing territorial war with the gold miners, Michael Palin explores this vast and disparate nation in his inimitable way. He journeys into the heart of the Amazon rainforest. He travels down the North-East coast to meet the descendents of African slaves with their vibrant culture of rituals, festivals and music. He visits the shanty towns of Rio and the beaches of Copacabana and Ipanema. He goes to Sao Paolo where the rich commute by helicopter. He travels South to meet German and Japanese communities, meets supermodels in the making and wealthy gauchos in the Pantanal before ending his journey at the spectacular Iguaçu Falls.\",\n",
              " 'To earn the OCA Java SE Programmer I Certification, you have to know your Java inside out, and to pass the exam you need to understand the test itself. This book cracks open the questions, exercises and expectations you’ll face on the OCA exam so you’ll be ready and confident on the test day. OCA Java SE 8 Programmer I Certification Guide prepares Java developers for the 1Z0-808 with through coverage of Java topics typically found on the exam. Each chapter starts with a list of exam objectives mapped to section numbers, followed by sample questions and exercises that reinforces key concepts.',\n",
              " 'Learn to read with the classic Amelia Bedelia!\\nEver since Amelia Bedelia made her debut in 1963, young readers have been laughing out loud at the antics of this literal-minded but charming housekeeper.\\nAmelia Bedelia has never been camping in the great outdoors before. She\\'s trying her best to do exactly as she\\'s told, but pitching a tent is not the same as throwing it into the bushes, and catching a fish with your bare hands isn\\'t easy. As usual, the mixed-up housekeeper makes this camping trip one hugely entertaining adventure. This Level 2 I Can Read is perfect for kids who read on their own but still need a little help.\\n\"No child can resist Amelia [Bedelia] and her literal trips through the minefield of the English language—and no adult can fail to notice that she\\'s usually right when she\\'s wrong.\"—The New York Times Book Review',\n",
              " 'From the gym to the trails, join Kris Gethin on his journey to complete an IRONMAN.\\nAs a widely followed personal trainer and professional bodybuilder, Kris undoubtedly knows his way around the weight room. But as more of his peers became involved in endurance races, Kris became intrigued by the possibility of a new mental and physical challenge\\x97did he have what it took to grind out an IRONMAN, without sacrificing his muscle mass? Thus began Kris\\x92s mission to prove that all athletes will benefit from a combination of hybrid training, whether that be endurance athletes adding more strength training to their routine, or weightlifters incorporating cardio. He developed his 6-month training program as he went, integrating the insights of experts from multiple fields as well as the results from his own experiences.\\nKris put in the work to become a hybrid athlete\\x97from fine-tuning his nutrition, preventing injury and refining technique, preparing his body to take the immense beating an IRONMAN presents, and developing the mental techniques necessary to stay focused and on course. Do you have what it takes to become a MAN of IRON?',\n",
              " 'Published in the summer of 1949, George Orwell’s nineteen Eighty-Four is one of the most definitive texts of modern literature. Set in Oceania, one of the three inter-continental superstate that divided the world among themselves after a global war, Orwell’s masterful critique of the political structures of the time, works itself out through the story of Winston Smith, a man caught in the webs of a dystopian future, and his clandestine love affair with Julia, a young woman he meets during the course of his work for the government. As much as it is an entertaining read, nineteen Eighty-Four is also a brilliant, and more importantly, a timeless satirical attack on the social and political structures of the world.',\n",
              " \"Game Changer is the riveting memoir of Shahid Afridi, one of modern cricket's most controversial and accomplished practitioners. In 1996, as a teenager, Afridi shot to fame after hammering the fastest ODI century at the time. One of the world's greatest all-rounders, today, he holds the distinction of having hit the most number of sixes in the history of ODI cricket scooping the most wickets in T20s and winning the most player-of-the match awards in the same format. From his humble beginnings in the mountains of Pakistan's unruly northwest to the mean streets of Karachi and the county parks of southern England, Afridi tells his life story just the way he bats - instinctively, candidly and with no holds barred. In a career as unpredictable as his leg-break googlies and 'boom-boom' power hitting, Afridi has been many things - the lost kid focused on pulling his parents out of poverty, the desperate captain trying not to snitch on his corrupt teammates, the gallant Pashtun centurion staring down a hostile Indian crowd, and the bad boy at the centre of a ball-tampering scandal. In Game Changer, he sets the record straight once and for all. A must-read not only for his legion of fans across the world but also for those interested in cricket and Pakistan's future.\",\n",
              " 'The international and Sunday Times bestseller. Alleviate anxiety and soothe all stress with The Little Book of Calm Colouring from David Sinden and Victoria Kay, the perfectly-formed sought-after antidote to a busy life. Beautifully hand-illustrated and thoughtfully designed to be the perfect size for portability, you can now take colouring art therapy with you wherever you go. With beautiful anti-stress designs on quality paper, this gorgeous colouring book will help your creativity flourish. Take a short relaxing breather from your day to colour the calming images and feel inspired by the poignant quotations that accompany each elegant artwork.',\n",
              " '\"Complete with crackling fast dialogue, an edgy ambivalent plot, and the capacity to make his readers turn the page, this feels like Child\\'s breakthrough book into the mega-sellers. He is that good.\" (Daily Mail)\\n\\nJack Reacher, adrift in the hellish heat of a Texas summer.\\n\\nLooking for a lift through the vast empty landscape. A woman stops, and offers a ride. She is young, rich and beautiful.\\n\\nBut her husband\\'s in jail. When he comes out, he\\'s going to kill her.\\n\\nHer family\\'s hostile, she can\\'t trust the cops, and the lawyers won\\'t help. She is entangled in a web of lies and prejudice, hatred and murder.\\n\\nJack Reacher never could resist a lady in distress.\\n\\nAlthough the Jack Reacher novels can be read in any order, Echo Burning is 5th in the series.',\n",
              " 'The Other Side Of Midnight features a tale that explores deceit and manipulation, in the context of an obsession by a woman of power. Noelle Page and Catherine Alexander are the two main characters in this book, and the conflict between them is the main premise of the novel. A thorough character sketch is presented for both these characters in terms of their upbringing, attitudes, ideologies, etc.\\nNoelle Page is a born as a result of an extra-marital affair, and ends up blossoming into a beautiful woman. She has an air of superiority about her, and nourishes great ambitions. By the time she mature into a woman, his father sells her on the pretext of getting her a job as a model.\\nNoelle runs away to Paris where she meets the pilot, Larry Douglas, a person she falls in love with and who she remains obsessed with for the rest of her life. After Larry forgets about her love, Noelle swears vengeance. Catherine, on the other hand, is an insecure woman who comes from a family with broken dreams. She never had the courage to explore her sexuality in her youth, and so, became a social outcast with an inferiority complex.\\nCatherine eventually meets Larry and ends up marrying him. She becomes a bone of contention for Noelle, who having risen high up in social ladder, plans to have Larry all to herself. The themes of obsession and jealousy are constantly highlighted throughout the book. The reissue edition of The Other Side Of Midnight was published in 2005 by HarperCollins. It is available in the form of a paperback.',\n",
              " \"The fiercely honest, fearless, darkly funny autobiography of global tennis star Maria Sharapova\\n\\nIn the middle of the night, a father and his daughter step off a Greyhound bus in Florida and head straight to the Nick Bollettieri Tennis Academy. They ring the bell, though no one is expecting them and they don't speak English. They have arrived from Russia with just seven hundred dollars and the conviction that this six-year-old girl will be the world's next great tennis star. They are right.\\n\\nThis is Maria Sharapova's gripping and fearless autobiography, telling her story from her roots in the small Siberian town her parents fled to after the Chernobyl disaster, through her arrival in the US with nothing and her phenomenal rise to success - winning Wimbledon aged just seventeen - to the disasters that threatened her career and her fight back. Here the five-time Grand Slam winner gives us candid insights into her relationship with her father, who gave up his job and life in Russia to dedicate himself to his daughter; the truth behind her famous rivalry with Serena Williams; the injuries and suspension controversy that threatened to end it all; and her recent battle to get back on court.\\n\\nTold with the same combative, no-holds-barred attitude as her game, it's a story of crazy luck, mistakes, rivalries, sacrifice, survival and, above all, the constant, unwavering determination to win.\",\n",
              " \"Stress is a lot like love hard to define, but you know it when you feel it.\\nIn this classic work, 'How to Stop Worrying and Start Living', Carnegie offers a set of practical formulas that you can put to work today. It is a book packed with lessons that will last a lifetime and make that lifetime happier\\nThis book will explore the nature of stress and how it infiltrates every level of your life, including the physical, emotional, cognitive, relational and even spiritual. Through techniques that get to the heart of your unique stress response and an exploration of how stress can affect your relationships, you'll discover how to control stress instead of letting it control you. This book shows you how.\\nUsing the power of habit and several techniques for smoothing out the stressful wrinkles in our day-to-day lives, we'll move towards a real-world solution to living with less stress, more confidence and a deep spiritual resilience that will insulate you from the inevitable pressures of life.\\nThe target of the book is to help readers understand what suits their respective lives best to help them reframe it in a constructive manner, subtracting worry from it and how they could focus on living each day with joy and contentment.\",\n",
              " \"Experience the joie de vivre with this revolutionary non-diet book that is changing the way women eat and live everywhere\\n\\nHow do French women do it? This is the book that unlocks the simple secrets of 'the French paradox' - how to enjoy food and stay slim and healthy. Classy, chic and expertly well-written, this is the book that we have all been waiting for. It's the ultimate non-diet book; instead, showing how to eat with balance, control and above all pleasure. Eat, like a French woman.\",\n",
              " 'A stunning thriller for the twenty-first century from the world’s number one storyteller.\\nThe Matarese dynasty, first encountered in The Matarese Circle, is back in all its questionable glory and evil. And the one man with enough knowledge to stop it, CIA case officer Cameron Pryce, may not have enough time. His challenge: to follow the trail of blood money and stone-cold killers to the heart of this deadly conspiracy. To do so, he must enlist the help of legendary retired agent, Brandon Scofield – the only man ever to penetrate the Matarese and survive – entice him out of his Caribbean hideaway and back to the Matarese circle of death.\\nAs the noose tightens and the panic spreads, Scofield discovers that the Matarese web has broken new ground – deep inside the CIA.',\n",
              " 'Offers 85 treks and 50 trekking options through the greatest mountains range on earth. This book is geared for the beginner, but also offers trekking options for the more experienced and adventurous.',\n",
              " 'A comprehensive and practical guide to drawing cartoons successfully with expert projects and exercises.',\n",
              " 'Batman vs Superman – The Greatest Battles is a compilation of five stories that feature deadly battles between the two of the biggest members of the Justice League. Superman is the Man of Steel, he is fast and invincible. Batman is The Dark Knight, the biggest crime fighter who can lay deadly traps for his enemies. As long as they are fighting against the criminal side by side they are unbeatable. But when the villains cause a rift between them, they turn into enemies and use all the tricks in their power to eliminate each other.\\nFans get a treat when the two superheros, sometimes called Gods, battle with each other. Here is when the reader finds himself in the difficult situation of rooting for one of the two equally beloved Superstars.\\nThis is a compilation of comics to give a taste of the greatness of the two big DC Stars. It is a great read for new and old fans who want to see the best of the Superman vs Batman series. This comic has been issued as a foretaste of the superheros just ahead of the impending release of the 2016 movie. The books have superior artwork and a glossy finish.\\nAbout the author:\\nGeoff Johns, Frank Miller, Jim Lee, Jeff Loeb, Brian Azzarello, Joe and Jack Kelly, Geoff Jones, Scott Snyder, Greg Capullo.\\nThese are the authors of the five stories in the comic book compilation featuring Batman and Superman. They are all acclaimed writers for DC Comics with formidable credits to their name. They have been involved in making the DC Comic stars into intriguing and interesting characters that the fans cannot get enough of.',\n",
              " '‘A huge, immersive, violent, compassionate read’ Ian Rankin\\nThe explosive, highly anticipated conclusion to the epic Cartel trilogy from the New York Times bestselling author of The Force.\\n‘Focused, angry, suspenseful, occasionally hilarious, always hugely entertaining.’ Stephen King\\nThe war has come home.\\nFor more than forty years, Art Keller has been on the front lines of America’s longest conflict: the war on drugs. His obsession with defeating the godfather of the Sinaloa Cartel – Adán Barrera – has cost him the people he loves, even taken a piece of his soul.\\n \\nNow Keller is elevated to the highest ranks of the DEA, only to find that in destroying one monster he has created thirty more that are wreaking chaos in his beloved Mexico. And not just there.\\n \\nFighting to end the heroin epidemic scourging America, Keller finds himself surrounded by an incoming administration that’s in bed with the very drug traffickers that Keller is trying to bring down.\\n \\nFrom the slums of Guatemala to the marbled corridors of Washington, D.C., Winslow follows a new generation of narcos, cops, addicts, politicians, and mere children fleeing the violence for the chance of a life in a new country.\\n \\nA shattering tale of vengeance, corruption and justice, The Border is an unflinching portrait of modern America, a story of – and for – our time.',\n",
              " 'Elon Musk, the South African--born entrepreneur who made his first fortune with Internet companies such as PayPal, has risen to global prominence as the visionary CEO of both Tesla Motors and SpaceX, two companies with self-proclaimed missions to improve life as we know it and better secure the future of humanity. For the first time, the most insightful, thought-provoking and revealing quotes from this entrepreneurial engineer have been compiled into a single book. Rocket Man: Elon Musk In His Own Words is a comprehensive guide to the inner workings of the man dubbed \"the real Tony Stark.\" Hundreds of his best quotes, comprising thoughts on business, clean energy, innovation, engineering, technology, space, electric vehicles, entrepreneurship, life lessons and more, provide an intimate and direct look into Silicon Valley\\'s most ambitious industrialist. Even with no prior experience in the complex, ultra-high-barrier-of-entry automotive and space industries, Musk has excelled. Tesla, the first successful American car startup in more than 90 years, received more than 325,000 reservations for its economical Model 3 in a single week--advancing the company\\'s cause to \"accelerate the advent of sustainable transport\" via affordable, reliable electric vehicles. SpaceX, the first private company to launch, orbit and recover a rocket and dock at the International Space Station, has drastically reduced the cost of launching and manufacturing reusable spacecraft, which the company sees as the first step toward its \"ultimate goal\" of making life multiplanetary. In the words of Richard Branson, \"Whatever skeptics have said can\\'t be done, Elon has gone out and made real.\" How could a young man who at one time seemed like \"just\" another Internet entrepreneur have gone on to build two highly disruptive companies and innovate technologies related to everything from electric batteries to rocket manufacturing? There\\'s no better way to learn than through his own words. This book curates Musk quotes from interviews, public appearances, online postings, company blogs, press releases and more. What emerges is a \\'word portrait\\' of the man whose companies\\' swift rise to the top will undoubtedly keep their status-quo competitors scrambling to keep up.',\n",
              " \"Now a major BBC film, directed by Oscar-winner Christopher Hampton (Atonement and Dangerous Liaisons), starring Vanessa Redgrave and Olivia Colman.\\nAngelfield House stands abandoned and forgotten. It was once home to the March family - fascinating, manipulative Isabelle, brutal, dangerous Charlie, and the wild, untamed twins, Emmeline and Adeline. But Angelfield House hides a chilling secret which strikes at the very heart of each of them, tearing their lives apart...\\nNow Margaret Lea is investigating Angelfield's past - and the mystery of the March family starts to unravel. What has Angelfield been hiding? What is its connection with the enigmatic writer Vida Winter? And what is the secret that strikes at the heart of Margaret's own, troubled life?\\nAs Margaret digs deeper, two parallel stories unfold, and the tale she uncovers sheds a disturbing light on her own life...\",\n",
              " 'The most up-to-date edition of the world’s bestselling thesaurus, Roget’s International Thesaurus, 7th Edition, gives writers of all levels an unparalleled aid in using language with precision, grace, and power.\\nThe most comprehensive, user-friendly thesaurus available, Roget’s features more than 325,000 words and phrases, including more than 2,000 all-new entries that reflect the very latest in culture and technology, from “alpha male” to“zero tolerance.” The seventh edition has reduced archaic terminology and added 50 new word lists, providing greater ease-of-use and accessibility than any other writer’s reference book on the market. An indispensable asset for students, writers, reporters, and editors, Roget’s International Thesaurus, 7th Edition is your key to unlocking the power of language.',\n",
              " \"An Actor Prepares is the most famous acting training book ever to have been written and the work of Stanislavski has inspired generations of actors and trainers. This translation was the first to introduce Stanislavski's 'system' to the English speaking world and has stood the test of time in acting classes to this day. Stanislavski here deals with the inward preparation an actor must undergo in order to explore a role to the full. He introduces the concepts of the 'magic if' units and objectives, of emotion memory, of the super-objective and many more now famous rehearsal aids. Now available in the Bloomsbury Revelations series to mark the 150th anniversary of Stanislavski's birth, this is an essential read for actors, directors and anyone interested in the art of drama.\",\n",
              " 'Some deserts are hot, and some deserts are cold. There is not much water in deserts, and it is difficult for animals to live in them. Ladybird Readers is a graded reading series of traditional tales, popular characters, modern stories, and non-fiction, written for young learners of English as a foreign or second language. Beautifully illustrated and carefully written, the series combines the best of Ladybird content with the structured language progression that will help children develop their reading, writing, speaking, listening and critical thinking skills. The six levels of Readers and Activity Books follow the CEFR framework and include language activities that provide preparation for the Cambridge English: Young Learners (YLE) exams. BBC Earth: Deserts, a Level 1 Reader, is Pre-A1 in the CEFR framework and supports YLE Starters exams. Short sentences contain a maximum of two clauses, using the present tense and some simple adjectives.',\n",
              " 'A man is shot at in a juvenile reform home – but someone else dies…\\n\\nMiss Marple senses danger when she visits a friend living in a Victorian mansion which doubles as a rehabilitiation centre for delinquents. Her fears are confirmed when a youth fires a revolver at the administrator, Lewis Serrocold. Neither is injured. But a mysterious visitor, Mr Gilbrandsen, is less fortunate – shot dead simultaneously in another part of the building.\\n\\nPure coincidence? Miss Marple thinks not and vows to discover the real reason for Mr Gilbrandsen’s visit.',\n",
              " \"Tenali Raman was a court jester, an intelligent advisor and one of the ashtadiggajas (elephants serving as pillars and taking care of all the eight sides) in the Bhuvana Vijayam (Royal Court) of the famed Emperor of Vijayanagar Empire (City of Joy) in Karnataka – Sri Krishna Deva Raya (1509-1529), the model ruler par excellence to Ashoka, Samudra Gupta and Harsha Vardhana. Tenali Raman was an embodiment of acute wit and humour and an admirable poet of knowledge, shrewdness and ingenuity. In a short span, the legacy left behind by Tenali Raman attained eternity. All these qualities of Tenali Raman have been fully explored and displayed in this collection of vibrant fables and anecdotes. The book is a marvellous treasury of legends of Tenali Raman and Emperor Raya which evokes a long lost, never-never land: an enchanted world of alert wits and tricky gossips; crafty crooks with biting tongues, valiant brigands and an assorted cluster of uncommon common people. Narrated by the author and superbly illustrated, “Fix Your Problems–The Tenali Raman Way” is an engaging blend of earthly wisdom and sparkling humour which deal with concepts that have certain timelessness. Each story is followed by terse moral and incalculable snippets which are usually that little extra that brings the reader a little more closer to his goal on the way to realisation. Every story purveys a pithy folk wisdom that triumphs over all trials and tribulations. The moralistic traits sagaciously portrayed by these stories intend to develop a series of impacts that can reinforce certain key ideas by the rational mind of the readers in all facets of life and propel them to the top in every endeavour. The stories' various layers of meaning educates, informs, advises, enthuses, inspires and amuses and thus have a teaching effect which makes this book a must read for every aspiring individual who wants to race ahead in the world of opportunities and cusses. The book also exposes how richly endowed Bharata Khanda (India before invasions) had been in the east in the field of wisdom and knowledge down the ages of which the west is ignorant.\",\n",
              " '“This book is a breakthrough, a lyrical, powerful, science-based narrative that actually shows us how to get better (much better) at the things we care about.”—Seth Godin, author of Linchpin\\n\\n“Anyone who wants to get better at anything should read [Peak]. Rest assured that the book is not mere theory. Ericsson’s research focuses on the real world, and he explains in detail, with examples, how all of us can apply the principles of great performance in our work or in any other part of our lives.”—Fortune\\n\\nAnders Ericsson has made a career studying chess champions, violin virtuosos, star athletes, and memory mavens. Peak distills three decades of myth-shattering research into a powerful learning strategy that is fundamentally different from the way people traditionally think about acquiring new abilities. Whether you want to stand out at work, improve your athletic or musical performance, or help your child achieve academic goals, Ericsson’s revolutionary methods will show you how to improve at almost any skill that matters to you.\\n \\n“The science of excellence can be divided into two eras: before Ericsson and after Ericsson. His groundbreaking work, captured in this brilliantly useful book, provides us with a blueprint for achieving the most important and life-changing work possible: to become a little bit better each day.”—Dan Coyle, author of The Talent Code\\n \\n“Ericsson’s research has revolutionized how we think about human achievement. If everyone would take the lessons of this book to heart, it could truly change the world.”—Joshua Foer, author of Moonwalking with Einstein\\n\\n ',\n",
              " \"Reveals the stories of the deathless masters, the siddhas Agastyar and Boganathar, who belonged to the '18 Siddha Tradition', famous among the Tamil speaking people of southern India and Babaji, the immortal master made famous by Yogananda's Autobiography of a Yogi.\",\n",
              " \"From big cats to elephants and indigenous communities, Wild Encounters is a must-have for nature lovers, conservationists, and anyone who is inspired by all that remains wild. David Yarrow travels from pole to pole and continent to continent to visit frozen Arctic tundras, vast African deserts, primordial rain forests, and remote villages, inviting us to truly connect with subjects we mistakenly think we have seen before. Yarrow takes the familiar—lions, elephants, tigers, polar bears—and makes it new again by creating iconic images that deliberately connect with us at a highly emotional level.  For more than two decades, this legendary wildlife photographer has been putting himself in harm's way to capture the most unbelievable close-up animal photography, amassing an incomparable photographic portfolio, spanning six continents. Driven by a passion for sharing and preserving Earth's last great wild cultures and species, Yarrow is as much a conservationist as a photographer and artist. His work has transcended wildlife photography and is now collected and shown as fine art in some of the most famed galleries around the world. Featuring 160 of his most breathtaking photographs, Wild Encounters offers a truly intimate view of some of the world's most compelling—and threatened—species and captures the splendor and very soul of what remains wild and free in our world through portraits that feel close enough to touch.\",\n",
              " \"When Pollyanna Whittier goes to live with her sourtempered aunt after her father's death, things seem bad enough, but then a dreadful accident ensues. However, Pollyanna's sunny nature and good humour prove to have an astonishing effect on all around her, and this wonderful tale of how cheerfulness can conquer adversity has remained one of the world's most popular children's books since its first publication in 1913. In Pollyanna Grows Up, the only sequel written by Porter herself, Pollyanna finds that that, despite being cured of her health problems, adulthood brings fresh challenges to be overcome.\",\n",
              " 'This wide-ranging dictionary contains a wealth of information on all aspects of history, from prehistory right up to the present day. Over 4,000 clear, concise entries include biographies of key figures in world history (living and dead), separate entries for every country in the world (summarising key historical events) and in-depth entries on religious and political movements, international organizations and major conflicts and events and their after-effects.\\nFor this new edition, existing entries have been revised and updated to reflect the very latest global events including changes in leadership, wars, political situations and the statistical information given for each country (population counts, currency, languages, religions). New entries have been included for key figures who have recently come to prominence and world events.\\nThe book also contains twenty-five detailed maps linked to key historical events and topics. These include the African slave trade, the Black Death and the Normandy campaign. Also included are over 200 country maps. The dictionary is enhanced by entry-level web links which are accessed via a dedicated companion website.\\nEncyclopedic in scope, this ambitious A to Z provides an excellent overview of world history both for students and anyone with an interest in the subject.\\nBook Features:\\nFully revised and updated history dictionary, containing over 4,000 clear, concise and accessible entries\\nIncludes biographies of key figures in world history, from Alexander the Great to Pope Francis, Elizabeth I to Nelson Mandela\\nInternational coverage: detailed entries for every country in the world\\nIn-depth entries on religious and political movements, international organizations and key battles and their after-effects\\nDetailed maps covering events and topics such as Black Death, the American War of Independence and the Ottoman Empire\\nEntry-level web links listed and regularly updated on a dedicated companion website',\n",
              " 'The Daylight War, the eagerly anticipated third volume in Brett’s internationally bestselling Demon Cycle, continues the epic tale of humanity’s last stand against an army of demons.\\nON THE NIGHT OF A NEW MOON ALL SHADOWS DEEPEN AND THE DEMONS RISE.\\nArlen Bales understands the threat better than anyone. Resisting the coreling plague has shaped him into a weapon so powerful he has attracted enemies both above and below ground.\\nBut as Arlen prepares his people for battle, a daylight war approaches from the south. Out of the desert rides an army led by a man who believes his destiny is to unite humanity against the demons, willingly or not.\\nOnce, Ahmann Jardir and Arlen were like brothers. Now they are the bitterest of rivals, and Jardir’s ambition is matched only by the magic wielded by his first wife, Inevera, a powerful priestess.\\nThe corelings will attack in thirty nights, and the only men capable of defeating them are divided against each other by the most deadly demons of all: those that lurk in the human heart.',\n",
              " \"This comprehensive volume contains some of Jim Corbett's best known books including My India, a memoir of the days he spent among the people of India; Jungle Lore, which talks about the fragility of nature and his despair at humanity's estrangement from the environment around them; and Tree Tops, wherein he recalls his final days in the Kenyan game reserves. Written in Corbett's clear, simple style and enlivened by his descriptions of jungle sights and sounds and village life, this is a must-read for those interested in wildlife and tiger tales.\",\n",
              " 'DESCRIPTION\\nThis book teaches young readers how to program using the Swift\\nlanguage while they develop iOS apps. Readers gain fundamental\\nprogramming skills along the way.\\nHello Swift! is a how-to guide to programming iOS apps with the Swift\\nlanguage, written from a kid’s perspective. This approachable, wellillustrated,\\nstep-by-step guide takes readers from very beginning\\nprogramming concepts through creating complete apps. It begins by\\nteaching the basics of what an app is and what is needed to build one.\\nFrom there, readers will create their first simple app while learning\\nmore about programming with Swift. Readers conquer important\\nprogramming concepts that can be used with other programming\\nlanguages.\\n KEY FEATURES\\n• Crystal-clear explanations\\n• Engaging images and straightforward examples\\n• Hands-on lessons\\n• Exercises that encourage critical thinking\\n• Practical instructions for building and deploying apps on the\\nApp Store\\nAUDIENCE\\nWhile written with a younger audience in mind, this book is for readers\\nof any age with no programming experience who want to enjoy\\nlearning to program while creating iOS apps.\\nABOUT THE TECHNOLOGY\\nSwift is Apple’s new robust and intuitive programming language for iOS.\\nIt’s free and open source and can be used to build apps for iOS, Mac,\\nApple TV, and Apple Watch.',\n",
              " \"First published in 1983 and now available with a new introduction by the author, Gardner's trailblazing book revolutionized the worlds of education and psychology by positing that rather than a single type of intelligence, we have several- most of which are neglected by standard testing and educational methods.\",\n",
              " '#1 New York Times bestselling author Karen Marie Moning picks up where Shadowfever leaves off with Iced, the sixth book in her blockbuster Fever series.\\n\\nThe year is 1 AWC—After the Wall Crash. The Fae are free and hunting us. It’s a war zone out there, and no two days are alike. I’m Dani O’Malley, the chaos-filled streets of Dublin are my home, and there’s no place I’d rather be.\\n\\nDani “Mega” O’Malley plays by her own set of rules—and in a world overrun by Dark Fae, her biggest rule is: Do what it takes to survive. Possessing rare talents and the all-powerful Sword of Light, Dani is more than equipped for the task. In fact, she’s one of the rare humans who can defend themselves against the Unseelie. But now, amid the pandemonium, her greatest gifts have turned into serious liabilities.\\n\\nDani’s ex–best friend, MacKayla Lane, wants her dead, the terrifying Unseelie princes have put a price on her head, and Inspector Jayne, the head of the police force, is after her sword and will stop at nothing to get it. What’s more, people are being mysteriously frozen to death all over the city, encased on the spot in sub-zero, icy tableaux.\\n\\nWhen Dublin’s most seductive nightclub gets blanketed in hoarfrost, Dani finds herself at the mercy of Ryodan, the club’s ruthless, immortal owner. He needs her quick wit and exceptional skill to figure out what’s freezing Fae and humans dead in their tracks—and Ryodan will do anything to ensure her compliance.\\n\\nDodging bullets, fangs, and fists, Dani must strike treacherous bargains and make desperate alliances to save her beloved Dublin—before everything and everyone in it gets iced.\\n\\nLook for all of Karen Marie Moning’s sensational Fever novels:\\nDARKFEVER | BLOODFEVER | FAEFEVER | DREAMFEVER | SHADOWFEVER | ICED | BURNED | FEVERBORN | FEVERSONG\\n\\nPraise for Iced\\n\\n“Moning returns to the heady world of her Fever series, and the results are addictive and consistently surprising. . . . The best elements of Moning’s sensual, shadowy epic are still here, from the sensual and enigmatic Fae to the super-alpha heroes and the breathless pace of their escalating conflicts. At its heart is a heroine whose development is likely to become the stuff of legends as this unforgettable, haunting series continues to evolve.”—RT Book Reviews\\n\\n“This is one of my favorite 2012 reads . . . It’s engaging, hilarious, amazing and Dani is going to be one heck of a woman.”—USA Today\\n\\n“A gripping story that combines excellent storytelling with believable characters that are rendered both superhuman and superbly human, with emotional fragility and psychological vulnerability in an unstable world fraught with danger . . . Fast-paced, with nonstop action set in a fascinating urban fantasy world of Dublin under siege, this is a smart, bold and textured success.”─Kirkus Reviews\\n\\n“Moning is a master storyteller. I don’t know how she does it, but she begs me to get on my knees and pay worship to the woman who has brought me the best, most labyrinthine stories and characters I’ve ever had the privilege to get to know. She weaves brilliantly, unapologetically, and without exception, and she has threaded the needle into me and I’ve been pulled, over and over, into her tapestry, and I don’t think I’m ever getting out. Iced is no exception.”—The Bawdy Book Blog (five-starred review)',\n",
              " \"Key Words with Peter and Jane uses the most frequently met words in the English language as a starting point for learning to read successfully and confidently. The Key Words reading scheme is scientifically researched and world renowned. In book 3a, Peter and Jane have fun doing things they like in 36 new words including 'me', 'tea', 'bed' and 'give'. Once this book has been completed, the child moves on to book 3b. The Key Words with Peter and Jane books work because each of the key words is introduced gradually and repeated frequently. This builds confidence in children when they recognise these key words on sight (also known as the 'look and say' method of learning). Examples of key words are: the, one, two, he. There are 12 levels, each with 3 books: a, b, and c. Series a: Gradually introduces new words Series b: Provides further practise of words featured in the 'a' series Series c: Links reading with writing and phonics. All the words that have been introduced in each 'a' and 'b' book are also reinforced in the 'c' books\",\n",
              " 'The classic World War II thriller from the acclaimed master of action and suspense. Now reissued in a new cover style.\\nOne winter night, seven men and a woman are parachuted onto a mountainside in wartime Germany. Their objective: an apparently inaccessible castle, headquarters of the Gestapo. Their mission: to rescue a crashed American general before the Nazi interrogators can force him to reveal secret D-Day plans.',\n",
              " \"The new Longman Basic English Dictionary offers all the information on the English language that young learners need to know. It contains thousands of simple examples based on the Longman Corpus Network, up-to-date coverage including new words relating to computers and the Internet, easy definitions and fun illustrations. For Sale in Indian subcontinent onlyFeatures • 12,000 words with definitions written using a defining vocabulary of just 1,600 words • Fun illustrations to make learning new words easier and more enjoyable • Mini picture dictionary in colour with pages on Sports, Transport, Clothes etc. • Usage notes based on common students' errors found in the Longman Learners' Corpus • Workbook introduction with exercises to practice using the dictionary\",\n",
              " \"The micekings are in a panic. The village's best cook is ill, and until she recovers, there's no delicious stew to eat! Geronimo Stiltonord departs immediately in search of a cure for her. But on the way, he ends up snout-to-snout with terrifying dragons! Can he make it back with his fur intact?There was a mystery in New Mouse City's Egyptian Mouseum! The Black Papyrus, an ancient document that reveals the secret of eternal youth, had disappeared. The mouseum's director asked me to help him find it. Yikes -- those Egyptian artifacts freak me out! Luckily, a secret agent came to our aid... but could we trust him? It was up to us to recover the precious scroll!\",\n",
              " \"A Topsy and Tim story. Topsy and Tim have fun meeting all kinds of interesting animals at the farm. What happens when they meet some geese? Read it yourself with Ladybird is one of Ladybird's best-selling reading series. For over thirty-five years it has helped young children who are learning to read develop and improve their reading skills. Each Read it yourself book is very carefully written to include many key, high-frequency words that are vital for learning to read, as well as a limited number of story words that are introduced and practised throughout. Simple sentences and frequently repeated words help to build the confidence of beginner readers and the four different levels of books support children all the way from very first reading practice through to independent, fluent reading. Each book has been carefully checked by educational consultants and can be read independently at home or used in a guided reading session at school. Further content includes comprehension puzzles, helpful notes for parents, carers and teachers, and book band information for use in schools. Topsy and Tim: At the Farm is a Level 1 Read it yourself book, suitable for very early readers who are ready to take their first steps in reading real stories. Each simple story uses a small number of frequently repeated words.\",\n",
              " \"Dr Julius No is a man with a mysterious past. Nobody knows what secrets are hidden on his Caribbean island, and all those who have attempted to investigate further have disappeared.\\n\\nWhen two British agents go missing in Jamaica, Bond is sent to investigate. Battling the Doctor's twin obsessions with power and pain, he uncovers the true nature of his opponent's covert operation - but he must undergo a deadly assault course before he can destroy the Doctor's plans once and for all.\",\n",
              " 'A spellbinding novel of romantic obsession.\\n\\nVernon Deyre is a sensitive and brilliant musician, even a genius. But there is a high price to be paid for his talent, especially by his family and the two women in his life. His sheltered childhood in the home he loves has not prepared Vernon for the harsh reality of his adult years, and in order to write the great masterpiece of his life, he has to make a crucial decision with no time left to count the cost…\\n\\nFamous for her ingenious crime books and plays, Agatha Christie also wrote about crimes of the heart, six bittersweet and very personal novels, as compelling and memorable as the best of her work.',\n",
              " 'The second edition of Video Production has been thoroughly revised to include a new chapter on Multimedia Production apart from a score of new topics combined with new images and anecdotes.\\n\\nBeginning with creating an understanding of visual grammar for video production, the book goes on to discuss the parts of a video camera, the roles of the personnel involved, and the three phases of video production-pre-production, production, and post-production. It moves from the conceptual to the practical-discussing in detail scriptwriting, lighting, sound, and editing; single-camera and multi-camera production processes; and the techniques involved in electronic news gathering and electronic field production. Video and broadcast technology and the various delivery options available in India and abroad are also discussed in detail.',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "def embedding(cat_df,column):\n",
        " max_words = 10000\n",
        " maxlen = 13095\n",
        " tokenizer = Tokenizer(num_words=max_words)\n",
        " tokenizer.fit_on_texts(cat_df[column])\n",
        " sequences = tokenizer.texts_to_sequences(cat_df[column])\n",
        " word_index = tokenizer.word_index\n",
        " print('Found %s unique tokens.' % len(word_index))\n",
        " return sequences,word_index\n",
        "#data = pad_sequences(sequences, maxlen=maxlen)\n",
        "#labels = encoded_df['Label']\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(max_words, 32, input_length=maxlen))\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "#model.summary()\n",
        "#history = model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "awp3Rms4HT-1"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s,w = embedding(cat_df,'Synopsis')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7KeAaiZJp75",
        "outputId": "0d30fc3a-463e-4dcd-9c77-c7585a7ea631"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 46323 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBSZDNwuHsrv",
        "outputId": "b5ae317b-2aeb-41c0-d036-9d3bd201d1ca"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'and': 2, 'of': 3, 'a': 4, 'to': 5, 'in': 6, 'is': 7, 'for': 8, 'with': 9, 'his': 10, 'that': 11, 'as': 12, 'on': 13, 'this': 14, 'from': 15, 'book': 16, 'it': 17, 'by': 18, 'he': 19, 'an': 20, 'her': 21, 'you': 22, 'has': 23, 'are': 24, 'one': 25, 'but': 26, 'who': 27, 'all': 28, 'at': 29, 'will': 30, 'how': 31, 'new': 32, 'their': 33, 'be': 34, 'was': 35, 'life': 36, 'world': 37, 'she': 38, 'have': 39, 'about': 40, 'most': 41, 'your': 42, 'into': 43, 'more': 44, 'they': 45, 'can': 46, 'or': 47, 'what': 48, 'when': 49, 'first': 50, 'been': 51, 'story': 52, 'not': 53, 'up': 54, 'time': 55, 'its': 56, 'also': 57, 'out': 58, 'which': 59, 'him': 60, 'love': 61, 'through': 62, 'best': 63, 'author': 64, 'only': 65, 'man': 66, 'years': 67, 'than': 68, 'now': 69, 'them': 70, 'we': 71, 'like': 72, 'series': 73, 'our': 74, 'these': 75, 'so': 76, 'over': 77, 'times': 78, 'no': 79, 'other': 80, 'way': 81, 'each': 82, 'india': 83, 'own': 84, 'two': 85, 'just': 86, 'work': 87, 'help': 88, 'make': 89, 'books': 90, 'english': 91, 'people': 92, 'find': 93, 'learn': 94, 'well': 95, 'every': 96, 'after': 97, 'if': 98, 'i': 99, 'even': 100, 'us': 101, 'many': 102, 'there': 103, 'history': 104, 'ever': 105, 'language': 106, 'young': 107, 'readers': 108, 'do': 109, '–': 110, 'some': 111, 'get': 112, 'never': 113, 'words': 114, 'great': 115, 'novel': 116, 'stories': 117, 'guide': 118, 'must': 119, 'edition': 120, 'year': 121, 'such': 122, 'read': 123, 'indian': 124, 'written': 125, 'key': 126, 'learning': 127, 'use': 128, 'had': 129, 'day': 130, 'where': 131, 'family': 132, 'back': 133, 'before': 134, 'while': 135, 'between': 136, '1': 137, '•': 138, 'war': 139, 'data': 140, 'bestselling': 141, 'being': 142, 'would': 143, 'then': 144, 'power': 145, 'york': 146, 'including': 147, 'students': 148, 'very': 149, 'writing': 150, 'old': 151, 'lives': 152, 'both': 153, 'could': 154, 'were': 155, 'features': 156, 'three': 157, 'know': 158, 'art': 159, 'any': 160, 'those': 161, 'made': 162, 'take': 163, 'includes': 164, 'business': 165, 'questions': 166, 'much': 167, 'become': 168, 'human': 169, 'set': 170, 'death': 171, 'full': 172, 'why': 173, 'long': 174, 'step': 175, 'modern': 176, 'using': 177, 'published': 178, 'down': 179, 'heart': 180, 'reading': 181, 'game': 182, 'journey': 183, 'around': 184, 'home': 185, 'need': 186, 'together': 187, \"it's\": 188, 'real': 189, 'good': 190, 'easy': 191, 'end': 192, 'greatest': 193, 'political': 194, 'go': 195, 'everything': 196, 'against': 197, 'woman': 198, 'different': 199, 'behind': 200, 'part': 201, 'past': 202, 'takes': 203, \"'\": 204, 'high': 205, 'last': 206, 'secret': 207, 'perfect': 208, 'four': 209, 'classic': 210, 'available': 211, 'yet': 212, 'powerful': 213, 'design': 214, 'provides': 215, 'important': 216, 'women': 217, 'practice': 218, 'school': 219, 'personal': 220, 'skills': 221, 'known': 222, 'information': 223, 'course': 224, 'off': 225, 'based': 226, 'across': 227, 'simple': 228, 'century': 229, 'social': 230, 'collection': 231, 'bestseller': 232, 'along': 233, 'techniques': 234, 'come': 235, 'chapter': 236, 'once': 237, 'himself': 238, 'makes': 239, 'self': 240, 'age': 241, 'science': 242, 'making': 243, 'want': 244, 'award': 245, 'right': 246, 'comes': 247, 'second': 248, 'better': 249, 'true': 250, 'used': 251, 'popular': 252, 'tale': 253, 'children': 254, 'complete': 255, 'international': 256, 'understand': 257, 'always': 258, '2': 259, 'friends': 260, 'american': 261, 'men': 262, 'concepts': 263, 'top': 264, 'things': 265, 'career': 266, 'five': 267, 'test': 268, 'knowledge': 269, 'understanding': 270, 'still': 271, 'beautiful': 272, 'level': 273, 'city': 274, 'today': 275, 'experience': 276, 'practical': 277, 'little': 278, 'offers': 279, 'girl': 280, 'without': 281, 'see': 282, 'since': 283, 'too': 284, 'may': 285, 'master': 286, 'team': 287, 'characters': 288, 'introduction': 289, 'my': 290, 'country': 291, 'future': 292, 'create': 293, 'works': 294, 'dark': 295, 'anyone': 296, 'study': 297, 'volume': 298, 'body': 299, 'look': 300, 'page': 301, 'film': 302, 'same': 303, 'deep': 304, 'essential': 305, 'case': 306, 'comprehensive': 307, 'under': 308, 'clear': 309, 'mind': 310, 'unique': 311, 'house': 312, 'does': 313, 'examples': 314, 'style': 315, 'again': 316, 'programming': 317, \"world's\": 318, 'build': 319, 'truth': 320, 'various': 321, 'few': 322, 'shows': 323, 'secrets': 324, 'place': 325, 'change': 326, 'finds': 327, 'exercises': 328, 'face': 329, 'found': 330, 'adventure': 331, 'big': 332, 'contains': 333, 'account': 334, 'computer': 335, 'soon': 336, 'system': 337, 'away': 338, 'next': 339, 'left': 340, 'until': 341, 'something': 342, 'success': 343, 'original': 344, 'discover': 345, 'whether': 346, 'knows': 347, 'grammar': 348, 'days': 349, 'really': 350, 'fans': 351, 'here': 352, 'live': 353, 'another': 354, 'father': 355, 'r': 356, 'thriller': 357, 'whose': 358, 'reader': 359, 'me': 360, \"you'll\": 361, 'winning': 362, 'famous': 363, 'number': 364, 'sunday': 365, 'should': 366, 'training': 367, 'text': 368, 'daily': 369, 'did': 370, 'keep': 371, 'put': 372, 'fun': 373, 'child': 374, 'c': 375, 'begins': 376, 'extraordinary': 377, 'dead': 378, 'cricket': 379, 'friend': 380, 'ideas': 381, 'brilliant': 382, 'murder': 383, 'form': 384, '3': 385, 'web': 386, 'answers': 387, 'later': 388, 'short': 389, 'king': 390, 'lost': 391, 'fascinating': 392, 'far': 393, 'among': 394, 'major': 395, 'writer': 396, 'because': 397, 'living': 398, 'updated': 399, 'during': 400, 'national': 401, 'give': 402, 'mysterious': 403, 'society': 404, 'global': 405, 'crime': 406, 'born': 407, 'topics': 408, 'illustrated': 409, 'basic': 410, 'wants': 411, 'include': 412, 'gives': 413, 'night': 414, 'others': 415, 'media': 416, '”': 417, 'working': 418, 'everyone': 419, 'successful': 420, 'tells': 421, 'pages': 422, 'approach': 423, 'mystery': 424, 'nothing': 425, 'technology': 426, 'often': 427, 'development': 428, 'black': 429, 'paperback': 430, 'think': 431, 'research': 432, 'throughout': 433, 'action': 434, 'looking': 435, 'turn': 436, 'within': 437, 'show': 438, 'state': 439, 'light': 440, 'comics': 441, 'john': 442, 'process': 443, 'politics': 444, 'word': 445, 'became': 446, 'small': 447, 'hand': 448, 'applications': 449, 'british': 450, 'write': 451, 'dictionary': 452, 'play': 453, 'epic': 454, 'thought': 455, 'london': 456, 'music': 457, 'events': 458, 'digital': 459, 'public': 460, 'almost': 461, 'inside': 462, 'building': 463, 'son': 464, 'control': 465, 'early': 466, 'boy': 467, 'special': 468, \"he's\": 469, 'million': 470, 'present': 471, 'called': 472, 'leading': 473, 'fiction': 474, 'side': 475, 'problems': 476, 'tools': 477, 'order': 478, 'loved': 479, 'novels': 480, 'designed': 481, 'following': 482, 'hard': 483, 'adventures': 484, \"'a\": 485, 'mother': 486, 'provide': 487, 'earth': 488, 'comic': 489, 'search': 490, 'security': 491, 'run': 492, 'free': 493, 'illustrations': 494, 'follow': 495, '10': 496, 'herself': 497, 'head': 498, 'beyond': 499, 'wife': 500, 'six': 501, 'job': 502, 'reveals': 503, 'going': 504, 'penguin': 505, 'funny': 506, 'latest': 507, 'name': 508, 'it’s': 509, 'twenty': 510, 'brings': 511, 'might': 512, 'ancient': 513, 'highly': 514, 'drawing': 515, 'covers': 516, 'needs': 517, 'person': 518, 'professional': 519, 'selling': 520, 'daughter': 521, 'finally': 522, 'detailed': 523, 'reference': 524, 'common': 525, 'running': 526, 'fast': 527, 'enough': 528, 'save': 529, 'nature': 530, 'david': 531, \"'the\": 532, 'relationship': 533, 's': 534, 'exam': 535, 'start': 536, 'hero': 537, 'dangerous': 538, 'close': 539, 'hands': 540, 'becomes': 541, 'yourself': 542, \"india's\": 543, 'able': 544, 'performance': 545, 'seen': 546, 'complex': 547, 'tintin': 548, 'stop': 549, 'chapters': 550, 'ways': 551, 'online': 552, 'intelligence': 553, 'themselves': 554, 'analysis': 555, 'code': 556, 'vocabulary': 557, 'south': 558, 'bring': 559, 'literature': 560, '5': 561, 'battle': 562, 'government': 563, 'ready': 564, 'beginning': 565, 'cover': 566, 'explains': 567, 'lessons': 568, 'software': 569, 'contemporary': 570, 'artist': 571, 'economic': 572, 'culture': 573, 'writers': 574, 'speaking': 575, 'college': 576, 'forces': 577, 'content': 578, 'creative': 579, 'dr': 580, 'third': 581, 'magic': 582, 'prize': 583, 'material': 584, 'company': 585, 'machine': 586, 'tell': 587, 'java': 588, 'section': 589, 'issues': 590, 'ultimate': 591, 'goes': 592, 'develop': 593, 'fight': 594, 'advanced': 595, 'useful': 596, 'someone': 597, '000': 598, 'theory': 599, 'range': 600, 'husband': 601, 'single': 602, 'force': 603, 'police': 604, 'marketing': 605, 'town': 606, 'principles': 607, 'rich': 608, 'given': 609, 'however': 610, 'meet': 611, 'turns': 612, 'decades': 613, 'detective': 614, 'graphic': 615, 'star': 616, 'getting': 617, 'gripping': 618, 'color': 619, 'role': 620, 'ten': 621, 'notes': 622, 'insight': 623, 'red': 624, 'struggle': 625, 'taking': 626, 'presents': 627, 'dream': 628, 'explore': 629, 'insights': 630, 'having': 631, 'systems': 632, 'chance': 633, 'upon': 634, 'games': 635, 'thing': 636, 'general': 637, 'america': 638, '4': 639, 'group': 640, 'dreams': 641, 'let': 642, 'winner': 643, 'former': 644, 'revised': 645, 'python': 646, 'tips': 647, 'list': 648, \"she's\": 649, 'class': 650, 'inspired': 651, 'improve': 652, 'strength': 653, 'peter': 654, 'entire': 655, 'advice': 656, 'private': 657, 'brought': 658, 'line': 659, 'student': 660, 'historical': 661, 'helps': 662, 'fully': 663, 'compelling': 664, 'expert': 665, 'parents': 666, 'authors': 667, 'changed': 668, 'subject': 669, 'told': 670, 'hope': 671, 'seven': 672, 'moving': 673, \"can't\": 674, 'created': 675, 'final': 676, 'means': 677, 'character': 678, 'acclaimed': 679, 'hilarious': 680, 'money': 681, 'legendary': 682, 'strong': 683, 'narrative': 684, 'core': 685, 'remarkable': 686, 'several': 687, 'food': 688, 'meets': 689, 'tests': 690, 'came': 691, 'jack': 692, 'classics': 693, 'childhood': 694, 'changes': 695, 'challenges': 696, 'turned': 697, 'experiences': 698, 'detail': 699, 'feel': 700, 'french': 701, 'explores': 702, 'field': 703, 'question': 704, 'growing': 705, 'white': 706, 'terms': 707, 'forever': 708, 'world’s': 709, 'truly': 710, 'race': 711, 'sets': 712, 'featuring': 713, 'review': 714, 'blood': 715, 'further': 716, 'millions': 717, 'whole': 718, 'lies': 719, 'gets': 720, 'included': 721, 'university': 722, 'exciting': 723, 'interviews': 724, 'kind': 725, 'passion': 726, 'nation': 727, 'law': 728, 'date': 729, 'health': 730, 'non': 731, 'sense': 732, 'deadly': 733, \"don't\": 734, 'got': 735, 'seems': 736, 'drawn': 737, 'army': 738, 'possible': 739, 'james': 740, 'killer': 741, 'eye': 742, 'michael': 743, 'anything': 744, 'management': 745, 'industry': 746, 'depth': 747, 'point': 748, 'plan': 749, 'travel': 750, 'focus': 751, 'answer': 752, 'matter': 753, 'alone': 754, 'justice': 755, 'survive': 756, 'filled': 757, 'rise': 758, 'football': 759, 'lead': 760, 'marriage': 761, 'west': 762, 'mission': 763, 'party': 764, 'sports': 765, 'thinking': 766, 'determined': 767, 'biggest': 768, 'states': 769, 'coverage': 770, 'win': 771, 'fall': 772, 'brain': 773, 'followed': 774, '8': 775, 'brand': 776, 'remains': 777, 'teach': 778, 'post': 779, 'half': 780, 'internet': 781, 'wide': 782, 'hidden': 783, '’': 784, 'fact': 785, 'confidence': 786, 'rules': 787, 'led': 788, 'evil': 789, 'though': 790, 'movement': 791, 'basics': 792, 'ago': 793, 'brother': 794, 'view': 795, 'empire': 796, 'united': 797, 'trying': 798, \"you're\": 799, 'photography': 800, 'changing': 801, 'taken': 802, 'alive': 803, 'thrilling': 804, 'sport': 805, 'packed': 806, 'stand': 807, 'hundreds': 808, 'methods': 809, 'algorithms': 810, 'choice': 811, \"doesn't\": 812, 'picture': 813, 'projects': 814, 'biography': 815, 'land': 816, '6': 817, 'stunning': 818, 'ideal': 819, 'support': 820, 'freedom': 821, 'open': 822, 'path': 823, 'difficult': 824, 'started': 825, 'application': 826, 'hit': 827, 'universe': 828, '100': 829, 'foreign': 830, 'captain': 831, 'call': 832, 'natural': 833, 'structures': 834, 'tales': 835, 'edge': 836, 'coming': 837, 'survival': 838, 'renowned': 839, 'generation': 840, 'identity': 841, 'space': 842, 'memory': 843, 'moment': 844, 'ability': 845, 'creating': 846, 'voice': 847, 'heroes': 848, 'quickly': 849, 'gandhi': 850, 'fate': 851, 'local': 852, 'relationships': 853, 'began': 854, 'companion': 855, 'access': 856, 'effective': 857, 'dog': 858, 'memoir': 859, 'describes': 860, 'colour': 861, 'films': 862, 'itself': 863, 'achieve': 864, 'hold': 865, 'recent': 866, 'street': 867, 'beloved': 868, 'german': 869, 'president': 870, 'studies': 871, 'cultural': 872, 'discovers': 873, 'value': 874, '7': 875, 'happened': 876, 'independent': 877, 'journalist': 878, 'sister': 879, 'join': 880, 'solve': 881, 'essays': 882, 'india’s': 883, 'less': 884, 'forced': 885, 'move': 886, 'j': 887, 'languages': 888, 'worked': 889, 'problem': 890, 'stage': 891, 'players': 892, 'details': 893, 'photographs': 894, 'lee': 895, 'return': 896, 'inspiring': 897, 'george': 898, 'aspects': 899, 'sometimes': 900, 'took': 901, 'letters': 902, 'activities': 903, 'unexpected': 904, 'model': 905, 'eyes': 906, 'visual': 907, 'rather': 908, 'suddenly': 909, 'b': 910, 'spiritual': 911, 'begin': 912, 'current': 913, 'bad': 914, 'challenge': 915, 'strange': 916, 'piece': 917, 'learned': 918, 'court': 919, 'continues': 920, 'break': 921, 'beauty': 922, 'images': 923, 'follows': 924, 'already': 925, 'apart': 926, 'finding': 927, 'believe': 928, 'artists': 929, 'philosophy': 930, 'experts': 931, 'island': 932, 'moments': 933, 'room': 934, 'escape': 935, 'engineering': 936, 'hindi': 937, 'powers': 938, 'accessible': 939, 'sure': 940, 'news': 941, 'else': 942, 'romance': 943, 'debut': 944, 'autobiography': 945, 'fire': 946, 'wrong': 947, 'gone': 948, 'everyday': 949, 'fantasy': 950, 'interesting': 951, 'named': 952, 'program': 953, 'levels': 954, 'cold': 955, 'definitive': 956, 'east': 957, 'introduces': 958, 'killed': 959, 'uses': 960, 'planet': 961, 'idea': 962, 'kids': 963, 'd': 964, 'beautifully': 965, 'e': 966, 'say': 967, 'method': 968, 'figures': 969, 'leave': 970, 'plus': 971, 'amazing': 972, 'player': 973, 'looks': 974, 'introduced': 975, 'reads': 976, 'completely': 977, 'professor': 978, 'try': 979, 'stay': 980, 'won': 981, 'parts': 982, 'variety': 983, 'courage': 984, 'mark': 985, 'impossible': 986, 'happy': 987, 'economy': 988, 'patterns': 989, 'friendship': 990, 'practices': 991, 'tom': 992, 'missing': 993, 'terrifying': 994, 'experienced': 995, 'doing': 996, 'loves': 997, 'cross': 998, 'cambridge': 999, 'deeply': 1000, 'trust': 1001, 'stars': 1002, 'incredible': 1003, 'thousands': 1004, 'months': 1005, 'soul': 1006, 'reality': 1007, 'nearly': 1008, 'wall': 1009, 'figure': 1010, 'portrait': 1011, 'cat': 1012, 'concept': 1013, 'sold': 1014, 'thirty': 1015, 'club': 1016, 'literary': 1017, 'phrases': 1018, 'shocking': 1019, 'project': 1020, 'attack': 1021, 'middle': 1022, 'fear': 1023, 'starting': 1024, 'towards': 1025, 'lot': 1026, 'mobile': 1027, 'perspective': 1028, 'late': 1029, 'hindu': 1030, 'physical': 1031, 'fresh': 1032, 'drama': 1033, 'exams': 1034, 'batman': 1035, 'starts': 1036, 'architecture': 1037, 'teacher': 1038, 'treasure': 1039, 'danger': 1040, 'front': 1041, 'robert': 1042, 'speak': 1043, 'simply': 1044, 'facts': 1045, 'engaging': 1046, 'pieces': 1047, 'leaders': 1048, 'desperate': 1049, 'praise': 1050, 'meaning': 1051, 'threat': 1052, 'quest': 1053, 'iconic': 1054, 'dragon': 1055, 'harry': 1056, 'cannot': 1057, 'strategies': 1058, 'leads': 1059, 'inspiration': 1060, 'network': 1061, 'share': 1062, 'he’s': 1063, 'went': 1064, 'done': 1065, 'ordinary': 1066, 'financial': 1067, 'eight': 1068, 'revolution': 1069, 'super': 1070, 'growth': 1071, 'market': 1072, 'kill': 1073, 'north': 1074, 'sea': 1075, \"there's\": 1076, 'framework': 1077, 'legend': 1078, 'becoming': 1079, 'enemy': 1080, 'video': 1081, 'broken': 1082, 'met': 1083, 'title': 1084, 'amazon': 1085, 'teaches': 1086, 'potential': 1087, 'centuries': 1088, 'decision': 1089, 'services': 1090, 'mr': 1091, 'japanese': 1092, 'elements': 1093, 'library': 1094, 'england': 1095, 'decides': 1096, 'god': 1097, 'interview': 1098, 'genius': 1099, 'vast': 1100, 'terrible': 1101, 'feature': 1102, 'worldwide': 1103, 'developed': 1104, 'certain': 1105, 'era': 1106, 'teachers': 1107, 'manga': 1108, 'entertaining': 1109, 'deal': 1110, 'village': 1111, 'asked': 1112, 'pick': 1113, 'titles': 1114, 'de': 1115, '12': 1116, 'mail': 1117, 'lived': 1118, 'rest': 1119, 'lord': 1120, 'independence': 1121, 'authoritative': 1122, 'wisdom': 1123, 'format': 1124, 'you’ll': 1125, 'scenes': 1126, 'service': 1127, 'source': 1128, 'k': 1129, 'peace': 1130, 'stranger': 1131, 'enjoy': 1132, 'steps': 1133, 'perhaps': 1134, 'falls': 1135, 'guardian': 1136, 'above': 1137, 'plot': 1138, 'interest': 1139, 'vision': 1140, 'copies': 1141, 'humanity': 1142, 'provided': 1143, 'gain': 1144, 'rare': 1145, 'built': 1146, 'programs': 1147, 'audio': 1148, 'played': 1149, 'gift': 1150, 'teaching': 1151, 'revenge': 1152, \"isn't\": 1153, 'civil': 1154, 'jane': 1155, 'office': 1156, 'paper': 1157, 'wild': 1158, 'holds': 1159, 'solutions': 1160, 'previous': 1161, 'murdered': 1162, 'league': 1163, 'offer': 1164, 'communication': 1165, 'magazine': 1166, 'romantic': 1167, 'girls': 1168, 'writes': 1169, 'khan': 1170, 'violence': 1171, 'birth': 1172, 'returns': 1173, 'numerous': 1174, 'knew': 1175, 'rock': 1176, 'types': 1177, 'globe': 1178, 'risk': 1179, 'hours': 1180, 'presented': 1181, 'faced': 1182, 'critical': 1183, 'examination': 1184, 'discovery': 1185, 'spent': 1186, 'twentieth': 1187, '2018': 1188, 'nine': 1189, 'ages': 1190, 'despite': 1191, 'community': 1192, 'she’s': 1193, 'sections': 1194, 'influence': 1195, 'scientific': 1196, 'providing': 1197, 'medical': 1198, 'protect': 1199, 'prime': 1200, 'discovered': 1201, 'bookshelf': 1202, 'fitness': 1203, 'blue': 1204, 'spirit': 1205, 'suspense': 1206, 'learners': 1207, 'reach': 1208, 'brutal': 1209, 'giving': 1210, 'movie': 1211, 'technologies': 1212, 'turning': 1213, 'chief': 1214, 'impact': 1215, 'developing': 1216, 'road': 1217, 'functions': 1218, 'match': 1219, 'breaking': 1220, 'large': 1221, 'cd': 1222, 'safe': 1223, 'billion': 1224, 'hundred': 1225, 'technical': 1226, 'plans': 1227, 'listening': 1228, 'investigation': 1229, 'easily': 1230, 'places': 1231, '20': 1232, 'richard': 1233, 'entries': 1234, 'washington': 1235, 'marvel': 1236, 'exercise': 1237, 'destroy': 1238, 'rights': 1239, 'explanations': 1240, 'computers': 1241, 'playing': 1242, 'huge': 1243, \"that's\": 1244, 'decade': 1245, 'record': 1246, 'education': 1247, 'cost': 1248, 'generations': 1249, 'price': 1250, 'summer': 1251, 'points': 1252, 'exclusive': 1253, 'cases': 1254, 'p': 1255, 'ones': 1256, 'paris': 1257, 'structure': 1258, 'fifty': 1259, 'leader': 1260, 'instead': 1261, '9': 1262, 'fighting': 1263, 'interested': 1264, 'driven': 1265, 'multiple': 1266, 'skill': 1267, 'spring': 1268, 'masterpiece': 1269, 'animals': 1270, 'shares': 1271, 'tv': 1272, 'talks': 1273, 'professionals': 1274, 'whom': 1275, 'competitive': 1276, 'involved': 1277, 'alex': 1278, 'train': 1279, 'sent': 1280, 'scientists': 1281, 'covered': 1282, 'wrote': 1283, 'letter': 1284, 'express': 1285, 'needed': 1286, 'progress': 1287, 'finest': 1288, 'addition': 1289, 'air': 1290, 'outside': 1291, 'usage': 1292, 'honest': 1293, 'water': 1294, 'tragedy': 1295, 'quite': 1296, 'inner': 1297, 'brilliantly': 1298, 'prove': 1299, 'chinese': 1300, 'specific': 1301, 'influential': 1302, 'prepare': 1303, 'seem': 1304, 'desire': 1305, 'loss': 1306, 'europe': 1307, '11': 1308, 'environment': 1309, 'explain': 1310, 'especially': 1311, 'agent': 1312, 'attention': 1313, 'result': 1314, 'trilogy': 1315, 'says': 1316, 'sentences': 1317, 'enemies': 1318, 'considered': 1319, 'image': 1320, 'draw': 1321, 'quick': 1322, 'covering': 1323, 'celebrated': 1324, 'resource': 1325, 'moves': 1326, 'awards': 1327, 'kingdom': 1328, 'carefully': 1329, 'energy': 1330, 'traditional': 1331, 'standard': 1332, 'streets': 1333, 'comedy': 1334, 'policy': 1335, 'helped': 1336, 'related': 1337, 'lie': 1338, 'reveal': 1339, 'forms': 1340, 'evidence': 1341, 'mountain': 1342, 'democracy': 1343, 'thoughts': 1344, 'mean': 1345, 'manner': 1346, 'exactly': 1347, 'wealth': 1348, 'act': 1349, 'ex': 1350, 'director': 1351, 'calvin': 1352, 'results': 1353, 'networks': 1354, 'car': 1355, 'track': 1356, 'vital': 1357, 'unforgettable': 1358, 'co': 1359, 'leaving': 1360, 'widely': 1361, 'emotional': 1362, 'leaves': 1363, 'creativity': 1364, 'laugh': 1365, 'intimate': 1366, 'actually': 1367, 'magical': 1368, 'stress': 1369, 'example': 1370, 'tasks': 1371, 'u': 1372, 'explained': 1373, 'm': 1374, 'valuable': 1375, 'extensive': 1376, 'struggles': 1377, 'cast': 1378, 'wars': 1379, 'importance': 1380, 'cities': 1381, 'exploring': 1382, 'western': 1383, 'tricks': 1384, 'greater': 1385, 'official': 1386, 'trail': 1387, 'tries': 1388, 'goal': 1389, 'jobs': 1390, 'cloud': 1391, 'ielts': 1392, '2013': 1393, 'guy': 1394, '“the': 1395, 'delhi': 1396, 'evolution': 1397, 'wit': 1398, 'faces': 1399, 'v': 1400, '30': 1401, 'attempt': 1402, 'terror': 1403, 'ends': 1404, 'pakistan': 1405, '2012': 1406, 'riveting': 1407, 'golden': 1408, 'happiness': 1409, 'spy': 1410, 'type': 1411, 'statistics': 1412, 'cinema': 1413, 'translation': 1414, 'publisher': 1415, 'yoga': 1416, 'pain': 1417, 'legal': 1418, 'enhance': 1419, 'quality': 1420, 'google': 1421, 'corporate': 1422, 'user': 1423, 'die': 1424, 'favourite': 1425, 'astonishing': 1426, 'revealed': 1427, 'phonics': 1428, 'destiny': 1429, 'violent': 1430, 'wanted': 1431, 'mental': 1432, 'joy': 1433, 'situations': 1434, 'china': 1435, 'alike': 1436, 'runs': 1437, 'anecdotes': 1438, 'happens': 1439, 'leadership': 1440, 'highest': 1441, 'trouble': 1442, 'balance': 1443, 'humor': 1444, 'frequently': 1445, 'sql': 1446, 'discusses': 1447, 'queen': 1448, \"'i\": 1449, 'resources': 1450, 'stephen': 1451, 'lifetime': 1452, 'hobbes': 1453, 'speed': 1454, 'dramatic': 1455, 'note': 1456, 'am': 1457, 'revealing': 1458, 'apply': 1459, 'motion': 1460, 'adapted': 1461, 'witty': 1462, 'gold': 1463, 'asterix': 1464, 'patel': 1465, 'friendly': 1466, 'married': 1467, 'shape': 1468, 'tiger': 1469, 'x': 1470, 'month': 1471, 'crisis': 1472, 'mysteries': 1473, 'scene': 1474, 'moon': 1475, 'mumbai': 1476, '2016': 1477, 'relevant': 1478, 'oxford': 1479, 'sun': 1480, 'near': 1481, 'subjects': 1482, 'door': 1483, 'landscape': 1484, 'economics': 1485, 'inspire': 1486, 'adults': 1487, 'criminal': 1488, 'purpose': 1489, 'ball': 1490, 'fourth': 1491, 'context': 1492, 'period': 1493, 'ii': 1494, 'storytelling': 1495, 'religious': 1496, 'alongside': 1497, 'poetry': 1498, 'manual': 1499, 'guides': 1500, 'reason': 1501, 'required': 1502, 'eventually': 1503, 'smart': 1504, 'areas': 1505, 'significant': 1506, 'intelligent': 1507, 'ahead': 1508, 'rule': 1509, 'microsoft': 1510, 'couple': 1511, 'fundamental': 1512, 'humour': 1513, 'creation': 1514, 'individual': 1515, 'minds': 1516, 'revolutionary': 1517, \"they're\": 1518, 'wonderful': 1519, 'brothers': 1520, 'faith': 1521, 'ice': 1522, 'planning': 1523, '15': 1524, 'product': 1525, 'bill': 1526, 'devastating': 1527, 'agency': 1528, 'selection': 1529, 'russian': 1530, 'least': 1531, 'invaluable': 1532, 'message': 1533, 'publication': 1534, 'l': 1535, 'choose': 1536, 'ground': 1537, 'passionate': 1538, 'favorite': 1539, 'divided': 1540, 'strike': 1541, 'magnificent': 1542, 'singh': 1543, 'kept': 1544, 'said': 1545, 'examinations': 1546, 'translated': 1547, 'technique': 1548, 'scale': 1549, '2017': 1550, 'sap': 1551, 'countries': 1552, 'facing': 1553, 'helpful': 1554, 'caught': 1555, 'till': 1556, '2011': 1557, 'intellectual': 1558, 'ladybird': 1559, 'longer': 1560, 'preparation': 1561, 'additional': 1562, 'hot': 1563, 'thousand': 1564, 'beginner': 1565, 'sinister': 1566, 'expanded': 1567, 'care': 1568, 'kate': 1569, 'phenomenon': 1570, 'dc': 1571, 'academy': 1572, 'sex': 1573, 'collects': 1574, 'psychological': 1575, 'decisions': 1576, 'avoid': 1577, 'preparing': 1578, \"today's\": 1579, 'talent': 1580, 'tour': 1581, '200': 1582, 'table': 1583, 'version': 1584, 'cartoons': 1585, 'starring': 1586, 'died': 1587, 'ensure': 1588, 'military': 1589, 'muslim': 1590, 'martin': 1591, 'effectively': 1592, 'classical': 1593, 'paced': 1594, 'although': 1595, 'inspirational': 1596, 'effects': 1597, 'links': 1598, 'continue': 1599, 'situation': 1600, 'holmes': 1601, 'minister': 1602, 'creator': 1603, 'partner': 1604, 'shadow': 1605, 'scientist': 1606, 'gods': 1607, 'guitar': 1608, 'tradition': 1609, 'drawings': 1610, 'telegraph': 1611, 'wonder': 1612, 'beginners': 1613, 'choices': 1614, 'easier': 1615, 'consequences': 1616, 'chaos': 1617, 'sir': 1618, 'object': 1619, 'dynamic': 1620, 'prison': 1621, 'charming': 1622, 'children’s': 1623, 'journal': 1624, 'exploration': 1625, 'texts': 1626, 'challenging': 1627, 'companies': 1628, 'steve': 1629, 'vivid': 1630, 'conflict': 1631, \"you've\": 1632, 'demons': 1633, 'bond': 1634, 'band': 1635, 'selected': 1636, 'bold': 1637, 'sees': 1638, 'talk': 1639, 'grace': 1640, 'imagination': 1641, 'multi': 1642, 'lose': 1643, 'doctor': 1644, 'remain': 1645, 'tool': 1646, 'encounter': 1647, 'solving': 1648, 'miss': 1649, 'position': 1650, 'surprising': 1651, 'sherlock': 1652, 'grow': 1653, 'press': 1654, 'arts': 1655, 'foundation': 1656, 'youth': 1657, 'cup': 1658, 'products': 1659, 'crucial': 1660, 'examines': 1661, 'memories': 1662, 'officer': 1663, 'emotions': 1664, 'papers': 1665, 'plays': 1666, 'double': 1667, 'africa': 1668, 'topic': 1669, 'website': 1670, 'chronicles': 1671, 'twelve': 1672, 'competition': 1673, 'models': 1674, 'william': 1675, 'script': 1676, 'wedding': 1677, 'main': 1678, 'gradually': 1679, 'fortune': 1680, 'helping': 1681, 'worlds': 1682, 'week': 1683, 'physics': 1684, 'fellow': 1685, 'serve': 1686, 'everywhere': 1687, 'researched': 1688, 'watch': 1689, 'prince': 1690, 'happen': 1691, 'internationally': 1692, 'lonely': 1693, 'weekly': 1694, 'memorable': 1695, 'waiting': 1696, 'etc': 1697, 'instructions': 1698, 'henry': 1699, '2014': 1700, 'activity': 1701, 'total': 1702, 'athletes': 1703, 'clever': 1704, 'seemingly': 1705, 'explosive': 1706, 'animal': 1707, 'hell': 1708, 'concise': 1709, 'authentic': 1710, 'developers': 1711, 'lady': 1712, 'unknown': 1713, 'creed': 1714, 'walk': 1715, 'hunt': 1716, 'succeed': 1717, 'uk': 1718, 'rising': 1719, 'screen': 1720, 'explanation': 1721, 'weight': 1722, 'serious': 1723, 'perform': 1724, 'minutes': 1725, 'warrior': 1726, 'gave': 1727, 'movies': 1728, 'battles': 1729, 'medicine': 1730, 'scholars': 1731, 'grand': 1732, 'particular': 1733, 'japan': 1734, 'users': 1735, 'instruction': 1736, 'terrorism': 1737, 'grade': 1738, 'described': 1739, 'volumes': 1740, 'accounts': 1741, 'indians': 1742, 'guidance': 1743, 'familiar': 1744, 'controversial': 1745, 'clearly': 1746, 'immediately': 1747, 'newly': 1748, 'artificial': 1749, 'wish': 1750, 'cutting': 1751, 'humorous': 1752, 'hollywood': 1753, 'uncover': 1754, 'tower': 1755, 'travels': 1756, 'legacy': 1757, 'distinguished': 1758, 'deeper': 1759, 'victim': 1760, 'sound': 1761, 'telling': 1762, 'breathtaking': 1763, 'designs': 1764, 'crew': 1765, 'lines': 1766, 'spoken': 1767, 'colouring': 1768, 'communicate': 1769, 'rescue': 1770, 'mountains': 1771, 'mix': 1772, 'pronunciation': 1773, 'taught': 1774, 'production': 1775, 'com': 1776, 'wise': 1777, 'television': 1778, 'prose': 1779, 'box': 1780, '50': 1781, 'profound': 1782, 'views': 1783, 'cut': 1784, 'feelings': 1785, 'paul': 1786, 'miles': 1787, \"assassin's\": 1788, 'completed': 1789, 'frank': 1790, 'drive': 1791, 'princess': 1792, 'connection': 1793, 'weeks': 1794, 'song': 1795, 'ruthless': 1796, 'talking': 1797, 'limits': 1798, 'neural': 1799, 'ambitious': 1800, 'recipes': 1801, 'buy': 1802, 'ambition': 1803, 'opens': 1804, 'deals': 1805, 'largest': 1806, 'humans': 1807, 'writings': 1808, 'bodies': 1809, 'articles': 1810, 'cause': 1811, 'fears': 1812, 'lovers': 1813, 'chess': 1814, 'speech': 1815, 'stone': 1816, 'poor': 1817, 'themes': 1818, 'tennis': 1819, 'families': 1820, 'opportunities': 1821, 'containing': 1822, 'strategy': 1823, 'textbook': 1824, 'featured': 1825, 'excel': 1826, 'garfield': 1827, 'overcome': 1828, 'green': 1829, 'traces': 1830, 'eating': 1831, 'founder': 1832, 'dealing': 1833, 'forward': 1834, 'opening': 1835, 'teams': 1836, 'keeping': 1837, 'fan': 1838, 'ali': 1839, 'sequel': 1840, 'held': 1841, 'desert': 1842, 'ultimately': 1843, 'anniversary': 1844, 'centre': 1845, 'can’t': 1846, 'collected': 1847, 'attacks': 1848, 'boys': 1849, 'you’re': 1850, 'draws': 1851, 'urban': 1852, 'realizes': 1853, 'either': 1854, 'objects': 1855, 'javascript': 1856, 'pocket': 1857, 'photos': 1858, 'add': 1859, 'thus': 1860, 'showing': 1861, 'names': 1862, 'trip': 1863, 'killing': 1864, 'conspiracy': 1865, 'imagined': 1866, 'terrorist': 1867, 'organization': 1868, 'remote': 1869, 'advertising': 1870, 'capital': 1871, 'correct': 1872, 'disaster': 1873, 'audience': 1874, 'task': 1875, 'falling': 1876, 'sisters': 1877, 'campaign': 1878, 'area': 1879, 'administration': 1880, 'pass': 1881, 'saga': 1882, 'boyfriend': 1883, 'print': 1884, 'forgotten': 1885, 'aspiring': 1886, 'manage': 1887, 'learns': 1888, 'sophie': 1889, 'closer': 1890, 'intrigue': 1891, 'straight': 1892, 'target': 1893, 'sam': 1894, 'accident': 1895, 'ask': 1896, 'morning': 1897, 'innovation': 1898, 'unlikely': 1899, 'storm': 1900, 'meanwhile': 1901, 'computing': 1902, 'brief': 1903, 'insightful': 1904, 'threatens': 1905, 'platform': 1906, 'members': 1907, 'offering': 1908, 'lawyer': 1909, 'kid': 1910, \"america's\": 1911, 'sources': 1912, 'feels': 1913, 'anatomy': 1914, 'values': 1915, 'chilling': 1916, 'poverty': 1917, 'bank': 1918, \"i'm\": 1919, 'background': 1920, 'components': 1921, 'mathematics': 1922, 'sounds': 1923, 'twists': 1924, 'seventy': 1925, 'faster': 1926, 'seeks': 1927, 'identify': 1928, 'baby': 1929, 'event': 1930, 'lists': 1931, 'felt': 1932, 'piano': 1933, 'pitt': 1934, 'irresistible': 1935, 'dedicated': 1936, 're': 1937, 'database': 1938, 'publishing': 1939, 'tested': 1940, 'whatever': 1941, 'anti': 1942, 'sky': 1943, 'determination': 1944, 'classes': 1945, 'cards': 1946, 'cartoon': 1947, 'cars': 1948, 'introduce': 1949, 'troubled': 1950, 'innovative': 1951, 'enhanced': 1952, 'allows': 1953, 'benefits': 1954, 'meeting': 1955, 'doesn’t': 1956, 'thorough': 1957, 'utterly': 1958, 'eat': 1959, 'opportunity': 1960, 'thinks': 1961, 't': 1962, 'spanish': 1963, 'torn': 1964, 'camera': 1965, 'prepared': 1966, 'testing': 1967, 'worst': 1968, 'ranging': 1969, 'captured': 1970, 'editions': 1971, 'reacher': 1972, 'chris': 1973, 'disciplines': 1974, 'translations': 1975, 'groundbreaking': 1976, 'bloody': 1977, 'ride': 1978, 'machines': 1979, 'travelling': 1980, 'ray': 1981, 'effect': 1982, 'pre': 1983, 'appear': 1984, 'harvard': 1985, 'managing': 1986, 'conversation': 1987, 'heat': 1988, 'youtube': 1989, 'hear': 1990, 'affair': 1991, 'seeking': 1992, 'kashmir': 1993, 'lover': 1994, 'celebration': 1995, 'feet': 1996, 'buried': 1997, 'transformed': 1998, 'keeps': 1999, 'strip': 2000, 'indispensable': 2001, 'shared': 2002, 'religion': 2003, 'quotes': 2004, 'due': 2005, 'actors': 2006, 'bound': 2007, 'originally': 2008, 'matters': 2009, 'elite': 2010, 'betrayal': 2011, 'ease': 2012, 'entry': 2013, 'accompanied': 2014, 'extreme': 2015, 'studying': 2016, 'none': 2017, 'scott': 2018, 'fit': 2019, 'cancer': 2020, 'academic': 2021, 'believes': 2022, 'puzzles': 2023, 'suitable': 2024, 'shaped': 2025, 'native': 2026, 'personalities': 2027, 'necessary': 2028, 'tried': 2029, 'region': 2030, 'equally': 2031, 'calls': 2032, 'nehru': 2033, 'tragic': 2034, 'destruction': 2035, 'european': 2036, 'statistical': 2037, 'supreme': 2038, 'received': 2039, 'saved': 2040, 'functional': 2041, 'gorgeous': 2042, 'puts': 2043, 'represents': 2044, 'shot': 2045, 'iron': 2046, 'increasingly': 2047, 'ceo': 2048, 'meditation': 2049, 'sight': 2050, 'innocent': 2051, 'france': 2052, 'diverse': 2053, 'instant': 2054, 'expressions': 2055, 'brown': 2056, 'poet': 2057, 'appeal': 2058, 'laws': 2059, '2008': 2060, 'fields': 2061, 'app': 2062, 'numbers': 2063, 'implement': 2064, 'composition': 2065, 'bringing': 2066, 'walks': 2067, 'weapon': 2068, 'suicide': 2069, 'rose': 2070, 'added': 2071, 'solved': 2072, 'pattern': 2073, 'approaches': 2074, 'tamil': 2075, 'coach': 2076, 'transform': 2077, 'martial': 2078, 'crash': 2079, 'visit': 2080, '18': 2081, 'emphasis': 2082, 'ship': 2083, 'there’s': 2084, 'combines': 2085, 'recounts': 2086, 'musical': 2087, 'intense': 2088, 'thoroughly': 2089, 'worth': 2090, 'don’t': 2091, 'nor': 2092, 'server': 2093, 'difference': 2094, 'manager': 2095, 'bed': 2096, 'intriguing': 2097, 'respected': 2098, 'existence': 2099, 'timeless': 2100, 'basis': 2101, 'sentence': 2102, 'mirror': 2103, 'younger': 2104, 'spectacular': 2105, 'fbi': 2106, 'claim': 2107, 'seek': 2108, 'introductions': 2109, 'distance': 2110, 'records': 2111, 'styles': 2112, 'materials': 2113, 'dollar': 2114, 'efforts': 2115, 'assassin': 2116, 'files': 2117, 'novelist': 2118, 'release': 2119, 'storyteller': 2120, 'captivating': 2121, 'losing': 2122, 'positive': 2123, 'quiet': 2124, 'fame': 2125, 'birthday': 2126, 'hearts': 2127, 'observer': 2128, 'relations': 2129, 'asia': 2130, '—': 2131, '16': 2132, 'focuses': 2133, 'psychology': 2134, 'female': 2135, 'tough': 2136, 'diary': 2137, 'achieved': 2138, '20th': 2139, 'feeling': 2140, 'fail': 2141, 'normal': 2142, 'circumstances': 2143, 'hour': 2144, 'thrones': 2145, 'unprecedented': 2146, 'aims': 2147, 'introducing': 2148, 'surprise': 2149, 'nations': 2150, 'firm': 2151, 'rome': 2152, 'status': 2153, 'mass': 2154, 'senior': 2155, '2019': 2156, 'capture': 2157, 'saw': 2158, 'excellent': 2159, 'stands': 2160, 'enduring': 2161, 'brutally': 2162, 'asks': 2163, 'racing': 2164, 'attempts': 2165, 'rival': 2166, 'shooting': 2167, 'illuminating': 2168, 'personality': 2169, 'universal': 2170, 'champion': 2171, 'smith': 2172, 'iot': 2173, 'enable': 2174, 'murderer': 2175, 'oriented': 2176, 'tech': 2177, 'allow': 2178, 'origins': 2179, 'joe': 2180, 'islamic': 2181, 'lesson': 2182, 'dance': 2183, 'diagrams': 2184, 'acting': 2185, 'contents': 2186, 'solution': 2187, 'spanning': 2188, 'prevent': 2189, 'vibrant': 2190, 'corruption': 2191, 'germany': 2192, 'caste': 2193, 'proven': 2194, 'released': 2195, 'overview': 2196, 'operations': 2197, \"hbr's\": 2198, 'chosen': 2199, 'poignant': 2200, 'intermediate': 2201, 'sexual': 2202, 'elegant': 2203, 'landmark': 2204, 'provoking': 2205, 'member': 2206, 'logic': 2207, 'forty': 2208, 'phone': 2209, 'transformation': 2210, 'debate': 2211, 'hospital': 2212, 'fifth': 2213, 'aid': 2214, 'vs': 2215, 'maps': 2216, 'established': 2217, 'programme': 2218, 'informative': 2219, 'workbook': 2220, 'masters': 2221, 'steel': 2222, 'glory': 2223, 'gabriel': 2224, 'specially': 2225, 'touch': 2226, 'forest': 2227, 'conversations': 2228, 'linked': 2229, 'hotel': 2230, '‘the': 2231, 'ocean': 2232, 'haunting': 2233, '14': 2234, 'grown': 2235, 'regular': 2236, 'colleagues': 2237, 'setting': 2238, 'talented': 2239, 'shortlisted': 2240, 'lively': 2241, 'entirely': 2242, 'remember': 2243, 'schools': 2244, 'entrepreneur': 2245, 'trade': 2246, 'meant': 2247, 'songs': 2248, 'successfully': 2249, 'goals': 2250, 'trained': 2251, 'anticipated': 2252, 'rural': 2253, 'population': 2254, 'soldiers': 2255, 'dying': 2256, \"i've\": 2257, 'discussion': 2258, 'jean': 2259, 'robin': 2260, 'craft': 2261, 'mistakes': 2262, 'emperor': 2263, 'according': 2264, 'anne': 2265, 'murders': 2266, 'increasing': 2267, 'term': 2268, 'similar': 2269, 'check': 2270, '24': 2271, 'highlights': 2272, 'darkest': 2273, 'investigate': 2274, 'recently': 2275, 'ourselves': 2276, 'copy': 2277, 'foreword': 2278, 'solid': 2279, 'entertainment': 2280, 'board': 2281, 'that’s': 2282, 'safety': 2283, 'demon': 2284, 'devices': 2285, 'returning': 2286, 'higher': 2287, 'pop': 2288, 'valley': 2289, 'armed': 2290, 'pictures': 2291, 'reflect': 2292, 'low': 2293, \"won't\": 2294, 'theories': 2295, 'central': 2296, 'bourne': 2297, 'fine': 2298, 'finish': 2299, 'trusted': 2300, 'fat': 2301, 'percent': 2302, 'fundamentals': 2303, '500': 2304, 'mining': 2305, 'islam': 2306, 'combining': 2307, 'standards': 2308, 'icon': 2309, 'scandal': 2310, 'bridge': 2311, 'degree': 2312, 'superman': 2313, 'toward': 2314, 'delight': 2315, 'wake': 2316, 'trees': 2317, 'definitions': 2318, 'glimpse': 2319, 'brave': 2320, 'sharing': 2321, 'adult': 2322, 'appeared': 2323, 'flying': 2324, 'neither': 2325, 'aws': 2326, 'tables': 2327, 'assessment': 2328, 'elizabeth': 2329, 'produced': 2330, 'healing': 2331, 'today’s': 2332, 'complicated': 2333, 'ryan': 2334, 'spider': 2335, 'hopes': 2336, 'documents': 2337, 'unusual': 2338, 'ben': 2339, 'reflects': 2340, 'responsible': 2341, 'candid': 2342, \"children's\": 2343, 'roman': 2344, 'lincoln': 2345, 'arms': 2346, 'australia': 2347, 'oil': 2348, \"father's\": 2349, 'sarah': 2350, \"man's\": 2351, 'earlier': 2352, 'abilities': 2353, 'discipline': 2354, 'individuals': 2355, 'combination': 2356, 'sized': 2357, 'darkness': 2358, 'confidently': 2359, 'willing': 2360, 'proper': 2361, 'captures': 2362, 'rachel': 2363, 'politicians': 2364, 'verbs': 2365, 'repeated': 2366, '2015': 2367, 'structured': 2368, 'evening': 2369, 'bollywood': 2370, 'union': 2371, \"didn't\": 2372, 'designer': 2373, 'serial': 2374, 'decided': 2375, 'archie': 2376, 'authored': 2377, 'accused': 2378, 'arrives': 2379, 'hiding': 2380, 'actor': 2381, 'odds': 2382, 'beneath': 2383, 'emma': 2384, 'receives': 2385, 'pay': 2386, 'fought': 2387, 'jeeves': 2388, 'goku': 2389, 'believed': 2390, 'poirot': 2391, 'lifestyle': 2392, 'stake': 2393, 'dirk': 2394, 'internal': 2395, 'seat': 2396, 'shadows': 2397, 'defined': 2398, 'bear': 2399, 'raw': 2400, 'paintings': 2401, 'reporter': 2402, 'failure': 2403, 'adam': 2404, 'remained': 2405, 'massive': 2406, 'celebrity': 2407, 'fashion': 2408, '80': 2409, '2007': 2410, 'sketches': 2411, 'screenwriting': 2412, 'pack': 2413, 'struggling': 2414, 'practise': 2415, 'average': 2416, 'triumph': 2417, 'programmers': 2418, 'spend': 2419, 'russia': 2420, 'myth': 2421, 'standing': 2422, 'hill': 2423, 'simon': 2424, 'trapped': 2425, 'store': 2426, '21st': 2427, 'krishna': 2428, 'muscle': 2429, 'processing': 2430, 'ai': 2431, 'function': 2432, '700': 2433, 'genres': 2434, 'la': 2435, 'walking': 2436, 'handsome': 2437, 'secure': 2438, 'handle': 2439, 'explaining': 2440, 'grasp': 2441, 'reports': 2442, 'confident': 2443, 'worse': 2444, 'snow': 2445, 'defeat': 2446, 'hide': 2447, 'enigmatic': 2448, 'chronicle': 2449, 'belief': 2450, 'mighty': 2451, 'poems': 2452, 'thinkers': 2453, 'editor': 2454, 'decide': 2455, 'fly': 2456, 'reasons': 2457, 'artwork': 2458, 'argues': 2459, 'groups': 2460, 'benefit': 2461, 'raised': 2462, 'charts': 2463, '17': 2464, 'sleep': 2465, 'navigate': 2466, 'client': 2467, 'illustrate': 2468, 'forget': 2469, 'ups': 2470, 'pursuit': 2471, 'aspect': 2472, 'bbc': 2473, 'jenny': 2474, 'previously': 2475, 'dangers': 2476, \"read'\": 2477, 'h': 2478, 'hugely': 2479, 'confront': 2480, 'offered': 2481, 'radio': 2482, 'medium': 2483, 'holding': 2484, 'besides': 2485, 'hall': 2486, 'flow': 2487, 'wonders': 2488, 'injury': 2489, 'achievements': 2490, 'victims': 2491, 'sweeping': 2492, 'encounters': 2493, 'ad': 2494, 'bright': 2495, 'coast': 2496, 'luck': 2497, 'f': 2498, 'organized': 2499, 'constantly': 2500, 'asking': 2501, 'bit': 2502, 'birds': 2503, 'sporting': 2504, 'nuclear': 2505, 'strikes': 2506, 'size': 2507, 'measure': 2508, 'moral': 2509, 'particularly': 2510, 'direct': 2511, 'link': 2512, 'obsession': 2513, 'doubt': 2514, 'handy': 2515, 'extra': 2516, 'flight': 2517, 'improving': 2518, 'isn’t': 2519, 'pressure': 2520, 'discussed': 2521, 'voices': 2522, 'microservices': 2523, 'emerging': 2524, 'nowhere': 2525, 'absolute': 2526, 'unlike': 2527, 'scheme': 2528, 'capable': 2529, 'bar': 2530, \"country's\": 2531, 'climate': 2532, 'knight': 2533, 'hunter': 2534, 'host': 2535, 'glossary': 2536, 'ending': 2537, 'affairs': 2538, 'bestsellers': 2539, 'twist': 2540, 'stolen': 2541, 'actual': 2542, '13': 2543, 'clive': 2544, 'guided': 2545, 'runners': 2546, 'healthy': 2547, '·': 2548, 'objectives': 2549, 'lands': 2550, 'season': 2551, 'catch': 2552, 'slowly': 2553, 'soviet': 2554, 'issue': 2555, 'oliver': 2556, 'breaks': 2557, 'cartoonist': 2558, 'processes': 2559, 'window': 2560, 'grows': 2561, 'addresses': 2562, 'fantastic': 2563, 'bruce': 2564, 'jason': 2565, 'define': 2566, 'begun': 2567, 'comprehension': 2568, 'sample': 2569, 'charm': 2570, 'christian': 2571, 'searching': 2572, 'countless': 2573, 'chicago': 2574, 'jackson': 2575, 'errors': 2576, 'trinity': 2577, 'designing': 2578, 'governance': 2579, 'proves': 2580, 'coding': 2581, 'traditions': 2582, 'teachings': 2583, 'listen': 2584, 'jungle': 2585, 'moved': 2586, 'wealthy': 2587, 'tense': 2588, 'candidates': 2589, 'calling': 2590, 'parties': 2591, 'daring': 2592, '2009': 2593, 'thrown': 2594, 'recognition': 2595, 'typical': 2596, 'earned': 2597, 'curious': 2598, 'keyboard': 2599, 'tensorflow': 2600, 'lighting': 2601, 'muscles': 2602, 'diet': 2603, 'enter': 2604, 'recommended': 2605, 'expression': 2606, 'disappeared': 2607, 'angeles': 2608, '25': 2609, 'thomas': 2610, 'equal': 2611, 'address': 2612, 'jones': 2613, 'workings': 2614, 'perfectly': 2615, 'graduate': 2616, 'nazi': 2617, 'dies': 2618, 'dazzling': 2619, 'laughter': 2620, 'realize': 2621, 'interactive': 2622, 'storage': 2623, 'guidelines': 2624, 'connected': 2625, 'disease': 2626, 'fierce': 2627, 'blog': 2628, 'printed': 2629, 'puzzle': 2630, 'speaks': 2631, 'grew': 2632, 'larger': 2633, 'intended': 2634, 'builds': 2635, 'loving': 2636, 'failed': 2637, 'closely': 2638, 'legends': 2639, 'throne': 2640, 'institutions': 2641, 'escaped': 2642, 'yes': 2643, 'roots': 2644, 'lego': 2645, 'facebook': 2646, 'luke': 2647, 'imagine': 2648, 'index': 2649, 'doctors': 2650, 'maybe': 2651, 'incident': 2652, 'readable': 2653, 'glorious': 2654, 'spread': 2655, 'naruto': 2656, 'christmas': 2657, 'bitter': 2658, 'send': 2659, 'handbook': 2660, 'hardcover': 2661, 'appearance': 2662, 'disturbing': 2663, 'synonyms': 2664, 'syllabus': 2665, 'apps': 2666, 'initial': 2667, \"author's\": 2668, 'map': 2669, 'noble': 2670, 'radical': 2671, 'eleven': 2672, 'witnessed': 2673, 'shopping': 2674, 'tata': 2675, 'extremely': 2676, 'sign': 2677, 'putting': 2678, 'suspect': 2679, 'designers': 2680, 'description': 2681, 'origin': 2682, 'undergraduate': 2683, \"what's\": 2684, \"'one\": 2685, 'analyses': 2686, 'reputation': 2687, 'sweet': 2688, '“a': 2689, 'marry': 2690, 'prominent': 2691, 'reaching': 2692, 'artistic': 2693, 'continent': 2694, 'scope': 2695, 'appears': 2696, 'incredibly': 2697, 'devoted': 2698, 'endurance': 2699, 'nobel': 2700, 'document': 2701, 'welcome': 2702, 'eve': 2703, 'veteran': 2704, 'directly': 2705, 'villains': 2706, 'election': 2707, 'celebrate': 2708, 'sachin': 2709, 'clarity': 2710, 'translators': 2711, 'sons': 2712, 'defining': 2713, 'races': 2714, 'emotion': 2715, 'border': 2716, 'sides': 2717, 'june': 2718, 'los': 2719, 'differences': 2720, 'nightmare': 2721, 'complexity': 2722, 'g': 2723, 'essay': 2724, 'horror': 2725, 'dating': 2726, 'efficient': 2727, 'organizations': 2728, 'careers': 2729, 'charles': 2730, 'videos': 2731, 'tiny': 2732, '2010': 2733, 'industrial': 2734, 'crazy': 2735, 'per': 2736, 'lots': 2737, 'saying': 2738, 'likes': 2739, 'resist': 2740, 'corner': 2741, 'engineers': 2742, 'kinds': 2743, 'dan': 2744, 'pen': 2745, 'environmental': 2746, 'witness': 2747, 'file': 2748, 'seemed': 2749, 'britain': 2750, 'actions': 2751, 'obelix': 2752, 'premier': 2753, 'passed': 2754, 'rarely': 2755, 'lucid': 2756, 'crimes': 2757, 'trends': 2758, 'handling': 2759, 'portraits': 2760, 'fish': 2761, 'avengers': 2762, 'primary': 2763, 'virtual': 2764, 'exceptional': 2765, 'screenplay': 2766, 'photographers': 2767, 'effort': 2768, 'shattered': 2769, 'combat': 2770, 'embrace': 2771, 'philip': 2772, 'silent': 2773, 'ghost': 2774, 'site': 2775, 'delightful': 2776, 'cool': 2777, 'arranged': 2778, 'authority': 2779, 'heights': 2780, 'suffering': 2781, \"one's\": 2782, 'essentials': 2783, 'collaboration': 2784, 'youngest': 2785, 'everest': 2786, 'lethal': 2787, 'varied': 2788, 'painful': 2789, 'gained': 2790, 'immense': 2791, 'expect': 2792, 'enterprise': 2793, 'haunted': 2794, 'locked': 2795, 'citizens': 2796, 'except': 2797, 'threatened': 2798, 'lay': 2799, 'habits': 2800, 'boss': 2801, 'stronger': 2802, 'sought': 2803, 'outstanding': 2804, 'illustrator': 2805, 'imaginative': 2806, 'pair': 2807, 'recommendations': 2808, 'constant': 2809, 'depression': 2810, 'claire': 2811, 'creates': 2812, 'push': 2813, 'cruel': 2814, 'arthur': 2815, 'emerge': 2816, 'uncovers': 2817, 'mary': 2818, 'creatures': 2819, 'invisible': 2820, 'athlete': 2821, 'pulitzer': 2822, 'unravel': 2823, 'heard': 2824, 'colonial': 2825, 'child’s': 2826, 'clash': 2827, 'humble': 2828, 'net': 2829, 'guru': 2830, 'presentation': 2831, 'injuries': 2832, '40': 2833, 'publishers': 2834, 'base': 2835, 'apple': 2836, 'executive': 2837, 'center': 2838, 'silver': 2839, 'walter': 2840, 'greed': 2841, 'park': 2842, 'thirteen': 2843, 'camp': 2844, 'loud': 2845, 'marked': 2846, '22': 2847, 'jon': 2848, 'combined': 2849, 'networking': 2850, 'honesty': 2851, 'collections': 2852, 'connect': 2853, 'harrowing': 2854, 'applied': 2855, 'expertise': 2856, 'gates': 2857, 'threats': 2858, 'reviews': 2859, 'reporting': 2860, 'bone': 2861, 'concerns': 2862, 'guilt': 2863, 'concerned': 2864, 'analyze': 2865, 'expected': 2866, 'sharp': 2867, 'peak': 2868, 'monsters': 2869, 'royal': 2870, 'o': 2871, 'jr': 2872, 'roy': 2873, 'sixteen': 2874, 'bonus': 2875, 'caused': 2876, 'unable': 2877, 'collecting': 2878, 'entrepreneurs': 2879, 'fifteen': 2880, 'wait': 2881, 'demands': 2882, 'convinced': 2883, 'mathematical': 2884, 'popularity': 2885, 'potter': 2886, 'platforms': 2887, 'azure': 2888, 'hunting': 2889, 'ultra': 2890, 'increase': 2891, 'theoretical': 2892, 'codes': 2893, 'laid': 2894, 'unparalleled': 2895, '2005': 2896, 'implementation': 2897, 'managers': 2898, 'risks': 2899, 'serves': 2900, 'round': 2901, 'association': 2902, 'possibly': 2903, 'sophisticated': 2904, 'cult': 2905, 'bigger': 2906, 'indeed': 2907, 'interests': 2908, 'institute': 2909, 'illness': 2910, 'teenager': 2911, 'windows': 2912, 'accurate': 2913, 'workout': 2914, 'tackle': 2915, 'idioms': 2916, 'exist': 2917, 'winter': 2918, 'scratch': 2919, 'trial': 2920, 'animation': 2921, 'williams': 2922, 'breath': 2923, 'tigers': 2924, 'sword': 2925, \"'an\": 2926, 'mughal': 2927, 'emerged': 2928, 'requirements': 2929, 'units': 2930, 'finance': 2931, 'painting': 2932, 'gifted': 2933, 'engagement': 2934, 'promise': 2935, 'photographer': 2936, 'resistance': 2937, '‘a': 2938, 'beings': 2939, 'spelling': 2940, 'sort': 2941, 'agatha': 2942, 'blockchain': 2943, 'awareness': 2944, 'focusing': 2945, 'owner': 2946, 'knowing': 2947, 'dozens': 2948, 'modi': 2949, 'agents': 2950, 'genre': 2951, 'card': 2952, 'sidney': 2953, 'commentary': 2954, 'advantage': 2955, 'march': 2956, 'hair': 2957, 'distant': 2958, 'truths': 2959, 'pioneering': 2960, 'august': 2961, 'lettering': 2962, 'compilation': 2963, 'daughters': 2964, 'observations': 2965, 'depths': 2966, 'mythology': 2967, 'broke': 2968, 'enjoyed': 2969, \"'a'\": 2970, 'provocative': 2971, 'engaged': 2972, 'assassins': 2973, 'abandoned': 2974, 'nate': 2975, 'savage': 2976, 'surface': 2977, 'discovering': 2978, 'objective': 2979, 'extended': 2980, 'steven': 2981, 'warning': 2982, 'sheer': 2983, 'watching': 2984, 'mrs': 2985, 'obama': 2986, 'billionaire': 2987, 'einstein': 2988, 'cultures': 2989, 'comfort': 2990, 'stuff': 2991, 'tree': 2992, 'alternative': 2993, 'kinsella': 2994, 'ford': 2995, 'hogwarts': 2996, 'analytics': 2997, 'swimming': 2998, 'sudden': 2999, 'claims': 3000, 'enormous': 3001, 'buddha': 3002, 'campaigns': 3003, \"who's\": 3004, 'pleasure': 3005, 'gun': 3006, 'blockbuster': 3007, 'realises': 3008, \"wasn't\": 3009, 'monster': 3010, 'uncle': 3011, 'strings': 3012, 'direction': 3013, 'sixth': 3014, 'mid': 3015, \"couldn't\": 3016, 'combine': 3017, 'vividly': 3018, 'supposed': 3019, 'audiences': 3020, 'italian': 3021, 'superb': 3022, 'achievement': 3023, 'mad': 3024, 'instantly': 3025, 'arguments': 3026, 'movements': 3027, 'excitement': 3028, 'silence': 3029, 'calm': 3030, 'contribution': 3031, 'architect': 3032, 'managed': 3033, 'shy': 3034, 'grey': 3035, 'shades': 3036, 'manchester': 3037, 'treatment': 3038, 'keys': 3039, 'carry': 3040, 'african': 3041, 'amid': 3042, 'arrested': 3043, 'killers': 3044, 'heads': 3045, 'demonstrate': 3046, 'hat': 3047, 'youíll': 3048, 'governments': 3049, 'turner': 3050, 'wildly': 3051, 'charged': 3052, 'clancy': 3053, 'execution': 3054, 'watched': 3055, 'monkey': 3056, 'returned': 3057, 'kingdoms': 3058, 'investigator': 3059, 'woods': 3060, 'daniel': 3061, 'corrupt': 3062, 'throw': 3063, 'soldier': 3064, 'edited': 3065, 'republic': 3066, 'harsh': 3067, 'spot': 3068, 'anna': 3069, '…': 3070, 'vengeance': 3071, 'plenty': 3072, 'le': 3073, 'warm': 3074, 'workouts': 3075, 'authorities': 3076, 'older': 3077, 'acts': 3078, 'otherwise': 3079, 'naturally': 3080, 'coaches': 3081, 'maintain': 3082, 'package': 3083, 'regarded': 3084, 'civilization': 3085, 'sacred': 3086, 'libraries': 3087, 'loyalty': 3088, 'ronaldo': 3089, 'revered': 3090, 'n': 3091, 'roles': 3092, 'annual': 3093, 'planned': 3094, 'rivals': 3095, 'pitch': 3096, 'indira': 3097, 'attractive': 3098, 'medieval': 3099, 'presence': 3100, 'policies': 3101, 'usa': 3102, 'active': 3103, 'printing': 3104, 'descriptions': 3105, 'interpretation': 3106, 'graphics': 3107, 'jim': 3108, 'revelations': 3109, 'hardware': 3110, 'existing': 3111, 'examine': 3112, 'clients': 3113, 'eighteen': 3114, 'ken': 3115, 'sales': 3116, 'tribute': 3117, 'courses': 3118, 'diamond': 3119, 'compiled': 3120, 'joined': 3121, 'serving': 3122, 'relentless': 3123, 'passing': 3124, 'charge': 3125, 'andrew': 3126, 'male': 3127, 'ambitions': 3128, 'thoughtful': 3129, 'saving': 3130, 'dragons': 3131, 'fell': 3132, 'absorbing': 3133, 'colourful': 3134, 'signs': 3135, 'estate': 3136, 'threaten': 3137, 'blocks': 3138, 'releases': 3139, 'ring': 3140, 'notorious': 3141, 'thief': 3142, 'fastest': 3143, 'random': 3144, 'totally': 3145, 'curiosity': 3146, 'delves': 3147, 'classification': 3148, 'exhilarating': 3149, 'battling': 3150, 'soccer': 3151, \"student's\": 3152, 'z': 3153, 'cussler': 3154, 'actress': 3155, 'dick': 3156, 'anthony': 3157, 'editors': 3158, 'shift': 3159, 'engineer': 3160, 'establish': 3161, 'essence': 3162, 'philosophical': 3163, 'operating': 3164, 'guaranteed': 3165, 'continued': 3166, 'throws': 3167, 'bare': 3168, 'taylor': 3169, 'conditions': 3170, 'options': 3171, 'emergency': 3172, 'conclusion': 3173, 'placed': 3174, 'guilty': 3175, 'dedication': 3176, 'muslims': 3177, 'urgent': 3178, 'defence': 3179, 'linear': 3180, 'foremost': 3181, 'ghosts': 3182, 'photo': 3183, 'tim': 3184, 'stopping': 3185, 'oracle': 3186, 'loose': 3187, 'probably': 3188, 'diaries': 3189, 'profession': 3190, 'exhaustive': 3191, \"family's\": 3192, 'fourteen': 3193, 'spirited': 3194, 'perspectives': 3195, 'protection': 3196, 'broad': 3197, 'promises': 3198, 'demand': 3199, 'innocence': 3200, 'judy': 3201, 'duty': 3202, 'automation': 3203, 'formats': 3204, 'balls': 3205, 'ian': 3206, 'pace': 3207, 'convenient': 3208, 'bag': 3209, 'absolutely': 3210, 'classroom': 3211, \"city's\": 3212, 'commander': 3213, \"they've\": 3214, 'maximum': 3215, 'january': 3216, '300': 3217, 'allowed': 3218, 'myths': 3219, 'conquer': 3220, 'developer': 3221, 'startling': 3222, 'gender': 3223, 'presenting': 3224, 'pro': 3225, 'masterful': 3226, 'houses': 3227, 'operation': 3228, 'recognized': 3229, 'lightning': 3230, 'monthly': 3231, 'served': 3232, 'length': 3233, 'involving': 3234, 'directors': 3235, 'critically': 3236, 'achieving': 3237, 'you’ve': 3238, 'parallel': 3239, 'capacity': 3240, 'score': 3241, 'everybody': 3242, 'infrastructure': 3243, 'jokes': 3244, 'lack': 3245, 'stroke': 3246, 'tea': 3247, 'fair': 3248, 'don': 3249, 'champions': 3250, 'fateful': 3251, 'iii': 3252, 'creators': 3253, 'understood': 3254, 'informed': 3255, 'fallen': 3256, 'isolated': 3257, '400': 3258, 'precious': 3259, 'plane': 3260, 'edward': 3261, 'quotations': 3262, 'vicious': 3263, 'supported': 3264, 'deaths': 3265, 'tension': 3266, 'dive': 3267, 'mistake': 3268, 'madness': 3269, 'superstar': 3270, 'busy': 3271, 'foundations': 3272, 'anxiety': 3273, 'expanding': 3274, 'rapidly': 3275, 'logical': 3276, 'respect': 3277, 'mike': 3278, 'choosing': 3279, 'marks': 3280, 'directed': 3281, 'waste': 3282, 'nick': 3283, 'journeys': 3284, 'universities': 3285, 'committed': 3286, 'recovery': 3287, 'touching': 3288, 'demonstrates': 3289, 'kidnapped': 3290, 'businesses': 3291, 'minute': 3292, 'blend': 3293, 'circle': 3294, 'cia': 3295, 'engrossing': 3296, 'dinner': 3297, 'palace': 3298, 'wave': 3299, 'technological': 3300, 'invasion': 3301, 'fabulous': 3302, 'ended': 3303, 'matt': 3304, 'blow': 3305, 'clues': 3306, 'mood': 3307, 'wonderfully': 3308, 'francis': 3309, 'conducted': 3310, 'didn’t': 3311, 'sardar': 3312, 'mere': 3313, 'commitment': 3314, 'editorial': 3315, 'via': 3316, 'limited': 3317, 'wind': 3318, 'newspaper': 3319, 'destroyed': 3320, 'journalism': 3321, 'mortal': 3322, 'scales': 3323, 'reliable': 3324, 'caesar': 3325, 'archives': 3326, 'i’m': 3327, 'handwriting': 3328, 'punctuation': 3329, 'leg': 3330, 'kung': 3331, 'adopted': 3332, 'clark': 3333, 'allies': 3334, 'sit': 3335, 'van': 3336, 'messi': 3337, 'comfortable': 3338, 'conventional': 3339, '1970s': 3340, 'archer': 3341, 'hole': 3342, 'elusive': 3343, 'nobody': 3344, 'pushed': 3345, 'kapoor': 3346, 'shown': 3347, 'glass': 3348, 'they’re': 3349, 'cricketer': 3350, 'educational': 3351, 'dawn': 3352, 'formidable': 3353, 'currently': 3354, 'performing': 3355, 'climbing': 3356, 'river': 3357, 'stock': 3358, 'supports': 3359, 'electronic': 3360, 'markets': 3361, 'precise': 3362, 'renaissance': 3363, 'anywhere': 3364, 'speakers': 3365, 'graded': 3366, 'ingenious': 3367, 'collective': 3368, 'architectural': 3369, 'bizarre': 3370, 'charismatic': 3371, 'symbol': 3372, 'himalayas': 3373, 'species': 3374, 'clock': 3375, 'produce': 3376, 'newest': 3377, 'bible': 3378, 'messages': 3379, 'span': 3380, 'beliefs': 3381, 'eccentric': 3382, 'makers': 3383, 'row': 3384, 'reaches': 3385, '2001': 3386, 'saffron': 3387, 'investigative': 3388, 'dollars': 3389, 'trademark': 3390, 'count': 3391, 'calculus': 3392, 'heroic': 3393, 'october': 3394, 'false': 3395, 'skin': 3396, 'exchange': 3397, 'drugs': 3398, 'funniest': 3399, 'hired': 3400, 'kevin': 3401, 'consciousness': 3402, 'alexander': 3403, 'charlie': 3404, 'accomplished': 3405, 'gathering': 3406, 'sixty': 3407, 'mahatma': 3408, 'netflix': 3409, 'evergreen': 3410, '”—the': 3411, 'tear': 3412, 'expand': 3413, 'cycle': 3414, 'command': 3415, 'hate': 3416, 'php': 3417, 'rabbit': 3418, 'wizard': 3419, 'behaviour': 3420, 'mafia': 3421, 'kumar': 3422, 'grant': 3423, 'apis': 3424, 'adrian': 3425, 'kafka': 3426, 'seventeen': 3427, 'sector': 3428, 'reinforcement': 3429, 'condition': 3430, 'enables': 3431, 'hercule': 3432, 'focused': 3433, 'rid': 3434, 'backdrop': 3435, 'headed': 3436, 'san': 3437, 'fill': 3438, 'stages': 3439, 'revolves': 3440, 'chain': 3441, 'perilous': 3442, 'passions': 3443, 'infamous': 3444, 'waters': 3445, 'tutorials': 3446, 'heartbreak': 3447, 'grief': 3448, 'evocative': 3449, 'colorful': 3450, 'lifelong': 3451, 'mastering': 3452, 'elections': 3453, 'nationalism': 3454, 'politician': 3455, 'embarks': 3456, 'obsessed': 3457, 'pulls': 3458, 'eager': 3459, 'amount': 3460, 'asian': 3461, 'biographies': 3462, 'beach': 3463, 'accompany': 3464, 'sparks': 3465, 'lift': 3466, 'beat': 3467, 'alphabet': 3468, 'dramatically': 3469, 'associated': 3470, 'widow': 3471, 'defend': 3472, 'pitfalls': 3473, 'amateur': 3474, 'regime': 3475, 'affect': 3476, 'intricate': 3477, 'tongue': 3478, 'shapes': 3479, 'eminent': 3480, 'coffee': 3481, 'plants': 3482, 'advances': 3483, 'inheritance': 3484, 'extensively': 3485, 'stan': 3486, 'sitting': 3487, 'prey': 3488, 'girlfriend': 3489, 'strangers': 3490, 'crystal': 3491, 'pursue': 3492, 'california': 3493, 'display': 3494, 'mysteriously': 3495, 'thesaurus': 3496, 'economist': 3497, 'formula': 3498, 'careful': 3499, 'ninja': 3500, 'billions': 3501, 'looked': 3502, 'publications': 3503, 'integration': 3504, 'impressive': 3505, 'wins': 3506, 'glamorous': 3507, 'twisted': 3508, 'unit': 3509, 'origami': 3510, 'delivers': 3511, 'paths': 3512, 'household': 3513, '0': 3514, 'giant': 3515, 'pull': 3516, 'driving': 3517, 'bottom': 3518, 'fearless': 3519, 'deliver': 3520, 'bombay': 3521, 'gathered': 3522, 'chairman': 3523, 'tactics': 3524, 'suit': 3525, 'heavy': 3526, 'won’t': 3527, 'channel': 3528, 'manuscript': 3529, 'heir': 3530, 'maintaining': 3531, 'woven': 3532, 'systematic': 3533, 'route': 3534, 'lays': 3535, 'split': 3536, 'download': 3537, 'realistic': 3538, 'secretary': 3539, 'shock': 3540, 'alice': 3541, 'certainly': 3542, 'seasoned': 3543, 'spellbinding': 3544, 'mouse': 3545, 'roger': 3546, 'gang': 3547, 'ashes': 3548, 'stick': 3549, 'ill': 3550, 'afraid': 3551, '1947': 3552, 'rainbow': 3553, 'seminal': 3554, 'angel': 3555, 'desires': 3556, 'certification': 3557, 'ranks': 3558, 'sensational': 3559, 'triumphs': 3560, 'flash': 3561, 'quirky': 3562, 'colours': 3563, 'continuing': 3564, 'nominated': 3565, '60': 3566, 'cameras': 3567, 'dig': 3568, 'mischievous': 3569, 'barely': 3570, 'floor': 3571, '1971': 3572, 'temple': 3573, 'addictive': 3574, 'november': 3575, 'insider': 3576, 'assistant': 3577, 'earn': 3578, 'castle': 3579, 'egypt': 3580, 'mastery': 3581, 'seas': 3582, 'survived': 3583, 'grammatical': 3584, '1960s': 3585, 'courageous': 3586, 'drills': 3587, 'somehow': 3588, 'wherever': 3589, 'landscapes': 3590, 'julia': 3591, 'terrorists': 3592, 'meanings': 3593, 'realities': 3594, 'dust': 3595, 'followers': 3596, 'anger': 3597, 'separate': 3598, \"patel's\": 3599, 'destined': 3600, 'conflicts': 3601, 'joins': 3602, '19': 3603, 'afghanistan': 3604, 'jewish': 3605, 'aged': 3606, 'linux': 3607, 'bryson': 3608, 'construction': 3609, 'panic': 3610, 'bomb': 3611, 'contact': 3612, 'andy': 3613, 'sensation': 3614, 'acquire': 3615, 'foot': 3616, 'sensitive': 3617, 'peaceful': 3618, 'americans': 3619, 'odd': 3620, 'olympic': 3621, 'hanuman': 3622, 'tendulkar': 3623, 'lecture': 3624, 'harper': 3625, 'budding': 3626, 'noah': 3627, 'pokémon': 3628, 'spirits': 3629, 'scholar': 3630, 'exquisite': 3631, 'contain': 3632, 'admired': 3633, 'likely': 3634, 'turbulent': 3635, 'southern': 3636, 'directions': 3637, 'report': 3638, 'appropriate': 3639, 'cricketers': 3640, 'enlightenment': 3641, 'embark': 3642, 'signature': 3643, 'practitioners': 3644, 'arrays': 3645, 'burning': 3646, 'allon': 3647, \"we've\": 3648, 'experiments': 3649, 'criticism': 3650, 'onto': 3651, 'nights': 3652, 'underworld': 3653, 'hostile': 3654, 'longest': 3655, 'neil': 3656, 'pulled': 3657, 'retired': 3658, 'ace': 3659, 'fragile': 3660, 'tensions': 3661, 'drives': 3662, 'parent': 3663, 'guard': 3664, 'eternal': 3665, 'tournament': 3666, 'phenomenal': 3667, 'aim': 3668, 'climbers': 3669, 'richly': 3670, 'matches': 3671, 'susan': 3672, 'customers': 3673, 'stakes': 3674, 'stretch': 3675, 'causes': 3676, 'chase': 3677, 'delivery': 3678, 'visionary': 3679, 'couldn’t': 3680, 'teenage': 3681, 'loses': 3682, 'trace': 3683, 'farm': 3684, 'sustainable': 3685, 'producer': 3686, 'pakistani': 3687, 'stopped': 3688, 'kalam': 3689, 'stood': 3690, 'crack': 3691, 'stumbles': 3692, 'goodreads': 3693, 'crush': 3694, 'closest': 3695, 'alter': 3696, 'accidentally': 3697, 'swim': 3698, 'omnibus': 3699, 'vanished': 3700, 'proof': 3701, 'exposed': 3702, 'surveillance': 3703, 'ordered': 3704, 'superheroes': 3705, 'italy': 3706, 'distinctive': 3707, 'warriors': 3708, \"king's\": 3709, 'algorithm': 3710, 'cefr': 3711, 'developments': 3712, 'oldest': 3713, 'lights': 3714, 'respond': 3715, 'aptitude': 3716, 'hitler': 3717, 'billy': 3718, 'greek': 3719, 'below': 3720, 'awkward': 3721, 'accept': 3722, 'staff': 3723, 'rely': 3724, 'describing': 3725, 'deepest': 3726, 'congress': 3727, 'amongst': 3728, 'capturing': 3729, 'celebrates': 3730, 'underlying': 3731, 'editor’s': 3732, 'behavior': 3733, 'kannada': 3734, 'holiday': 3735, 'pregnant': 3736, 'destination': 3737, 'paid': 3738, 'wildlife': 3739, 'brands': 3740, 'manages': 3741, 'se': 3742, 'jeffrey': 3743, 'html': 3744, 'opened': 3745, 'railway': 3746, 'fu': 3747, 'garden': 3748, 'irish': 3749, 'collect': 3750, 'rage': 3751, 'drug': 3752, 'eighty': 3753, 'response': 3754, 'biting': 3755, 'hunted': 3756, 'abstract': 3757, 'survivors': 3758, 'urdu': 3759, 'boat': 3760, 'expedition': 3761, 'awaited': 3762, 'www': 3763, 'attraction': 3764, 'dave': 3765, 'surrounded': 3766, 'shop': 3767, 'capabilities': 3768, 'routine': 3769, 'parliament': 3770, 'prestigious': 3771, 'tracy': 3772, 'lawyers': 3773, 'observation': 3774, 'animated': 3775, 'drone': 3776, 'accompanying': 3777, 'increased': 3778, 'guns': 3779, 'highs': 3780, 'victory': 3781, 'stark': 3782, 'measures': 3783, 'wider': 3784, 'counter': 3785, 'runner': 3786, 'fault': 3787, 'credit': 3788, 'learnt': 3789, 'expose': 3790, 'specifically': 3791, 'ivan': 3792, '21': 3793, 'interact': 3794, 'ruin': 3795, 'lower': 3796, 'proud': 3797, 'endless': 3798, 'author’s': 3799, 'anthology': 3800, 'memoirs': 3801, 'antonyms': 3802, 'pride': 3803, 'ethics': 3804, '26': 3805, 'sole': 3806, 'investigating': 3807, 'sacrifice': 3808, 'theme': 3809, 'marathon': 3810, 'refuses': 3811, 'tribe': 3812, 'voyage': 3813, 'novice': 3814, 'emotionally': 3815, 'banks': 3816, 'honour': 3817, 'matthew': 3818, 'trials': 3819, 'inimitable': 3820, 'assassination': 3821, 'al': 3822, 'narratives': 3823, 'booker': 3824, 'ted': 3825, 'taste': 3826, 'importantly': 3827, 'criminals': 3828, 'tortured': 3829, 'finished': 3830, 'rate': 3831, 'marie': 3832, 'lore': 3833, 'revision': 3834, 'bell': 3835, 'apache': 3836, 'deploy': 3837, 'equipment': 3838, 'devotion': 3839, 'courtney': 3840, 'witcher': 3841, 'maus': 3842, 'clue': 3843, 'engage': 3844, 'martin’s': 3845, 'realm': 3846, 'bent': 3847, 'weapons': 3848, 'positions': 3849, 'soft': 3850, 'bones': 3851, 'orphan': 3852, 'keen': 3853, 'narrated': 3854, 'studio': 3855, 'reinforced': 3856, 'dates': 3857, 'plain': 3858, 'nail': 3859, 'defeated': 3860, 'connections': 3861, 'describe': 3862, 'july': 3863, 'beginnings': 3864, 'yorker': 3865, '2000': 3866, 'chemistry': 3867, 'notebook': 3868, 'fights': 3869, \"woman's\": 3870, 'lows': 3871, 'failures': 3872, 'clean': 3873, 'bengal': 3874, 'mysql': 3875, 'surrounding': 3876, 'photographic': 3877, 'usual': 3878, 'session': 3879, '150': 3880, 'references': 3881, 'christopher': 3882, 'interior': 3883, 'historic': 3884, 'buildings': 3885, 'coaching': 3886, 'scoring': 3887, 'sheets': 3888, 'blake': 3889, 'anonymous': 3890, 'tremendous': 3891, 'moscow': 3892, 'invented': 3893, 'victorian': 3894, 'darker': 3895, 'friendships': 3896, 'brink': 3897, 'fighter': 3898, 'programmer': 3899, 'patients': 3900, 'disappears': 3901, 'meticulously': 3902, 'horrors': 3903, 'relevance': 3904, 'staying': 3905, 'fairy': 3906, 'timely': 3907, 'st': 3908, 'narrator': 3909, 'commerce': 3910, 'tumultuous': 3911, 'corpse': 3912, 'showcases': 3913, 'espionage': 3914, 'holy': 3915, 'appreciation': 3916, 'mentor': 3917, 'upside': 3918, 'consider': 3919, 'joining': 3920, 'constitution': 3921, 'kings': 3922, 'wing': 3923, 'tracks': 3924, 'preface': 3925, 'hindol': 3926, 'component': 3927, 'thanks': 3928, 'cunning': 3929, 'ghoul': 3930, 'trump': 3931, 'locate': 3932, 'albert': 3933, 'iraq': 3934, 'gap': 3935, 'trainer': 3936, 'crowd': 3937, 'folk': 3938, 'commercial': 3939, 'stretching': 3940, 'literally': 3941, 'exposure': 3942, 'nazis': 3943, 'launch': 3944, 'defense': 3945, 'speaker': 3946, 'confused': 3947, 'shiva': 3948, 'array': 3949, 'consists': 3950, 'mothers': 3951, 'lucky': 3952, 'bengali': 3953, 'wooster': 3954, 'merely': 3955, 'breathing': 3956, \"'this\": 3957, 'opponent': 3958, 'sri': 3959, 'alien': 3960, 'wishes': 3961, 'thrill': 3962, 'medal': 3963, 'arrived': 3964, 'layout': 3965, 'twitter': 3966, 'sell': 3967, 'rooms': 3968, 'emerges': 3969, 'graphs': 3970, 'israeli': 3971, 'launched': 3972, 'roll': 3973, 'exact': 3974, 'summit': 3975, 'ear': 3976, 'instalment': 3977, 'wilderness': 3978, 'attitude': 3979, 'unleashed': 3980, '2020': 3981, 'despair': 3982, 'jonathan': 3983, 'rosie': 3984, 'liberal': 3985, 'jargon': 3986, 'february': 3987, 'pressures': 3988, 'souls': 3989, 'nineteenth': 3990, 'implementing': 3991, 'haunt': 3992, 'heavily': 3993, 'hailed': 3994, 'lets': 3995, '2003': 3996, 'coloring': 3997, 'enthusiasts': 3998, '1980s': 3999, 'langdon': 4000, 'motivation': 4001, 'travelled': 4002, 'lewis': 4003, 'solo': 4004, 'pursued': 4005, 'unicorn': 4006, 'larry': 4007, 'opinion': 4008, 'arm': 4009, 'prior': 4010, 'tokyo': 4011, 'shadowy': 4012, 'limit': 4013, 'consistently': 4014, 'virtually': 4015, 'plots': 4016, 'regional': 4017, 'introductory': 4018, 'jump': 4019, 'arrival': 4020, 'judges': 4021, 'dreamed': 4022, 'towering': 4023, 'blasts': 4024, 'journalists': 4025, 'talents': 4026, 'wanting': 4027, 'twice': 4028, 'hills': 4029, 'destructive': 4030, 'integrated': 4031, 'ruled': 4032, 'pure': 4033, 'w': 4034, 'democratic': 4035, 'somewhere': 4036, \"mother's\": 4037, 'injured': 4038, 'deluxe': 4039, 'majority': 4040, 'factors': 4041, 'arrive': 4042, 'please': 4043, 'spark': 4044, 'exposes': 4045, 'department': 4046, 'currency': 4047, 'opposite': 4048, 'dialogue': 4049, 'seventh': 4050, 'analytical': 4051, 'block': 4052, 'guests': 4053, 'redemption': 4054, 'station': 4055, 'pretty': 4056, 'musician': 4057, 'plotting': 4058, 'revelatory': 4059, 'significance': 4060, 'stretches': 4061, \"women's\": 4062, 'teeth': 4063, 'elaborate': 4064, 'retirement': 4065, 'maria': 4066, 'midst': 4067, 'circus': 4068, 'symbols': 4069, 'spell': 4070, 'muhammad': 4071, 'forbidden': 4072, 'america’s': 4073, 'monitor': 4074, 'wodehouse': 4075, 'bertie': 4076, 'deception': 4077, 'sat': 4078, 'amy': 4079, 'horrific': 4080, 'logan': 4081, 'maker': 4082, 'editing': 4083, 'transforms': 4084, 'relax': 4085, 'exotic': 4086, 'dear': 4087, 'uplifting': 4088, 'shall': 4089, 'sanskrit': 4090, 'ruler': 4091, 'databases': 4092, 'brutality': 4093, 'scores': 4094, 'underground': 4095, 'myriad': 4096, 'shots': 4097, 'passes': 4098, 'celebrities': 4099, 'toughest': 4100, 'profile': 4101, 'declared': 4102, 'unfolds': 4103, 'visits': 4104, 'gifts': 4105, 'ran': 4106, 'frozen': 4107, 'crossing': 4108, 'awesome': 4109, 'implications': 4110, 'expectations': 4111, 'fry': 4112, 'empathy': 4113, 'apartment': 4114, 'transport': 4115, 'teen': 4116, 'toy': 4117, 'routines': 4118, 'contained': 4119, 'privacy': 4120, \"people's\": 4121, \"we're\": 4122, 'shifts': 4123, 'scenarios': 4124, 'usually': 4125, 'partners': 4126, 'commonly': 4127, 'louis': 4128, 'radically': 4129, 'priceless': 4130, '1999': 4131, 'backed': 4132, 'himalayan': 4133, 'diversity': 4134, 'kiss': 4135, 'recognise': 4136, 'continents': 4137, 'comedian': 4138, 'fatal': 4139, 'enters': 4140, 'boundaries': 4141, 'enthralling': 4142, 'jawaharlal': 4143, 'navy': 4144, 'businessman': 4145, 'foe': 4146, 'glittering': 4147, 'hybrid': 4148, 'fiercely': 4149, 'ed': 4150, 'alan': 4151, 'determine': 4152, 'chasing': 4153, 'brains': 4154, 'sorting': 4155, 'progression': 4156, 'yle': 4157, 'striking': 4158, 'educated': 4159, 'narration': 4160, '1996': 4161, 'settle': 4162, 'clothes': 4163, 'investment': 4164, 'discussing': 4165, 'unpredictable': 4166, 'warming': 4167, 'amidst': 4168, 'subcontinent': 4169, 'rom': 4170, 'numa': 4171, 'cry': 4172, 'letting': 4173, 'hits': 4174, 'debates': 4175, 'officers': 4176, 'knife': 4177, 'knack': 4178, 'twin': 4179, 'simplicity': 4180, 'pointers': 4181, 'philosopher': 4182, 'mouth': 4183, 'unlock': 4184, 'frequency': 4185, 'giants': 4186, 'suggests': 4187, 'shoulder': 4188, 'spine': 4189, 'studied': 4190, 'mini': 4191, 'mesmerizing': 4192, 'buying': 4193, 'sciences': 4194, 'suspects': 4195, 'realise': 4196, 'julius': 4197, 'forests': 4198, 'zero': 4199, 'desperately': 4200, 'zoo': 4201, 'showed': 4202, 'tinkle': 4203, 'leverage': 4204, 'influenced': 4205, 'paints': 4206, 'turmoil': 4207, 'enjoyable': 4208, 'surprisingly': 4209, 'founding': 4210, 'zen': 4211, 'lanka': 4212, 'sights': 4213, 'kindness': 4214, 'apparent': 4215, 'modules': 4216, 'consultants': 4217, 'cell': 4218, 'cash': 4219, 'composed': 4220, 'finalist': 4221, 'layer': 4222, 'secretly': 4223, 'fictional': 4224, 'horse': 4225, \"he'd\": 4226, 'strips': 4227, 'occur': 4228, 'weather': 4229, 'atlantic': 4230, 'deceit': 4231, 'sworn': 4232, 'murderous': 4233, 'nineteen': 4234, 'feminism': 4235, 'invites': 4236, 'realized': 4237, 'pillars': 4238, 'crafted': 4239, 'batsman': 4240, 'carrying': 4241, 'formed': 4242, 'domestic': 4243, 'traffic': 4244, 'mankind': 4245, 'rahul': 4246, 'earliest': 4247, 'terminology': 4248, 'climber': 4249, 'mexico': 4250, 'upper': 4251, 'certified': 4252, 'profoundly': 4253, 'item': 4254, 'resolve': 4255, 'adapt': 4256, 'footballer': 4257, 'architects': 4258, 'grab': 4259, 'jordan': 4260, 'philosophers': 4261, 'motivate': 4262, 'searches': 4263, 'microservice': 4264, 'hottest': 4265, 'happily': 4266, 'kicking': 4267, 'scientifically': 4268, 'genuine': 4269, 'raising': 4270, 'gym': 4271, 'sends': 4272, 'collins': 4273, 'pilot': 4274, 'famed': 4275, 'masses': 4276, 'smile': 4277, '1000': 4278, 'restaurant': 4279, 'prepares': 4280, 'buddhist': 4281, 'luffy': 4282, 'greatness': 4283, 'indomitable': 4284, 'locke': 4285, 'catching': 4286, 'aunt': 4287, 'heading': 4288, 'haddock': 4289, 'tired': 4290, 'tibet': 4291, 'grip': 4292, 'appendices': 4293, 'newspapers': 4294, 'pounding': 4295, 'discoveries': 4296, 'triumphant': 4297, 'mistress': 4298, 'jeff': 4299, 'pros': 4300, 'branding': 4301, 'inevitable': 4302, 'raj': 4303, 'doom': 4304, 'strengths': 4305, '1930s': 4306, 'visually': 4307, 'boring': 4308, 'restore': 4309, 'abroad': 4310, 'escapes': 4311, 'da': 4312, '2002': 4313, 'perfection': 4314, 'portrayal': 4315, 'rocks': 4316, 'country’s': 4317, 'updates': 4318, 'inspector': 4319, 'disappearance': 4320, 'mention': 4321, 'autobiographical': 4322, 'bullets': 4323, 'largely': 4324, 'treat': 4325, 'contributions': 4326, 'constructed': 4327, 'mixing': 4328, 'musicians': 4329, 'roberts': 4330, 'lean': 4331, 'tone': 4332, 'francisco': 4333, 'kane': 4334, 'bravery': 4335, 'jojo': 4336, 'chandra': 4337, 'domain': 4338, 'enthusiasm': 4339, 'lily': 4340, 'summary': 4341, 'sites': 4342, 'kitchen': 4343, 'primarily': 4344, 'nlp': 4345, 'attempting': 4346, 'concurrency': 4347, 'sebastian': 4348, 'stream': 4349, 'teammates': 4350, 'notice': 4351, 'tagore': 4352, 'guys': 4353, 'curriculum': 4354, 'watercolor': 4355, 'streams': 4356, 'activist': 4357, 'nutrition': 4358, 'midnight': 4359, 'progressive': 4360, 'manning': 4361, 'seo': 4362, 'cds': 4363, 'biology': 4364, 'seeing': 4365, 'lake': 4366, 'css': 4367, 'incorporates': 4368, 'handful': 4369, 'mindfulness': 4370, 'compassionate': 4371, 'museum': 4372, 'villain': 4373, 'survey': 4374, 'aftermath': 4375, 'mostly': 4376, 'incisive': 4377, 'unflinching': 4378, 'irreverent': 4379, 'lectures': 4380, 'reflections': 4381, 'uniquely': 4382, 'sheldon': 4383, 'what’s': 4384, 'interactions': 4385, 'revelation': 4386, 'strategic': 4387, 'hatred': 4388, 'crossed': 4389, 'minor': 4390, 'properly': 4391, 'pivotal': 4392, 'stuck': 4393, 'evolved': 4394, 'organisation': 4395, 'therefore': 4396, 'officials': 4397, 'partition': 4398, 'lane': 4399, 'rain': 4400, 'council': 4401, 'december': 4402, 'april': 4403, 'reinforce': 4404, 'heroine': 4405, 'efficiently': 4406, 'tony': 4407, 'rings': 4408, 'passengers': 4409, 'scripts': 4410, 'morgan': 4411, 'iran': 4412, 'collapse': 4413, 'eisner': 4414, 'engine': 4415, 'recognize': 4416, 'treacherous': 4417, 'passage': 4418, 'beer': 4419, 'gather': 4420, 'stirring': 4421, 'maximize': 4422, 'chart': 4423, 'hoping': 4424, 'faithful': 4425, 'sin': 4426, 'miller': 4427, 'discuss': 4428, 'enhancing': 4429, 'lasting': 4430, 'location': 4431, 'texas': 4432, 'man’s': 4433, 'critics': 4434, 'horrifying': 4435, 'attracted': 4436, 'thrive': 4437, '1973': 4438, 'instincts': 4439, 'climb': 4440, 'watson': 4441, 'bride': 4442, 'customer': 4443, \"book'\": 4444, 'retelling': 4445, 'illustrates': 4446, 'nose': 4447, 'sleepy': 4448, 'helen': 4449, 'weaves': 4450, 'ensures': 4451, 'geographic': 4452, 'patient': 4453, 'tomorrow': 4454, 'duke': 4455, 'overwhelming': 4456, 'wear': 4457, 'barack': 4458, 'abraham': 4459, 'superpower': 4460, 'spin': 4461, 'analyst': 4462, 'hostage': 4463, 'titled': 4464, 'prisoner': 4465, 'loyal': 4466, 'possibilities': 4467, 'becky': 4468, 'acclaim': 4469, 'witnesses': 4470, 'transition': 4471, 'dominated': 4472, 'torture': 4473, 'rocket': 4474, 'gavin': 4475, 'superpowers': 4476, 'issued': 4477, 'prep': 4478, 'sonic': 4479, 'expertly': 4480, 'survivor': 4481, 'rebus': 4482, 'cognitive': 4483, 'maths': 4484, 'develops': 4485, 'sita': 4486, 'qualities': 4487, 'nice': 4488, \"parents'\": 4489, 'cousin': 4490, 'corbett': 4491, 'challenged': 4492, 'arlen': 4493, 'flexibility': 4494, 'cricketing': 4495, 'gre': 4496, 'kernel': 4497, 'shocked': 4498, 'dealt': 4499, 'chi': 4500, 'efficiency': 4501, 'weaving': 4502, 'ongoing': 4503, 'trading': 4504, 'homeland': 4505, 'franchise': 4506, 'barcelona': 4507, 'correctly': 4508, 'inhabitants': 4509, 'formal': 4510, 'colleague': 4511, 'trap': 4512, 'ross': 4513, 'trauma': 4514, 'archive': 4515, 'reasoning': 4516, 'atmosphere': 4517, 'daisy': 4518, 'remembers': 4519, 'mount': 4520, 'sanjay': 4521, 'repertoire': 4522, 'recordings': 4523, 'heartfelt': 4524, 'untold': 4525, 'propaganda': 4526, 'fiery': 4527, 'investigates': 4528, 'rogue': 4529, 'wry': 4530, 'aimed': 4531, 'crown': 4532, '23': 4533, 'inequality': 4534, 'successes': 4535, 'forth': 4536, 'hungry': 4537, 'angry': 4538, 'attempted': 4539, 'critique': 4540, 'heartbreaking': 4541, 'judge': 4542, 'pioneer': 4543, 'en': 4544, 'colleges': 4545, 'mayhem': 4546, 'vote': 4547, 'uncertainty': 4548, 'postgraduate': 4549, 'argument': 4550, \"'c'\": 4551, 'conquered': 4552, 'complexities': 4553, 'productive': 4554, 'perceptive': 4555, 'sara': 4556, 'brian': 4557, 'spoke': 4558, 'unravels': 4559, 'playful': 4560, 'profit': 4561, 'sexuality': 4562, 'tracing': 4563, 'threatening': 4564, 'boarding': 4565, 'undercover': 4566, 'performances': 4567, 'athletic': 4568, 'maxwell': 4569, 'quintessential': 4570, 'church': 4571, 'discussions': 4572, 'flesh': 4573, 'narrow': 4574, 'regions': 4575, 'administrator': 4576, 'spirituality': 4577, 'thin': 4578, 'odyssey': 4579, 'genghis': 4580, 'deadliest': 4581, 'framed': 4582, 'legs': 4583, 'scenario': 4584, 'generate': 4585, 'concentration': 4586, 'seller': 4587, 'update': 4588, 'mega': 4589, 'ignore': 4590, 'establishing': 4591, 'spends': 4592, 'zone': 4593, 'bend': 4594, 'climax': 4595, 'novella': 4596, 'device': 4597, 'passages': 4598, 'glamour': 4599, 'overall': 4600, 'arctic': 4601, 'proving': 4602, 'entertain': 4603, 'gentle': 4604, 'hence': 4605, 'gauls': 4606, 'examining': 4607, '250': 4608, 'harder': 4609, 'unless': 4610, 'appendix': 4611, 'arguably': 4612, 'precisely': 4613, 'prisoners': 4614, 'remi': 4615, 'plague': 4616, 'clare': 4617, 'ruth': 4618, 'drop': 4619, 'instruments': 4620, 'moyes': 4621, 'equality': 4622, 'scary': 4623, 'wondered': 4624, 'predict': 4625, 'electric': 4626, 'damon': 4627, 'seriously': 4628, 'avoiding': 4629, 'suspenseful': 4630, '2006': 4631, 'monitoring': 4632, 'cure': 4633, 'dialogues': 4634, 'unfamiliar': 4635, 'attain': 4636, 'adventurous': 4637, 'northern': 4638, 'verb': 4639, 'galaxy': 4640, 'embedded': 4641, 'wrenching': 4642, 'kit': 4643, 'compassion': 4644, 'suffered': 4645, 'spectrum': 4646, 'programmes': 4647, 'external': 4648, 'tender': 4649, 'astounding': 4650, 'iv': 4651, 'brazil': 4652, 'unpublished': 4653, 'eastern': 4654, 'evaluate': 4655, 'rumours': 4656, 'jennifer': 4657, 'picked': 4658, 'thea': 4659, '101': 4660, 'crashes': 4661, 'reign': 4662, 'jealousy': 4663, 'reminder': 4664, \"it'\": 4665, 'communist': 4666, 'bored': 4667, 'cats': 4668, 'swift': 4669, 'idyllic': 4670, 'gotham': 4671, 'agassi': 4672, 'intent': 4673, 'aids': 4674, 'fifa': 4675, 'spending': 4676, 'corridors': 4677, 'tribes': 4678, 'conduct': 4679, 'cook': 4680, '2004': 4681, 'configuration': 4682, 'waking': 4683, 'demanding': 4684, 'targeted': 4685, 'schemes': 4686, 'inc': 4687, 'executives': 4688, 'answering': 4689, 'manufacturing': 4690, 'extent': 4691, 'kills': 4692, 'boston': 4693, 'debt': 4694, 'carol': 4695, 'cheeky': 4696, 'weaknesses': 4697, 'frequent': 4698, 'accuracy': 4699, 'inherited': 4700, 'initially': 4701, '27': 4702, 'affordable': 4703, \"century's\": 4704, 'square': 4705, 'stimulating': 4706, 'brush': 4707, '1998': 4708, 'quantum': 4709, 'pirates': 4710, 'territory': 4711, 'motor': 4712, 'screenwriter': 4713, 'devil': 4714, 'dvd': 4715, 'straightforward': 4716, 'superhero': 4717, 'intensity': 4718, 'bat': 4719, 'ideals': 4720, 'greece': 4721, 'inherent': 4722, 'enchanting': 4723, 'competitors': 4724, 'acknowledged': 4725, 'clauses': 4726, 'versions': 4727, 'drinking': 4728, 'mahabharata': 4729, 'dinosaurs': 4730, 'flat': 4731, 'thread': 4732, 'sparkling': 4733, 'malcolm': 4734, 'element': 4735, 'producing': 4736, '1993': 4737, 'hunger': 4738, 'conceptual': 4739, 'struck': 4740, 'rush': 4741, 'allowing': 4742, 'reclaim': 4743, 'monumental': 4744, 'flora': 4745, 'crushing': 4746, 'settled': 4747, 'audacious': 4748, 'cope': 4749, 'walls': 4750, 'phase': 4751, 'privileged': 4752, 'agrees': 4753, '1990s': 4754, 'modeling': 4755, 'pupil': 4756, 'raise': 4757, 'leonardo': 4758, 'nicholas': 4759, 'supernatural': 4760, 'goddess': 4761, 'immediate': 4762, 'jungles': 4763, 'revolutionized': 4764, 'syntax': 4765, 'touches': 4766, 'yellow': 4767, 'nasty': 4768, 'confronted': 4769, 'rapid': 4770, 'battlefield': 4771, 'lara': 4772, 'recorded': 4773, 'narendra': 4774, 'requires': 4775, 'cried': 4776, 'singing': 4777, 'categories': 4778, 'supplement': 4779, 'purchase': 4780, 'sale': 4781, 'hanging': 4782, 'email': 4783, 'geography': 4784, 'fishburne': 4785, 'portfolio': 4786, 'olympics': 4787, 'widespread': 4788, 'bang': 4789, 'beast': 4790, 'angular': 4791, 'conditioning': 4792, 'pregnancy': 4793, 'digest': 4794, 'ambedkar': 4795, 'dare': 4796, 'doors': 4797, 'precision': 4798, 'ingredients': 4799, 'laurie': 4800, 'malala': 4801, 'reportage': 4802, 'paint': 4803, 'oprah': 4804, 'triathlon': 4805, 'tax': 4806, 'cube': 4807, 'fabric': 4808, 'bodybuilding': 4809, 'serverless': 4810, 'sheds': 4811, 'suspicion': 4812, 'heal': 4813, 'dilemma': 4814, 'pirate': 4815, 'monastery': 4816, 'scholarship': 4817, 'solitude': 4818, 'consistent': 4819, 'godfather': 4820, 'optimize': 4821, 'phones': 4822, 'existed': 4823, 'namely': 4824, 'vol': 4825, 'collectible': 4826, 'performed': 4827, 'suspicious': 4828, 'singer': 4829, 'swedish': 4830, 'delivered': 4831, 'un': 4832, 'heroism': 4833, 'ruins': 4834, \"book's\": 4835, 'longing': 4836, 'convey': 4837, 'acceptance': 4838, 'trick': 4839, 'charms': 4840, 'sellers': 4841, 'neighbours': 4842, 'hearted': 4843, 'drones': 4844, 'terrific': 4845, 'targets': 4846, 'relentlessly': 4847, 'clouds': 4848, 'ancestors': 4849, 'razor': 4850, 'singular': 4851, 'heritage': 4852, 'applying': 4853, 'succeeded': 4854, 'interface': 4855, 'roller': 4856, 'luxury': 4857, 'procedures': 4858, 'pursuing': 4859, 'headlines': 4860, 'fortunes': 4861, 'afternoon': 4862, 'championships': 4863, 'encountered': 4864, 'confusion': 4865, 'lawrence': 4866, 'unleash': 4867, 'immersive': 4868, 'assault': 4869, 'jake': 4870, 'vietnam': 4871, 'ms': 4872, 'ccna': 4873, 'delve': 4874, 'ethical': 4875, 'highlighting': 4876, 'downs': 4877, 'perception': 4878, 'evokes': 4879, 'loveable': 4880, 'candidate': 4881, \"say'\": 4882, 'weird': 4883, 'danielle': 4884, 'animator': 4885, 'downloadable': 4886, 'lucy': 4887, 'separately': 4888, 'subtle': 4889, 'transforming': 4890, 'magazines': 4891, 'brandon': 4892, 'yours': 4893, 'mixed': 4894, 'delivering': 4895, 'congo': 4896, 'ignored': 4897, 'occasionally': 4898, 'gaul': 4899, 'sigma': 4900, 'tesla': 4901, 'manipulate': 4902, 'activists': 4903, 'regularly': 4904, 'consultant': 4905, 'tracking': 4906, 'submarine': 4907, 'dogs': 4908, 'rolls': 4909, 'doomed': 4910, 'builder': 4911, 'consumer': 4912, 'unimaginable': 4913, 'thrust': 4914, 'reached': 4915, 'slow': 4916, 'isis': 4917, 'grandmother': 4918, 'inter': 4919, \"they'll\": 4920, 'outlines': 4921, 'beasts': 4922, 'workflows': 4923, 'bose': 4924, 'q': 4925, 'integral': 4926, 'colors': 4927, 'folding': 4928, 'rape': 4929, 'drove': 4930, 'exile': 4931, 'strengthen': 4932, 'module': 4933, 'kurt': 4934, 'austin': 4935, 'frightening': 4936, 'shoot': 4937, 'raymond': 4938, 'poker': 4939, 'hannah': 4940, 'duo': 4941, 'elementary': 4942, 'tomb': 4943, 'historian': 4944, 'cyber': 4945, 'johnson': 4946, 'mile': 4947, 'minded': 4948, 'leigh': 4949, 'slums': 4950, 'attitudes': 4951, '‘i': 4952, 'reflecting': 4953, 'appetite': 4954, 'investigations': 4955, 'gentleman': 4956, 'secondary': 4957, 'thriving': 4958, 'illuminates': 4959, 'fred': 4960, 'mermaid': 4961, 'calcutta': 4962, 'math': 4963, 'knees': 4964, 'twisty': 4965, 'notable': 4966, 'dot': 4967, 'verbal': 4968, 'courtroom': 4969, 'prophet': 4970, 'venture': 4971, 'satire': 4972, 'cooking': 4973, 'seal': 4974, 'listed': 4975, 'louise': 4976, 'pacific': 4977, 'tutorial': 4978, 'berlin': 4979, 'error': 4980, \"life's\": 4981, 'divine': 4982, 'entrance': 4983, 'br': 4984, 'critic': 4985, 'painted': 4986, 'virus': 4987, 'mars': 4988, 'whilst': 4989, 'controversy': 4990, 'opinions': 4991, 'leisure': 4992, 'regression': 4993, 'crises': 4994, 'rice': 4995, 'industries': 4996, 'catherine': 4997, 'hinduism': 4998, 'selenium': 4999, 'feluda': 5000, 'environments': 5001, 'definitely': 5002, 'founded': 5003, 'nepal': 5004, 'malware': 5005, 'fonts': 5006, 'ka': 5007, 'hunters': 5008, 'mythical': 5009, 'masterpieces': 5010, 'awe': 5011, 'unrivalled': 5012, 'healthier': 5013, 'budget': 5014, 'anil': 5015, 'russell': 5016, 'prolific': 5017, 'cruelty': 5018, 'owners': 5019, 'recommend': 5020, 'cosmos': 5021, 'advance': 5022, 'hoped': 5023, 'foster': 5024, 'suffer': 5025, 'lovely': 5026, 'stops': 5027, 'characteristic': 5028, 'powerfully': 5029, 'belong': 5030, 'spaces': 5031, 'swept': 5032, 'strangely': 5033, 'spectator': 5034, 'statements': 5035, 'vowed': 5036, 'fingertips': 5037, 'baseball': 5038, 'dutch': 5039, 'slave': 5040, 'simultaneously': 5041, 'burn': 5042, 'victoria': 5043, 'societies': 5044, 'happier': 5045, 'infinity': 5046, 'worry': 5047, 'logo': 5048, 'reward': 5049, 'aware': 5050, 'formation': 5051, 'heels': 5052, 'confession': 5053, 'punch': 5054, 'tailor': 5055, 'considerations': 5056, 'tight': 5057, 'kidnapping': 5058, 'distinct': 5059, 'customs': 5060, 'happening': 5061, 'promised': 5062, 'silicon': 5063, 'circles': 5064, 'raped': 5065, 'beaten': 5066, '1992': 5067, 'meaningful': 5068, 'dozen': 5069, 'fraught': 5070, 'antics': 5071, 'rivalry': 5072, 'psychologist': 5073, 'belongs': 5074, 'monk': 5075, 'one’s': 5076, \"'look\": 5077, \"'b'\": 5078, 'terrain': 5079, 'sas': 5080, 'compromise': 5081, 'glance': 5082, 'steal': 5083, \"pakistan's\": 5084, 'deployment': 5085, 'villages': 5086, 'ibm': 5087, 'keyes': 5088, 'picking': 5089, 'family’s': 5090, 'sprawling': 5091, 'manipulation': 5092, 'breakthrough': 5093, 'max': 5094, 'parker': 5095, 'phrase': 5096, 'tiwari': 5097, 'tackling': 5098, 'commit': 5099, 'waves': 5100, 'physique': 5101, 'principal': 5102, 'verge': 5103, 'manhattan': 5104, 'rivers': 5105, 'highlighted': 5106, 'string': 5107, 'divide': 5108, 'rebels': 5109, 'mess': 5110, 'sleeping': 5111, 'surely': 5112, 'impress': 5113, 'lust': 5114, 'invitation': 5115, 'evan': 5116, 'enabling': 5117, 'recording': 5118, 'worthy': 5119, 'feared': 5120, 'abuse': 5121, 'hitherto': 5122, 'lal': 5123, 'accepted': 5124, 'reluctant': 5125, 'fluent': 5126, 'housing': 5127, 'treasures': 5128, 'laughs': 5129, 'brotherhood': 5130, 'kgb': 5131, 'supporting': 5132, 'bobby': 5133, 'narrates': 5134, 'vampire': 5135, 'siblings': 5136, 'plotted': 5137, 'ezio': 5138, 'scheming': 5139, 'enjoying': 5140, 'explorers': 5141, 'touched': 5142, 'guest': 5143, 'pool': 5144, 'saint': 5145, 'rhyme': 5146, 'religions': 5147, 'unstoppable': 5148, 'insane': 5149, 'treachery': 5150, 'trivia': 5151, 'tied': 5152, 'awarded': 5153, 'album': 5154, 'county': 5155, 'corporations': 5156, 'vinci': 5157, 'baker': 5158, 'scared': 5159, 'visiting': 5160, \"hasn't\": 5161, 'creations': 5162, 'dev': 5163, 'pioneers': 5164, 'contrast': 5165, 'concern': 5166, 'uncertain': 5167, 'adulthood': 5168, 'carter': 5169, 'occupation': 5170, 'eyed': 5171, 'apparently': 5172, 'controlled': 5173, 'liverpool': 5174, 'stevens': 5175, 'spatial': 5176, 'harmony': 5177, 'intrigues': 5178, 'refreshing': 5179, '1990': 5180, 'holidays': 5181, 'fox': 5182, 'packages': 5183, 'integrate': 5184, 'hawking': 5185, 'oca': 5186, 'facets': 5187, 'genetic': 5188, 'marcus': 5189, 'bay': 5190, 'exception': 5191, 'rama': 5192, 'believing': 5193, 'shane': 5194, 'vulnerable': 5195, 'devastated': 5196, 'darkly': 5197, 'missions': 5198, 'geronimo': 5199, 'karate': 5200, 'findings': 5201, 'sexy': 5202, 'zynpagua': 5203, 'granted': 5204, 'jesus': 5205, 'fargo': 5206, 'lantern': 5207, 'improvement': 5208, 'rusty': 5209, 'hal': 5210, 'notation': 5211, 'afterword': 5212, '1997': 5213, 'mummy': 5214, 'wizarding': 5215, 'travellers': 5216, 'limitations': 5217, 'fake': 5218, 'phrasebook': 5219, 'legacies': 5220, 'html5': 5221, 'recalls': 5222, 'weak': 5223, 'curse': 5224, 'paradigm': 5225, 'dynasty': 5226, 'shaolin': 5227, 'sustained': 5228, 'whale': 5229, 'ally': 5230, 'unaware': 5231, 'seasons': 5232, 'identifying': 5233, 'electronics': 5234, 'informal': 5235, 'closed': 5236, 'ipl': 5237, 'encyclopedia': 5238, 'items': 5239, 'surprises': 5240, 'carl': 5241, 'runaway': 5242, 'balloon': 5243, 'assignment': 5244, 'gaming': 5245, 'administrative': 5246, 'israel': 5247, 'orwell': 5248, 'menace': 5249, 'satisfying': 5250, 'connor': 5251, 'locations': 5252, 'proportions': 5253, 'wearing': 5254, 'sorrow': 5255, 'drew': 5256, 'ann': 5257, 'heartwarming': 5258, 'snowy': 5259, 'argued': 5260, 'savvy': 5261, 'bird': 5262, 'towns': 5263, 'confronts': 5264, 'dropped': 5265, 'intuitive': 5266, 'diagnosis': 5267, 'lavishly': 5268, 'fits': 5269, 'mastermind': 5270, 'victories': 5271, 'intensely': 5272, 'superior': 5273, 'eliminate': 5274, 'steady': 5275, 'donald': 5276, 'possess': 5277, 'instance': 5278, '32': 5279, 'relating': 5280, 'considerable': 5281, 'christie': 5282, 'authenticity': 5283, 'innovations': 5284, 'madrid': 5285, 'upbringing': 5286, 'unfortunately': 5287, 'fails': 5288, 'principle': 5289, 'select': 5290, 'negative': 5291, 'confrontation': 5292, 'experiment': 5293, 'breathless': 5294, 'phil': 5295, 'mature': 5296, 'tricky': 5297, 'richest': 5298, 'psyche': 5299, 'sudha': 5300, 'altogether': 5301, 'y': 5302, 'gruesome': 5303, 'alternate': 5304, 'lab': 5305, 'hacking': 5306, 'gray': 5307, '600': 5308, 'norman': 5309, 'graham': 5310, 'assigned': 5311, 'speeches': 5312, 'honor': 5313, 'laughing': 5314, 'workflow': 5315, 'excellence': 5316, 'templates': 5317, 'understands': 5318, 'navigating': 5319, 'sad': 5320, 'guessing': 5321, \"page'\": 5322, 'diana': 5323, 'entered': 5324, 'elephant': 5325, 'journals': 5326, 'operative': 5327, 'fuel': 5328, 'pharaoh': 5329, 'recover': 5330, 'recipient': 5331, 'catastrophe': 5332, 'sensual': 5333, 'arab': 5334, 'marple': 5335, 'targaryen': 5336, 'lang': 5337, 'chords': 5338, 'flee': 5339, 'gothic': 5340, 'identities': 5341, 'transformative': 5342, 'immensely': 5343, 'overcoming': 5344, 'filmmaking': 5345, 'barred': 5346, 'mountaineering': 5347, 'blind': 5348, 'struggled': 5349, 'unbelievable': 5350, 'autumn': 5351, 'grit': 5352, 'landing': 5353, 'trains': 5354, 'unexpectedly': 5355, 'powered': 5356, 'custom': 5357, 'variations': 5358, 'feast': 5359, 'warren': 5360, 'parenting': 5361, 'stunningly': 5362, 'failing': 5363, 'cooperation': 5364, 'newfound': 5365, 'diplomatic': 5366, 'domination': 5367, 'vijay': 5368, 'sen': 5369, 'thick': 5370, 'grips': 5371, \"she'll\": 5372, 'websites': 5373, 'peek': 5374, 'aside': 5375, 'mentally': 5376, 'fix': 5377, 'tie': 5378, 'unseen': 5379, 'consumers': 5380, 'protagonist': 5381, 'resilience': 5382, 'prodigy': 5383, 'cooper': 5384, 'lunch': 5385, 'learner': 5386, '1939': 5387, 'incorporated': 5388, 'amusing': 5389, 'latin': 5390, \"years'\": 5391, 'stayed': 5392, '1985': 5393, 'homes': 5394, 'elsewhere': 5395, 'lying': 5396, 'adaptation': 5397, 'connecting': 5398, 'founders': 5399, 'carries': 5400, 'senses': 5401, 'appearing': 5402, 'magician': 5403, 'receive': 5404, 'gandhi’s': 5405, 'seconds': 5406, 'raises': 5407, '1994': 5408, 'evolving': 5409, 'september': 5410, 'regardless': 5411, 'gasp': 5412, \"let's\": 5413, 'wits': 5414, 'bane': 5415, 'spells': 5416, 'funeral': 5417, 'farcical': 5418, '1988': 5419, 'professions': 5420, 'restless': 5421, 'bus': 5422, 'resulted': 5423, 'affecting': 5424, 'permanent': 5425, 'planets': 5426, 'publish': 5427, 'generated': 5428, 'comments': 5429, 'ma': 5430, 'unite': 5431, 'incidents': 5432, 'rituals': 5433, 'lin': 5434, 'frameworks': 5435, 'streaming': 5436, 'baffling': 5437, 'quit': 5438, 'gardner': 5439, 'sits': 5440, 'meticulous': 5441, 'dignity': 5442, 'junior': 5443, 'log': 5444, 'assume': 5445, 'nicole': 5446, 'mein': 5447, 'dimensions': 5448, 'wounds': 5449, 'rebel': 5450, 'bowler': 5451, 'testament': 5452, 'westeros': 5453, 'elderly': 5454, 'methodology': 5455, 'boost': 5456, 'stomach': 5457, 'encourage': 5458, 'protocols': 5459, 'organs': 5460, 'expensive': 5461, 'ruby': 5462, 'jardir': 5463, 'brilliance': 5464, 'laxman': 5465, 'pierce': 5466, 'anika': 5467, 'golf': 5468, 'anderson': 5469, 'anand': 5470, 'mystic': 5471, 'pencils': 5472, 'unfolding': 5473, 'emily': 5474, 'templars': 5475, 'caroline': 5476, 'mole': 5477, 'trevor': 5478, 'stilton': 5479, 'verse': 5480, 'openstack': 5481, 'ariba': 5482, 'patterson': 5483, '1986': 5484, 'reduce': 5485, 'christie’s': 5486, 'plant': 5487, 'centres': 5488, 'wives': 5489, 'burned': 5490, 'controls': 5491, 'healthcare': 5492, 'continuous': 5493, 'steeped': 5494, 'hardback': 5495, 'fi': 5496, 'dose': 5497, 'wwe': 5498, 'hart': 5499, 'robie': 5500, 'reissue': 5501, 'painter': 5502, 'bike': 5503, 'plunges': 5504, 'eric': 5505, 'filmmaker': 5506, 'weekend': 5507, 'pressing': 5508, 'mp3': 5509, 'unputdownable': 5510, 'adventurer': 5511, 'searing': 5512, '1975': 5513, 'fury': 5514, 'suzuki': 5515, 'separated': 5516, 'loneliness': 5517, 'tackles': 5518, 'terry': 5519, 'extend': 5520, 'suite': 5521, 'rss': 5522, 'archival': 5523, 'satisfy': 5524, 'mischief': 5525, 'isbn': 5526, 'cop': 5527, 'overnight': 5528, 'denied': 5529, 'captivated': 5530, 'chooses': 5531, 'slam': 5532, 'dominance': 5533, 'antidote': 5534, 'economists': 5535, 'quiz': 5536, 'servers': 5537, 'distributed': 5538, 'footsteps': 5539, \"history's\": 5540, 'quietly': 5541, '1963': 5542, 'balanced': 5543, 'lisa': 5544, 'spain': 5545, 'ferguson': 5546, 'kick': 5547, 'definition': 5548, 'catastrophic': 5549, 'combinations': 5550, 'sequence': 5551, 'emergence': 5552, 'gathers': 5553, 'championship': 5554, '48': 5555, 'assistance': 5556, 'manifesto': 5557, 'marx': 5558, 'supply': 5559, 'biological': 5560, 'traveller': 5561, 'freelance': 5562, 'agencies': 5563, 'morality': 5564, 'miranda': 5565, 'ritual': 5566, 'doubts': 5567, 'immortal': 5568, 'asset': 5569, 'safely': 5570, 'hasn’t': 5571, 'corners': 5572, 'isabel': 5573, 'slavery': 5574, 'interaction': 5575, 'scottish': 5576, 'responses': 5577, 'formulas': 5578, 'loops': 5579, 'entrepreneurial': 5580, 'oh': 5581, '1964': 5582, 'thrills': 5583, 'servant': 5584, 'input': 5585, 'compendium': 5586, 'dean': 5587, 'attacked': 5588, 'spins': 5589, 'destroying': 5590, 'armies': 5591, 'tide': 5592, 'risen': 5593, 'conan': 5594, 'delicious': 5595, 'dictionaries': 5596, 'involves': 5597, 'reckoning': 5598, 'illegal': 5599, 'clubs': 5600, 'egg': 5601, 'foes': 5602, 'greatly': 5603, 'ambassador': 5604, 'bosch': 5605, 'sean': 5606, 'shark': 5607, 'detectives': 5608, 'underpants': 5609, 'ties': 5610, 'thieves': 5611, 'root': 5612, 'lesser': 5613, 'obstacles': 5614, 'possibility': 5615, 'dared': 5616, 'tibetan': 5617, 'restorer': 5618, 'clerk': 5619, 'require': 5620, 'absurd': 5621, '1974': 5622, 'endure': 5623, 'rhetoric': 5624, 'flynn': 5625, 'aristocratic': 5626, 'altering': 5627, 'intel': 5628, 'scattered': 5629, 'rulers': 5630, 'studios': 5631, 'tears': 5632, 'relatives': 5633, 'joseph': 5634, 'nuances': 5635, 'peers': 5636, 'bharat': 5637, 'vice': 5638, 'holistic': 5639, \"aren't\": 5640, 'plunge': 5641, 'pursues': 5642, 'attorney': 5643, 'father’s': 5644, 'layers': 5645, '1950s': 5646, 'soaring': 5647, 'chronicling': 5648, 'unthinkable': 5649, 'joker': 5650, 'demonstration': 5651, 'agenda': 5652, 'delicate': 5653, 'button': 5654, 'wartime': 5655, 'riches': 5656, 'surviving': 5657, 'dry': 5658, 'outer': 5659, 'bush': 5660, 'enchanted': 5661, 'sick': 5662, 'icons': 5663, 'merchant': 5664, 'advent': 5665, 'appreciate': 5666, 'instructors': 5667, 'miracle': 5668, 'carnegie': 5669, 'chose': 5670, 'unity': 5671, 'locals': 5672, 'bangladesh': 5673, 'rhythm': 5674, 'proved': 5675, 'drink': 5676, 'greats': 5677, 'mackintosh': 5678, 'horizon': 5679, 'depicts': 5680, 'shifting': 5681, 'superbly': 5682, 'bears': 5683, 'blank': 5684, 'visitors': 5685, 'meetings': 5686, 'prism': 5687, 'examiner': 5688, 'nation’s': 5689, 'hugh': 5690, 'jazz': 5691, 'perils': 5692, 'camps': 5693, \"child's\": 5694, 'wolf': 5695, 'fulfill': 5696, 'citizen': 5697, 'operators': 5698, 'illustrious': 5699, 'thrillers': 5700, 'researchers': 5701, 'respective': 5702, 'wakes': 5703, 'ecosystem': 5704, 'banking': 5705, 'guarantee': 5706, 'sweep': 5707, 'con': 5708, 'mechanics': 5709, 'concrete': 5710, 'physically': 5711, 'poster': 5712, 'ayurveda': 5713, 'sociology': 5714, \"writer's\": 5715, 'reproduced': 5716, 'vintage': 5717, 'palin': 5718, \"students'\": 5719, 'bilingual': 5720, 'motivated': 5721, 'option': 5722, 'electrifying': 5723, 'priya': 5724, \"sister's\": 5725, 'terrified': 5726, 'scored': 5727, 'riding': 5728, 'jerusalem': 5729, 'entertained': 5730, 'bowling': 5731, '“i': 5732, 'basketball': 5733, 'nightmares': 5734, 'argue': 5735, 'indo': 5736, 'arnold': 5737, 'cinematography': 5738, 'welfare': 5739, 'ya': 5740, 'stella': 5741, '35': 5742, 'vsphere': 5743, 'intricacies': 5744, 'stylish': 5745, 'trigger': 5746, 'quarter': 5747, 'awakening': 5748, 'dominate': 5749, 'undisputed': 5750, 'forensic': 5751, 'specialist': 5752, 'splendid': 5753, 'visualization': 5754, 'life’s': 5755, 'clothing': 5756, 'employment': 5757, 'bob': 5758, 'dressing': 5759, \"tezuka's\": 5760, 'siddhartha': 5761, 'slightly': 5762, 'superstars': 5763, 'missed': 5764, 'shortly': 5765, 'reflection': 5766, 'edgar': 5767, 'masterwork': 5768, 'hannibal': 5769, 'ram': 5770, 'aesthetics': 5771, 'eagerly': 5772, 'poses': 5773, 'settings': 5774, '1991': 5775, 'personally': 5776, 'boot': 5777, 'progresses': 5778, 'whirlwind': 5779, 'dirty': 5780, '75': 5781, 'chopra': 5782, 'harold': 5783, 'reverse': 5784, 'vba': 5785, 'ingenuity': 5786, 'retain': 5787, 'sorts': 5788, 'statement': 5789, 'organised': 5790, 'outrageous': 5791, 'representing': 5792, 'effortlessly': 5793, 'statesman': 5794, 'humility': 5795, 'talked': 5796, 'rifles': 5797, 'economies': 5798, 'costs': 5799, 'troubles': 5800, 'labour': 5801, '28': 5802, 'comprehend': 5803, 'architectures': 5804, 'ruskin': 5805, 'fingers': 5806, 'suggestions': 5807, 'racial': 5808, 'aesthetic': 5809, 'division': 5810, 'coaster': 5811, 'independently': 5812, 'tormented': 5813, 'ana': 5814, 'who’s': 5815, 'adams': 5816, 'subsequent': 5817, 'festival': 5818, 'fired': 5819, 'addressing': 5820, 'neighbour': 5821, 'chest': 5822, 'eloquent': 5823, 'educators': 5824, 'angels': 5825, 'teamwork': 5826, 'forged': 5827, 'airport': 5828, \"he'll\": 5829, 'karl': 5830, 'deserts': 5831, 'inform': 5832, 'betrayed': 5833, \"shouldn't\": 5834, 'organizational': 5835, 'containers': 5836, 'frederick': 5837, 'cuddly': 5838, 'sings': 5839, 'swami': 5840, 'poetic': 5841, 'devastation': 5842, 'exceptions': 5843, 'debugging': 5844, 'guidebook': 5845, 'variables': 5846, 'tall': 5847, 'babies': 5848, 'canvas': 5849, 'grave': 5850, 'eighth': 5851, 'vows': 5852, 'unsettling': 5853, 'wounded': 5854, 'visuals': 5855, 'abducted': 5856, 'dress': 5857, 'communal': 5858, 'cristiano': 5859, 'shoulders': 5860, 'factor': 5861, 'testimony': 5862, 'partnership': 5863, 'fabled': 5864, 'saitama': 5865, 'damage': 5866, 'throwing': 5867, 'mason': 5868, 'wondering': 5869, \"lee's\": 5870, 'theatre': 5871, 'fascinated': 5872, 'shelter': 5873, 'backs': 5874, 'rebuild': 5875, 'appointed': 5876, 'adored': 5877, 'colgan': 5878, 'gary': 5879, 'sabotage': 5880, 'article': 5881, 'princely': 5882, 'dangerously': 5883, 'shah': 5884, 'siege': 5885, 'advocate': 5886, 'invincible': 5887, 'isabelle': 5888, 'julie': 5889, '70': 5890, 'blueprint': 5891, 'depend': 5892, 'practicing': 5893, 'ebook': 5894, 'polish': 5895, 'height': 5896, 'punjab': 5897, 'identifies': 5898, 'tycoon': 5899, 'imperial': 5900, 'rigorous': 5901, 'delights': 5902, 'delightfully': 5903, 'corpus': 5904, 'displays': 5905, 'poised': 5906, 'sub': 5907, 'legions': 5908, 'isaacson': 5909, 'von': 5910, 'detailing': 5911, 'wooden': 5912, 'swiss': 5913, 'creature': 5914, 'brett': 5915, 'wicked': 5916, 'hacker': 5917, 'presidency': 5918, 'decline': 5919, 'heist': 5920, 'agile': 5921, 'farmer': 5922, 'colonel': 5923, 'hangs': 5924, '1977': 5925, '1984': 5926, 'wade': 5927, 'income': 5928, 'dad': 5929, 'instructor': 5930, 'physicist': 5931, 'relatable': 5932, 'inevitably': 5933, 'tourist': 5934, 'pencil': 5935, '19th': 5936, 'millennium': 5937, 'chord': 5938, 'relate': 5939, 'mat': 5940, 'pretend': 5941, 'convincing': 5942, 'profiles': 5943, 'toefl': 5944, 'steadily': 5945, 'equipped': 5946, 'zoe': 5947, \"sarah's\": 5948, 'festivals': 5949, 'sweden': 5950, 'wheel': 5951, 'improved': 5952, 'amazingly': 5953, 'difficulties': 5954, 'watches': 5955, 'strongest': 5956, 'feedback': 5957, 'plagued': 5958, 'workplace': 5959, 'gear': 5960, 'bullet': 5961, 'mine': 5962, 'penned': 5963, 'empires': 5964, 'api': 5965, 'socio': 5966, 'clifton': 5967, 'virginia': 5968, 'evolutionary': 5969, 'upcoming': 5970, 'reich': 5971, 'jimmy': 5972, 'chamber': 5973, 'themed': 5974, 'nonsense': 5975, 'institution': 5976, 'surprised': 5977, 'detection': 5978, 'disney': 5979, 'homework': 5980, 'concurrent': 5981, 'roland': 5982, 'periods': 5983, 'wears': 5984, 'relative': 5985, 'hitting': 5986, 'douglas': 5987, 'habit': 5988, 'unfold': 5989, 'ottoman': 5990, 'ashton': 5991, 'checkpoint': 5992, 'rewarding': 5993, 'vanishes': 5994, 'fearsome': 5995, \"'cussler\": 5996, 'politically': 5997, 'eerie': 5998, 'havoc': 5999, 'exists': 6000, 'hel': 6001, 'rivanah': 6002, 'gut': 6003, 'salander': 6004, 'wood': 6005, 'kerala': 6006, 'novelists': 6007, \"'if\": 6008, 'optimization': 6009, 'relation': 6010, 'zlatan': 6011, 'dutt': 6012, 'cinematic': 6013, 'railways': 6014, 'uncompromising': 6015, 'downfall': 6016, 'ruling': 6017, 'firsthand': 6018, 'murakami': 6019, 'thank': 6020, 'luca': 6021, 'foods': 6022, 'placing': 6023, 'stats': 6024, 'baldacci': 6025, 'motorcycle': 6026, 'traps': 6027, 'secretive': 6028, 'stylist': 6029, 'believable': 6030, 'kelly': 6031, 'rajan': 6032, 'bowden': 6033, 'chaotic': 6034, 'laying': 6035, 'gurus': 6036, 'backgrounds': 6037, 'portrayed': 6038, 'outlook': 6039, 'gaiman': 6040, 'pilgrimage': 6041, 'dystopian': 6042, 'involvement': 6043, 'businessmen': 6044, 'prospects': 6045, 'sydney': 6046, 'ranked': 6047, 'privilege': 6048, 'robinson': 6049, '1942': 6050, 'smiling': 6051, 'cargo': 6052, 'joke': 6053, 'andre': 6054, 'wings': 6055, 'promote': 6056, 'calligraphy': 6057, 'deploying': 6058, 'plains': 6059, 'altered': 6060, 'commentator': 6061, 'hr': 6062, 'considers': 6063, 'android': 6064, 'affected': 6065, 'imaginary': 6066, 'plastic': 6067, 'surgery': 6068, 'explored': 6069, 'proficiency': 6070, 'guarded': 6071, 'avenge': 6072, 'strokes': 6073, 'textbooks': 6074, 'fraud': 6075, 'grisham': 6076, 'mississippi': 6077, 'hugo': 6078, 'illustrative': 6079, 'hurt': 6080, 'posts': 6081, 'drivers': 6082, 'functioning': 6083, 'aspirants': 6084, 'jam': 6085, 'hyperactive': 6086, 'appeals': 6087, 'robots': 6088, 'fund': 6089, 'tina': 6090, 'paragraph': 6091, 'convince': 6092, \"novel'\": 6093, 'relying': 6094, 'forge': 6095, 'nephew': 6096, '“': 6097, 'integrating': 6098, 'bet': 6099, 'adopt': 6100, 'he’ll': 6101, 'humankind': 6102, 'ruined': 6103, 'vikram': 6104, 'lyrical': 6105, 'misery': 6106, 'quizzes': 6107, 'spreading': 6108, 'confessions': 6109, 'anybody': 6110, 'jose': 6111, 'vegas': 6112, 'michelle': 6113, \"rackham's\": 6114, 'column': 6115, 'bald': 6116, 'kingsbridge': 6117, 'forgetting': 6118, 'district': 6119, 'gate': 6120, 'pushing': 6121, 'forefront': 6122, 'viesturs': 6123, 'adventurers': 6124, 'messaging': 6125, 'patrick': 6126, 'ladies': 6127, 'misunderstood': 6128, 'encompasses': 6129, 'elected': 6130, \"we'll\": 6131, 'lurking': 6132, 'interfaces': 6133, 'assess': 6134, '1980': 6135, 'tyler': 6136, 'predictions': 6137, 'awaits': 6138, 'optimal': 6139, \"gandhi's\": 6140, 'princeton': 6141, 'josh': 6142, 'collision': 6143, 'historians': 6144, 'yorkshire': 6145, 'britain’s': 6146, 'comprises': 6147, 'daunting': 6148, 'displaced': 6149, 'represent': 6150, 'dull': 6151, 'rajiv': 6152, 'poison': 6153, 'excited': 6154, 'premise': 6155, 'padma': 6156, 'branch': 6157, 'portrays': 6158, 'consumed': 6159, 'pleasures': 6160, 'observed': 6161, 'margaret': 6162, 'sally': 6163, 'dominant': 6164, 'forming': 6165, '52': 6166, 'governor': 6167, 'oscar': 6168, 'sparked': 6169, 'sail': 6170, 'originality': 6171, 'tolerance': 6172, 'naked': 6173, 'revisit': 6174, 'they’ve': 6175, '90': 6176, \"marvel's\": 6177, 'leo': 6178, 'crippling': 6179, 'exhibition': 6180, 'fulfil': 6181, 'loyalties': 6182, 'brewing': 6183, 'guardians': 6184, 'evelyn': 6185, 'feminist': 6186, '120': 6187, 'ratna': 6188, 'ltd': 6189, 'illustration': 6190, 'electrical': 6191, 'wireless': 6192, 'celebrating': 6193, 'wizardry': 6194, 'vehicles': 6195, 'applicable': 6196, 'initiatives': 6197, 'winterfell': 6198, 'lords': 6199, 'grim': 6200, 'trainers': 6201, 'deserted': 6202, 'occurred': 6203, 'orders': 6204, '1969': 6205, 'coincidence': 6206, 'deliciously': 6207, 'sharma': 6208, \"men's\": 6209, 'inspires': 6210, 'rank': 6211, 'shanghai': 6212, 'shut': 6213, 'slip': 6214, 'unearth': 6215, 'compare': 6216, 'les': 6217, 'rises': 6218, 'ranch': 6219, 'visualize': 6220, 'paradise': 6221, 'anime': 6222, 'aspen': 6223, 'analysts': 6224, 'hook': 6225, 'fuzzy': 6226, 'branches': 6227, 'coin': 6228, 'crisp': 6229, 'instagram': 6230, 'maintenance': 6231, 'dharma': 6232, 'pearl': 6233, 'assist': 6234, 'affection': 6235, 'prejudices': 6236, 'request': 6237, 'promising': 6238, 'witchcraft': 6239, 'syd': 6240, 'paula': 6241, 'ascent': 6242, 'manners': 6243, 'invaders': 6244, 'remembered': 6245, 'semi': 6246, 'adolescence': 6247, 'equip': 6248, 'swords': 6249, 'accidental': 6250, 'sketch': 6251, 'prompts': 6252, 'residents': 6253, 'momentous': 6254, 'compete': 6255, 'employees': 6256, 'mansion': 6257, 'freak': 6258, 'amateurs': 6259, 'orange': 6260, 'viewed': 6261, 'motives': 6262, 'radhika': 6263, 'dalit': 6264, 'moby': 6265, 'shell': 6266, 'primal': 6267, 'burroughs': 6268, \"beginner's\": 6269, 'bending': 6270, 'favourites': 6271, 'jacob': 6272, 'ownership': 6273, 'dagger': 6274, 'hooked': 6275, 'divorce': 6276, '1967': 6277, 'natasha': 6278, 'cells': 6279, 'plunged': 6280, 'nemesis': 6281, 'slash': 6282, 'akira': 6283, 'cole': 6284, 'gains': 6285, 'midoriya': 6286, 'coal': 6287, 'jquery': 6288, \"hitler's\": 6289, 'sheep': 6290, 'cinematographer': 6291, 'ramayana': 6292, 'omega': 6293, 'soha': 6294, 'tabata': 6295, 'layered': 6296, 'marathi': 6297, 'bitcoin': 6298, 'clay': 6299, 'characteristics': 6300, 'embracing': 6301, 'clan': 6302, 'beside': 6303, 'footballers': 6304, 'navigation': 6305, 'smoothly': 6306, 'spotlight': 6307, 'blossom': 6308, 'oppression': 6309, \"nation's\": 6310, 'enlightening': 6311, 'necessity': 6312, 'noted': 6313, 'connects': 6314, 'applies': 6315, 'conviction': 6316, 'entranced': 6317, 'conceived': 6318, '1979': 6319, 'altitude': 6320, 'flair': 6321, 'icy': 6322, 'opposition': 6323, 'okay': 6324, 'potent': 6325, 'perpetrators': 6326, 'bjp': 6327, 'negotiating': 6328, 'depicted': 6329, 'fiancé': 6330, 'seductive': 6331, 'remarkably': 6332, 'endlessly': 6333, 'ensuring': 6334, 'spots': 6335, 'skies': 6336, 'disasters': 6337, 'rational': 6338, 'cuts': 6339, 'portable': 6340, 'shortcuts': 6341, 'mainstream': 6342, 'mapping': 6343, \"'it\": 6344, 'approaching': 6345, 'snake': 6346, 'ios': 6347, 'centered': 6348, 'damaged': 6349, 'brooklyn': 6350, 'paying': 6351, 'café': 6352, 'unfortunate': 6353, \"calvin's\": 6354, 'ward': 6355, 'clutches': 6356, 'explanatory': 6357, 'striving': 6358, 'expands': 6359, '7th': 6360, 'stack': 6361, 'startup': 6362, 'stanford': 6363, 'airline': 6364, 'react': 6365, 'protected': 6366, 'gay': 6367, 'devious': 6368, 'improvements': 6369, 'harness': 6370, 'automate': 6371, 'browser': 6372, 'earl': 6373, 'inclusive': 6374, 'linguistics': 6375, 'modernity': 6376, 'nelson': 6377, 'presentations': 6378, 'youthful': 6379, 'saturday': 6380, 'jackie': 6381, 'degrees': 6382, 'intertwined': 6383, 'invited': 6384, 'aided': 6385, 'chances': 6386, 'geometry': 6387, 'marian': 6388, 'holly': 6389, 'laura': 6390, 'uplift': 6391, 'sugar': 6392, '1946': 6393, 'jamie': 6394, 'spielberg': 6395, 'grandfather': 6396, 'crowded': 6397, 'controversies': 6398, 'canada': 6399, 'hardly': 6400, 'harris': 6401, 'wilbur': 6402, 'peaks': 6403, 'occupied': 6404, 'producers': 6405, 'thematic': 6406, 'sands': 6407, 'hindus': 6408, 'minority': 6409, 'equivalent': 6410, \"alfred's\": 6411, 'assignments': 6412, 'shining': 6413, 'dizzying': 6414, '1982': 6415, 'inventor': 6416, 'strife': 6417, 'cathedral': 6418, 'summoned': 6419, 'defeats': 6420, 'destroys': 6421, 'sailor': 6422, 'inquiry': 6423, 'mineral': 6424, 'ideological': 6425, 'socrates': 6426, 'cosmic': 6427, 'skilled': 6428, 'intensive': 6429, 'defines': 6430, 'headstrong': 6431, 'vishnu': 6432, 'oneself': 6433, \"insider's\": 6434, 'chocolate': 6435, 'flowers': 6436, 'unconventional': 6437, 'arrow': 6438, 'distribution': 6439, 'afford': 6440, 'advisor': 6441, 'bleak': 6442, 'epics': 6443, 'spies': 6444, 'suits': 6445, 'pushes': 6446, \"dk's\": 6447, '©': 6448, 'propelled': 6449, 'routing': 6450, 'rust': 6451, 'chaplin': 6452, 'unstable': 6453, 'acquired': 6454, 'macros': 6455, 'deserves': 6456, 'shatter': 6457, 'peer': 6458, 'offices': 6459, 'rao': 6460, 'employed': 6461, 'stormy': 6462, \"husband's\": 6463, 'preserve': 6464, 'scholarly': 6465, 'penny': 6466, 'fishing': 6467, 'practised': 6468, 'opera': 6469, 'arresting': 6470, 'obscure': 6471, 'summaries': 6472, 'shakespeare': 6473, 'shattering': 6474, 'romans': 6475, 'maine': 6476, 'cabinet': 6477, 'franklin': 6478, 'prosperity': 6479, 'abdul': 6480, 'stones': 6481, 'longs': 6482, 'enigma': 6483, 'collide': 6484, 'endured': 6485, 'bangalore': 6486, 'rooted': 6487, 'jahangir': 6488, 'favour': 6489, 'actionable': 6490, 'assemble': 6491, 'jeremy': 6492, 'arduous': 6493, 'gordon': 6494, 'entwined': 6495, 'adversary': 6496, 'costa': 6497, 'orphaned': 6498, 'collaborative': 6499, 'agree': 6500, 'unsuspecting': 6501, 'representation': 6502, 'obsessive': 6503, 'entangled': 6504, 'unspeakable': 6505, 'chemical': 6506, 'voted': 6507, 'exclusively': 6508, 'lifting': 6509, 'plato': 6510, 'compulsive': 6511, 'effortless': 6512, 'fluently': 6513, 'tube': 6514, 'closing': 6515, 'marjane': 6516, 'barbara': 6517, 'attending': 6518, 'sake': 6519, 'carved': 6520, 'gaining': 6521, 'cracking': 6522, 'correspondence': 6523, 'millionaire': 6524, 'tara': 6525, 'et': 6526, 'maya': 6527, 'poets': 6528, 'tap': 6529, 'alibaba': 6530, 'relatively': 6531, 'alicia': 6532, 'imprisoned': 6533, 'kitty': 6534, 'shed': 6535, 'workers': 6536, 'arise': 6537, 'passport': 6538, 'gangsters': 6539, 'bind': 6540, 'cute': 6541, 'titans': 6542, 'pulling': 6543, 'chronological': 6544, 'inventions': 6545, 'lambda': 6546, 'conscience': 6547, 'descent': 6548, 'scalable': 6549, 'flexible': 6550, 'luna': 6551, 'baghdad': 6552, 'rough': 6553, 'regimen': 6554, 'compact': 6555, 'jail': 6556, 'bin': 6557, 'reactive': 6558, 'viewers': 6559, 'thwarted': 6560, 'continuation': 6561, 'disastrous': 6562, 'fuelled': 6563, 'telugu': 6564, 'musk': 6565, 'unhappy': 6566, 'sticker': 6567, 'jews': 6568, 'georgia': 6569, 'professionally': 6570, 'continually': 6571, 'documenting': 6572, 'christianity': 6573, 'collapses': 6574, 'numerical': 6575, 'invention': 6576, 'wishing': 6577, 'jill': 6578, 'witch': 6579, 'tribulations': 6580, 'flower': 6581, 'communities': 6582, \"earth's\": 6583, 'topsy': 6584, 'exposing': 6585, 'accounting': 6586, 'driver': 6587, 'countryside': 6588, 'substance': 6589, 'k2': 6590, 'novices': 6591, 'grades': 6592, 'amounts': 6593, 'casino': 6594, 'jet': 6595, 'endearing': 6596, 'prosper': 6597, 'customize': 6598, 'instructional': 6599, \"beat'\": 6600, 'difficulty': 6601, 'http': 6602, 'rubber': 6603, 'laurel': 6604, 'sultan': 6605, 'emmy': 6606, 'chakraborty': 6607, 'stable': 6608, 'puri': 6609, 'hbo': 6610, 'chomsky': 6611, 'flung': 6612, 'robotics': 6613, 'adversaries': 6614, 'akbar': 6615, 'minerals': 6616, 'hints': 6617, 'marine': 6618, 'suggested': 6619, \"she'd\": 6620, 'motivational': 6621, 'traveling': 6622, 'algebra': 6623, 'ample': 6624, 'pradesh': 6625, 'gem': 6626, 'proverbs': 6627, 'nationalist': 6628, 'xml': 6629, '1965': 6630, 'gene': 6631, 'moderately': 6632, 'splunk': 6633, 'servicenow': 6634, 'arun': 6635, 'contemporaries': 6636, 'mindful': 6637, 'serenity': 6638, 'productivity': 6639, 'collaborate': 6640, 'fundamentally': 6641, 'insurance': 6642, 'blowing': 6643, 'hong': 6644, 'possessed': 6645, 'warmth': 6646, 'lionel': 6647, 'mario': 6648, '\\x96': 6649, 'extract': 6650, 'construct': 6651, 'punishment': 6652, 'osamu': 6653, 'inception': 6654, 'jessica': 6655, 'tangled': 6656, 'succeeds': 6657, 'secrecy': 6658, 'fog': 6659, 'queues': 6660, 'mcqs': 6661, 'embassy': 6662, 'monstrous': 6663, 'ink': 6664, 'heady': 6665, 'simplified': 6666, '2nd': 6667, 'expects': 6668, 'forsaken': 6669, 'phenomenally': 6670, 'spreads': 6671, 'facial': 6672, 'imprisonment': 6673, 'timeline': 6674, 'spiral': 6675, 'cherished': 6676, 'fashioned': 6677, 'smuggling': 6678, 'checked': 6679, \"year'\": 6680, 'satirical': 6681, 'unlocking': 6682, 'noir': 6683, 'redefine': 6684, 'delighted': 6685, 'englishman': 6686, 'ordeal': 6687, 'succession': 6688, \"you'd\": 6689, 'relaxing': 6690, 'viral': 6691, 'tick': 6692, 'fractured': 6693, 'volunteer': 6694, 'bach': 6695, 'obvious': 6696, 'notions': 6697, 'thereby': 6698, 'appealing': 6699, 'sweat': 6700, '31': 6701, 'associate': 6702, 'finale': 6703, 'smoke': 6704, 'ceremony': 6705, 'proclaimed': 6706, \"wodehouse's\": 6707, 'retreat': 6708, 'execute': 6709, 'troubleshooting': 6710, 'arrangement': 6711, 'heaven': 6712, 'departure': 6713, 'culmination': 6714, 'postures': 6715, 'hip': 6716, 'generally': 6717, \"everyone's\": 6718, 'nurse': 6719, \"brown's\": 6720, 'endings': 6721, 'candidly': 6722, 'bottle': 6723, 'assured': 6724, 'boxes': 6725, 'misfits': 6726, 'athens': 6727, 'pyramid': 6728, 'communicating': 6729, 'investors': 6730, 'devi': 6731, 'multimedia': 6732, 'psychic': 6733, '3d': 6734, 'tuning': 6735, 'hackers': 6736, 'occasion': 6737, 'couples': 6738, 'reminds': 6739, 'theft': 6740, 'vivekananda': 6741, 'expeditions': 6742, 'recurring': 6743, 'erotic': 6744, 'matched': 6745, 'survives': 6746, 'alliance': 6747, 'worksheets': 6748, 'nearby': 6749, 'temples': 6750, 'monuments': 6751, 'turtle': 6752, 'ticking': 6753, 'rollercoaster': 6754, \"'funny\": 6755, 'randall': 6756, 'handed': 6757, 'experiencing': 6758, 'shashi': 6759, 'forbes': 6760, 'montgomery': 6761, 'encourages': 6762, 'nikki': 6763, 'toll': 6764, 'adrenaline': 6765, 'barton': 6766, 'reform': 6767, 'convicted': 6768, 'noun': 6769, 'candour': 6770, 'getafix': 6771, 'potion': 6772, 'ranges': 6773, 'cairo': 6774, 'pandemic': 6775, 'panel': 6776, 'examined': 6777, 'prevented': 6778, 'sentiments': 6779, 'mob': 6780, 'responsibility': 6781, 'prequel': 6782, 'towers': 6783, 'stature': 6784, 'adds': 6785, 'strengthening': 6786, 'mexican': 6787, 'completion': 6788, 'stabbed': 6789, 'persistence': 6790, 'folklore': 6791, \"artist's\": 6792, 'praised': 6793, 'repeatedly': 6794, 'bryan': 6795, 'snippets': 6796, 'hometown': 6797, 'racism': 6798, 'communications': 6799, 'dinosaur': 6800, 'majestic': 6801, 'laia': 6802, 'incomparable': 6803, 'recruitment': 6804, 'we’re': 6805, 'electricity': 6806, 'persuasion': 6807, 'assets': 6808, 'hbo’s': 6809, 'daenerys': 6810, 'practitioner': 6811, 'teenagers': 6812, 'roald': 6813, \"'you\": 6814, '800': 6815, 'book’s': 6816, 'occasions': 6817, 'fed': 6818, 'buffett': 6819, 'heartache': 6820, 'vallabhbhai': 6821, 'stoic': 6822, '1950': 6823, 'lovingly': 6824, 'stability': 6825, 'enriched': 6826, 'campus': 6827, 'copyright': 6828, 'systematically': 6829, 'histories': 6830, 'bits': 6831, 'hood': 6832, 'abduction': 6833, 'matching': 6834, 'forgiveness': 6835, 'renewed': 6836, 'illusion': 6837, 'colored': 6838, 'elixir': 6839, 'annie': 6840, 'marrying': 6841, 'wondrous': 6842, 'victor': 6843, 'encompass': 6844, 'causing': 6845, 'sanity': 6846, 'switch': 6847, 'sackett': 6848, 'judgement': 6849, 'luckily': 6850, 'shores': 6851, '5th': 6852, \"wouldn't\": 6853, 'leaps': 6854, 'lovelace': 6855, 'pioneered': 6856, 'persuade': 6857, 'refuse': 6858, 'genesis': 6859, 'mina': 6860, 'protecting': 6861, 'edinburgh': 6862, 'commission': 6863, 'astute': 6864, 'shake': 6865, 'packs': 6866, 'desolate': 6867, 'twisting': 6868, 'switzerland': 6869, 'kidnap': 6870, 'descriptive': 6871, 'evans': 6872, 'dini': 6873, 'jury': 6874, 'edgy': 6875, 'penelope': 6876, 'photograph': 6877, 'lama': 6878, 'troubling': 6879, 'sacrificing': 6880, 'indra': 6881, 'ugly': 6882, 'ash': 6883, 'swing': 6884, 'johnny': 6885, 'patron': 6886, 'nonfiction': 6887, \"company's\": 6888, 'australian': 6889, 'accelerate': 6890, 'menon': 6891, 'reprint': 6892, 'ravi': 6893, \"'another\": 6894, 'flaws': 6895, 'cheating': 6896, 'contract': 6897, 'wholly': 6898, 'echoes': 6899, 'losses': 6900, 'lahore': 6901, 'dots': 6902, 'courts': 6903, \"'in\": 6904, 'depends': 6905, 'dimension': 6906, 'prospect': 6907, 'pour': 6908, 'addicted': 6909, 'des': 6910, 'eighteenth': 6911, 'starters': 6912, '1981': 6913, 'hemingway': 6914, 'refuge': 6915, '007': 6916, 'instances': 6917, 'howard': 6918, 'exploited': 6919, 'urge': 6920, 'flees': 6921, 'creatively': 6922, 'monica': 6923, 'trails': 6924, 'lion': 6925, 'i’ve': 6926, 'atlantis': 6927, 'ayaan': 6928, 'stardom': 6929, 'disorder': 6930, 'owned': 6931, 'dressed': 6932, 'classmates': 6933, 'generosity': 6934, 'longlisted': 6935, 'output': 6936, 'educate': 6937, 'boundless': 6938, 'underneath': 6939, 'twinkle': 6940, 'negotiations': 6941, 'looms': 6942, 'hector': 6943, 'wired': 6944, 'virat': 6945, 'adding': 6946, 'manifested': 6947, 'lynch': 6948, 'krav': 6949, 'negotiation': 6950, \"reader's\": 6951, 'scam': 6952, 'benjamin': 6953, 'bid': 6954, 'shaping': 6955, 'cormoran': 6956, \"storyteller'\": 6957, 'husbands': 6958, 'mates': 6959, 'fantastically': 6960, 'ashok': 6961, 'arya': 6962, 'opencv': 6963, 'amber': 6964, 'practically': 6965, 'instrument': 6966, 'atomic': 6967, 'minecraft': 6968, 'fills': 6969, 'entering': 6970, 'sacco': 6971, 'rendered': 6972, 'styled': 6973, 'reverend': 6974, 'blends': 6975, 'syria': 6976, 'fortress': 6977, 'knit': 6978, '\\x94': 6979, 'signed': 6980, 'casey': 6981, 'estranged': 6982, 'harm': 6983, 'fascist': 6984, 'slaves': 6985, 'therapy': 6986, 'noelle': 6987, 'comprising': 6988, 'twins': 6989, 'polar': 6990, 'dublin': 6991, 'covert': 6992, 'keeper': 6993, 'satyajit': 6994, 'glimpses': 6995, 'wreck': 6996, 'dropping': 6997, 'viann': 6998, 'climbed': 6999, 'saves': 7000, 'instinct': 7001, 'tableau': 7002, 'machinations': 7003, 'abundance': 7004, 'pentland': 7005, 'lavish': 7006, 'sweeps': 7007, 'versus': 7008, 'masterclass': 7009, 'wilson': 7010, 'reeling': 7011, 'companions': 7012, 'textile': 7013, 'dimensional': 7014, 'uhtred': 7015, 'tarzan': 7016, 'squires': 7017, 'redesigned': 7018, 'dravid': 7019, 'leonard': 7020, 'intrepid': 7021, 'chicken': 7022, \"time'\": 7023, 'triple': 7024, 'substantial': 7025, 'yuvraj': 7026, 'sessions': 7027, 'assumes': 7028, \"planet's\": 7029, 'alternatives': 7030, 'solar': 7031, 'answered': 7032, 'imagining': 7033, 'port': 7034, 'classified': 7035, 'kabir': 7036, 'poem': 7037, 'django': 7038, 'taliban': 7039, 'vaastu': 7040, 'himalaya': 7041, 'amit': 7042, 'probability': 7043, 'girlboss': 7044, 'bhagat': 7045, 'ichigo': 7046, 'decker': 7047, 'commands': 7048, 'lydia': 7049, 'ayako': 7050, 'wyndham': 7051, 'caraval': 7052, 'hibernate': 7053, 'anarchic': 7054, 'represented': 7055, 'relaxation': 7056, 'metro': 7057, 'constraints': 7058, '‘this': 7059, 'blooded': 7060, 'knights': 7061, \"world'\": 7062, 'sequences': 7063, 'ravages': 7064, 'counts': 7065, 'demonstrated': 7066, 'vi': 7067, 'intuition': 7068, 'squad': 7069, 'fixing': 7070, '1961': 7071, 'considering': 7072, 'analyse': 7073, 'sikkim': 7074, 'cosmopolitan': 7075, 'walked': 7076, 'persuasive': 7077, 'grylls': 7078, 'canadian': 7079, 'satisfaction': 7080, 'downloaded': 7081, 'liberty': 7082, 'demonstrations': 7083, 'ninth': 7084, 'aboard': 7085, 'establishment': 7086, 'eyewitness': 7087, 'org': 7088, 'absence': 7089, 'staggering': 7090, 'proudly': 7091, 'passionately': 7092, 'fated': 7093, 'ware': 7094, 'molly': 7095, 'traits': 7096, 'fantasies': 7097, 'dumped': 7098, 'famously': 7099, 'rollicking': 7100, 'uproarious': 7101, 'claimed': 7102, 'nora': 7103, 'whimsical': 7104, 'tyrannical': 7105, 'opponents': 7106, 'singapore': 7107, 'wimbledon': 7108, 'consumption': 7109, 'contributing': 7110, 'gujarat': 7111, 'reckless': 7112, 'contributed': 7113, 'robust': 7114, 'fitting': 7115, 'highlight': 7116, 'establishes': 7117, 'stuart': 7118, 'resort': 7119, 'pc': 7120, 'soil': 7121, 'committing': 7122, 'qualified': 7123, 'macabre': 7124, 'inferno': 7125, 'artefacts': 7126, 'declaration': 7127, 'devote': 7128, 'nba': 7129, 'stills': 7130, 'replete': 7131, 'micro': 7132, 'rishi': 7133, 'completing': 7134, \"london's\": 7135, 'viable': 7136, 'jay': 7137, 'significantly': 7138, 'drunken': 7139, 'conservation': 7140, 'harvey': 7141, 'workshop': 7142, 'queries': 7143, 'balancing': 7144, 'methodologies': 7145, 'adoption': 7146, 'operational': 7147, 'stressful': 7148, 'strict': 7149, 'inclusion': 7150, 'evaluation': 7151, 'marries': 7152, 'betty': 7153, 'bastard': 7154, 'chronology': 7155, 'gambling': 7156, 'robbery': 7157, 'treatise': 7158, 'chased': 7159, 'suspected': 7160, 'grayson': 7161, 'seized': 7162, 'outsider': 7163, 'nicknamed': 7164, \"teacher's\": 7165, 'posed': 7166, 'starred': 7167, 'weave': 7168, 'murky': 7169, 'insists': 7170, 'surrender': 7171, 'fiona': 7172, 'extracting': 7173, 'disruptive': 7174, 'persistent': 7175, 'concludes': 7176, 'grounds': 7177, 'motley': 7178, 'poorly': 7179, 'rand': 7180, 'gaulish': 7181, 'travelers': 7182, 'oral': 7183, 'frantic': 7184, 'fearing': 7185, 'stanley': 7186, 'migration': 7187, 'malegaon': 7188, 'questionable': 7189, 'mourinho': 7190, 'arrogant': 7191, 'trophy': 7192, 'joys': 7193, 'hedge': 7194, 'duncan': 7195, 'finger': 7196, 'ba': 7197, 'lone': 7198, 'workshops': 7199, 'undoubtedly': 7200, 'las': 7201, 'thompson': 7202, 'metal': 7203, 'confusing': 7204, 'assortment': 7205, 'expressed': 7206, 'cameron': 7207, 'unmatched': 7208, 'flyers': 7209, 'gleaned': 7210, 'theater': 7211, 'interweaving': 7212, 'motive': 7213, 'meter': 7214, 'feat': 7215, 'po': 7216, 'khanna': 7217, 'enthralled': 7218, 'mechanical': 7219, 'walker': 7220, 'reduced': 7221, 'dashing': 7222, 'hierarchy': 7223, 'aiming': 7224, 'reforms': 7225, 'jackal': 7226, 'hana': 7227, 'contexts': 7228, 'katie': 7229, 'globalization': 7230, 'uncovering': 7231, 'gems': 7232, 'supplies': 7233, 'writer’s': 7234, 'referred': 7235, 'essentially': 7236, 'hyderabad': 7237, 'dreamt': 7238, 'unified': 7239, 'analysing': 7240, 'injustice': 7241, 'refreshingly': 7242, 'worker': 7243, 'reclusive': 7244, 'hotels': 7245, 'shocks': 7246, 'shield': 7247, 'earning': 7248, 'fathers': 7249, 'foul': 7250, '99': 7251, 'forgets': 7252, 'sheet': 7253, 'luther': 7254, 'intellect': 7255, 'comedians': 7256, 'filmmakers': 7257, 'hardships': 7258, 'anew': 7259, 'murphy': 7260, 'investing': 7261, 'stores': 7262, 'twilight': 7263, 'tapestry': 7264, 'bc': 7265, 'synthesis': 7266, 'solitary': 7267, 'goldman': 7268, 'desperation': 7269, 'mask': 7270, 'mystical': 7271, 'theo': 7272, 'colony': 7273, 'mba': 7274, 'macro': 7275, 'diabolical': 7276, 'documentary': 7277, 'legion': 7278, 'diplomat': 7279, 'wastes': 7280, 'reissued': 7281, 'leav': 7282, 'immune': 7283, 'edges': 7284, 'feisty': 7285, 'jewels': 7286, 'kaneki': 7287, 'lois': 7288, 'workbooks': 7289, 'stickers': 7290, 'trove': 7291, 'hearing': 7292, 'caribbean': 7293, 'jumped': 7294, 'fool': 7295, 'defiant': 7296, '1987': 7297, 'domains': 7298, 'incorporate': 7299, 'diagnosed': 7300, 'tessa': 7301, 'rates': 7302, 'inch': 7303, 'alphabets': 7304, 'integrity': 7305, 'saunders': 7306, 'yahoo': 7307, 'empty': 7308, 'undertaken': 7309, 'badly': 7310, 'operate': 7311, 'borders': 7312, 'portray': 7313, 'experimental': 7314, 'nixon': 7315, 'acute': 7316, 'arpeggios': 7317, 'pronouns': 7318, 'ears': 7319, 'assumptions': 7320, 'purposes': 7321, 'predator': 7322, 'immigrant': 7323, 'dense': 7324, 'starks': 7325, 'isaac': 7326, 'executed': 7327, '1962': 7328, 'banned': 7329, 'farmers': 7330, 'unsolved': 7331, 'intimacy': 7332, '64': 7333, 'onslaught': 7334, '1989': 7335, 'nina': 7336, 'defy': 7337, 'cleverly': 7338, 'greg': 7339, 'pet': 7340, 'realised': 7341, 'sufi': 7342, 'disappear': 7343, 'bhushan': 7344, 'scams': 7345, 'barriers': 7346, 'addressed': 7347, 'convert': 7348, 'assumed': 7349, 'caring': 7350, 'setbacks': 7351, 'arrest': 7352, 'relief': 7353, 'thinker': 7354, 'repercussions': 7355, 'samantha': 7356, 'escaping': 7357, 'christine': 7358, 'cake': 7359, 'kampf': 7360, 'outcome': 7361, 'osho': 7362, 'richness': 7363, 'pit': 7364, 'warne': 7365, 'maverick': 7366, 'afghan': 7367, 'authentication': 7368, 'capability': 7369, 'composer': 7370, 'linking': 7371, 'possesses': 7372, 'armchair': 7373, 'ponder': 7374, 'admit': 7375, 'transformations': 7376, 'literacy': 7377, 'boldly': 7378, 'bedroom': 7379, 'scars': 7380, 'surgeon': 7381, 'cryptic': 7382, 'verses': 7383, 'researching': 7384, \"killer's\": 7385, 'hang': 7386, 'interconnected': 7387, 'stranded': 7388, 'promotion': 7389, 'adjectives': 7390, 'prowess': 7391, 'smooth': 7392, 'a1': 7393, 'i’ll': 7394, 'intact': 7395, 'heroines': 7396, 'kumaon': 7397, 'affirming': 7398, 'urquhart': 7399, 'murdering': 7400, 'shame': 7401, 'kris': 7402, \"cricket's\": 7403, 'ideologies': 7404, 'worrying': 7405, 'belonged': 7406, 'neglected': 7407, 'sheltered': 7408, 'allen': 7409, 'deserve': 7410, 'luis': 7411, 'ajax': 7412, 'lyrics': 7413, 'jerry': 7414, 'stripped': 7415, 'retail': 7416, 'exquisitely': 7417, 'adorable': 7418, 'moran': 7419, 'snakes': 7420, 'facility': 7421, 'amma': 7422, 'unconscious': 7423, 'charges': 7424, 'collectors': 7425, 'boruto': 7426, 'he’d': 7427, 'threads': 7428, 'charting': 7429, 'gritty': 7430, 'researcher': 7431, 'explodes': 7432, 'aditya': 7433, 'surreal': 7434, 'tenth': 7435, 'manipulated': 7436, '1948': 7437, 'successive': 7438, 'globally': 7439, 'moore': 7440, 'uncomfortable': 7441, 'uncovered': 7442, 'liz': 7443, 'dishes': 7444, 'reluctantly': 7445, 'hapless': 7446, 'recipe': 7447, 'webdriver': 7448, 'visions': 7449, 'salient': 7450, 'blomkvist': 7451, 'kennedy': 7452, 'outcomes': 7453, 'neighbourhood': 7454, 'generating': 7455, 'partly': 7456, 'feed': 7457, 'ethan': 7458, 'carried': 7459, 'banerjee': 7460, '―': 7461, 'hawkins': 7462, 'mughals': 7463, 'gardens': 7464, 'equations': 7465, 'j2ee': 7466, 'outwit': 7467, 'kirkus': 7468, 'di': 7469, 'graduates': 7470, 'compass': 7471, 'passive': 7472, 'divorced': 7473, 'forsyth': 7474, 'bihar': 7475, 'singh’s': 7476, 'kim': 7477, 'casting': 7478, \"'best\": 7479, 'toe': 7480, 'lieutenant': 7481, 'simpler': 7482, 'circuit': 7483, 'unix': 7484, 'jolly': 7485, 'jules': 7486, 'screenplays': 7487, 'mines': 7488, 'crosses': 7489, 'geralt': 7490, 'located': 7491, 'rigs': 7492, 'spiegelman': 7493, 'ias': 7494, 'tarot': 7495, 'pl': 7496, 'budgeting': 7497, 'tail': 7498, 'assembled': 7499, 'employs': 7500, 'knee': 7501, 'kolkata': 7502, 'isabella': 7503, 'templar': 7504, 'kong': 7505, 'monks': 7506, 'timing': 7507, 'protector': 7508, 'eminently': 7509, 'neymar': 7510, 'refused': 7511, 'footballing': 7512, 'accepts': 7513, 'clandestine': 7514, 'underbelly': 7515, 'rendition': 7516, 'furthermore': 7517, 'similarities': 7518, 'infused': 7519, 'doomsday': 7520, 'nathan': 7521, 'drake': 7522, 'neck': 7523, 'stacks': 7524, 'holes': 7525, 'combed': 7526, 'ideology': 7527, 'opposites': 7528, 'differently': 7529, 'irrevocably': 7530, 'inventive': 7531, 'nevertheless': 7532, 'calculated': 7533, 'unleashes': 7534, 'soaked': 7535, 'battlefields': 7536, 'footnotes': 7537, 'wandering': 7538, 'shaking': 7539, 'imagery': 7540, 'veer': 7541, 'theirs': 7542, 'bittersweet': 7543, 'eliminating': 7544, '1983': 7545, 'vulnerability': 7546, 'nathaniel': 7547, 'oskar': 7548, 'hawk': 7549, 'reserve': 7550, 'claws': 7551, 'clips': 7552, 'imperative': 7553, 'realization': 7554, 'mortality': 7555, 'whispers': 7556, 'snyder': 7557, 'nadal': 7558, 'kay': 7559, 'couch': 7560, 'illusions': 7561, 'expressing': 7562, 'opposed': 7563, 'crossroads': 7564, 'priest': 7565, 'eventful': 7566, 'load': 7567, 'goodbye': 7568, 'replace': 7569, 'regarding': 7570, 'exploit': 7571, 'functionality': 7572, 'menu': 7573, 'coursebook': 7574, 'hides': 7575, \"england's\": 7576, 'regrets': 7577, 'sidemen': 7578, 'succinct': 7579, 'dire': 7580, 'conqueror': 7581, 'frontier': 7582, 'brooks': 7583, 'florence': 7584, 'decipher': 7585, 'kissinger': 7586, 'seamlessly': 7587, 'affections': 7588, 'immigration': 7589, '38': 7590, 'furious': 7591, 'notion': 7592, 'frame': 7593, 'consensus': 7594, 'cheek': 7595, 'annapurna': 7596, 'valued': 7597, 'estimated': 7598, 'teens': 7599, 'subsequently': 7600, 'availability': 7601, 'clustering': 7602, 'dwells': 7603, 'waited': 7604, 'veronica': 7605, 'triangle': 7606, 'contrary': 7607, 'contradictions': 7608, 'joyce': 7609, 'bewildering': 7610, 'samples': 7611, 'acted': 7612, 'sided': 7613, 'scotland': 7614, 'shoe': 7615, 'everyone’s': 7616, \"'it's\": 7617, 'flies': 7618, 'patriotism': 7619, 'uniform': 7620, 'interpret': 7621, 'stellar': 7622, 'structural': 7623, 'whoever': 7624, 'thirties': 7625, 'resonate': 7626, 'genuinely': 7627, 'pura': 7628, 'factory': 7629, 'outlander': 7630, 'rip': 7631, 'ninety': 7632, 'spans': 7633, 'portion': 7634, 'mishaps': 7635, 'elephants': 7636, 'ceos': 7637, 'twenties': 7638, 'roads': 7639, 'lecturer': 7640, 'kirito': 7641, 'virtuoso': 7642, \"thriller'\": 7643, 'branded': 7644, 'confess': 7645, 'jinx': 7646, 'prepositions': 7647, 'diamonds': 7648, 'karen': 7649, 'doyle': 7650, 'druid': 7651, 'samjhauta': 7652, \"'saffron\": 7653, \"terrorism'\": 7654, 'reliability': 7655, 'masterminds': 7656, 'mitchell': 7657, 'portuguese': 7658, 'stuffed': 7659, 'reviewer': 7660, 'recollections': 7661, 'destinies': 7662, 'exploits': 7663, 'vienna': 7664, \"here's\": 7665, 'saints': 7666, 'shoes': 7667, 'residence': 7668, 'universally': 7669, 'guarding': 7670, 'incomplete': 7671, 'cotton': 7672, 'boundary': 7673, 'teeming': 7674, 'tutor': 7675, 'adversity': 7676, 'roses': 7677, 'judgment': 7678, 'mallory': 7679, 'sending': 7680, 'elias': 7681, 'abbreviations': 7682, 'filling': 7683, 'ph': 7684, 'distress': 7685, 'quartet': 7686, 'steer': 7687, 'observers': 7688, 'scorer': 7689, 'colossal': 7690, \"funny'\": 7691, \"'hilarious\": 7692, 'warehouse': 7693, 'judicial': 7694, \"year's\": 7695, 'translate': 7696, 'embarrassing': 7697, 'satisfied': 7698, 'herald': 7699, 'mentioned': 7700, 'idealism': 7701, 'sengupta': 7702, 'clashes': 7703, \"sengupta's\": 7704, 'ancestral': 7705, 'utter': 7706, 'pillar': 7707, 'tallest': 7708, 'constantinople': 7709, 'neighbor': 7710, 'forcing': 7711, 'nerves': 7712, 'reserved': 7713, 'horrible': 7714, 'blossoms': 7715, 'bliss': 7716, 'unbridled': 7717, 'pete': 7718, 'frenzy': 7719, 'venice': 7720, 'marriages': 7721, 'scandals': 7722, 'posted': 7723, 'fable': 7724, 'organize': 7725, 'mother’s': 7726, 'eldest': 7727, 'merit': 7728, 'catholic': 7729, 'paired': 7730, 'bahadur': 7731, 'shastri': 7732, 'oregon': 7733, 'captive': 7734, 'perseverance': 7735, 'seth': 7736, 'buzzfeed': 7737, 'adrift': 7738, 'impart': 7739, 'shri': 7740, 'cissp': 7741, 'realizing': 7742, 'hawthorne': 7743, 'shrouded': 7744, 'wordplay': 7745, 'annotations': 7746, 'organizing': 7747, 'rebellion': 7748, 'masterly': 7749, 'pm': 7750, 'bravest': 7751, \"britain's\": 7752, 'wilde': 7753, 'gruffalo': 7754, 'presidents': 7755, 'fitzgerald': 7756, 'dungeon': 7757, 'gripped': 7758, 'thirst': 7759, 'compared': 7760, 'unwittingly': 7761, 'ghouls': 7762, 'rewritten': 7763, 'void': 7764, 'ridiculous': 7765, 'operatives': 7766, 'om': 7767, 'labor': 7768, 'columns': 7769, 'scarlett': 7770, 'strongly': 7771, 'meals': 7772, 'bro': 7773, 'outlining': 7774, 'sunshine': 7775, 'insatiable': 7776, 'halahala': 7777, 'eclipse': 7778, 'bearing': 7779, 'captivate': 7780, 'apocalypse': 7781, 'gaze': 7782, 'florida': 7783, 'brooke': 7784, 'shatters': 7785, 'taut': 7786, 'regain': 7787, 'paranoia': 7788, 'intrigued': 7789, 'stumbled': 7790, 'comprehensible': 7791, 'northeast': 7792, 'suspicions': 7793, 'bhutan': 7794, 'symptoms': 7795, 'coastal': 7796, 'seaside': 7797, '1907': 7798, 'persepolis': 7799, 'swiftly': 7800, 'sequential': 7801, 'dentist': 7802, 'sealed': 7803, 'latter': 7804, 'frustrated': 7805, 'eleventh': 7806, 'indias': 7807, 'impending': 7808, 'enthusiast': 7809, 'casual': 7810, 'rajasthan': 7811, 'nadu': 7812, 'topple': 7813, 'encouraging': 7814, 'extending': 7815, 'deck': 7816, 'bibliography': 7817, 'skillfully': 7818, 'nursery': 7819, 'psychopath': 7820, 'mercy': 7821, 'implementations': 7822, 'rebecca': 7823, 'overwhelmed': 7824, 'chambers': 7825, 'mom': 7826, 'mindset': 7827, 'fallon': 7828, 'trips': 7829, 'binding': 7830, 'proposed': 7831, 'practising': 7832, 'organ': 7833, 'addiction': 7834, 'feats': 7835, 'sixties': 7836, 'inventory': 7837, \"you'\": 7838, 'civilizations': 7839, 'revenue': 7840, 'vogue': 7841, 'restored': 7842, 'grandson': 7843, 'luminaries': 7844, 'abusive': 7845, 'farewell': 7846, '1959': 7847, 'lauren': 7848, 'unbeatable': 7849, 'geoff': 7850, 'eats': 7851, 'hedgehog': 7852, 'antoine': 7853, 'alaska': 7854, 'cow': 7855, 'doodle': 7856, 'devotees': 7857, 'betrayals': 7858, 'property': 7859, 'petey': 7860, 'titan': 7861, 'miraculous': 7862, 'rankin': 7863, 'messy': 7864, 'trek': 7865, 'erik': 7866, 'feud': 7867, 'potions': 7868, 'angela': 7869, 'evolve': 7870, 'mistaken': 7871, 'exploding': 7872, 'societal': 7873, 'prints': 7874, 'metrics': 7875, 'lucas': 7876, 'anatomical': 7877, 'tenses': 7878, 'counting': 7879, 'mesmerising': 7880, 'judiciary': 7881, 'addict': 7882, 'dealer': 7883, 'beating': 7884, \"rowling's\": 7885, 'hers': 7886, 'immerse': 7887, 'mock': 7888, 'docker': 7889, 'maga': 7890, 'rewards': 7891, 'secular': 7892, 'assurance': 7893, 'fled': 7894, 'bloodshed': 7895, 'alcohol': 7896, 'nepali': 7897, 'nerve': 7898, 'winners': 7899, '16th': 7900, 'bikes': 7901, 'leon': 7902, 'lasts': 7903, 'hopeful': 7904, 'jinnah': 7905, 'hindustani': 7906, 'rhythms': 7907, 'warfare': 7908, 'optical': 7909, 'embarked': 7910, 'wendy': 7911, 'people’s': 7912, 'quote': 7913, 'impacts': 7914, 'distraction': 7915, \"friend's\": 7916, 'feynman': 7917, 'tip': 7918, 'endgame': 7919, 'arabian': 7920, 'bargain': 7921, 'aging': 7922, 'aircraft': 7923, 'admission': 7924, 'we’ve': 7925, 'electoral': 7926, 'reviled': 7927, 'acumen': 7928, 'masked': 7929, 'eaters': 7930, \"king'\": 7931, 'lifts': 7932, 'lee’s': 7933, 'shelf': 7934, 'prejudice': 7935, 'barrier': 7936, 'digs': 7937, 'doubles': 7938, 'raman': 7939, 'pole': 7940, 'accessed': 7941, 'encouraged': 7942, '1954': 7943, 'mikael': 7944, 'kristin': 7945, 'burgess': 7946, \"boys'\": 7947, 'myself': 7948, 'caitlin': 7949, 'retold': 7950, 'charmed': 7951, 'bite': 7952, 'configure': 7953, 'sergeant': 7954, 'influences': 7955, 'lure': 7956, 'cabin': 7957, 'payal': 7958, 'delving': 7959, 'senator': 7960, 'prefer': 7961, 'moriarty': 7962, '45': 7963, 'randy': 7964, 'pausch': 7965, 'aren’t': 7966, 'cookbook': 7967, 'lenses': 7968, 'lloyd': 7969, 'artistry': 7970, 'dirt': 7971, 'maid': 7972, 'atlee': 7973, 'vocal': 7974, 'socialist': 7975, 'kali': 7976, 'openly': 7977, 'sharpest': 7978, 'dale': 7979, 'unforgiving': 7980, 'remaining': 7981, 'dilemmas': 7982, 'filmed': 7983, 'seize': 7984, 'noam': 7985, 'introvert': 7986, 'storylines': 7987, 'lens': 7988, 'indies': 7989, \"cussler's\": 7990, 'vastly': 7991, 'reader’s': 7992, 'saddam': 7993, 'furiously': 7994, 'invent': 7995, 'holocaust': 7996, 'processor': 7997, 'flashcards': 7998, 'anastasia': 7999, \"'s\": 8000, 'mathematician': 8001, 'todd': 8002, 'misadventure': 8003, 'santosh': 8004, '¦': 8005, 'healed': 8006, 'motifs': 8007, 'bow': 8008, 'kratos': 8009, '“quirks”': 8010, 'quirkless': 8011, 'chef': 8012, 'surveys': 8013, 'royale': 8014, 'horowitz': 8015, 'pittacus': 8016, 'huffington': 8017, 'slaughter': 8018, 'weakness': 8019, 'dallas': 8020, 'weep': 8021, 'isolation': 8022, 'russians': 8023, 'rl': 8024, 'ubiquitous': 8025, 'sensors': 8026, 'ssc': 8027, 'lapena': 8028, 'beck': 8029, 'nigella': 8030, 'mythic': 8031, 'menacing': 8032, 'shines': 8033, 'alix': 8034, 'amos': 8035, 'shorthand': 8036, 'lightweight': 8037, 'identification': 8038, 'processors': 8039, \"ambedkar's\": 8040, 'arcgis': 8041, 'salesforce': 8042, 'wabi': 8043, 'sabi': 8044, 'selfish': 8045, 'publicly': 8046, 'rex': 8047, 'danny': 8048, 'posture': 8049, 'yogic': 8050, 'discourse': 8051, 'composing': 8052, 'aristotle': 8053, 'accomplish': 8054, 'agreed': 8055, 'hardened': 8056, 'fico': 8057, 'selections': 8058, 'travelogue': 8059, 'guha': 8060, 'allegations': 8061, 'administrators': 8062, 'fixed': 8063, 'ordering': 8064, 'curtain': 8065, \"baldacci's\": 8066, 'biographical': 8067, 'personnel': 8068, 'uncharted': 8069, 'flourish': 8070, 'traumatic': 8071, 'heaps': 8072, 'explosion': 8073, 'picks': 8074, 'flawed': 8075, 'fatigue': 8076, 'strewn': 8077, 'melodies': 8078, 'ode': 8079, 'arrangements': 8080, 'hussain': 8081, 'mundane': 8082, 'coincide': 8083, 'hairstyles': 8084, 'galleries': 8085, 'jurassic': 8086, 'hauntingly': 8087, 'condensed': 8088, 'coup': 8089, 'stint': 8090, 'parents’': 8091, 'freshly': 8092, 'landed': 8093, 'charmingly': 8094, 'burgeoning': 8095, 'labelled': 8096, 'unrelenting': 8097, 'bars': 8098, 'batsmen': 8099, 'consistency': 8100, \"team's\": 8101, 'phantom': 8102, 'owing': 8103, 'dynamics': 8104, '50th': 8105, 'floating': 8106, 'hallmarks': 8107, 'laughed': 8108, 'vacation': 8109, 'beaches': 8110, 'battled': 8111, 'kyle': 8112, 'belt': 8113, 'competing': 8114, 'outdoors': 8115, 'capitalism': 8116, 'heroin': 8117, 'wielded': 8118, 'primitive': 8119, 'unlocks': 8120, 'rivalries': 8121, \"football's\": 8122, 'scalability': 8123, 'accepting': 8124, 'signals': 8125, 'insiders': 8126, 'metaphor': 8127, 'category': 8128, 'hated': 8129, 'proposes': 8130, 'kareena': 8131, 'spice': 8132, 'rio': 8133, 'finals': 8134, 'stepping': 8135, 'pairs': 8136, 'prone': 8137, 'awakes': 8138, 'breakneck': 8139, 'sculptures': 8140, '65': 8141, 'lips': 8142, 'pilots': 8143, 'consisting': 8144, 'scarce': 8145, 'worldly': 8146, 'seattle': 8147, 'dreaming': 8148, 'firmly': 8149, 'revolutionize': 8150, 'horrendous': 8151, 'grandeur': 8152, 'hone': 8153, 'cisco': 8154, 'murderers': 8155, 'indexes': 8156, 'tucker': 8157, 'anymore': 8158, 'modular': 8159, 'organic': 8160, 'agriculture': 8161, 'stubborn': 8162, 'tribal': 8163, 'ensuing': 8164, 'convict': 8165, 'hastings': 8166, 'puzzling': 8167, 'inquisitive': 8168, 'tactical': 8169, 'sacrifices': 8170, 'apartheid': 8171, 'oceans': 8172, 'refusal': 8173, 'convenience': 8174, 'achieves': 8175, 'hotly': 8176, 'whenever': 8177, 'shrines': 8178, 'preserved': 8179, 'computational': 8180, 'deftly': 8181, 'emerald': 8182, \"haven't\": 8183, 'psychiatrist': 8184, 'weiss': 8185, 'joint': 8186, 'constitutes': 8187, 'entrepreneurship': 8188, 'probes': 8189, 'swashbuckling': 8190, 'evident': 8191, 'evoke': 8192, 'idle': 8193, 'bond’s': 8194, 'stunned': 8195, \"master's\": 8196, 'showcase': 8197, \"fiction'\": 8198, 'creepy': 8199, 'penetrating': 8200, 'rethink': 8201, 'egyptian': 8202, 'tyranny': 8203, 'kansas': 8204, 'company’s': 8205, 'energetic': 8206, 'samurai': 8207, 'rider': 8208, 'warnings': 8209, 'innovators': 8210, 'arises': 8211, 'culminating': 8212, 'grapple': 8213, 'obstacle': 8214, 'votes': 8215, 'guardiola': 8216, 'ticket': 8217, 'disappointments': 8218, 'nigel': 8219, 'melody': 8220, 'blues': 8221, 'burden': 8222, 'encompassing': 8223, 'pele': 8224, 'iranian': 8225, 'twelfth': 8226, 'lifeless': 8227, 'conservative': 8228, 'upheavals': 8229, 'anarchy': 8230, \"follett's\": 8231, 'resourceful': 8232, 'thwart': 8233, 'prophecy': 8234, 'corpses': 8235, 'emphasizes': 8236, 'marketplace': 8237, 'hashing': 8238, 'pulse': 8239, 'laboratory': 8240, 'sr': 8241, 'desktop': 8242, 'ironman': 8243, 'bureaucrats': 8244, 'phases': 8245, 'scripting': 8246, 'gilbert': 8247, 'deft': 8248, 'atop': 8249, 'doorstep': 8250, 'olivia': 8251, 'ember': 8252, 'proceeds': 8253, 'tragedies': 8254, 'rana': 8255, 'rejected': 8256, 'shirt': 8257, 'murty': 8258, 'interviewer': 8259, 'worshipped': 8260, 'malaysia': 8261, 'kenya': 8262, 'chanakya': 8263, 'strategist': 8264, 'invitations': 8265, 'desk': 8266, 'baked': 8267, 'bennett': 8268, 'visited': 8269, 'secluded': 8270, 'scribbles': 8271, 'laureate': 8272, 'boyhood': 8273, 'intention': 8274, 'smiles': 8275, 'magisterial': 8276, 'altar': 8277, 'lavanya': 8278, 'prevalent': 8279, 'socialism': 8280, 'mutual': 8281, 'anecdote': 8282, 'orientation': 8283, 'cleaning': 8284, 'fires': 8285, 'swan': 8286, 'santa': 8287, 'camille': 8288, 'hardest': 8289, 'fischer': 8290, 'resilient': 8291, 'breakthroughs': 8292, 'ardent': 8293, 'woman’s': 8294, 'tezuka': 8295, 'decorated': 8296, 'lifecycle': 8297, 'wine': 8298, \"'his\": 8299, 'tribune': 8300, 'mann': 8301, 'condemned': 8302, 'ribbon': 8303, 'click': 8304, 'voss': 8305, 'trophies': 8306, 'harley': 8307, 'label': 8308, 'standalone': 8309, 'illustrating': 8310, 'indelible': 8311, 'forging': 8312, 'narayan': 8313, 'hospitals': 8314, 'mice': 8315, 'pope': 8316, 'crossword': 8317, 'smarter': 8318, 'chatterjee': 8319, 'geographical': 8320, 'resolved': 8321, 'dexter': 8322, \"collector's\": 8323, 'inhabit': 8324, '—the': 8325, 'relates': 8326, 'viking': 8327, 'sleuth': 8328, 'octane': 8329, 'contrasting': 8330, 'consuming': 8331, 'savings': 8332, '1945': 8333, 'chennai': 8334, 'symbolic': 8335, 'ada': 8336, 'accessories': 8337, 'placement': 8338, '230': 8339, 'bernard': 8340, 'penetration': 8341, 'presidential': 8342, '1995': 8343, 'wrist': 8344, 'unanswered': 8345, 'conception': 8346, 'storytellers': 8347, 'recruit': 8348, 'burns': 8349, 'audrey': 8350, 'observing': 8351, 'falsely': 8352, 'idealistic': 8353, 'hoffman': 8354, 'sanctuary': 8355, 'retrieve': 8356, 'excruciating': 8357, 'craving': 8358, 'landmarks': 8359, 'logically': 8360, '1914': 8361, 'phonic': 8362, 'empress': 8363, 'defying': 8364, 'seventeenth': 8365, 'oblivion': 8366, 'rhus': 8367, 'centric': 8368, 'familiarity': 8369, 'ireland': 8370, 'debated': 8371, 'suffers': 8372, 'cage': 8373, 'waugh': 8374, 'scarlet': 8375, 'consulting': 8376, '‘an': 8377, 'marissa': 8378, 'bust': 8379, 'parks': 8380, 'infant': 8381, 'wrath': 8382, 'plates': 8383, 'deemed': 8384, 'cloth': 8385, 'diagnostic': 8386, 'joyful': 8387, 'fever': 8388, 'bianca': 8389, 'crab': 8390, 'intrinsic': 8391, 'slide': 8392, 'inches': 8393, 'stupid': 8394, 'promoting': 8395, 'mainly': 8396, 'acquisition': 8397, 'outline': 8398, 'strain': 8399, 'jaw': 8400, 'encouragement': 8401, 'fleeing': 8402, 'nervous': 8403, 'bradley': 8404, 'inducing': 8405, 'treasury': 8406, 'sundays': 8407, 'cartooning': 8408, 'favorites': 8409, 'cottage': 8410, 'groom': 8411, 'esteem': 8412, 'courtship': 8413, 'opaque': 8414, 'simone': 8415, 'infographics': 8416, 'collectable': 8417, 'saliva': 8418, 'utilizing': 8419, 'irrepressible': 8420, 'patience': 8421, 'kaur': 8422, 'associates': 8423, 'manic': 8424, 'wittily': 8425, 'ralph': 8426, 'untouched': 8427, 'agony': 8428, 'par': 8429, 'klein': 8430, 'refugees': 8431, 'petty': 8432, 'affirmation': 8433, 'inseparable': 8434, 'throat': 8435, 'bazaar': 8436, 'sweetheart': 8437, 'generous': 8438, 'transactions': 8439, 'morrison': 8440, 'storms': 8441, 'hadley': 8442, 'flood': 8443, 'tess': 8444, 'malone': 8445, 'unexplored': 8446, 'detect': 8447, 'philosophies': 8448, 'ron': 8449, 'mud': 8450, 'practiced': 8451, '“this': 8452, 'revival': 8453, 'readings': 8454, 'engines': 8455, 'chip': 8456, 'organisations': 8457, 'canon': 8458, 'finn': 8459, 'pens': 8460, 'modest': 8461, 'frames': 8462, 'devdutt': 8463, 'alabama': 8464, \"'read\": 8465, 'complemented': 8466, 'hike': 8467, 'blizzard': 8468, 'annotated': 8469, 'accurately': 8470, 'raging': 8471, 'dragged': 8472, 'suited': 8473, 'facilitate': 8474, 'sue': 8475, 'camping': 8476, 'outdoor': 8477, 'compose': 8478, 'avatar': 8479, 'relic': 8480, 'enabled': 8481, 'comment': 8482, 'possessing': 8483, 'unsaid': 8484, 'password': 8485, 'minimum': 8486, '1st': 8487, 'rowling': 8488, 'ignorance': 8489, 'worship': 8490, 'sells': 8491, 'shaw': 8492, 'elon': 8493, 'horrified': 8494, 'bustle': 8495, 'bombs': 8496, 'persons': 8497, 'dubbed': 8498, 'certainty': 8499, 'reunion': 8500, 'adverbs': 8501, 'preventing': 8502, 'conscious': 8503, 'physician': 8504, 'brightest': 8505, \"collectors'\": 8506, 'sketching': 8507, 'gotten': 8508, 'await': 8509, 'investigators': 8510, 'archaeologist': 8511, 'digests': 8512, \"smith's\": 8513, 'badass': 8514, 'marrow': 8515, 'cusp': 8516, 'immediacy': 8517, 'dysfunctional': 8518, 'connectivity': 8519, 'jo': 8520, 'witches': 8521, 'coins': 8522, 'eddie': 8523, 'facsimile': 8524, 'pink': 8525, 'soup': 8526, 'kapil': 8527, 'perks': 8528, 'cater': 8529, 'dip': 8530, 'adolescent': 8531, '1z0': 8532, 'expecting': 8533, \"'how\": 8534, 'keller': 8535, 'advancing': 8536, 'ensues': 8537, 'daylight': 8538, 'freezing': 8539, 'laden': 8540, '43': 8541, 'pather': 8542, 'panchali': 8543, 'ray’s': 8544, 'hallmark': 8545, 'ridden': 8546, 'energies': 8547, 'diving': 8548, 'humane': 8549, 'facilities': 8550, 'gulf': 8551, \"amy's\": 8552, 'tcs': 8553, 'murthy': 8554, '55': 8555, 'brimming': 8556, 'gerrard': 8557, 'custody': 8558, 'recovered': 8559, 'futures': 8560, 'heiress': 8561, 'grateful': 8562, 'tiffany': 8563, 'hysterical': 8564, 'stumble': 8565, 'mountaineers': 8566, 'ferrari': 8567, 'rages': 8568, 'darrow': 8569, 'complications': 8570, 'mogul': 8571, 'ella': 8572, 'diplomacy': 8573, \"me'\": 8574, 'labs': 8575, 'hive': 8576, 'envy': 8577, 'brady': 8578, 'mdash': 8579, 'annabelle': 8580, 'biographer': 8581, 'summon': 8582, 'productions': 8583, \"'life\": 8584, 'inmates': 8585, 'owls': 8586, 'murray': 8587, 'feminine': 8588, 'bleach': 8589, 'alchemy': 8590, 'tents': 8591, 'ellie': 8592, 'spices': 8593, 'solely': 8594, 'confines': 8595, 'entails': 8596, 'owen': 8597, 'dixon': 8598, 'novoneel': 8599, 'aung': 8600, 'jaipur': 8601, 'zakir': 8602, \"history'\": 8603, 'aspirations': 8604, 'lurks': 8605, 'rosemary': 8606, 'istanbul': 8607, 'requests': 8608, 'forgive': 8609, 'textiles': 8610, 'requiring': 8611, 'che': 8612, 'intimidating': 8613, 'thrilled': 8614, 'lenin': 8615, 'dependent': 8616, 'probe': 8617, 'securing': 8618, 'inexplicably': 8619, 'russ': 8620, 'freed': 8621, 'winds': 8622, 'understandable': 8623, 'prosecution': 8624, 'iraqi': 8625, 'defies': 8626, 'manipur': 8627, 'gilded': 8628, 'mia': 8629, 'naga': 8630, 'niall': 8631, 'imran': 8632, 'ass': 8633, 'rat': 8634, 'potato': 8635, 'spss': 8636, 'offerings': 8637, 'museums': 8638, 'derek': 8639, 'bands': 8640, 'halls': 8641, 'syndicate': 8642, 'gorgeously': 8643, 'depict': 8644, 'haunts': 8645, 'bulma': 8646, 'orbs': 8647, 'odessa': 8648, 'actively': 8649, 'boosting': 8650, 'seminars': 8651, 'realism': 8652, 'lilly': 8653, 'constitutional': 8654, 'amartya': 8655, 'mukherjee': 8656, 'resonance': 8657, 'documentation': 8658, 'cent': 8659, 'dhruv': 8660, 'sandman': 8661, 'prospective': 8662, 'stealing': 8663, 'sai': 8664, 'pollution': 8665, 'vector': 8666, 'recall': 8667, 'invoicing': 8668, 'sofa': 8669, 'pronounce': 8670, 'breadth': 8671, 'expansive': 8672, 'garde': 8673, 'holder': 8674, 'flavor': 8675, 'attend': 8676, 'rubik’s': 8677, 'coconut': 8678, 'chronic': 8679, 'jenna': 8680, 'interviewed': 8681, 'reaper': 8682, 'arduino': 8683, 'kublai': 8684, 'shari': 8685, 'qaeda': 8686, 'captaincy': 8687, 'duties': 8688, 'chamberlain': 8689, 'contradictory': 8690, 'atmospheric': 8691, 'nets': 8692, 'keras': 8693, 'vagabond': 8694, 'heller': 8695, 'consolidate': 8696, 'ruttie': 8697, 'reddy': 8698, 'burger': 8699, 'container': 8700, 'logos': 8701, 'isro': 8702, 'airbender': 8703, 'bo': 8704, 'pounds': 8705, 'prescribed': 8706, 'sandberg': 8707, 'verdict': 8708, 'devops': 8709, 'puller': 8710, 'watchmen': 8711, 'saadiq': 8712, 'juliet': 8713, 'competent': 8714, 'hyperledger': 8715, 'adrian’s': 8716, 'tella': 8717, 'astrid': 8718, 'mumford': 8719, 'shay': 8720, 'ble': 8721, 'nuanced': 8722, 'analyzing': 8723, 'deployed': 8724, 'enforce': 8725, 'painstaking': 8726, \"hollywood's\": 8727, 'nationally': 8728, 'blending': 8729, 'prefers': 8730, 'alert': 8731, 'prehistoric': 8732, 'han': 8733, 'kicks': 8734, 'burial': 8735, 'visitor': 8736, 'performer': 8737, 'bacon': 8738, 'translator': 8739, 'commissioned': 8740, 'gratitude': 8741, 'transfer': 8742, 'sooner': 8743, 'beneficial': 8744, 'vii': 8745, 'viii': 8746, 'merchants': 8747, 'originated': 8748, 'consummate': 8749, 'resignation': 8750, 'recognizing': 8751, 'hulk': 8752, 'artifacts': 8753, 'promotional': 8754, 'enrich': 8755, 'smash': 8756, 'reel': 8757, 'marlowe': 8758, 'naval': 8759, 'tracked': 8760, 'harpercollins': 8761, 'oversized': 8762, 'orphanage': 8763, 'sometime': 8764, 'supporter': 8765, 'constructive': 8766, 'doug': 8767, 'elle': 8768, 'albums': 8769, 'excess': 8770, 'turkish': 8771, 'excerpts': 8772, 'novelisation': 8773, 'shiny': 8774, 'suspended': 8775, 'confirms': 8776, 'skilfully': 8777, 'descends': 8778, 'treasured': 8779, 'entitled': 8780, 'jolted': 8781, 'bowie': 8782, 'vajpayee': 8783, 'assam': 8784, 'massacre': 8785, 'conference': 8786, 'overwhelm': 8787, 'samuel': 8788, 'hilariously': 8789, 'tin': 8790, 'executing': 8791, 'yagami': 8792, 'shinigami': 8793, 'hypocrisy': 8794, 'beatles': 8795, 'marvelous': 8796, 'rugby': 8797, 'info': 8798, 'priced': 8799, 'incorrect': 8800, 'recurrent': 8801, 'escalating': 8802, 'bursting': 8803, 'conquering': 8804, 'wrought': 8805, 'divides': 8806, 'navarone': 8807, 'muscular': 8808, 'riots': 8809, 'vortex': 8810, 'premium': 8811, 'wellbeing': 8812, 'washed': 8813, 'gas': 8814, 'aadhaar': 8815, 'tm': 8816, 'blogs': 8817, 'ratan': 8818, 'invested': 8819, 'expansion': 8820, 'scaling': 8821, 'purity': 8822, 'el': 8823, 'palms': 8824, 'durable': 8825, 'encyclopaedia': 8826, 'arjuna': 8827, 'sybex': 8828, 'tier': 8829, 'spotted': 8830, 'defied': 8831, 'authorized': 8832, 'nostalgia': 8833, 'pains': 8834, 'neel': 8835, 'purest': 8836, 'hates': 8837, 'unexplained': 8838, 'jealous': 8839, \"publisher's\": 8840, 'cellular': 8841, 'tablets': 8842, 'configuring': 8843, '37': 8844, 'unsettled': 8845, '125': 8846, 'exhibit': 8847, 'referenced': 8848, 'confronting': 8849, 'meditative': 8850, 'blackmail': 8851, 'ferocious': 8852, 'jersey': 8853, 'candy': 8854, 'piercing': 8855, 'armstrong': 8856, 'propel': 8857, 'dakota': 8858, 'kobe': 8859, 'rebellious': 8860, 'intentions': 8861, 'marking': 8862, 'drums': 8863, 'declare': 8864, 'craftsman': 8865, 'mechanism': 8866, 'capitalist': 8867, 'sleepless': 8868, 'scream': 8869, 'multifaceted': 8870, 'serene': 8871, 'downright': 8872, 'mckinsey': 8873, 'president’s': 8874, 'racist': 8875, 'wronged': 8876, 'rhuk': 8877, 'reviewed': 8878, 'justin': 8879, 'manor': 8880, 'approximately': 8881, 'sapiens': 8882, 'karamazov': 8883, 'breakdown': 8884, \"written'\": 8885, 'questioning': 8886, 'immersed': 8887, 'autobiographies': 8888, 'proposals': 8889, 'fey': 8890, 'gerard': 8891, 'highlands': 8892, 'elegance': 8893, 'bench': 8894, 'spreadsheet': 8895, 'karnataka': 8896, 'deccan': 8897, '130': 8898, 'coloured': 8899, 'convention': 8900, 'selves': 8901, 'vine': 8902, 'waits': 8903, 'eradicate': 8904, 'elaborates': 8905, 'empower': 8906, 'villagers': 8907, 'cnn': 8908, 'roaring': 8909, 'shortest': 8910, 'crows': 8911, 'unscrupulous': 8912, 'companionship': 8913, 'mayor': 8914, 'cycling': 8915, 'intelligently': 8916, 'rugged': 8917, 'poisoned': 8918, 'angle': 8919, 'taps': 8920, 'amazed': 8921, 'cave': 8922, 'splendour': 8923, 'centers': 8924, 'chakra': 8925, 'railroad': 8926, 'korea': 8927, 'conversational': 8928, 'alarming': 8929, 'nikola': 8930, 'mustafa': 8931, 'conveys': 8932, 'hurting': 8933, 'sponsored': 8934, 'erstwhile': 8935, 'garner': 8936, 'praveen': 8937, 'accountable': 8938, 'wildest': 8939, 'salary': 8940, 'mitch': 8941, 'incorporating': 8942, 'endeavours': 8943, 'showdown': 8944, 'craig': 8945, 'wandered': 8946, 'novellas': 8947, 'improbable': 8948, 'alpine': 8949, 'au': 8950, 'somebody': 8951, 'paulo': 8952, 'lit': 8953, \"society's\": 8954, 'corresponding': 8955, 'tantalizing': 8956, 'preserving': 8957, 'hound': 8958, 'pound': 8959, 'idiom': 8960, 'trio': 8961, 'eyeball': 8962, 'unimpressive': 8963, 'fend': 8964, 'famine': 8965, 'starvation': 8966, 'headquarters': 8967, 'austen': 8968, 'wonderland': 8969, 'provoke': 8970, 'longtime': 8971, 'narasimha': 8972, 'sensitivity': 8973, 'massachusetts': 8974, 'recruited': 8975, 'ins': 8976, 'roadmap': 8977, 'distances': 8978, 'extinction': 8979, 'gallant': 8980, 'rescues': 8981, 'accomplishments': 8982, 'sandy': 8983, 'warring': 8984, 'commandant': 8985, 'departments': 8986, 'webcomic': 8987, 'squirrel': 8988, 'purchasing': 8989, 'shore': 8990, 'weddings': 8991, 'involve': 8992, 'impoverished': 8993, '77': 8994, 'eddard': 8995, 'aka': 8996, 'prizes': 8997, 'emperors': 8998, 'sighted': 8999, 'dahl': 9000, 'misunderstandings': 9001, 'bury': 9002, '3rd': 9003, 'handwritten': 9004, 'fascinate': 9005, 'wallflower': 9006, 'exposé': 9007, 'sci': 9008, 'dramas': 9009, 'ironically': 9010, 'carve': 9011, 'impeccable': 9012, 'maintained': 9013, 'cofounder': 9014, 'realist': 9015, 'redefining': 9016, 'warned': 9017, 'suresh': 9018, 'wills': 9019, 'nap': 9020, 'pie': 9021, 'matarese': 9022, 'untimely': 9023, 'ignoring': 9024, 'imaginations': 9025, 'waitzkin': 9026, 'pinnacle': 9027, 'city’s': 9028, 'powerhouse': 9029, 'cartoonists': 9030, 'wayward': 9031, 'alienation': 9032, \"'he\": 9033, 'booklet': 9034, 'deceptively': 9035, 'ironic': 9036, 'crusade': 9037, 'participate': 9038, 'raghu': 9039, 'roof': 9040, 'contentious': 9041, 'empowering': 9042, 'honestly': 9043, 'game’s': 9044, 'tantrums': 9045, 'meltdown': 9046, 'masterfully': 9047, 'fleet': 9048, \"gulzar's\": 9049, 'adaptations': 9050, \"shakespeare's\": 9051, 'arsenal': 9052, 'lacks': 9053, 'bought': 9054, 'crashed': 9055, 'goa': 9056, 'naomi': 9057, 'looming': 9058, 'arriving': 9059, 'verve': 9060, \"girl's\": 9061, 'invade': 9062, 'arguing': 9063, 'burma': 9064, 'fort': 9065, 'wrestling': 9066, 'sublime': 9067, 'opposing': 9068, 'inhabited': 9069, 'inexorably': 9070, 'scheffler': 9071, 'scarecrow': 9072, 'cryptography': 9073, 'eleanor': 9074, 'ned': 9075, 'impose': 9076, 'flame': 9077, 'villa': 9078, 'amsterdam': 9079, 'deranged': 9080, 'dispatched': 9081, 'tommy': 9082, 'maze': 9083, 'sprayed': 9084, 'yearns': 9085, 'replaced': 9086, 'wow': 9087, 'lush': 9088, 'deb': 9089, 'master’s': 9090, 'candor': 9091, 'downey': 9092, 'flourishing': 9093, 'caped': 9094, 'discouraged': 9095, 'bowl': 9096, 'evocation': 9097, 'recommendation': 9098, 'invaded': 9099, 'demanded': 9100, \"khan's\": 9101, 'riven': 9102, 'sing': 9103, 'vikramaditya': 9104, 'wily': 9105, 'arch': 9106, 'cracks': 9107, 'antonio': 9108, \"ladybird's\": 9109, 'carers': 9110, 'transmission': 9111, 'pilgrim': 9112, '00': 9113, 'strangest': 9114, 'dna': 9115, 'grappling': 9116, 'marc': 9117, 'advancement': 9118, 'perceived': 9119, 'navigates': 9120, 'honed': 9121, 'stray': 9122, 'smartest': 9123, 'labyrinth': 9124, 'tearing': 9125, 'catches': 9126, 'century’s': 9127, 'coma': 9128, 'boom': 9129, 'castafiore': 9130, 'herge': 9131, 'lotus': 9132, 'sharks': 9133, 'nigeria': 9134, 'repeat': 9135, 'abdominals': 9136, 'inhale': 9137, 'exhale': 9138, 'distracted': 9139, 'alfred': 9140, 'contest': 9141, 'clinical': 9142, \"'my\": 9143, 'summers': 9144, 'epidemic': 9145, 'launches': 9146, 'appalling': 9147, 'sage': 9148, 'silently': 9149, 'mohan': 9150, 'sacrificed': 9151, 'graduating': 9152, 'remind': 9153, 'annoying': 9154, 'slim': 9155, 'sunset': 9156, 'mornings': 9157, 'crescent': 9158, 'abridged': 9159, 'alliances': 9160, 'valiant': 9161, 'repression': 9162, 'collaborator': 9163, 'revive': 9164, 'swallows': 9165, 'regard': 9166, 'comeback': 9167, 'gadgets': 9168, 'infiltrate': 9169, 'revolutions': 9170, '18th': 9171, 'finely': 9172, 'tag': 9173, 'clarify': 9174, 'enlightened': 9175, 'dusty': 9176, 'mercedes': 9177, 'pragmatic': 9178, 'garbage': 9179, 'documented': 9180, 'harrison': 9181, 'pedagogy': 9182, 'brent': 9183, 'guile': 9184, 'blinding': 9185, 'blazing': 9186, 'sectors': 9187, 'contribute': 9188, 'literal': 9189, 'salt': 9190, 'misses': 9191, 'columnist': 9192, 'comprise': 9193, '29': 9194, 'movers': 9195, 'magicians': 9196, 'forthcoming': 9197, 'mum': 9198, 'extraordinarily': 9199, 'liked': 9200, 'crow': 9201, 'wolves': 9202, 'marketoonist': 9203, 'trenches': 9204, 'marketers': 9205, 'poorest': 9206, 'arvind': 9207, 'accusations': 9208, 'deconstructs': 9209, 'uneasy': 9210, 'cluster': 9211, 'bunch': 9212, 'commanding': 9213, 'tangle': 9214, 'schedule': 9215, 'flames': 9216, 'federer': 9217, 'drastically': 9218, 'bounds': 9219, 'coben': 9220, 'explorations': 9221, 'nasa': 9222, 'storyline': 9223, 'undergo': 9224, 'peculiar': 9225, 'refugee': 9226, 'vanish': 9227, 'figured': 9228, 'helicopter': 9229, 'generics': 9230, 'football’s': 9231, 'mercilessly': 9232, 'billingham': 9233, 'predictable': 9234, 'dogged': 9235, 'tonight': 9236, 'recursion': 9237, 'innings': 9238, '71': 9239, 'smoking': 9240, 'titanic': 9241, 'conditioned': 9242, 'seekers': 9243, 'funnybones': 9244, 'akshay': 9245, \"person's\": 9246, 'phrasal': 9247, 'akash': 9248, 'contributes': 9249, 'triggered': 9250, 'desired': 9251, 'cricket’s': 9252, 'predicts': 9253, 'threw': 9254, 'corporation': 9255, 'jodi': 9256, 'astronomy': 9257, 'monument': 9258, 'forthis': 9259, 'spectacularly': 9260, 'punches': 9261, 'tides': 9262, 'potentially': 9263, 'nadia': 9264, 'soothe': 9265, 'darkseid': 9266, 'deeds': 9267, 'bursts': 9268, 'boxing': 9269, 'vigilante': 9270, 'wipe': 9271, 'obsessions': 9272, 'vance': 9273, \"'will\": 9274, \"night'\": 9275, \"classic'\": 9276, 'playboy': 9277, 'housekeeper': 9278, 'seizing': 9279, 'housewife': 9280, 'motivating': 9281, 'transcendent': 9282, 'dubious': 9283, 'discrimination': 9284, 'conveying': 9285, 'wayne': 9286, 'scikit': 9287, 'linton': 9288, 'safer': 9289, 'jewel': 9290, 'fascination': 9291, 'irregular': 9292, 'trend': 9293, 'keith': 9294, 'guiding': 9295, 'rocky': 9296, 'underwater': 9297, 'boxed': 9298, 'rs': 9299, 'arjun': 9300, 'compression': 9301, 'coveted': 9302, 'promotes': 9303, 'extension': 9304, 'wales': 9305, 'novelty': 9306, 'prices': 9307, '1940': 9308, 'retains': 9309, 'cites': 9310, 'unsung': 9311, 'vampires': 9312, 'nair': 9313, \"corbett's\": 9314, 'madeline': 9315, \"bertie's\": 9316, 'confinement': 9317, \"down'\": 9318, '36': 9319, 'scoop': 9320, 'rai': 9321, 'wheelchair': 9322, 'employee': 9323, 'separation': 9324, 'spite': 9325, 'offensive': 9326, 'chan': 9327, 'refining': 9328, 'afridi': 9329, 'wickets': 9330, 'defeating': 9331, 'reporters': 9332, 'crooks': 9333, 'pithy': 9334, 'advises': 9335, 'violin': 9336, 'sunny': 9337, 'begs': 9338, 'labyrinthine': 9339, 'longman': 9340, 'broadcast': 9341, 'decent': 9342, 'trajectory': 9343, '1955': 9344, 'pig': 9345, 'parenthood': 9346, 'consequently': 9347, 'flavour': 9348, 'idol': 9349, 'fulfilling': 9350, 'dread': 9351, 'troy': 9352, 'recovering': 9353, 'rated': 9354, 'additionally': 9355, 'owning': 9356, 'joona': 9357, 'impression': 9358, 'atrocities': 9359, 'murder…': 9360, 'crafts': 9361, 'supercar': 9362, 'breakdowns': 9363, 'bi': 9364, 'incapable': 9365, 'millennia': 9366, 'datasets': 9367, 'breakfast': 9368, 'additions': 9369, 'possession': 9370, 'interrupted': 9371, '1970': 9372, 'curve': 9373, 'experimenting': 9374, 'anton': 9375, 'removed': 9376, 'yousafzai': 9377, 'descend': 9378, 'specialized': 9379, 'graveyard': 9380, 'anya': 9381, 'yesterday': 9382, 'kindle': 9383, 'compositions': 9384, 'keywords': 9385, 'mentality': 9386, 'lengths': 9387, 'paranormal': 9388, 'collector': 9389, 'professors': 9390, 'panther': 9391, 'inspirations': 9392, 'kohli': 9393, 'conversion': 9394, 'parking': 9395, 'magnum': 9396, 'attract': 9397, 'interiors': 9398, 'iphone': 9399, '1b': 9400, '10th': 9401, 'celeste': 9402, 'collaborations': 9403, 'pays': 9404, 'shoots': 9405, '42': 9406, 'steals': 9407, 'panoramic': 9408, 'misaki': 9409, 'usui': 9410, 'varnams': 9411, 'adi': 9412, 'bounty': 9413, 'federal': 9414, 'cuba': 9415, 'lisbeth': 9416, 'ranking': 9417, 'readership': 9418, 'azad': 9419, 'dispute': 9420, 'broader': 9421, 'whitney': 9422, 'mirrors': 9423, 'maharaja': 9424, 'agonizing': 9425, 'deciding': 9426, 'befriends': 9427, 'unearthly': 9428, 'luxurious': 9429, 'fewer': 9430, '1862': 9431, 'casts': 9432, 'unearths': 9433, 'wreak': 9434, 'resurrected': 9435, 'tape': 9436, 'popularly': 9437, 'switching': 9438, 'scenery': 9439, 'moreover': 9440, '103': 9441, 'playwright': 9442, 'guards': 9443, 'civilian': 9444, 'lawson': 9445, 'leslie': 9446, 'deprecating': 9447, 'stated': 9448, 'missile': 9449, 'officially': 9450, 'bridges': 9451, 'they’ll': 9452, 'inappropriate': 9453, 'crumbling': 9454, 'comparisons': 9455, 'motherhood': 9456, 'babur': 9457, 'epoch': 9458, 'obviously': 9459, 'guitarists': 9460, 'deputy': 9461, 'tune': 9462, 'blaise': 9463, 'skull': 9464, 'bullying': 9465, 'limbs': 9466, 'protest': 9467, 'paradox': 9468, 'vorderman': 9469, 'djinn': 9470, 'songwriter': 9471, 'mould': 9472, 'singers': 9473, 'servants': 9474, 'stamina': 9475, 'gossip': 9476, 'hears': 9477, 'appalachian': 9478, 'tantra': 9479, 'exceed': 9480, 'ambrose': 9481, 'fpv': 9482, 'subjective': 9483, 'iacocca': 9484, 'neal': 9485, 'jihad': 9486, 'ambiguity': 9487, 'fantastical': 9488, '1920s': 9489, 'infinite': 9490, '33': 9491, 'paramount': 9492, 'evaluating': 9493, 'inherits': 9494, 'juggling': 9495, 'cocktail': 9496, 'boats': 9497, 'sabrina': 9498, 'ganesha': 9499, 'uber': 9500, 'calculations': 9501, 'stretched': 9502, 'filming': 9503, 'conjunction': 9504, 'persona': 9505, 'sassy': 9506, 'contempt': 9507, 'perceptions': 9508, 'pinker': 9509, 'familiarise': 9510, 'segregation': 9511, 'lauded': 9512, 'coalition': 9513, 'multicolour': 9514, 'uttar': 9515, 'scheduling': 9516, 'css3': 9517, 'corleone': 9518, 'percentage': 9519, 'mercenary': 9520, \"'brilliant\": 9521, '8086': 9522, 'dreamer': 9523, 'aggressive': 9524, 'converted': 9525, 'bread': 9526, 'outlaws': 9527, 'specifications': 9528, 'sizzling': 9529, 'mccloud': 9530, 'chiffre': 9531, 'obesity': 9532, 'routes': 9533, 'dhondutai': 9534, 'mode': 9535, 'milan': 9536, \"organization's\": 9537, 'royce': 9538, 'warded': 9539, 'swarm': 9540, 'tooth': 9541, 'uml': 9542, 'pataudi': 9543, 'aviation': 9544, 'drafts': 9545, 'properties': 9546, 'b2': 9547, 'minimal': 9548, 'boo': 9549, 'tapescripts': 9550, 'imperfect': 9551, 'blandings': 9552, 'sinha': 9553, 'vicar': 9554, 'protestant': 9555, 'homeless': 9556, 'devised': 9557, 'balloons': 9558, 'elves': 9559, 'diwali': 9560, 'veil': 9561, 'endorsed': 9562, 'soap': 9563, \"'tense\": 9564, 'augmented': 9565, 'alexa': 9566, 'rig': 9567, 'dink': 9568, 'conventions': 9569, 'dancing': 9570, 'monday': 9571, 'oath': 9572, 'bloodline': 9573, 'smaller': 9574, 'earth’s': 9575, 'charnwood': 9576, 'cares': 9577, 'patna': 9578, 'israelis': 9579, 'dent': 9580, 'cranston': 9581, \"batman's\": 9582, 'computation': 9583, 'rin': 9584, 'vmware': 9585, 'hygge': 9586, 'struts': 9587, 'kasparov': 9588, 'duck': 9589, 'beatrix': 9590, 'jvm': 9591, 'bhojpuri': 9592, 'marco': 9593, 'polo': 9594, 'ke': 9595, 'claiming': 9596, 'rejection': 9597, 'retaining': 9598, 'deceased': 9599, 'confirmed': 9600, 'photojournalism': 9601, 'accolades': 9602, 'concerning': 9603, 'socially': 9604, 'pan': 9605, 'opium': 9606, 'vitality': 9607, 'comparative': 9608, 'distract': 9609, 'selecting': 9610, 'transports': 9611, 'mate': 9612, 'sasuke': 9613, 'graph': 9614, 'nana': 9615, 'japan’s': 9616, 'detours': 9617, 'napkin': 9618, 'cigarettes': 9619, 'trafficking': 9620, 'gaps': 9621, 'bets': 9622, 'stem': 9623, 'streak': 9624, 'undertook': 9625, 'tasked': 9626, 'macdonald': 9627, 'astrology': 9628, 'happenings': 9629, 'glued': 9630, 'lecter': 9631, 'wherein': 9632, 'rips': 9633, 'quo': 9634, 'inextricably': 9635, 'deliberately': 9636, 'distinguish': 9637, 'filters': 9638, 'suspension': 9639, 'barefoot': 9640, 'dylan': 9641, 'ahern': 9642, 'edith': 9643, 'ransom': 9644, 'timed': 9645, 'recreates': 9646, 'purchased': 9647, 'arizona': 9648, 'fragments': 9649, 'tore': 9650, \"moving'\": 9651, 'deed': 9652, 'rajesh': 9653, 'dickens': 9654, 'bachelor': 9655, 'catapulted': 9656, 'recognizes': 9657, \"light's\": 9658, 'nouns': 9659, 'assertion': 9660, 'doll': 9661, 'vanishing': 9662, 'odi': 9663, 'jammu': 9664, 'slips': 9665, 'sedaris': 9666, 'sharper': 9667, 'arena': 9668, 'mcenroe': 9669, 'locker': 9670, 'indoors': 9671, 'calming': 9672, 'criteria': 9673, 'steadfast': 9674, 'ate': 9675, 'enterprises': 9676, 'olympiad': 9677, 'mill': 9678, 'compute': 9679, 'gestures': 9680, 'reborn': 9681, 'shedding': 9682, \"david's\": 9683, 'beckham': 9684, 'reaction': 9685, 'repetitive': 9686, 'taxes': 9687, 'lovable': 9688, 'admirers': 9689, 'malady': 9690, 'gamut': 9691, 'etiquette': 9692, 'spaceman': 9693, 'drooling': 9694, 'recollection': 9695, 'belongings': 9696, 'ominous': 9697, 'storied': 9698, 'selfless': 9699, 'varying': 9700, 'dennis': 9701, 'bryant': 9702, 'conceal': 9703, 'reuse': 9704, 'contours': 9705, 'dropout': 9706, 'ecology': 9707, 'habitats': 9708, 'otherworldly': 9709, 'mit': 9710, 'thug': 9711, 'triggers': 9712, 'bumper': 9713, 'crop': 9714, 'proposal': 9715, 'goldberg': 9716, 'sentiment': 9717, 'homo': 9718, 'entity': 9719, 'adapts': 9720, '54': 9721, \"'there\": 9722, 'astonishingly': 9723, 'happiest': 9724, 'poirot’s': 9725, 'formative': 9726, 'conflicting': 9727, 'marital': 9728, 'claude': 9729, 'neat': 9730, 'accumulated': 9731, 'robot': 9732, 'concisely': 9733, 'honeymoon': 9734, 'awakened': 9735, 'scandalous': 9736, 'greedy': 9737, 'rescued': 9738, 'nursing': 9739, 'spare': 9740, 'baaz': 9741, 'siddhant': 9742, 'spirals': 9743, 'perennial': 9744, 'programmed': 9745, \"'truly\": 9746, 'therapist': 9747, 'booklist': 9748, 'fray': 9749, 'fogg': 9750, 'ruthlessness': 9751, \"miller's\": 9752, 'memorize': 9753, 'guinness': 9754, 'danish': 9755, 'experimentation': 9756, 'owes': 9757, 'blown': 9758, \"'wonderfully\": 9759, 'herman': 9760, 'merciless': 9761, 'extraction': 9762, 'urgently': 9763, 'taita': 9764, 'citadel': 9765, 'primer': 9766, 'pronoun': 9767, 'inadequate': 9768, 'ayn': 9769, 'atlas': 9770, 'petersburg': 9771, '1943': 9772, 'aspire': 9773, 'goths': 9774, 'colloquial': 9775, 'metaphors': 9776, 'tainted': 9777, 'concluded': 9778, 'coined': 9779, 'upa': 9780, 'oppose': 9781, 'indisputable': 9782, 'managerial': 9783, 'kirby': 9784, 'cache': 9785, 'pep': 9786, 'eras': 9787, 'aura': 9788, 'bells': 9789, 'lightly': 9790, 'lou': 9791, 'honorary': 9792, 'doubly': 9793, 'escalates': 9794, 'barren': 9795, 'howl': 9796, 'wasn’t': 9797, 'academics': 9798, 'niece': 9799, 'beset': 9800, 'initiated': 9801, 'valleys': 9802, 'jumping': 9803, 'lucidity': 9804, 'pact': 9805, 'rainforest': 9806, 'a2': 9807, 'idiomatic': 9808, 'longevity': 9809, 'liberation': 9810, 'buddhism': 9811, 'impassioned': 9812, 'keepers': 9813, 'attracting': 9814, 'intellectuals': 9815, 'culminates': 9816, 'advocates': 9817, 'witnessing': 9818, 'meteoric': 9819, 'humiliation': 9820, 'wiped': 9821, 'tent': 9822, '1924': 9823, 'uniting': 9824, 'mutilated': 9825, 'generals': 9826, 'carnage': 9827, 'laia’s': 9828, 'grandparents': 9829, '3000': 9830, 'phillips': 9831, 'comedies': 9832, 'imaging': 9833, 'utilize': 9834, 'grounded': 9835, 'mixture': 9836, 'declining': 9837, 'drought': 9838, 'interviewee': 9839, \"driver's\": 9840, '1938': 9841, 'roosevelt': 9842, 'lannister': 9843, 'king’s': 9844, 'submit': 9845, 'emphasizing': 9846, 'assert': 9847, 'surya': 9848, 'hats': 9849, 'realisation': 9850, 'tuned': 9851, 'abap': 9852, 'mobility': 9853, 'dares': 9854, 'animations': 9855, 'loomis': 9856, 'investor': 9857, 'के': 9858, 'है': 9859, 'excesses': 9860, 'das': 9861, 'ohio': 9862, 'heap': 9863, 'feudal': 9864, 'opulence': 9865, 'princes': 9866, 'preferred': 9867, 'tuck': 9868, 'ministry': 9869, 'nagaland': 9870, 'consideration': 9871, 'brooding': 9872, \"writer'\": 9873, 'bills': 9874, 'strictly': 9875, 'dominating': 9876, 'droppings': 9877, 'cops': 9878, 'admits': 9879, 'briefly': 9880, \"japan's\": 9881, 'honored': 9882, 'mounting': 9883, 'vet': 9884, 'catcher': 9885, \"obama's\": 9886, 'indiana': 9887, 'theatrical': 9888, 'fusion': 9889, 'sins': 9890, \"comics'\": 9891, 'activism': 9892, 'suburb': 9893, 'confidant': 9894, 'spawned': 9895, 'pi': 9896, 'framing': 9897, 'manmohan': 9898, 'restaurants': 9899, 'halt': 9900, 'calmly': 9901, 'repair': 9902, 'judith': 9903, 'warlord': 9904, 'admiral': 9905, 'gulzar': 9906, 'quran': 9907, 'commentaries': 9908, 'gillian': 9909, 'vincenzi': 9910, 'unlucky': 9911, 'bumbling': 9912, 'scroll': 9913, 'hamlet': 9914, 'grisly': 9915, \"'with\": 9916, 'ramachandra': 9917, '‘you': 9918, 'geek': 9919, 'reticent': 9920, 'hardcore': 9921, 'underestimated': 9922, 'forbidding': 9923, 'inventors': 9924, 'turing': 9925, 'meteorite': 9926, \"tintin's\": 9927, 'zog': 9928, 'goodwin': 9929, \"genius'\": 9930, 'supporters': 9931, 'repackaged': 9932, 'telegram': 9933, 'compulsively': 9934, \"china's\": 9935, 'intends': 9936, 'incorrigible': 9937, 'hamburg': 9938, 'innocents': 9939, 'grisha': 9940, 'sumptuous': 9941, 'dives': 9942, 'powerpoint': 9943, 'sc': 9944, 'rebirth': 9945, 'crossfire': 9946, 'righteous': 9947, 'absurdity': 9948, 'ira': 9949, 'laced': 9950, 'noticed': 9951, 'easiest': 9952, 'marlon': 9953, 'brando': 9954, 'obtained': 9955, 'crusader': 9956, 'treated': 9957, 'depicting': 9958, 'recommender': 9959, 'watterson': 9960, 'pat': 9961, 'disparate': 9962, 'eternity': 9963, 'endeavour': 9964, 'governed': 9965, 'behave': 9966, 'historically': 9967, 'façade': 9968, 'moody': 9969, 'devas': 9970, 'illuminate': 9971, 'disrupt': 9972, 'picasso': 9973, 'polio': 9974, 'vowels': 9975, 'banished': 9976, 'stumbling': 9977, 'shields': 9978, 'initiative': 9979, 'dismissed': 9980, 'speculation': 9981, 'burman': 9982, 'piper': 9983, 'succumb': 9984, '1857': 9985, 'clothbound': 9986, 'soviets': 9987, 'rohan': 9988, 'strengthens': 9989, 'neuroscience': 9990, 'traditionally': 9991, 'backbone': 9992, 'morbid': 9993, 'plausible': 9994, 'automatically': 9995, 'winters': 9996, 'sorcerers': 9997, 'omens': 9998, 'bodybuilders': 9999, 'brunton': 10000, 'sino': 10001, \"through'\": 10002, 'leopard': 10003, 'vimal': 10004, 'dailies': 10005, 'fortunate': 10006, 'unapologetically': 10007, 'lap': 10008, 'illuminated': 10009, 'terribly': 10010, 'cassandra': 10011, 'shadowhunter': 10012, 'wouldn’t': 10013, 'reinventing': 10014, 'acknowledgements': 10015, 'martyr': 10016, 'silenced': 10017, 'elegantly': 10018, 'smiley': 10019, 'brittany': 10020, 'stressed': 10021, 'distinction': 10022, 'subhash': 10023, 'ban': 10024, 'sachs': 10025, 'openness': 10026, 'valid': 10027, 'rani': 10028, 'upsc': 10029, 'starlet': 10030, 'disturbed': 10031, 'highway': 10032, 'stole': 10033, 'centred': 10034, 'carpet': 10035, 'campfire': 10036, 'versatile': 10037, 'burton': 10038, 'ebay': 10039, 'boasts': 10040, 'conjunctions': 10041, 'exuberant': 10042, 'baltimore': 10043, 'cons': 10044, 'attendant': 10045, 'pulp': 10046, 'fraction': 10047, 'capullo': 10048, 'credits': 10049, 'ego': 10050, 'encryption': 10051, 'arithmetic': 10052, 'fruit': 10053, 'si': 10054, 'mills': 10055, 'linkedin': 10056, 'conflicted': 10057, 'karla': 10058, 'resurface': 10059, 'eden': 10060, 'demystifies': 10061, 'shutter': 10062, 'villainous': 10063, 'venus': 10064, 'harlan': 10065, 'fabulously': 10066, 'depiction': 10067, 'heredity': 10068, 'wiping': 10069, 'exhausted': 10070, 'pattanaik': 10071, 'horses': 10072, 'shaken': 10073, 'dreaded': 10074, 'recount': 10075, 'pertaining': 10076, 'ulf': 10077, 'norse': 10078, 'likeable': 10079, 'typically': 10080, 'flag': 10081, \"we'd\": 10082, 'friday': 10083, 'maggie': 10084, 'glimpsed': 10085, 'schoolboy': 10086, 'urged': 10087, 'onwards': 10088, 'yearning': 10089, 'constructs': 10090, 'leagues': 10091, '1911': 10092, 'manipulating': 10093, 'policeman': 10094, \"emma's\": 10095, \"archer's\": 10096, 'explode': 10097, 'consultation': 10098, 'modelling': 10099, 'template': 10100, 'entities': 10101, 'batting': 10102, 'predicting': 10103, 'sadness': 10104, 'silk': 10105, 'operator': 10106, 'merge': 10107, 'faculty': 10108, 'troublesome': 10109, 'esteemed': 10110, 'colin': 10111, 'revel': 10112, 'feeds': 10113, 'clusters': 10114, 'complement': 10115, 'psy': 10116, 'valiantly': 10117, 'observes': 10118, 'crippled': 10119, 'ravana': 10120, 'exiled': 10121, 'imposed': 10122, 'superlative': 10123, 'solace': 10124, 'comical': 10125, 'bubble': 10126, 'satellite': 10127, 'adviser': 10128, 'sufficient': 10129, 'rescuing': 10130, \"other's\": 10131, 'botham': 10132, 'purposeful': 10133, 'crushed': 10134, 'unwanted': 10135, 'gerry': 10136, 'sleeper': 10137, 'shimla': 10138, 'greenaway': 10139, 'levi': 10140, 'updating': 10141, 'screenwriters': 10142, 'anyway': 10143, 'stanton': 10144, 'jacket': 10145, 'val': 10146, 'unwavering': 10147, 'prominence': 10148, 'foundational': 10149, 'cli': 10150, 'restful': 10151, 'responsive': 10152, 'rahman': 10153, 'resounding': 10154, 'affectionate': 10155, 'lamora': 10156, 'factions': 10157, 'protagonists': 10158, 'kashmiri': 10159, 'assertive': 10160, 'bloomwood': 10161, 'mayo': 10162, 'bookseller': 10163, 'worm': 10164, 'frontline': 10165, 'sneak': 10166, 'composers': 10167, 'fluency': 10168, 'gujarati': 10169, 'punjabi': 10170, 'duel': 10171, 'tournaments': 10172, 'drops': 10173, \"'unputdownable\": 10174, \"irresistible'\": 10175, 'butler': 10176, 'faber': 10177, 'jesse': 10178, 'harriet': 10179, 'minister’s': 10180, 'ghetto': 10181, 'milieu': 10182, 'ramifications': 10183, 'tharoor': 10184, 'fledged': 10185, 'sibyl': 10186, 'temporary': 10187, 'plethora': 10188, 'perarnau': 10189, 'munich': 10190, 'wreckage': 10191, 'helpless': 10192, 'priority': 10193, 'undead': 10194, 'karan': 10195, 'eva': 10196, 'extracts': 10197, 'modes': 10198, 'retribution': 10199, 'finances': 10200, 'melt': 10201, 'evie': 10202, 'guts': 10203, 'conspiracies': 10204, 'comprehensively': 10205, 'acrylic': 10206, 'afar': 10207, 'optimistic': 10208, 'sophistication': 10209, 'hadoop': 10210, 'solomon': 10211, 'breaches': 10212, 'nesbo': 10213, 'susie': 10214, 'eloquently': 10215, 'stalking': 10216, 'tightly': 10217, 'gussie': 10218, 'knots': 10219, 'ho': 10220, 'fold': 10221, 'alphabetical': 10222, 'digging': 10223, 'vegetarian': 10224, 'kashmiris': 10225, 'install': 10226, 'raina': 10227, 'amelia': 10228, 'bodybuilder': 10229, 'changer': 10230, 'insecure': 10231, 'foer': 10232, 'bales': 10233, 'corelings': 10234, 'lurk': 10235, 'dani': 10236, 'stew': 10237, 'lucidly': 10238, 'superiors': 10239, 'wiser': 10240, 'genes': 10241, 'shady': 10242, 'liberating': 10243, 'arabic': 10244, 'linna': 10245, 'psychiatric': 10246, 'unbelievably': 10247, 'pajaro': 10248, 'attacking': 10249, 'ripley': 10250, 'tattooed': 10251, 'nuts': 10252, 'seduction': 10253, 'gerald': 10254, 'homeopathy': 10255, 'pacy': 10256, 'ageing': 10257, 'levy': 10258, 'achievers': 10259, 'cultivated': 10260, '8th': 10261, 'installment': 10262, 'occasional': 10263, 'unerring': 10264, 'allegiance': 10265, 'vikings': 10266, 'belly': 10267, 'reported': 10268, 'corps': 10269, \"game's\": 10270, 'amanda': 10271, '1952': 10272, 'dilip': 10273, 'attributes': 10274, 'poppy': 10275, 'sprinkleofglitter': 10276, \"laughter'\": 10277, 'kurukshetra': 10278, 'tourists': 10279, 'gidwani': 10280, \"york's\": 10281, 'frustration': 10282, 'baroque': 10283, \"i'll\": 10284, \"brilliant'\": 10285, 'blogger': 10286, 'sincere': 10287, 'pressed': 10288, 'backing': 10289, 'adobe': 10290, \"cuckoo's\": 10291, 'dehra': 10292, 'directing': 10293, 'troubleshoot': 10294, 'schedules': 10295, 'meghalaya': 10296, 'peril': 10297, 'bras': 10298, 'quicker': 10299, '350': 10300, 'sharpen': 10301, 'slay': 10302, 'caioli': 10303, 'sphere': 10304, 'snapchat': 10305, 'mickey': 10306, 'defending': 10307, 'unwillingly': 10308, 'opus': 10309, 'affects': 10310, 'instructive': 10311, 'flawless': 10312, 'comfortably': 10313, 'ng': 10314, 'cherry': 10315, 'normally': 10316, 'galbraith': 10317, 'outsiders': 10318, 'numbered': 10319, 'colonies': 10320, 'buenos': 10321, 'aires': 10322, 'purely': 10323, \"ray's\": 10324, 'attached': 10325, 'meal': 10326, 'celestial': 10327, 'gideon': 10328, 'karin': 10329, 'rampage': 10330, 'ok': 10331, 'avid': 10332, 'hopeless': 10333, 'mitnick': 10334, 'eerily': 10335, 'quotation': 10336, 'pin': 10337, 'sizes': 10338, 'cursed': 10339, 'trusting': 10340, 'rolling': 10341, 'joyous': 10342, 'civilians': 10343, 'regimes': 10344, 'reflected': 10345, 'ailments': 10346, \"town's\": 10347, 'derived': 10348, 'drinks': 10349, 'vendor': 10350, 'citation': 10351, 'fairness': 10352, 'pasts': 10353, 'procedure': 10354, 'subramanian': 10355, 'unbearable': 10356, '1978': 10357, 'flamboyant': 10358, 'segment': 10359, 'extremists': 10360, 'fingerstyle': 10361, 'negotiate': 10362, 'secured': 10363, 'sketchbook': 10364, 'demise': 10365, 'decisive': 10366, 'sabha': 10367, 'govern': 10368, 'stern': 10369, 'respectively': 10370, 'announced': 10371, 'smartphone': 10372, 'awaken': 10373, 'restlessness': 10374, 'banker': 10375, \"disney's\": 10376, 'infested': 10377, 'maurice': 10378, 'blistering': 10379, 'computations': 10380, 'killings': 10381, 'toilets': 10382, 'bolts': 10383, \"apple's\": 10384, \"schaum's\": 10385, 'modesty': 10386, 'snap': 10387, 'shannon': 10388, 'margo': 10389, 'butterflies': 10390, 'gallery': 10391, 'misfortune': 10392, 'genetics': 10393, 'misstep': 10394, 'garcia': 10395, 'linda': 10396, 'avoids': 10397, 'optimizing': 10398, 'millennial': 10399, 'charlotte': 10400, 'eligible': 10401, 'quincy': 10402, 'manuscripts': 10403, 'culturally': 10404, 'bookscan': 10405, 'mechanisms': 10406, 'racer': 10407, 'zones': 10408, '2022': 10409, '1972': 10410, 'leap': 10411, 'pointed': 10412, 'hosseini': 10413, 'sarajevo': 10414, 'fates': 10415, 'adolf': 10416, 'dalai': 10417, 'weights': 10418, 'correspondent': 10419, 'headline': 10420, 'enmity': 10421, 'conquest': 10422, \"muhammad's\": 10423, 'foreigner': 10424, 'cycles': 10425, 'burgers': 10426, 'honey': 10427, 'nominee': 10428, 'occult': 10429, 'necessarily': 10430, 'coral': 10431, 'greetings': 10432, 'madhubani': 10433, 'mani': 10434, 'alley': 10435, 'lock': 10436, 'mansell': 10437, 'aide': 10438, 'invoice': 10439, 'stairs': 10440, 'compels': 10441, 'rhyming': 10442, 'borges': 10443, 'receptionist': 10444, 'hindustan': 10445, 'zafar': 10446, 'consume': 10447, 'sustain': 10448, 'yarn': 10449, 'chronicler': 10450, 'statecraft': 10451, 'grover': 10452, 'zach': 10453, 'microprocessor': 10454, 'microcontroller': 10455, 'mohammed': 10456, 'agreement': 10457, 'curry': 10458, 'genocide': 10459, 'technicians': 10460, 'crashing': 10461, 'misused': 10462, 'revisions': 10463, 'pitted': 10464, 'tastes': 10465, 'dl': 10466, 'chatbots': 10467, 'grid': 10468, 'pose': 10469, 'immigrants': 10470, 'blows': 10471, 'keepsake': 10472, 'deviation': 10473, 'gateway': 10474, 'resume': 10475, 'walsh': 10476, 'unlimited': 10477, \"valentine's\": 10478, 'safeguard': 10479, 'brearley': 10480, 'deliverer': 10481, 'spear': 10482, 'citizenship': 10483, 'birla': 10484, 'treehouse': 10485, 'elf': 10486, 'andrea': 10487, 'sen’nin': 10488, 'mohandas': 10489, 'hi': 10490, 'richer': 10491, 'china’s': 10492, 'shade': 10493, 'homicide': 10494, 'ripped': 10495, 'pitman': 10496, 'phonetic': 10497, 'endangered': 10498, 'perkins': 10499, 'font': 10500, 'du': 10501, 'hammer': 10502, 'eliza': 10503, 'giridharadas': 10504, 'dhoni': 10505, 'tinker': 10506, 'abel': 10507, 'manavas': 10508, 'zarathustra': 10509, 'associations': 10510, 'stunts': 10511, '4th': 10512, 'baba': 10513, 'jerome': 10514, \"modi's\": 10515, 'edmond': 10516, 'megha': 10517, '13th': 10518, 'berger': 10519, 'rbi': 10520, 'physiology': 10521, 'hussein': 10522, '\\x95': 10523, 'listens': 10524, 'gospel': 10525, 'marathons': 10526, 'phelps': 10527, 'fitz': 10528, 'benchmark': 10529, 'mindy': 10530, 'reece': 10531, 'shirley': 10532, 'felix': 10533, 'liberalism': 10534, 'emoji': 10535, 'vanya': 10536, 'baali': 10537, \"harrison's\": 10538, 'mcqueen': 10539, 'scala': 10540, 'ocp': 10541, \"power'\": 10542, 'burnt': 10543, 'corrections': 10544, 'vividness': 10545, \"magazine's\": 10546, 'chirunning': 10547, 'farther': 10548, 'katherine': 10549, 'nile': 10550, 'passenger': 10551, 'malayalam': 10552, 'accursed': 10553, 'statue': 10554, 'jacques': 10555, 'ingeniously': 10556, 'transparency': 10557, 'royalty': 10558, 'anthropologist': 10559, 'naruto’s': 10560, 'resolution': 10561, 'spencer': 10562, 'stored': 10563, 'truss': 10564, 'satoru': 10565, 'boyne': 10566, 'read’': 10567, 'screenshots': 10568, 'transaction': 10569, 'xi': 10570, 'auto': 10571, 'prostitution': 10572, 'advised': 10573, \"buddha's\": 10574, 'commentators': 10575, 'wrestlemania': 10576, 'burglary': 10577, 'appreciated': 10578, 'revolving': 10579, 'naughty': 10580, 'acquaintances': 10581, 'dossier': 10582, 'rigor': 10583, 'accountability': 10584, 'margin': 10585, 'disgraced': 10586, 'margins': 10587, 'transporting': 10588, 'savour': 10589, 'tremendously': 10590, 'mac': 10591, 'beethoven': 10592, 'sonata': 10593, 'dubai': 10594, 'funky': 10595, 'inking': 10596, 'reshape': 10597, 'coterie': 10598, 'abrsm': 10599, 'glasses': 10600, 'underappreciated': 10601, \"lover's\": 10602, 'gradual': 10603, 'tragically': 10604, 'besharam': 10605, 'dissects': 10606, 'inexplicable': 10607, 'gross': 10608, 'ove': 10609, 'assistants': 10610, 'persist': 10611, 'prisha': 10612, 'biz': 10613, 'pickwick': 10614, 'serialized': 10615, 'drum': 10616, 'stalker': 10617, \"writers'\": 10618, 'unfolded': 10619, 'ibsen’s': 10620, 'haruki': 10621, 'moth': 10622, \"planet'\": 10623, 'sportsman': 10624, 'greer': 10625, 'needing': 10626, 'foothold': 10627, '900': 10628, 'hordes': 10629, 'dissolute': 10630, 'tramp': 10631, 'tropical': 10632, 'ulysses': 10633, 'sinking': 10634, 'higgins': 10635, \"'as\": 10636, 'competitor': 10637, 'artworks': 10638, 'pilling': 10639, 'infiltrated': 10640, 'geared': 10641, 'yardstick': 10642, 'lazy': 10643, 'tank': 10644, 'firing': 10645, 'hay': 10646, 'climbs': 10647, 'dome': 10648, 'curl': 10649, 'comparing': 10650, 'alphabetically': 10651, 'invest': 10652, 'seduce': 10653, 'collocations': 10654, 'strides': 10655, '1956': 10656, 'suburban': 10657, 'drenched': 10658, '72': 10659, 'pdf': 10660, 'steele': 10661, 'dominates': 10662, 'clayton': 10663, 'diwekar': 10664, 'saif': 10665, 'dieting': 10666, 'carlos': 10667, 'ultimatum': 10668, 'biases': 10669, 'broaden': 10670, 'hitchens': 10671, 'collapsed': 10672, 'vanity': 10673, 'hygiene': 10674, 'educator': 10675, 'facade': 10676, 'bulls': 10677, 'storyboards': 10678, 'machiavelli': 10679, 'fables': 10680, 'entrusted': 10681, 'assassinated': 10682, 'truby': 10683, 'epiphanies': 10684, 'credibility': 10685, 'sailing': 10686, 'marvels': 10687, 'theranos': 10688, 'heinous': 10689, 'rifle': 10690, 'culprits': 10691, 'steam': 10692, 'lesbian': 10693, 'icnd1': 10694, 'pearson': 10695, 'cyanide': 10696, 'slaughtered': 10697, 'fullest': 10698, \"president's\": 10699, 'tedious': 10700, 'farming': 10701, 'sustainability': 10702, \"dostoyevsky's\": 10703, 'salvation': 10704, 'idiot': 10705, \"life'\": 10706, 'freud': 10707, 'comedic': 10708, 'perennially': 10709, 'mandela': 10710, 'northwest': 10711, 'homage': 10712, 'memos': 10713, 'mails': 10714, 'shipwreck': 10715, 'artful': 10716, 'planes': 10717, 'sparkle': 10718, 'ussr': 10719, 'maddy': 10720, 'bathing': 10721, 'blossoming': 10722, 'abyss': 10723, 'effectiveness': 10724, 'decode': 10725, 'projected': 10726, 'offline': 10727, 'wickedly': 10728, 'fletcher': 10729, 'treating': 10730, 'reunite': 10731, 'participation': 10732, 'philadelphia': 10733, 'troops': 10734, 'fraser': 10735, 'presumed': 10736, 'borrow': 10737, 'undertake': 10738, 'culprit': 10739, 'husband’s': 10740, 'preliminary': 10741, 'aditi': 10742, 'memorizing': 10743, 'spellings': 10744, 'philanthropist': 10745, '1940s': 10746, 'pleasant': 10747, 'guggenheim': 10748, 'chloe': 10749, \"obsession'\": 10750, 'profits': 10751, 'retreating': 10752, 'victorious': 10753, 'traitor': 10754, 'miniature': 10755, 'travis': 10756, 'drumm': 10757, 'sentenced': 10758, 'attained': 10759, 'enriching': 10760, 'luffy’s': 10761, 'preposition': 10762, 'dormant': 10763, 'kargil': 10764, 'ecstasy': 10765, 'glossy': 10766, 'sickle': 10767, 'korean': 10768, 'archaeological': 10769, 'scourge': 10770, 'respectable': 10771, 'ushered': 10772, 'bail': 10773, 'spectacle': 10774, 'naïve': 10775, 'gayle': 10776, 'harmonica': 10777, 'anita': 10778, 'increases': 10779, 'stricken': 10780, 'sao': 10781, 'cups': 10782, 'monarchy': 10783, 'prosperous': 10784, \"victim's\": 10785, 'sunken': 10786, 'thomson': 10787, 'dav': 10788, 'fleas': 10789, 'fetch': 10790, 'honing': 10791, 'sakura': 10792, 'kakashi': 10793, 'neighborhood': 10794, 'zombies': 10795, \"penguin's\": 10796, 'chekhov': 10797, 'bridging': 10798, 'characterised': 10799, 'vistas': 10800, 'liberate': 10801, 'willis': 10802, 'proofs': 10803, 'hawaii': 10804, 'wished': 10805, 'bucket': 10806, \"story'\": 10807, 'chhattisgarh': 10808, 'unafraid': 10809, 'artist’s': 10810, \"plato's\": 10811, 'viewpoints': 10812, '66': 10813, 'planetary': 10814, 'grail': 10815, 'unyielding': 10816, 'mandatory': 10817, 'holland': 10818, 'produces': 10819, 'descendants': 10820, 'pretends': 10821, 'hobb': 10822, 'exceptionally': 10823, 'performers': 10824, 'crude': 10825, 'alec': 10826, 'thumb': 10827, 'gerstner': 10828, 'competitiveness': 10829, 'daniels': 10830, 'crooked': 10831, 'topped': 10832, 'lowdown': 10833, 'contributor': 10834, 'vedic': 10835, 'aliens': 10836, 'lorien': 10837, 'interplay': 10838, 'twits': 10839, 'revolting': 10840, 'fowler': 10841, 'smoak': 10842, 'diagnose': 10843, 'analyzes': 10844, 'gadbois': 10845, 'lid': 10846, 'breed': 10847, \"'gripping\": 10848, 'successor': 10849, 'commando': 10850, 'dans': 10851, 'philanthropy': 10852, '“how': 10853, 'prescient': 10854, 'datta': 10855, 'का': 10856, 'quarrels': 10857, 'admire': 10858, 'desai': 10859, 'dreamers': 10860, 'license': 10861, \"india'\": 10862, 'disobedience': 10863, 'revisiting': 10864, 'protégé': 10865, 'maharajas': 10866, 'heavens': 10867, 'funds': 10868, 'coherent': 10869, 'builders': 10870, 'steelheart': 10871, 'concealed': 10872, 'maddox': 10873, 'tame': 10874, 'tattoo': 10875, 'willie': 10876, 'brick': 10877, 'trademarks': 10878, 'bros': 10879, '“what': 10880, 'offs': 10881, 'spur': 10882, 'fighters': 10883, 'encapsulates': 10884, 'pinch': 10885, 'foil': 10886, 'mizuki': 10887, 'vets': 10888, '1919': 10889, 'newsweek': 10890, 'junk': 10891, 'faded': 10892, 'upheaval': 10893, 'shorter': 10894, 'frightened': 10895, 'simplify': 10896, 'ships': 10897, '“not': 10898, 'quinn': 10899, 'imprint': 10900, \"sophie's\": 10901, 'youíre': 10902, 'segregated': 10903, 'payment': 10904, 'channels': 10905, 'restricted': 10906, 'accompanies': 10907, 'bpb': 10908, 'assessments': 10909, 'fade': 10910, '“an': 10911, 'hurdles': 10912, 'kurtz': 10913, 'upbeat': 10914, 'lineage': 10915, 'opener': 10916, 'remnants': 10917, 'l’amour': 10918, 'packaged': 10919, 'contributors': 10920, 'newborn': 10921, 'exhibited': 10922, 'tilly': 10923, 'bagshawe': 10924, 'jilly': 10925, 'caution': 10926, \"readable'\": 10927, 'mozart': 10928, 'mp': 10929, \"own'\": 10930, 'picturesque': 10931, 'prakash': 10932, 'sum': 10933, 'frail': 10934, 'hindutva': 10935, 'profiling': 10936, '‘how': 10937, 'captions': 10938, 'centurion': 10939, 'cato': 10940, 'scarred': 10941, 'slopes': 10942, 'mt': 10943, 'symbolize': 10944, 'sensuous': 10945, 'hoard': 10946, 'sicilian': 10947, 'chases': 10948, 'bats': 10949, 'assists': 10950, 'kearns': 10951, 'investments': 10952, 'pathways': 10953, 'withstand': 10954, 'rebellions': 10955, 'alluring': 10956, 'scots': 10957, 'rightful': 10958, 'sheldon’s': 10959, 'unifying': 10960, 'shinobi': 10961, 'determines': 10962, 'scion': 10963, 'malicious': 10964, 'memphis': 10965, 'boasting': 10966, 'tempted': 10967, 'stereotypes': 10968, 'turf': 10969, 'nails': 10970, 'reinforces': 10971, 'woody': 10972, 'nur': 10973, 'jahan': 10974, 'ministers': 10975, 'norms': 10976, 'silva': 10977, 'unorthodox': 10978, 'presided': 10979, 'peppered': 10980, 'whereas': 10981, 'pawn': 10982, 'irrespective': 10983, 'viciously': 10984, 'avenger': 10985, 'tolstoy': 10986, 'parable': 10987, 'nurses': 10988, 'cooks': 10989, 'tons': 10990, 'pandas': 10991, \"son's\": 10992, 'robb': 10993, 'peasants': 10994, 'steppe': 10995, 'hamilton': 10996, 'hardin': 10997, 'asuras': 10998, 'hack': 10999, 'nike': 11000, 'insider’s': 11001, 'guess': 11002, 'noise': 11003, 'reread': 11004, 'mayer': 11005, 'awful': 11006, 'del': 11007, 'conjures': 11008, 'mist': 11009, 'cited': 11010, 'myanmar': 11011, 'credited': 11012, 'alternating': 11013, '1931': 11014, 'time’': 11015, 'swinging': 11016, 'silly': 11017, 'tombs': 11018, 'wizards': 11019, 'favor': 11020, 'supremacy': 11021, 'civilisation': 11022, 'disappointment': 11023, 'psychologically': 11024, 'bickford': 11025, 'poll': 11026, 'mantle': 11027, 'handled': 11028, 'joints': 11029, 'canine': 11030, 'peru': 11031, \"herge's\": 11032, 'palm': 11033, 'cigars': 11034, 'sceptre': 11035, '714': 11036, 'picaros': 11037, 'alph': 11038, 'boko': 11039, 'haram': 11040, 'roulette': 11041, 'repetitions': 11042, 'tabletop': 11043, 'overhead': 11044, 'resting': 11045, 'slate': 11046, 'adept': 11047, 'scanning': 11048, 'someday': 11049, 'savagely': 11050, 'impenetrable': 11051, 'unleashing': 11052, 'isle': 11053, 'bath': 11054, 'helm': 11055, 'abbott': 11056, 'housekeeping': 11057, 'reputed': 11058, 'trusty': 11059, 'allie': 11060, 'blooms': 11061, 'insects': 11062, 'reservations': 11063, 'merry': 11064, \"geographic's\": 11065, 'gloriously': 11066, 'imposing': 11067, 'toni': 11068, 'paved': 11069, 'shadowhunters': 11070, 'sadie': 11071, 'kelley': 11072, 'fallout': 11073, 'optimum': 11074, 'hormonal': 11075, 'immortality': 11076, 'taoists': 11077, 'flowing': 11078, 'branson': 11079, 'virgin': 11080, \"spensa's\": 11081, \"humanity's\": 11082, 'mystifying': 11083, 'alignment': 11084, \"becky's\": 11085, 'disciple': 11086, 'porter': 11087, 'endorsement': 11088, 'bomber': 11089, 'institutional': 11090, 'painstakingly': 11091, 'entirety': 11092, 'arbitrary': 11093, \"sheldon's\": 11094, 'widest': 11095, 'avec': 11096, 'encore': 11097, 'arrogance': 11098, 'loathing': 11099, \"reading'\": 11100, 'welcomed': 11101, 'fiancée': 11102, 'exalted': 11103, 'aap': 11104, 'cheap': 11105, 'afterlife': 11106, 'unfinished': 11107, 'loans': 11108, 'she’d': 11109, \"readers'\": 11110, 'apprentice': 11111, 'inherit': 11112, 'vikander': 11113, 'jeopardy': 11114, 'waterstones': 11115, 'plaguing': 11116, 'aristocracy': 11117, 'chandler': 11118, '1933': 11119, 'rift': 11120, 'johns': 11121, 'chuck': 11122, 'handedly': 11123, 'symbolism': 11124, 'toughness': 11125, 'geology': 11126, 'geological': 11127, 'marketer': 11128, 'spying': 11129, 'patriarchy': 11130, 'shrine': 11131, 'clinic': 11132, 'ki': 11133, 'ravaged': 11134, 'malik': 11135, 'freeze': 11136, 'automated': 11137, 'supervised': 11138, 'query': 11139, 'engulfed': 11140, 'sparking': 11141, 'innate': 11142, 'kartik': 11143, 'connelly': 11144, 'clinton': 11145, 'emulated': 11146, 'readily': 11147, 'geneticist': 11148, 'stations': 11149, 'hiking': 11150, 'sadly': 11151, '650': 11152, 'tidy': 11153, 'queer': 11154, 'hashmi': 11155, 'mister': 11156, 'derives': 11157, 'sympathetic': 11158, 'jurek': 11159, \"'absolutely\": 11160, \"way'\": 11161, \"someone's\": 11162, 'exposition': 11163, 'freeman': 11164, 'accomplishment': 11165, 'hawking’s': 11166, 'stamping': 11167, 'marginalized': 11168, 'khalid': 11169, 'focussed': 11170, 'enhances': 11171, 'joshua': 11172, 'drifting': 11173, 'valjean': 11174, 'maiden': 11175, 'siberia': 11176, 'shipping': 11177, 'nn': 11178, 'technologists': 11179, 'surrounds': 11180, 'distressed': 11181, 'pamela': 11182, \"hardy's\": 11183, 'frankness': 11184, 'rope': 11185, 'fairly': 11186, 'threading': 11187, 'calendar': 11188, 'ridiculously': 11189, 'townsend': 11190, 'anders': 11191, 'demonstrating': 11192, 'guarantees': 11193, 'protocol': 11194, 'obsolete': 11195, 'niche': 11196, 'owns': 11197, 'smugglers': 11198, 'mantra': 11199, 'embodiment': 11200, 'gangs': 11201, 'stacked': 11202, 'widening': 11203, 'unravelled': 11204, 'coupled': 11205, \"mankind's\": 11206, 'cardinal': 11207, 'adjective': 11208, 'reasonable': 11209, 'sensible': 11210, 'sudeep': 11211, 'nagarkar': 11212, 'enlisted': 11213, 'bannock': 11214, 'hampshire': 11215, 'toys': 11216, 'pinfold': 11217, 'livery': 11218, \"'better\": 11219, 'field’s': 11220, 'zelda': 11221, 'chill': 11222, 'stare': 11223, 'culled': 11224, 'pivot': 11225, 'minimalist': 11226, 'overflowing': 11227, 'haven’t': 11228, 'hobby': 11229, 'mysore': 11230, 'momentum': 11231, 'schoolboys': 11232, 'unearthing': 11233, 'attributed': 11234, 'riddles': 11235, 'loads': 11236, 'irresistibly': 11237, 'willingness': 11238, 'spacex': 11239, 'trappings': 11240, 'attended': 11241, 'ellacott': 11242, 'perpetrator': 11243, 'escapades': 11244, 'pluralism': 11245, 'abstraction': 11246, 'approached': 11247, 'dismal': 11248, 'subhas': 11249, 'amish': 11250, 'convent': 11251, 'abandon': 11252, 'akshara': 11253, 'jha': 11254, 'chauhan': 11255, 'ranger': 11256, 'breaker': 11257, 'convolutional': 11258, 'reunited': 11259, 'counterparts': 11260, 'negotiator': 11261, 'missouri': 11262, 'caricatures': 11263, 'truck': 11264, 'depraved': 11265, 'typography': 11266, 'specimen': 11267, 'belle': 11268, 'crafting': 11269, 'tunnels': 11270, 'perished': 11271, 'absorbed': 11272, 'destinations': 11273, 'gamble': 11274, 'gaza': 11275, 'unemployed': 11276, 'situated': 11277, 'igcse': 11278, 'grouped': 11279, \"sport's\": 11280, 'bankruptcy': 11281, 'cape': 11282, 'countdown': 11283, 'jai': 11284, \"singh's\": 11285, 'stimulates': 11286, 'pharmaceutical': 11287, 'hint': 11288, 'zealand': 11289, 'emails': 11290, 'bulk': 11291, '1918': 11292, 'engagingly': 11293, 'priests': 11294, 'rogers': 11295, 'filthy': 11296, 'fortunately': 11297, 'synchronization': 11298, 'ellen': 11299, 'kidnappers': 11300, 'excuses': 11301, 'hunts': 11302, 'urgency': 11303, 'palaces': 11304, 'reminiscent': 11305, 'taj': 11306, 'thanos': 11307, 'gunslinger': 11308, 'fauna': 11309, 'newt': 11310, 'bassett': 11311, 'michigan': 11312, \"character's\": 11313, 'mehra': 11314, 'luthor': 11315, 'undying': 11316, 'mookerjee': 11317, 'capote': 11318, 'instrumental': 11319, 'colonialism': 11320, 'bustling': 11321, 'mapped': 11322, 'bedelia': 11323, 'cardio': 11324, 'karachi': 11325, 'echo': 11326, 'ladder': 11327, '85': 11328, 'treks': 11329, 'roget’s': 11330, 'accessibility': 11331, 'tenali': 11332, 'gupta': 11333, 'displayed': 11334, 'marvellous': 11335, 'educates': 11336, 'informs': 11337, 'acquiring': 11338, 'inviting': 11339, 'conservationist': 11340, 'cured': 11341, 'kenyan': 11342, 'jamaica': 11343, 'scriptwriting': 11344, 'wound': 11345, 'pets': 11346, 'betraying': 11347, 'heralded': 11348, 'scribbled': 11349, 'aired': 11350, 'referring': 11351, 'aroused': 11352, 'stocks': 11353, 'arora': 11354, 'rediscover': 11355, 'columbia': 11356, 'rewrite': 11357, 'webb': 11358, 'maintains': 11359, 'whisked': 11360, 'tumbling': 11361, 'trent': 11362, 'liam': 11363, 'skilful': 11364, 'dancer': 11365, 'brazilian': 11366, 'barbarian': 11367, 'cues': 11368, 'hype': 11369, 'bolder': 11370, 'illumination': 11371, 'fanatic': 11372, \"humankind's\": 11373, 'kidnaps': 11374, \"leo's\": 11375, 'dwarf': 11376, 'shower': 11377, 'unclear': 11378, 'impossibly': 11379, 'gogh': 11380, 'jamison': 11381, 'scorching': 11382, 'kisses': 11383, 'hodder': 11384, 'dahlia': 11385, 'dictator': 11386, 'hurricane': 11387, 'dalrymple': 11388, 'christians': 11389, 'embellished': 11390, 'dice': 11391, 'divinity': 11392, 'mild': 11393, 'wharton': 11394, 'magnitude': 11395, 'den': 11396, 'universes': 11397, 'cream': 11398, 'handlers': 11399, 'quantity': 11400, 'viz': 11401, 'stepfather': 11402, 'kissing': 11403, 'gustav’s': 11404, 'complicate': 11405, 'directory': 11406, 'mera': 11407, 'meg': 11408, 'andrews': 11409, 'nurture': 11410, 'tweets': 11411, 'litt': 11412, 'harbor': 11413, 'vie': 11414, 'closes': 11415, 'colorado': 11416, 'matthews': 11417, 'endures': 11418, 'picoult': 11419, 'pedagogical': 11420, 'rankings': 11421, \"'fascinating\": 11422, 'blink': 11423, 'infectious': 11424, 'cheerful': 11425, 'copper': 11426, 'newcomers': 11427, 'disk': 11428, 'catwoman': 11429, 'ivy': 11430, \"reacher's\": 11431, 'liberated': 11432, 'supposedly': 11433, 'salvage': 11434, 'enormously': 11435, 'pronounced': 11436, 'world’': 11437, 'wicket': 11438, 'lakes': 11439, 'sabotaged': 11440, \"legend's\": 11441, 'conquests': 11442, 'erika': 11443, 'mounts': 11444, 'inflict': 11445, 'shibumi': 11446, 'ailing': 11447, 'notebooks': 11448, 'chock': 11449, 'pale': 11450, 'skinned': 11451, 'mayfair': 11452, 'hushed': 11453, 'pseudonym': 11454, 'millennials': 11455, 'alters': 11456, 'reese': 11457, 'gabe': 11458, 'liesel': 11459, 'pamuk': 11460, 'borges’': 11461, 'constance': 11462, 'god’s': 11463, 'cwa': 11464, 'meerbach': 11465, 'hitler’s': 11466, 'videogames': 11467, 'andersen': 11468, 'corry': 11469, 'curtis': 11470, 'yard': 11471, 'dictatorship': 11472, 'sniper': 11473, 'gravity': 11474, 'newton': 11475, 'succeeding': 11476, 'delisle': 11477, 'strives': 11478, 'narrating': 11479, 'fairytale': 11480, 'rockefeller': 11481, 'vish': 11482, 'winfrey': 11483, 'multimillion': 11484, 'alerts': 11485, 'similarly': 11486, 'ipad': 11487, 'pony': 11488, '‘what': 11489, 'ml': 11490, 'nutritionist': 11491, 'delay': 11492, 'subway': 11493, 'funding': 11494, 'shonda': 11495, 'truest': 11496, 'mythological': 11497, 'satellites': 11498, 'organisms': 11499, 'tricolore': 11500, 'debuted': 11501, 'surfaces': 11502, 'props': 11503, 'strive': 11504, 'cabal': 11505, 'restoring': 11506, 'bunny': 11507, 'recognisable': 11508, 'topshe': 11509, 'supplemented': 11510, 'rails': 11511, 'undeniably': 11512, 'rob': 11513, 'aficionados': 11514, 'shack': 11515, 'az': 11516, 'hassan': 11517, 'brazen': 11518, 'meaningless': 11519, 'unbound': 11520, 'tempered': 11521, 'nda': 11522, 'lok': 11523, 'jarrett': 11524, 'hey': 11525, 'wimpy': 11526, 'foretold': 11527, 'spec': 11528, 'sidebars': 11529, 'noon': 11530, 'stewart': 11531, 'strands': 11532, 'vignettes': 11533, \"caesar's\": 11534, 'golfers': 11535, 'vitalstatistix': 11536, 'vedanta': 11537, 'maoists': 11538, '‘one': 11539, 'recursive': 11540, 'discrete': 11541, 'joni': 11542, 'kocienda': 11543, 'safari': 11544, 'replica': 11545, 'pictorial': 11546, '1968': 11547, 'backward': 11548, 'vendetta': 11549, \"'v'\": 11550, 'revisits': 11551, 'commissions': 11552, 'darkroom': 11553, 'tailed': 11554, 'coat': 11555, 'checks': 11556, 'atul': 11557, 'jalan': 11558, 'entrenched': 11559, \"hbo's\": 11560, 'herd': 11561, 'wasting': 11562, 'wealthiest': 11563, 'digitally': 11564, 'intervention': 11565, 'wrongs': 11566, 'superintendent': 11567, 'adores': 11568, 'reminded': 11569, 'diane': 11570, 'mckee': 11571, 'artefact': 11572, 'vigyan': 11573, 'overshadowed': 11574, 'deny': 11575, 'tens': 11576, 'guidebooks': 11577, 'blessing': 11578, 'rowell': 11579, 'gitanjali': 11580, 'worries': 11581, 'kite': 11582, 'freeza': 11583, 'snooker': 11584, 'peoples': 11585, 'utility': 11586, 'fearlessness': 11587, 'fore': 11588, 'punished': 11589, 'aamir': 11590, 'sonia': 11591, 'completes': 11592, 'stimulate': 11593, 'startups': 11594, 'bargained': 11595, 'impulse': 11596, 'weary': 11597, 'shone': 11598, 'epictetus': 11599, 'ang': 11600, 'fascism': 11601, 'salesman': 11602, 'ux': 11603, 'beguiling': 11604, 'saudi': 11605, 'resentment': 11606, 'allan': 11607, 'beijing': 11608, 'qi': 11609, 'sundarbans': 11610, 'fishermen': 11611, 'bass': 11612, 'racy': 11613, 'teller': 11614, 'acronyms': 11615, 'jandy': 11616, 'acquainted': 11617, 'undergoes': 11618, 'enquiry': 11619, 'diseases': 11620, 'arrange': 11621, 'tillman': 11622, 'matrix': 11623, 'autonomous': 11624, 'idiosyncratic': 11625, '1903': 11626, 'tryst': 11627, 'nuns': 11628, 'accompaniment': 11629, 'dancers': 11630, 'invoices': 11631, 'settlement': 11632, 'metaphorical': 11633, 'sh': 11634, 'falk': 11635, 'coates': 11636, 'aloud': 11637, 'repetition': 11638, 'disappearing': 11639, 'blurred': 11640, 'allied': 11641, 'ridge': 11642, 'strung': 11643, 'specializes': 11644, 'pharma': 11645, 'loom': 11646, 'costumes': 11647, 'dorian': 11648, 'anecdotal': 11649, 'enraged': 11650, 'byomkesh': 11651, 'bakshi': 11652, 'kaleidoscope': 11653, 'pertinent': 11654, 'versed': 11655, 'dowry': 11656, 'sidekick': 11657, 'layouts': 11658, 'concentrated': 11659, 'microprocessors': 11660, '8051': 11661, 'interfacing': 11662, 'reflective': 11663, 'interviewing': 11664, 'napoleon': 11665, 'approximate': 11666, 'grounding': 11667, 'cybersecurity': 11668, 'zetter': 11669, 'stuxnet': 11670, 'mindfully': 11671, 'mentzer': 11672, 'magnetic': 11673, 'populated': 11674, 'theodore': 11675, 'fargos': 11676, 'decorative': 11677, 'gambler': 11678, 'sweetness': 11679, '56': 11680, 'outbreak': 11681, 'cracked': 11682, 'fist': 11683, 'renna': 11684, 'namita': 11685, 'advantages': 11686, 'narayana': 11687, 'measured': 11688, 'removal': 11689, 'geeky': 11690, 'seles': 11691, 'khai': 11692, 'oberoi': 11693, 'bijapur': 11694, 'disorders': 11695, 'synergy': 11696, 'ark': 11697, 'compulsory': 11698, 'altair': 11699, 'panchayats': 11700, 'autonomy': 11701, 'manu': 11702, 'advertisements': 11703, 'ansible': 11704, 'durjoy': 11705, 'skeletor': 11706, 'doyle’s': 11707, 'sacks': 11708, 'jonah': 11709, 'smartphones': 11710, 'thorp': 11711, 'intersection': 11712, 'sullivan': 11713, 'eggs': 11714, 'ashish': 11715, 'mvc': 11716, 'helene': 11717, \"boy's\": 11718, 'pak': 11719, '1929': 11720, 'continuity': 11721, 'elites': 11722, 'miserable': 11723, 'monetary': 11724, 'embraced': 11725, 'alibaba’s': 11726, 'ee': 11727, 'nobles': 11728, 'recognised': 11729, 'dexterity': 11730, 'joey': 11731, \"memoir'\": 11732, 'unadulterated': 11733, 'graduation': 11734, 'romita': 11735, 'garcía': 11736, '39': 11737, 'preachy': 11738, 'sheela': 11739, 'thor': 11740, 'taxi': 11741, 'libya': 11742, 'anglo': 11743, 'interpreting': 11744, 'seldom': 11745, 'targaryens': 11746, 'siddartha': 11747, 'jefferson': 11748, 'minnie': 11749, 'fiftieth': 11750, 'rolled': 11751, 'aural': 11752, 'enlists': 11753, 'anchor': 11754, 'firms': 11755, 'farah': 11756, 'roshan': 11757, \"o'neil\": 11758, 'ankita': 11759, 'contacts': 11760, 'fossils': 11761, 'ginsburg': 11762, 'nest': 11763, 'transcripts': 11764, 'rigging': 11765, 'ped': 11766, 'year’s': 11767, 'adrenalin': 11768, 'ultramarathon': 11769, 'interwoven': 11770, 'resulting': 11771, 'aryavarta': 11772, 'fiscal': 11773, 'welsh': 11774, 'warli': 11775, 'rhys': 11776, 'sheryl': 11777, 'babbage': 11778, 'sahil': 11779, 'shikari': 11780, 'fellowship': 11781, 'vatican': 11782, \"holocaust'\": 11783, 'vladek': 11784, \"verne's\": 11785, 'edouard': 11786, 'supplementary': 11787, 'tenge': 11788, 'grindelwald': 11789, 'charnwood’s': 11790, 'riya': 11791, 'milk': 11792, 'dungeons': 11793, 'tome': 11794, 'neven': 11795, 'prevention': 11796, \"roy's\": 11797, 'bidar': 11798, 'linguistic': 11799, 'rowing': 11800, \"james's\": 11801, 'relaxed': 11802, 'angrejee': 11803, 'binny': 11804, 'booking': 11805, 'breezy': 11806, 'kautilya': 11807, 'desirable': 11808, 'trolls': 11809, 'jeans': 11810, 'behalf': 11811, 'maryam': 11812, 'precis': 11813, 'guernsey': 11814, 'riddell': 11815, 'sanchin': 11816, 'lucrative': 11817, 'audie': 11818, 'morpheus': 11819, 'madhubala': 11820, 'vansh': 11821, 'brody': 11822, 'datacenter': 11823, 'lottie': 11824, 'quidditch': 11825, 'procurement': 11826, 'pulizzi': 11827, 'है।': 11828, 'overly': 11829, 'odysseus': 11830, 'thorpe': 11831, 'halloween': 11832, 'reactor': 11833, 'vidal': 11834, 'monique': 11835, 'kroc': 11836, 'arianna': 11837, 'fudge': 11838, 'une': 11839, 'dell': 11840, 'rui': 11841, 'fp': 11842, 'worldview': 11843, 'reserves': 11844, 'personalized': 11845, 'pistol': 11846, 'asanas': 11847, 'finishing': 11848, 'thirteenth': 11849, 'governs': 11850, 'persecution': 11851, 'oppressive': 11852, 'wozniak': 11853, 'christensen': 11854, 'klaus': 11855, 'forum': 11856, '1840s': 11857, 'tai': 11858, 'grandmaster': 11859, 'taoist': 11860, 'illustrators': 11861, 'hokage': 11862, 'nietzsche': 11863, 'gifting': 11864, '‘it': 11865, 'tenderness': 11866, 'customizing': 11867, 'import': 11868, 'tend': 11869, 'irreverence': 11870, 'betting': 11871, 'enticing': 11872, 'rot': 11873, 'enjoyment': 11874, '1957': 11875, 'gustav': 11876, 'jung': 11877, 'newcomer': 11878, 'formerly': 11879, 'barber': 11880, 'fleeting': 11881, 'recalled': 11882, 'blast': 11883, 'folly': 11884, 'unpleasant': 11885, 'vehicle': 11886, 'responds': 11887, 'whisky': 11888, 'erin': 11889, 'recreation': 11890, 'cecilia': 11891, 'jenkins': 11892, 'britons': 11893, 'overcame': 11894, 'rigid': 11895, 'cathedrals': 11896, 'individually': 11897, 'naive': 11898, 'crilley': 11899, 'remove': 11900, 'shooter': 11901, 'financier': 11902, 'concluding': 11903, 'dawning': 11904, 'taboos': 11905, 'bookshop': 11906, 'lethem': 11907, 'deepak': 11908, 'graeme': 11909, 'idiots': 11910, 'rounds': 11911, 'atal': 11912, 'bihari': 11913, 'paradoxes': 11914, 'militant': 11915, '1932': 11916, 'huxley': 11917, 'prophecies': 11918, 'impressed': 11919, 'abandoning': 11920, 'augustus': 11921, 'parliamentary': 11922, 'ills': 11923, 'vancouver': 11924, 'malice': 11925, 'smoky': 11926, 'extends': 11927, 'bengaluru': 11928, 't20': 11929, 'relish': 11930, 'escalate': 11931, 'masterworks': 11932, 'purse': 11933, 'backpack': 11934, 'bitterness': 11935, 'tumour': 11936, 'wrapped': 11937, 'maniacal': 11938, 'rafael': 11939, \"federer's\": 11940, 'flawlessly': 11941, 'nationwide': 11942, 'caliber': 11943, 'soothing': 11944, 'chang': 11945, 'delusion': 11946, 'populism': 11947, 'trenchant': 11948, 'misguided': 11949, 'mistrust': 11950, 'girl’s': 11951, 'airplanes': 11952, 'rightly': 11953, 'etymology': 11954, 'curricula': 11955, 'tatas': 11956, 'krakauer': 11957, 'boots': 11958, 'hosts': 11959, 'syed': 11960, 'firewalls': 11961, 'initiation': 11962, 'visionaries': 11963, 'unassuming': 11964, 'soared': 11965, 'spearheaded': 11966, 'forceful': 11967, 'memorial': 11968, 'volcano': 11969, 'simulations': 11970, '“in': 11971, 'ventures': 11972, 'rujuta': 11973, 'adjust': 11974, 'bikini': 11975, 'euro': 11976, 'prodigal': 11977, 'trafford': 11978, 'glare': 11979, 'disciplinary': 11980, 'scramble': 11981, 'bias': 11982, 'breathe': 11983, 'leaf': 11984, \"'from\": 11985, 'underwent': 11986, 'peerless': 11987, 'cipher': 11988, 'stupendous': 11989, 'weirdos': 11990, 'boink': 11991, 'teamed': 11992, 'tate': 11993, 'decor': 11994, 'decorating': 11995, 'passageways': 11996, 'distils': 11997, 'automatic': 11998, 'songbooks': 11999, 'nowadays': 12000, 'swears': 12001, 'chemist': 12002, 'touchstone': 12003, 'shrek': 12004, 'hubris': 12005, 'mandala': 12006, 'squares': 12007, 'brigance': 12008, 'sensitively': 12009, 'realms': 12010, 'ecological': 12011, 'barnes': 12012, \"yalsa's\": 12013, 'cert': 12014, 'switches': 12015, 'schoolteacher': 12016, 'manservant': 12017, 'devoured': 12018, 'backup': 12019, 'haven': 12020, 'injection': 12021, 'donít': 12022, 'financing': 12023, 'iyer': 12024, 'erratic': 12025, 'blur': 12026, 'friedrich': 12027, 'panache': 12028, '46': 12029, 'consult': 12030, 'greene': 12031, 'correctness': 12032, 'nerd': 12033, 'bossy': 12034, 'amuse': 12035, 'volatile': 12036, \"'why\": 12037, 'jana': 12038, 'insecurity': 12039, 'wire': 12040, 'jain': 12041, 'phd': 12042, 'tori': 12043, 'bailey': 12044, 'aspiration': 12045, \"i'd\": 12046, 'minefield': 12047, \"'smart\": 12048, \"it'll\": 12049, 'deprived': 12050, '750': 12051, 'seed': 12052, 'cooperative': 12053, 'heart’s': 12054, 'illegitimate': 12055, 'kidnapper': 12056, 'pandit': 12057, '1889': 12058, 'he\\x92s': 12059, 'directs': 12060, 'hones': 12061, 'eclectic': 12062, 'widowed': 12063, 'carver': 12064, 'visceral': 12065, 'singhal': 12066, \"group's\": 12067, 'conglomerate': 12068, 'commissioner': 12069, \"'do\": 12070, \"'excellent\": 12071, 'misadventures': 12072, 'blade': 12073, 'tehran': 12074, 'breach': 12075, 'compromised': 12076, \"tale'\": 12077, 'surge': 12078, 'tenant': 12079, 'feasts': 12080, 'strangled': 12081, 'rubber…at': 12082, 'brass': 12083, \"rand's\": 12084, 'shrugged': 12085, '1905': 12086, '1951': 12087, \"fry's\": 12088, 'clerical': 12089, 'surround': 12090, 'familiarize': 12091, 'deepens': 12092, 'plagues': 12093, 'twain': 12094, 'explorer': 12095, 'sudan': 12096, 'postcolonial': 12097, 'incandescent': 12098, 'ascendancy': 12099, 'constitute': 12100, 'nationhood': 12101, \"kasab's\": 12102, 'simi': 12103, \"aseemanand's\": 12104, 'scot': 12105, 'manipulators': 12106, 'bombings': 12107, 'exonerated': 12108, 'flavoured': 12109, 'tagging': 12110, 'diminutive': 12111, 'entertainer': 12112, 'marching': 12113, 'alumni': 12114, 'harsha': 12115, 'bhogle': 12116, 'plight': 12117, 'appointment': 12118, 'adjusting': 12119, 'unidentified': 12120, 'upto': 12121, 'kitties': 12122, 'bookshelves': 12123, 'abound': 12124, 'summons': 12125, '80th': 12126, 'denmark': 12127, 'furthest': 12128, 'carefree': 12129, 'pristine': 12130, 'lexical': 12131, 'subtlety': 12132, 'outs': 12133, 'meyer': 12134, '5k': 12135, \"runner's\": 12136, 'yang': 12137, 'maoist': 12138, 'microcosm': 12139, 'stakeholders': 12140, 'spectators': 12141, 'pray': 12142, 'symposium': 12143, 'doting': 12144, 'unheard': 12145, 'piecing': 12146, 'oxygen': 12147, 'preternaturally': 12148, 'cautious': 12149, 'shrink': 12150, 'motto': 12151, \"krakauer's\": 12152, 'leavened': 12153, 'camaraderie': 12154, 'flaps': 12155, 'lifted': 12156, 'fulfilled': 12157, 'mongolian': 12158, 'territories': 12159, 'defiance': 12160, 'empire’s': 12161, 'boards': 12162, 'mca': 12163, 'outlandish': 12164, 'absurdities': 12165, 'acid': 12166, 'playbook': 12167, 'dwelling': 12168, 'men’s': 12169, 'outspoken': 12170, 'way—except': 12171, 'odisha': 12172, 'vibrantpublishers': 12173, 'toby': 12174, 'lowest': 12175, 'tyrion': 12176, 'dribble': 12177, 'bce': 12178, 'indus': 12179, 'peacock': 12180, 'nanda': 12181, 'ouster': 12182, 'fledgling': 12183, 'bewitching': 12184, 'enhancements': 12185, 'administering': 12186, 'oddities': 12187, 'discontents': 12188, 'raiders': 12189, '1902': 12190, 'documentaries': 12191, 'posters': 12192, 'derivatives': 12193, 'unending': 12194, 'coping': 12195, 'में': 12196, 'की': 12197, 'polemic': 12198, 'spared': 12199, 'statesmanship': 12200, 'consolidation': 12201, 'ensured': 12202, 'nawabs': 12203, 'neglect': 12204, 'subjugated': 12205, 'cloaked': 12206, 'jealousies': 12207, 'sood': 12208, 'nyt': 12209, 'tenacity': 12210, 'unpopular': 12211, 'bannerjee': 12212, 'burst': 12213, 'reckoners': 12214, 'auction': 12215, 'cosy': 12216, 'gobble': 12217, 'graduated': 12218, 'platonic': 12219, 'supportive': 12220, 'minifigure': 12221, 'dk': 12222, 'separates': 12223, 'competitions': 12224, 'generational': 12225, 'abe': 12226, 'prosecutor': 12227, 'decoration': 12228, 'catapults': 12229, 'tristan': 12230, 'contend': 12231, 'grudge': 12232, 'herriot’s': 12233, 'rye': 12234, 'midwest': 12235, '78': 12236, 'castes': 12237, 'sustaining': 12238, 'searchable': 12239, 'pleased': 12240, 'licensing': 12241, 'browsing': 12242, 'distinctions': 12243, 'decentralized': 12244, 'itís': 12245, 'pursuits': 12246, 'laptop': 12247, 'expressive': 12248, 'clone': 12249, 'ignores': 12250, 'responding': 12251, 'powershift': 12252, 'formulated': 12253, 'stripes': 12254, 'perforated': 12255, 'babri': 12256, 'masjid': 12257, 'auditore': 12258, 'sixteenth': 12259, 'shout': 12260, '”—publishers': 12261, 'cohen': 12262, 'em': 12263, 'tempestuous': 12264, 'jedi': 12265, 'skywalker': 12266, 'angoor': 12267, 'theological': 12268, 'sunni': 12269, 'shia': 12270, 'visible': 12271, 'sasha': 12272, 'vengeful': 12273, 'fruits': 12274, 'curiosities': 12275, 'accent': 12276, 'conjured': 12277, 'exhibits': 12278, 'maity': 12279, 'planners': 12280, 'stresses': 12281, 'dweller': 12282, 'ballet': 12283, 'ideally': 12284, 'selects': 12285, 'generic': 12286, 'prefect': 12287, 'coronation': 12288, 'geniuses': 12289, 'authorial': 12290, 'frankenstein': 12291, 'ascents': 12292, 'byron': 12293, 'muses': 12294, 'kent': 12295, 'surroundings': 12296, \"industry's\": 12297, 'bbc1': 12298, 'governing': 12299, 'fiercest': 12300, 'termed': 12301, 'compares': 12302, 'shrewd': 12303, 'makeup': 12304, 'blackwell': 12305, 'absent': 12306, 'demographic': 12307, 'exacting': 12308, 'backyard': 12309, 'classmate': 12310, 'whiskey': 12311, 'sympathizer': 12312, 'starved': 12313, 'converge': 12314, 'outcasts': 12315, \"'fast\": 12316, 'denies': 12317, 'wards': 12318, 'advancements': 12319, 'scrum': 12320, 'bca': 12321, 'transported': 12322, 'synthetic': 12323, 'harem': 12324, \"daughter's\": 12325, \"they'd\": 12326, 'whitehall': 12327, 'progressively': 12328, 'jolt': 12329, 'operates': 12330, 'console': 12331, 'iit': 12332, 'deliberate': 12333, 'peasant': 12334, 'ambush': 12335, 'chemicals': 12336, \"away'\": 12337, \"'very\": 12338, 'unlocked': 12339, 'honours': 12340, 'laila': 12341, 'lucknow': 12342, 'recommenders': 12343, 'metadata': 12344, 'filtering': 12345, 'wrangling': 12346, 'identified': 12347, 'kat': 12348, 'bloom': 12349, 'immortals': 12350, 'pervasive': 12351, 'weakened': 12352, 'wicketkeeper': 12353, 'jonny': 12354, 'janet': 12355, 'exterior': 12356, 'mainland': 12357, 'listeners': 12358, 'ignite': 12359, \"doctor's\": 12360, 'defender': 12361, 'conclusions': 12362, 'consonants': 12363, 'phonetics': 12364, 'counterpart': 12365, 'analog': 12366, 'modulation': 12367, 'slaughterhouse': 12368, 'rejects': 12369, 'loeb': 12370, 'annals': 12371, 'consist': 12372, 'ladders': 12373, 'yakshi': 12374, 'dara': 12375, 'lingering': 12376, 'brute': 12377, 'awash': 12378, 'intractable': 12379, 'beats': 12380, 'rd': 12381, 'woodward': 12382, 'doodles': 12383, 'ursula': 12384, 'reynolds': 12385, 'fiendish': 12386, 'bless': 12387, 'lured': 12388, 'provincial': 12389, 'geoffrey': 12390, 'coralie': 12391, 'antique': 12392, '1917': 12393, 'diploma': 12394, 'inca': 12395, 'braving': 12396, 'prediction': 12397, 'georges': 12398, 'brussels': 12399, 'piracy': 12400, 'unscathed': 12401, 'blades': 12402, 'postural': 12403, 'pelvis': 12404, 'neutral': 12405, 'modification': 12406, 'chin': 12407, 'carr': 12408, 'sampling': 12409, 'shallows': 12410, 'dissecting': 12411, 'contemplating': 12412, 'vagaries': 12413, 'nooyi': 12414, 'forums': 12415, 'nanotechnology': 12416, 'cola': 12417, 'cara': 12418, 'hacks': 12419, 'abused': 12420, 'grabs': 12421, 'awake': 12422, \"jane's\": 12423, 'bastards': 12424, 'appearances': 12425, 'gunfire': 12426, 'privy': 12427, 'bravely': 12428, 'rending': 12429, 'grainy': 12430, 'pitched': 12431, 'follower': 12432, 'overlooked': 12433, 'sprang': 12434, 'cardboard': 12435, 'astound': 12436, 'hyperbole': 12437, 'traced': 12438, 'arc': 12439, 'shimmering': 12440, 'midday': 12441, 'cityscapes': 12442, 'suitcase': 12443, 'jaded': 12444, 'unquestionably': 12445, 'infernal': 12446, 'girlfriends': 12447, 'blaze': 12448, 'incarceration': 12449, 'fluids': 12450, 'filter': 12451, 'surplus': 12452, 'provisions': 12453, 'legally': 12454, 'abruptly': 12455, \"'compelling\": 12456, 'embarrassed': 12457, 'manipulations': 12458, 'warn': 12459, 'alison': 12460, 'systemic': 12461, 'blanc': 12462, 'alps': 12463, 'deepening': 12464, 'sickness': 12465, 'crumble': 12466, 'multifarious': 12467, 'situational': 12468, 'trendy': 12469, 'hesitation': 12470, 'harmless': 12471, 'encyclopedic': 12472, 'refer': 12473, 'rests': 12474, 'barrington': 12475, 'flows': 12476, 'jnu': 12477, 'evangelist': 12478, 'scare': 12479, 'buses': 12480, 'screens': 12481, 'succinctly': 12482, 'eroticism': 12483, '1921': 12484, 'sadhu': 12485, 'partha': 12486, 'lightbringer': 12487, 'hid': 12488, 'mccarthy': 12489, 'wanders': 12490, 'stalked': 12491, 'diversion': 12492, 'geometric': 12493, 'tinder': 12494, 'pizza': 12495, 'hai': 12496, 'dazzled': 12497, 'invigorating': 12498, 'blanket': 12499, 'cleopatra': 12500, 'empowerment': 12501, 'angelou': 12502, 'swore': 12503, 'stirs': 12504, 'amulet': 12505, \"'watch\": 12506, \"bourne's\": 12507, 'disguised': 12508, 'americas': 12509, 'thugs': 12510, 'thorne': 12511, 'boiled': 12512, 'humphrey': 12513, 'drowning': 12514, \"'and\": 12515, 'dim': 12516, 'witted': 12517, 'cauldron': 12518, 'klopp': 12519, 'wenger': 12520, 'unbroken': 12521, \"esme's\": 12522, 'heartless': 12523, 'sled': 12524, 'hardship': 12525, 'elevated': 12526, 'purple': 12527, 'vp': 12528, 'devotional': 12529, 'chained': 12530, 'shantaram': 12531, 'prostitutes': 12532, 'exiles': 12533, 'apprenticeship': 12534, 'provoked': 12535, 'recounting': 12536, 'loan': 12537, 'srinagar': 12538, 'reappears': 12539, 'aperture': 12540, 'predicted': 12541, 'warehousing': 12542, \"pilkey's\": 12543, 'knocks': 12544, 'ramona': 12545, 'albeit': 12546, 'teddy': 12547, 'bibliographies': 12548, 'skeleton': 12549, 'brad': 12550, 'craftsmanship': 12551, 'epilogue': 12552, 'liberalization': 12553, 'receiving': 12554, 'balcony': 12555, 'summarize': 12556, 'rís': 12557, 'goddesses': 12558, 'vedas': 12559, 'jaya': 12560, 'scan': 12561, 'shuttle': 12562, 'elaborating': 12563, 'nutritional': 12564, 'unturned': 12565, 'fond': 12566, 'psychotherapist': 12567, 'summits': 12568, 'laser': 12569, 'denver': 12570, 'melodrama': 12571, \"'outstanding\": 12572, 'posthumous': 12573, 'phenomena': 12574, \"day's\": 12575, 'boulder': 12576, 'canyon': 12577, 'modified': 12578, 'footage': 12579, 'riveted': 12580, 'indianness': 12581, 'accommodation': 12582, 'declarations': 12583, 'bargaining': 12584, 'recounted': 12585, 'conducting': 12586, 'credible': 12587, 'lowe': 12588, \"'to\": 12589, \"'great\": 12590, 'dusk': 12591, 'populations': 12592, 'fisher': 12593, 'sloane': 12594, 'fs': 12595, 'specsavers': 12596, 'es': 12597, 'signal': 12598, 'sexes': 12599, 'hardy': 12600, 'parallels': 12601, 'iterators': 12602, 'flew': 12603, 'sleight': 12604, 'installing': 12605, 'reminiscences': 12606, 'fielding': 12607, 'fleming': 12608, 'cal': 12609, 'detox': 12610, 'raft': 12611, '96': 12612, 'distribute': 12613, 'outraged': 12614, 'shook': 12615, 'eliminated': 12616, 'mathura': 12617, 'horde': 12618, 'os': 12619, 'mortals': 12620, \"russia's\": 12621, 'perpetual': 12622, 'unspoken': 12623, 'dimple': 12624, 'agility': 12625, 'clause': 12626, 'hostages': 12627, 'life’': 12628, 'cross’s': 12629, '‘my': 12630, 'tours': 12631, 'alarm': 12632, 'anxieties': 12633, 'isles': 12634, 'zelda™': 12635, 'nostalgic': 12636, 'journalistic': 12637, 'whip': 12638, 'blunt': 12639, 'rumors': 12640, 'polity': 12641, 'authorization': 12642, 'github': 12643, 'js': 12644, 'lester': 12645, 'colleen': 12646, 'hoover': 12647, 'roommate': 12648, 'calculating': 12649, 'positivity': 12650, 'soundtrack': 12651, 'alchemist': 12652, 'venom': 12653, 'smitten': 12654, 'characterization': 12655, 'analytic': 12656, 'scribbling': 12657, 'parental': 12658, 'cuisine': 12659, 'mrinalini': 12660, 'boldest': 12661, 'shopaholic': 12662, 'crane': 12663, 'invading': 12664, 'dawood': 12665, 'abu': 12666, 'rapidex': 12667, 'benefited': 12668, 'unanimously': 12669, 'assamese': 12670, 'oriya': 12671, 'bidding': 12672, 'baffled': 12673, 'logs': 12674, 'unfair': 12675, 'kenton': 12676, 'brit': 12677, 'flip': 12678, 'blair': 12679, 'werner': 12680, 'masashi': 12681, 'mint': 12682, 'shelley': 12683, 'bore': 12684, 'paperbacks': 12685, 'brighter': 12686, 'bhaswati': 12687, 'recommends': 12688, 'gharana': 12689, 'madman': 12690, 'joyride': 12691, 'compulsion': 12692, 'emerson': 12693, 'conspiring': 12694, 'diner': 12695, \"sara's\": 12696, '‘it’s': 12697, \"einstein's\": 12698, 'walt': 12699, 'gorillas': 12700, 'pigs': 12701, 'basically': 12702, 'bolivian': 12703, 'extracted': 12704, 'czech': 12705, 'recap': 12706, 'courted': 12707, 'earthquake': 12708, 'stalks': 12709, 'lad': 12710, 'soak': 12711, 'vermont': 12712, 'devastatingly': 12713, 'wilt': 12714, 'remembering': 12715, 'viewpoint': 12716, 'babylon': 12717, 'rafah': 12718, 'dreadful': 12719, 'widows': 12720, 'palestine': 12721, 'contested': 12722, 'limelight': 12723, 'wren': 12724, 'exponential': 12725, 'debts': 12726, 'penniless': 12727, 'sexually': 12728, 'hypothesis': 12729, 'supplements': 12730, 'resisting': 12731, 'madcap': 12732, 'complaints': 12733, 'unnecessary': 12734, 'vr': 12735, 'beth': 12736, 'setup': 12737, 'solves': 12738, 'goodman': 12739, 'riotous': 12740, 'wwii': 12741, 'marquee': 12742, 'diction': 12743, 'strunk': 12744, 'archaeologists': 12745, 'kingston': 12746, 'friedman': 12747, 'awakens': 12748, 'habitat': 12749, 'screwed': 12750, 'radha': 12751, 'stieg': 12752, 'persian': 12753, 'chimamanda': 12754, 'ngozi': 12755, 'susannah': 12756, 'pal': 12757, 'confide': 12758, 'jess': 12759, 'crammed': 12760, 'knock': 12761, 'reckoned': 12762, 'incoming': 12763, 'sobering': 12764, 'truman': 12765, 'tempest': 12766, 'drunk': 12767, 'sammy': 12768, 'dreamy': 12769, 'pitching': 12770, 'thoughtfully': 12771, 'outcast': 12772, 'relational': 12773, 'classy': 12774, 'trekking': 12775, 'cartel': 12776, 'addicts': 12777, 'motors': 12778, 'scrambling': 12779, 'untamed': 12780, 'archaic': 12781, 'bloomsbury': 12782, 'mistakenly': 12783, 'pollyanna': 12784, 'brett’s': 12785, 'humanity’s': 12786, 'rides': 12787, 'iced': 12788, 'alpha': 12789, 'inaccessible': 12790, 'gestapo': 12791, 'departs': 12792, 'slept': 12793, 'outnumbered': 12794, 'characterize': 12795, 'pointing': 12796, 'praising': 12797, 'remedies': 12798, 'noisy': 12799, 'observe': 12800, 'abrar': 12801, 'lila': 12802, 'gopal': 12803, '‘he': 12804, 'chelsea': 12805, 'handpicked': 12806, 'luminous': 12807, 'coelho': 12808, 'earnest': 12809, 'trusts': 12810, 'crisply': 12811, 'ethnic': 12812, 'sleeps': 12813, 'lawn': 12814, 'thanksgiving': 12815, 'merdan': 12816, 'classrooms': 12817, 'gangster': 12818, 'dalits': 12819, 'fling': 12820, 'mulally': 12821, 'chrysler': 12822, 'lifeline': 12823, 'infighting': 12824, 'profitable': 12825, 'predictive': 12826, 'disgrace': 12827, 'ruthlessly': 12828, 'incessant': 12829, 'merits': 12830, 'troop': 12831, 'intimately': 12832, 'ambani': 12833, 'checkpoints': 12834, 'patricia': 12835, 'examiners': 12836, 'depending': 12837, 'slightest': 12838, 'conrad': 12839, 'adopting': 12840, 'trousers': 12841, 'whereby': 12842, 'tendency': 12843, 'ronson': 12844, 'shamed': 12845, 'institutes': 12846, 'corfu': 12847, 'regained': 12848, 'devotee': 12849, 'tycoons': 12850, 'fading': 12851, 'sarkar': 12852, 'ibt': 12853, 'mannered': 12854, 'fugitive': 12855, 'outsmart': 12856, 'sensory': 12857, 'rainy': 12858, 'mallya': 12859, 'rsquo': 12860, 'tangible': 12861, '128': 12862, \"who'd\": 12863, 'intoxicating': 12864, 'biblical': 12865, 'here’s': 12866, \"asia's\": 12867, 'duplicate': 12868, 'galley': 12869, \"'what\": 12870, 'palmer': 12871, 'drift': 12872, 'magnate': 12873, 'blessings': 12874, 'blockbusters': 12875, 'cynical': 12876, 'carlin': 12877, 'arkham': 12878, 'booming': 12879, 'bollywood’s': 12880, \"twist'\": 12881, 'selective': 12882, 'sibling': 12883, 'hopelessly': 12884, 'brew': 12885, \"poet's\": 12886, 'compellingly': 12887, 'lily’s': 12888, 'neurosurgeon': 12889, 'rupees': 12890, 'fixture': 12891, 'wreaks': 12892, 'flooding': 12893, 'grinding': 12894, 'foothills': 12895, 'supermarket': 12896, 'ivory': 12897, 'cho': 12898, 'showcased': 12899, 'bhagwan': 12900, 'pianist': 12901, 'accompaniments': 12902, 'busting': 12903, 'jamsetji': 12904, 'varieties': 12905, 'needless': 12906, 'malign': 12907, 'heavyweight': 12908, 'tapes': 12909, 'giordino': 12910, 'emulate': 12911, 'haller': 12912, '60th': 12913, 'westerner': 12914, 'rory': 12915, 'pikachu': 12916, 'rivanah’s': 12917, 'curtail': 12918, 'knocking': 12919, 'folks': 12920, 'weekends': 12921, 'nivrita': 12922, 'mccartney': 12923, 'starr': 12924, \"'is\": 12925, 'mcdermid': 12926, 'bedrock': 12927, 'giddy': 12928, 'munroe': 12929, 'haircut': 12930, 'brosh': 12931, 'markus': 12932, 'heartthrob': 12933, 'bala': 12934, 'songwriting': 12935, 'tsunami': 12936, 'intricately': 12937, 'vulnerabilities': 12938, 'bug': 12939, 'installation': 12940, 'lazlo': 12941, 'clint': 12942, 'outlaw': 12943, 'bebbanburg': 12944, 'saxons': 12945, 'violently': 12946, 'bernie': 12947, 'bolivia': 12948, 'hari': 12949, 'accumulation': 12950, '1920': 12951, 'adequate': 12952, 'rays': 12953, 'representations': 12954, 'attaining': 12955, 'morals': 12956, 'circulation': 12957, 'rent': 12958, 'legitimate': 12959, \"kashmir's\": 12960, 'collaborating': 12961, 'andie': 12962, 'sanders': 12963, 'ib': 12964, 'actresses': 12965, 'owe': 12966, 'hegemony': 12967, 'germans': 12968, 'consecutive': 12969, 'fringes': 12970, 'northeastern': 12971, 'heaped': 12972, 'preview': 12973, 'timid': 12974, 'transcends': 12975, 'ul': 12976, 'opar': 12977, 'mall': 12978, 'capstone': 12979, 'esol': 12980, 'ranjit': 12981, 'ganguly': 12982, 'swear': 12983, 'explicit': 12984, 'raid': 12985, 'turkey': 12986, 'fanatical': 12987, \"'no\": 12988, 'eldritch': 12989, 'illicit': 12990, 'testimonies': 12991, 'honoured': 12992, 'copywriter': 12993, 'beirut': 12994, 'sharia': 12995, 'revenues': 12996, 'vessel': 12997, 'inadvertently': 12998, 'sunil': 12999, 'mysticism': 13000, 'empowers': 13001, 'dales': 13002, 'reimagined': 13003, 'belgian': 13004, 'jayalalithaa': 13005, 'melbourne': 13006, 'apocalyptic': 13007, 'churchill': 13008, 'chandeliers': 13009, 'khel': 13010, 'hilo': 13011, 'settling': 13012, 'engulfs': 13013, 'enslaved': 13014, 'regaled': 13015, 'lecturers': 13016, 'psychologists': 13017, \"field's\": 13018, 'organise': 13019, 'plectrum': 13020, 'unfulfilling': 13021, 'asylum': 13022, 'multinational': 13023, \"government's\": 13024, 'exemplified': 13025, 'urging': 13026, 'citing': 13027, 'bernstein': 13028, 'loaded': 13029, 'invasive': 13030, 'dharmic': 13031, 'uphold': 13032, 'behaviours': 13033, \"forster's\": 13034, 'futurist': 13035, 'kurzweil': 13036, 'reigning': 13037, \"journey'\": 13038, 'aurangzeb': 13039, 'backstreets': 13040, 'velvet': 13041, 'montana': 13042, 'sheriff': 13043, \"alice's\": 13044, 'iterative': 13045, 'toolset': 13046, 'compatible': 13047, 'employing': 13048, 'routinely': 13049, 'handicap': 13050, 'haired': 13051, 'hyde': 13052, 'jen': 13053, 'maligned': 13054, 'kicked': 13055, 'blonde': 13056, 'naturalist': 13057, 'shine': 13058, 'helena': 13059, 'jp': 13060, 'prompted': 13061, 'dependence': 13062, 'sudoku': 13063, 'avery': 13064, 'spartan': 13065, \"gaiman's\": 13066, 'coraline': 13067, 'snaps': 13068, 'sharpening': 13069, 'hatched': 13070, 'sexton': 13071, 'nanny': 13072, 'wilds': 13073, 'lancaster': 13074, 'semantics': 13075, '450': 13076, 'ps': 13077, 'affiliated': 13078, 'counsel': 13079, 'subah': 13080, 'lyricist': 13081, 'tailored': 13082, 'hurry': 13083, 'swimmer': 13084, 'fluid': 13085, 'gemma': 13086, 'washing': 13087, 'arrows': 13088, 'shakespeare’s': 13089, 'keaton': 13090, 'narrowly': 13091, 'gladiatorial': 13092, 'contests': 13093, 'abbey': 13094, 'starter': 13095, 'nielsen': 13096, \"doyle's\": 13097, 'clearer': 13098, 'rail': 13099, 'standout': 13100, '1958': 13101, 'xkcd': 13102, 'haksar': 13103, 'attach': 13104, 'gohan': 13105, 'piccolo': 13106, 'rebuilt': 13107, 'tipu': 13108, 'syllabi': 13109, 'galore': 13110, 'georgie': 13111, 'chola': 13112, 'armour': 13113, 'untangle': 13114, 'amaze': 13115, \"media's\": 13116, 'toolkit': 13117, 'manufacturers': 13118, 'seinfeld': 13119, '11th': 13120, 'improvisation': 13121, 'butcher': 13122, 'mossad': 13123, 'putin': 13124, 'investigated': 13125, 'distilled': 13126, \"film's\": 13127, 'mocked': 13128, 'giraffe': 13129, 'skinning': 13130, 'delta': 13131, 'graphically': 13132, 'sayings': 13133, 'harappa': 13134, \"queen's\": 13135, 'levithan': 13136, 'tennessee': 13137, 'brisk': 13138, 'viewing': 13139, 'bess': 13140, 'cheat': 13141, 'ratnam': 13142, 'demystify': 13143, 'clearest': 13144, 'finesse': 13145, 'tuesday': 13146, 'romances': 13147, \"'original\": 13148, 'statisticians': 13149, 'dame': 13150, 'prodigious': 13151, 'itchy': 13152, 'bribes': 13153, 'polished': 13154, 'avoided': 13155, 'demonetization': 13156, 'milestone': 13157, 'tolkien': 13158, 'bedi': 13159, 'brash': 13160, 'startlingly': 13161, 'penn': 13162, 'impulsive': 13163, 'distraught': 13164, 'spills': 13165, 'stepped': 13166, 'yasuko': 13167, 'ishigami': 13168, 'yukawa': 13169, 'devise': 13170, 'wikileaks': 13171, 'lizard': 13172, 'riviera': 13173, 'deduction': 13174, 'posh': 13175, 'testbank': 13176, 'baldwin': 13177, 'microeconomics': 13178, \"google's\": 13179, 'playground': 13180, 'ringside': 13181, 'skeptical': 13182, 'cliff': 13183, 'frankie': 13184, 'jorge': 13185, 'excessive': 13186, 'envoy': 13187, 'peel': 13188, 'monopolies': 13189, 'courageously': 13190, 'blazingly': 13191, 'subsystems': 13192, 'grasping': 13193, 'frustrations': 13194, 'styling': 13195, 'studieslearning': 13196, 'checkglossary': 13197, 'termschapter': 13198, 'mistry': 13199, 'insecurities': 13200, 'memorization': 13201, 'mukesh': 13202, 'province': 13203, 'disciples': 13204, 'microcontrollers': 13205, '8085': 13206, '8096': 13207, 'bowlers': 13208, 'pooh': 13209, 'hoax': 13210, 'urbane': 13211, 'permutations': 13212, 'seventies': 13213, 'nobility': 13214, 'loop': 13215, 'rave': 13216, 'disappearances': 13217, 'kotler': 13218, 'frauds': 13219, 'inspectors': 13220, 'castles': 13221, 'fractals': 13222, 'sycamore': 13223, 'hubbard': 13224, 'nancy': 13225, 'asta': 13226, 'moonwalk': 13227, 'mastered': 13228, 'dothraki': 13229, 'pinkalicious': 13230, 'bastar': 13231, 'bee': 13232, 'sociological': 13233, 'homicidal': 13234, 'arcade': 13235, 'deepened': 13236, 'israel’s': 13237, 'grass': 13238, 'wright': 13239, 'decidedly': 13240, 'envelope': 13241, \"angel's\": 13242, 'comrades': 13243, 'boycott': 13244, 'disadvantages': 13245, 'sony': 13246, 'cooke': 13247, 'debug': 13248, 'indicators': 13249, 'internals': 13250, 'grants': 13251, 'legionary': 13252, \"'such\": 13253, 'quotient': 13254, 'photographed': 13255, 'harvest': 13256, 'paragraphs': 13257, 'inquisitiveness': 13258, 'surf': 13259, 'hobbies': 13260, 'optimism': 13261, 'asp': 13262, 'implemented': 13263, 'dances': 13264, 'civilized': 13265, \"noah's\": 13266, 'temperature': 13267, 'predecessor': 13268, 'unnoticed': 13269, 'shambhala': 13270, 'obtain': 13271, 'prized': 13272, 'simpson': 13273, 'prophesied': 13274, 'blamed': 13275, 'generative': 13276, 'rnn': 13277, 'lettered': 13278, 'sophia': 13279, 'wield': 13280, 'webber': 13281, 'fifteenth': 13282, 'nawab': 13283, 'culinary': 13284, 'verne': 13285, 'nutshell': 13286, 'lynne': 13287, 'painters': 13288, 'spiders': 13289, 'dempsey': 13290, 'regularization': 13291, 'scrooge': 13292, 'judged': 13293, 'estimate': 13294, 'longed': 13295, 'trojan': 13296, 'resigned': 13297, 'yield': 13298, 'miscellaneous': 13299, 'ashley': 13300, 'identical': 13301, 'revolved': 13302, 'snapshot': 13303, 'leopards': 13304, 'gurney': 13305, 'armand': 13306, 'feline': 13307, 'vegeta': 13308, 'diffusion': 13309, 'optics': 13310, 'studs': 13311, 'intersectionality': 13312, 'nikos': 13313, 'magazine’s': 13314, 'hire': 13315, 'davies': 13316, 'dostoevsky': 13317, 'nightmarish': 13318, 'seeds': 13319, 'automobile': 13320, 'portraying': 13321, 'consumerism': 13322, 'fundamentalism': 13323, 'austria': 13324, 'extortion': 13325, 'coups': 13326, 'assassinations': 13327, 'fermina': 13328, 'baptism': 13329, 'andrzej': 13330, 'sapkowski': 13331, 'sampler': 13332, 'francois': 13333, '1934': 13334, 'lions': 13335, 'holi': 13336, 'hercules': 13337, 'brahmin': 13338, 'equivalents': 13339, 'planner': 13340, 'vonnegut': 13341, 'obliged': 13342, 'syllabuses': 13343, 'whodunit': 13344, 'tricked': 13345, 'assuming': 13346, 'unveils': 13347, 'awry': 13348, 'draft': 13349, 'plural': 13350, 'modi’s': 13351, 'bob’s': 13352, 'johanna': 13353, 'angus': 13354, 'hostel': 13355, 'caters': 13356, 'floors': 13357, 'bharatiya': 13358, 'caves': 13359, 'tones': 13360, 'driverless': 13361, 'katrina': 13362, 'guitarist': 13363, 'jacqueline': 13364, 'suzanne': 13365, 'goretti': 13366, 'joshi': 13367, 'korak': 13368, 'seuss': 13369, 'psycho': 13370, 'multithreaded': 13371, 'preeti': 13372, 'baggage': 13373, 'maximizing': 13374, 'corbusier': 13375, 'gavaskar': 13376, 'kemp': 13377, 'belgium': 13378, \"'so\": 13379, 'juan': 13380, 'cabrillo': 13381, 'nun': 13382, 'sturdy': 13383, 'meantime': 13384, 'elemental': 13385, 'shading': 13386, 'rukia': 13387, 'incremental': 13388, 'floral': 13389, 'dreamland': 13390, 'faithfully': 13391, 'absorb': 13392, \"martin's\": 13393, 'specialists': 13394, 'feuds': 13395, 'browsers': 13396, 'prefixes': 13397, 'suffixes': 13398, 'tasker': 13399, 'ayesha': 13400, 'hundredth': 13401, '1923': 13402, 'woodrow': 13403, 'sid': 13404, 'shambu': 13405, 'bosses': 13406, 'illuminati': 13407, 'neha': 13408, 'inept': 13409, 'ict': 13410, 'fangirl': 13411, 'footed': 13412, 'foresight': 13413, 'roswell': 13414, \"fire'\": 13415, 'goku’s': 13416, 'toriyama’s': 13417, 'eventual': 13418, 'simmons': 13419, 'carlton': 13420, 'ganga': 13421, 'evidences': 13422, 'ricky': 13423, 'cbse': 13424, 'shankar': 13425, 'caller': 13426, 'notably': 13427, 'forecasting': 13428, 'ballroom': 13429, '225': 13430, 'queens': 13431, 'roy’s': 13432, 'amitabh': 13433, 'rudolf': 13434, 'chhavi': 13435, '1a': 13436, 'repute': 13437, 'presenter': 13438, \"hitchhiker's\": 13439, 'consolidated': 13440, 'andhra': 13441, 'naidu': 13442, 'catering': 13443, 'lillian': 13444, 'suppressed': 13445, 'marilyn': 13446, 'mao': 13447, 'dummies': 13448, 'ahmed': 13449, 'iq': 13450, 'marque': 13451, 'reed': 13452, 'quarterback': 13453, 'saviour': 13454, 'federation': 13455, 'kate’s': 13456, 'emsworth': 13457, 'batgirl': 13458, 'removable': 13459, 'phoebe': 13460, 'précis': 13461, 'grameen': 13462, 'zohar': 13463, 'alexis': 13464, 'horseback': 13465, 'contracts': 13466, 'mandrake': 13467, 'pakhi': 13468, 'jomny': 13469, 'sultanat': 13470, '1526': 13471, 'automata': 13472, 'elric': 13473, 'alchemical': 13474, 'edison': 13475, 'kathak': 13476, 'alain': 13477, 'burnout': 13478, 'sensuality': 13479, 'sweetie': 13480, 'wally': 13481, 'nightwing': 13482, 'partisans': 13483, 'chalisa': 13484, 'yash': 13485, \"alzheimer's\": 13486, \"leonardo's\": 13487, 'meira': 13488, 'akul': 13489, 'anthropocene': 13490, 'kayla': 13491, 'ivanov': 13492, 'jigs': 13493, 'गाँव': 13494, 'dyeing': 13495, 'khonoma': 13496, 'sieckmann': 13497, 'shanghvi': 13498, 'exhaustion': 13499, 'martínez': 13500, 'hiccup': 13501, 'gamache': 13502, 'badminton': 13503, 'madriani': 13504, 'nfl': 13505, 'shaymin': 13506, 'dorland’s': 13507, 'reena': 13508, 'botton': 13509, 'sumire': 13510, 'etta': 13511, '·the': 13512, 'phool': 13513, 'aur': 13514, 'ghulam': 13515, 'leviathan': 13516, 'formulates': 13517, \"hobbes's\": 13518, 'sipping': 13519, 'inspection': 13520, 'parlour': 13521, 'apex': 13522, 't’ai': 13523, 'aligned': 13524, 'cruise': 13525, 'nutritious': 13526, 'severe': 13527, 'sterling': 13528, 'ethic': 13529, 'thrones’': 13530, 'adulterous': 13531, 'unhappily': 13532, 'nadella': 13533, 'orient': 13534, 'tang': 13535, 'occupy': 13536, 'nosql': 13537, 'comforting': 13538, 'lightness': 13539, 'unerringly': 13540, 'hormone': 13541, \"winner's\": 13542, \"puzo's\": 13543, 'mosca': 13544, 'africans': 13545, 'cooked': 13546, 'stealth': 13547, 'penetrated': 13548, 'persuaded': 13549, 'committee': 13550, 'lalit': 13551, 'linger': 13552, 'contextualizes': 13553, 'interconnectedness': 13554, 'relive': 13555, 'hogan': 13556, 'tickets': 13557, 'michaels': 13558, 'dug': 13559, 'aficionado': 13560, 'clarified': 13561, 'torment': 13562, 'utilizes': 13563, 'gangtok': 13564, \"orwell's\": 13565, \"'all\": 13566, 'cliches': 13567, 'incendiary': 13568, 'swirl': 13569, 'fleetwood': 13570, 'coastline': 13571, 'bangkok': 13572, 'novelization': 13573, 'sequencing': 13574, 'fanatics': 13575, 'conspirators': 13576, 'indictment': 13577, 'derailed': 13578, \"abrsm's\": 13579, 'downloads': 13580, 'disoriented': 13581, 'crichton': 13582, 'strikingly': 13583, 'downward': 13584, 'bookstores': 13585, 'quirks': 13586, 'eggers': 13587, 'trailer': 13588, \"d'etat\": 13589, 'messenger': 13590, 'revisited': 13591, 'alarmingly': 13592, 'portly': 13593, 'evils': 13594, 'wielding': 13595, 'relocate': 13596, 'panels': 13597, 'shockwaves': 13598, 'compromises': 13599, 'ladybug': 13600, 'crowds': 13601, 'misinformation': 13602, 'violations': 13603, 'cfvs': 13604, 'empirical': 13605, 'buys': 13606, 'carolina': 13607, 'envisions': 13608, 'distrust': 13609, '34': 13610, \"himself'\": 13611, 'ferocity': 13612, \"nadal's\": 13613, 'ha': 13614, 'hijacked': 13615, 'disillusioned': 13616, 'cylinders': 13617, 'mechanic': 13618, 'seagull': 13619, 'unlikeliest': 13620, 'safeguarding': 13621, 'imagines': 13622, 'cm': 13623, 'worshipping': 13624, 'thrived': 13625, 'kapadia': 13626, \"game'\": 13627, 'aurora': 13628, 'nat': 13629, 'elastic': 13630, 'gesture': 13631, 'sioux': 13632, 'wyoming': 13633, 'motions': 13634, 'scouts': 13635, 'linguists': 13636, 'photographing': 13637, 'indicating': 13638, 'pte': 13639, 'b1': 13640, 'landlady': 13641, 'elder': 13642, 'bumps': 13643, 'shakes': 13644, 'yates': 13645, 'comptia': 13646, \"mike's\": 13647, 'innovate': 13648, 'drucker': 13649, 'stifling': 13650, 'cravings': 13651, 'romeo': 13652, 'portugal': 13653, 'fireworks': 13654, 'dietary': 13655, 'expectation': 13656, 'usability': 13657, 'cardiovascular': 13658, 'floods': 13659, 'rhythmic': 13660, 'hamstrings': 13661, 'achilles': 13662, \"william's\": 13663, 'hearty': 13664, 'imp': 13665, 'hitch': 13666, 'deported': 13667, 'unsparingly': 13668, 'spliff': 13669, \"'beautifully\": 13670, 'paige': 13671, 'furniture': 13672, 'jewelry': 13673, 'gauntlet': 13674, 'exploded': 13675, \"langdon's\": 13676, 'mehta': 13677, '4000': 13678, 'wavering': 13679, 'nickname': 13680, 'reinvent': 13681, 'upgraded': 13682, 'manuals': 13683, 'focusses': 13684, 'mortimer': 13685, 'environmentalism': 13686, 'propose': 13687, 'campuses': 13688, 'remaking': 13689, 'enlighten': 13690, 'sharply': 13691, 'persuasively': 13692, 'deborah': 13693, 'persia': 13694, 'multibillion': 13695, \"'chilling\": 13696, 'tranquility': 13697, 'booked': 13698, 'cheaper': 13699, 'monopoly': 13700, 'unrepentant': 13701, 'whites': 13702, 'unesco': 13703, 'plea': 13704, 'chair': 13705, 'frankly': 13706, 'resides': 13707, 'icnd2': 13708, 'dvds': 13709, 'simulator': 13710, 'paramours': 13711, 'replication': 13712, 'atlanta': 13713, 'hooking': 13714, 'ickenham': 13715, 'venkat': 13716, 'deadlines': 13717, 'deficit': 13718, 'riverdale': 13719, 'descendant': 13720, 'fyodor': 13721, 'alyosha': 13722, 'chronicled': 13723, 'confined': 13724, 'vivekananda’s': 13725, 'resident': 13726, 'yuri': 13727, 'government’s': 13728, 'imminent': 13729, 'chic': 13730, 'biased': 13731, 'faulkner': 13732, 'sylvia': 13733, 'rewarded': 13734, 'toxic': 13735, 'falcon': 13736, 'haryana': 13737, '86': 13738, 'buddies': 13739, 'tehmina': 13740, 'pulsating': 13741, 'nikita': 13742, 'robotic': 13743, 'synonymous': 13744, 'pivottables': 13745, 'vestiges': 13746, \"parent's\": 13747, 'ping': 13748, 'interactivity': 13749, 'ck': 13750, 'blisteringly': 13751, 'probing': 13752, 'movingly': 13753, 'integrates': 13754, 'gabaldon': 13755, 'watery': 13756, 'ventured': 13757, '”—people': 13758, 'phileas': 13759, 'delectable': 13760, 'boardroom': 13761, 'appointments': 13762, 'yogi': 13763, 'bmw': 13764, 'niels': 13765, 'preserves': 13766, 'zoey': 13767, 'mackenzie': 13768, 'starless': 13769, 'swordsman': 13770, 'noose': 13771, \"'heart\": 13772, \"talent'\": 13773, 'intellectually': 13774, 'poisonous': 13775, 'depictions': 13776, 'tanjore': 13777, 'donte': 13778, 'inoperable': 13779, 'refresh': 13780, 'renewal': 13781, 'rediscovered': 13782, 'humanist': 13783, 'world…': 13784, 'rowboat': 13785, 'wannabes': 13786, 'commanders': 13787, 'summation': 13788, \"'who\": 13789, '82': 13790, '1926': 13791, 'fountainhead': 13792, 'intend': 13793, 'sudanese': 13794, 'morton': 13795, 'holiness': 13796, 'unharmed': 13797, 'lyricism': 13798, 'invite': 13799, 'metropolitan': 13800, 'unravelling': 13801, 'clooney': 13802, 'pledged': 13803, 'westeros…': 13804, 'squire': 13805, 'dunk': 13806, 'cuckoo': 13807, 'waltz': 13808, 'hymn': 13809, 'rockets': 13810, 'intervals': 13811, 'skip': 13812, 'prayer': 13813, 'adoration': 13814, 'confounded': 13815, 'revolutionaries': 13816, 'boredom': 13817, 'problem—he': 13818, 'acknowledge': 13819, 'dante': 13820, 'taseer': 13821, 'collides': 13822, 'rally': 13823, 'anxious': 13824, 'ladakh': 13825, 'gently': 13826, 'repository': 13827, \"'we\": 13828, 'cultivate': 13829, \"decade'\": 13830, 'afterwards': 13831, 'structuring': 13832, 'contentment': 13833, 'wed': 13834, 'differing': 13835, 'vein': 13836, 'ket': 13837, \"python's\": 13838, 'locales': 13839, 'runner’s': 13840, 'grueling': 13841, 'pointer': 13842, 'catalyst': 13843, 'insurgency': 13844, 'prasad': 13845, 'ravage': 13846, 'brusatte': 13847, 'terrorizing': 13848, 'lava': 13849, 'bottled': 13850, 'optional': 13851, 'viesturs’s': 13852, 'agonies': 13853, 'stamped': 13854, '6th': 13855, 'heirs': 13856, 'saladin': 13857, 'london’s': 13858, 'martel': 13859, 'treason': 13860, 'brother’s': 13861, 'qualifying': 13862, 'aspirant': 13863, 'recruiting': 13864, 'gladwell': 13865, 'extremes': 13866, \"photographer's\": 13867, 'reliving': 13868, 'rounded': 13869, 'eloquence': 13870, '’s': 13871, 'turnaround': 13872, 'puzzled': 13873, 'entertains': 13874, 'fascinates': 13875, 'legged': 13876, 'f2': 13877, 'stevie': 13878, 'namaskara': 13879, 'sidhu': 13880, 'aw': 13881, 'kazi': 13882, \"delhi's\": 13883, 'chogyal': 13884, 'disgusting': 13885, \"heart'\": 13886, \"lifetime's\": 13887, 'breathtakingly': 13888, 'erp': 13889, 'supplier': 13890, 'compliance': 13891, 'lengthy': 13892, 'abhinav': 13893, \"ripley's\": 13894, 'afresh': 13895, 'anthologies': 13896, 'berkshire': 13897, 'buffett’s': 13898, 'prescription': 13899, 'cardiac': 13900, 'उर्दू': 13901, 'और': 13902, 'pragmatism': 13903, \"nehru's\": 13904, 'gurcharan': 13905, 'vaults': 13906, 'obscurity': 13907, 'reverses': 13908, 'jawahar': 13909, 'valabhbhai': 13910, 'decrepit': 13911, 'industrialization': 13912, 'gautam': 13913, 'saradindu': 13914, 'weaved': 13915, 'buffalo': 13916, 'johan': 13917, 'greener': 13918, 'climates': 13919, 'storing': 13920, 'bleed': 13921, 'escapism': 13922, 'montefiore': 13923, 'cheese': 13924, 'pasta': 13925, 'scofield': 13926, 'seduced': 13927, 'sucked': 13928, 'cami': 13929, 'tending': 13930, 'trenton': 13931, \"kids'\": 13932, 'configurations': 13933, 'licence': 13934, 'warner': 13935, 'triggering': 13936, 'overthrow': 13937, \"devil's\": 13938, 'dove': 13939, 'werewolf': 13940, 'miami': 13941, 'cane': 13942, 'preeminent': 13943, 'onward': 13944, 'depravity': 13945, 'ge': 13946, 'remastered': 13947, 'darrowby': 13948, 'salinger': 13949, \"forever'\": 13950, 'buttigieg': 13951, 'reinvention': 13952, 'appealed': 13953, 'scavengers': 13954, 'graceful': 13955, 'butalia': 13956, 'deserving': 13957, 'bourgeois': 13958, 'eli': 13959, 'restrictions': 13960, 'unavailable': 13961, 'sidelines': 13962, 'savior': 13963, 'poland': 13964, 'cryptographic': 13965, 'conduit': 13966, 'lighter': 13967, 'graphical': 13968, 'crying': 13969, 'annoyingly': 13970, 'advaita': 13971, 'saharan': 13972, 'rumoured': 13973, 'toffler': 13974, 'shunned': 13975, 'grocery': 13976, 'flourished': 13977, 'rallying': 13978, 'themost': 13979, 'kiki': 13980, '—and': 13981, 'fur': 13982, 'reinvented': 13983, 'nik': 13984, 'sanjana': 13985, 'assassin’s': 13986, 'neurological': 13987, 'instruct': 13988, 'grabbed': 13989, 'travelogues': 13990, 'variance': 13991, 'drifter': 13992, 'keyhole': 13993, 'metaphysical': 13994, 'theology': 13995, 'joel': 13996, \"luke's\": 13997, 'freely': 13998, 'bundle': 13999, 'annihilation': 14000, 'rt': 14001, 'thatcher': 14002, 'antony': 14003, 'edmund': 14004, 'pedestrian': 14005, 'stockholm': 14006, 'prophetic': 14007, 'gilead': 14008, 'vault': 14009, 'polemical': 14010, 'hazare': 14011, 'irreversible': 14012, 'agricultural': 14013, 'madras': 14014, 'alarms': 14015, 'gui': 14016, \"o'reilly\": 14017, \"scarrow's\": 14018, 'fourteenth': 14019, 'stronghold': 14020, 'afoot': 14021, 'eludes': 14022, \"fitzgerald's\": 14023, 'mcfarlane': 14024, 'contemplation': 14025, 'magically': 14026, 'allure': 14027, 'neumann': 14028, 'osborne': 14029, 'persuades': 14030, 'admissions': 14031, 'motivates': 14032, 'defences': 14033, 'implements': 14034, 'doris': 14035, 'lyndon': 14036, 'margery': 14037, 'tudor': 14038, 'extremism': 14039, 'celebrations': 14040, \"raven's\": 14041, 'ribs': 14042, 'vietnamese': 14043, 'flights': 14044, 'overcoat': 14045, 'smuggled': 14046, 'devout': 14047, 'peopled': 14048, 'uncommon': 14049, 'humdrum': 14050, 'drastic': 14051, 'redeemers': 14052, 'infinitely': 14053, 'conn': 14054, 'bardugo': 14055, 'angelique': 14056, 'mitali': 14057, 'obnoxious': 14058, 'somerset': 14059, 'multilingual': 14060, 'knowledgeable': 14061, 'screening': 14062, 'dash': 14063, 'fab': 14064, \"emperor's\": 14065, 'avantika': 14066, 'strengthened': 14067, 'proceedings': 14068, 'defect': 14069, 'ditch': 14070, 'meena': 14071, 'kumari': 14072, 'raja': 14073, 'manto': 14074, 'truthful': 14075, 'rehearsal': 14076, \"studios'\": 14077, 'hemsworth': 14078, 'ruffalo': 14079, 'johansson': 14080, 'renner': 14081, 'ultron': 14082, 'eduardo': 14083, 'risso': 14084, 'abiding': 14085, 'teaming': 14086, 'hampton': 14087, 'jasper': 14088, \"'leaves\": 14089, \"cleopatra's\": 14090, 'autocratic': 14091, 'employ': 14092, 'forif': 14093, 'guo': 14094, 'jing': 14095, 'patriot': 14096, 'enlarged': 14097, 'tendencies': 14098, 'combative': 14099, 'racked': 14100, 'innermost': 14101, 'imperialism': 14102, 'delineates': 14103, 'postcard': 14104, 'relies': 14105, 'gallo': 14106, 'poorer': 14107, 'comparison': 14108, '1941': 14109, 'savagery': 14110, 'footprints': 14111, '1887': 14112, 'baskervilles': 14113, '1915': 14114, 'bombing': 14115, 'graf': 14116, 'thorns': 14117, 'thursday': 14118, 'delayed': 14119, 'sagar': 14120, 'prithvi': 14121, 'darwin': 14122, 'shouldn’t': 14123, 'caustic': 14124, 'boon': 14125, 'categorization': 14126, 'criticized': 14127, 'oval': 14128, 'catana': 14129, 'earthsea': 14130, 'priestess': 14131, \"yosuke's\": 14132, 'mermaids': 14133, 'grids': 14134, 'mediocre': 14135, 'voracious': 14136, 'madame': 14137, 'cadences': 14138, 'enrichment': 14139, 'goodwill': 14140, 'pinpoint': 14141, 'dismisses': 14142, \"ottakar's\": 14143, 'suicidal': 14144, 'hellish': 14145, 'towel': 14146, 'sliding': 14147, 'diagonal': 14148, 'interweaves': 14149, 'industrialist': 14150, 'individuality': 14151, 'rushing': 14152, 'someone’s': 14153, 'disabled': 14154, 'greta': 14155, 'thrones―with': 14156, 'chapter―revitalizes': 14157, 'massing': 14158, 'kingdom’s': 14159, 'protective': 14160, 'summertime': 14161, 'epicurean': 14162, 'counterplots': 14163, 'perilously': 14164, 'hail': 14165, \"delavier's\": 14166, 'codename': 14167, 'kush': 14168, 'dalvi': 14169, 'clueless': 14170, \"again'\": 14171, 'terrifyingly': 14172, 'whispering': 14173, \"dad's\": 14174, \"jean's\": 14175, \"who'll\": 14176, 'family—and': 14177, 'analysed': 14178, 'tuna': 14179, 'bookstore': 14180, 'peterson': 14181, 'garhwal': 14182, 'oak': 14183, 'inclination': 14184, 'pandey': 14185, 'ruchir': 14186, 'godmen': 14187, 'elect': 14188, 'billie': 14189, 'gymnast': 14190, 'patti': 14191, 'deathly': 14192, 'jace': 14193, 'codex': 14194, 'ra': 14195, 'reinvents': 14196, 'miller’s': 14197, 'exchanges': 14198, 'planet’s': 14199, 'krell': 14200, 'spensa': 14201, 'sanderson': 14202, 'rothfuss': 14203, 'ew': 14204, 'teases': 14205, 'flourishes': 14206, 'aces': 14207, 'sleights': 14208, 'unsurpassed': 14209, 'uncomplicated': 14210, 'impressively': 14211, 'gurmehar': 14212, 'retreated': 14213, 'comprised': 14214, 'cures': 14215, 'coolest': 14216, 'toast': 14217, \"authors'\": 14218, 'fectly': 14219, 'compensation': 14220, 'cue': 14221, 'buzzwords': 14222, 'pipes': 14223, 'charisma': 14224, 'judd': 14225, 'tinsel': 14226, 'français': 14227, 'anglais': 14228, 'mots': 14229, 'bureau': 14230, 'pants': 14231, 'orhan': 14232, 'ackroyd': 14233, 'flowcharts': 14234, 'meridian': 14235, 'indirectly': 14236, 'harnessing': 14237, 'naturalistic': 14238, 'doctrines': 14239, 'recognizable': 14240, 'nonetheless': 14241, 'tempts': 14242, 'quips': 14243, 'popping': 14244, 'feeding': 14245, 'boldness': 14246, 'esquire': 14247, 'shawshank': 14248, \"'there's\": 14249, 'neighbors': 14250, 'amul': 14251, 'ads': 14252, 'rags': 14253, 'channeled': 14254, \"hadn't\": 14255, 'drowned': 14256, 'modal': 14257, \"jones's\": 14258, 'posthumously': 14259, \"hemingway's\": 14260, 'bartimaeus': 14261, \"elliot's\": 14262, 'daemon': 14263, 'olympian': 14264, 'zeus': 14265, 'stressing': 14266, 'rethinking': 14267, 'duchess': 14268, 'insurmountable': 14269, 'glitter': 14270, 'bogart': 14271, 'blinded': 14272, 'superheros': 14273, 'azzarello': 14274, 'jlpt': 14275, 'mayhew': 14276, 'ciphers': 14277, 'tails': 14278, 'sneakers': 14279, 'ingredient': 14280, 'arsene': 14281, 'pleasing': 14282, 'premiership': 14283, 'disarming': 14284, 'pathway': 14285, 'methodically': 14286, 'zeal': 14287, 'spicy': 14288, 'ogilvy': 14289, 'apt': 14290, '15th': 14291, 'pr': 14292, 'nsa': 14293, 'snowden': 14294, '90s': 14295, 'prague': 14296, 'keynote': 14297, 'painfully': 14298, 'tite': 14299, 'sharif': 14300, 'tidal': 14301, 'narrate': 14302, 'beggars': 14303, '‘in': 14304, 'lakhs': 14305, 'corrupted': 14306, 'abrupt': 14307, 'veterans': 14308, 'grammarian': 14309, 'preprocessing': 14310, 'virtue': 14311, 'grotesque': 14312, 'quimby': 14313, 'ashima': 14314, 'juggernaut': 14315, 'cuban': 14316, 'pacing': 14317, 'regulations': 14318, 'superiority': 14319, 'differ': 14320, 'complaint': 14321, 'harbouring': 14322, 'disappointing': 14323, 'classify': 14324, 'decoding': 14325, 'ancients': 14326, 'brahma': 14327, 'lakshmi': 14328, 'puranas': 14329, 'bestowed': 14330, 'justified': 14331, 'varg': 14332, 'depressed': 14333, 'trucks': 14334, 'stir': 14335, 'cocky': 14336, 'philosophic': 14337, 'aron': 14338, 'canyons': 14339, 'shorts': 14340, 'nearest': 14341, \"mountain's\": 14342, 'academia': 14343, 'nuggets': 14344, \"exam's\": 14345, 'belter': 14346, 'cavanagh': 14347, 'slick': 14348, 'clarke': 14349, 'tornado': 14350, 'hugo’s': 14351, 'misérables': 14352, 'buckingham': 14353, 'resign': 14354, 'giles': 14355, 'ea': 14356, 'pseudo': 14357, 'psus': 14358, 'instrumentation': 14359, '360': 14360, 'diagram': 14361, 'sportsmen': 14362, 'trescothick': 14363, 'nut': 14364, 'surpass': 14365, 'anguish': 14366, 'zurich': 14367, 'outputs': 14368, 'polymorphism': 14369, '89': 14370, 'pipelines': 14371, 'subversive': 14372, 'aphorisms': 14373, 'dissent': 14374, 'deceptive': 14375, 'swearing': 14376, 'homonyms': 14377, 'coexistence': 14378, 'affiliates': 14379, 'ibps': 14380, 'sbi': 14381, 'inquisitor': 14382, 'jezal': 14383, 'logen': 14384, 'magi': 14385, 'correlation': 14386, 'incite': 14387, 'indifferent': 14388, 'komarov': 14389, 'obey': 14390, 'manifestations': 14391, '“1000': 14392, 'century”': 14393, 'people―along': 14394, 'buddha―who': 14395, 'jibes': 14396, 'dwell': 14397, 'yorick': 14398, 'kremlin': 14399, 'minimize': 14400, 'reliance': 14401, 'modals': 14402, 'adverb': 14403, 'substitutions': 14404, 'friend’s': 14405, 'spoiled': 14406, 'england’s': 14407, 'vices': 14408, 'virtues': 14409, 'muse': 14410, '“one': 14411, 'clocks': 14412, 'cid': 14413, 'mandalas': 14414, 'interpretations': 14415, 'hectic': 14416, 'scans': 14417, 'lannisters': 14418, 'installments': 14419, 'census': 14420, 'society’s': 14421, 'intern': 14422, 'perfectionist': 14423, 'tempting': 14424, 'reverence': 14425, 'howell': 14426, 'touring': 14427, 'advertisement': 14428, 'sour': 14429, 'inventiveness': 14430, 'flush': 14431, 'deepen': 14432, 'paar': 14433, 'smallest': 14434, 'pickles': 14435, 'reap': 14436, 'cries': 14437, 'islamist': 14438, 'loveless': 14439, 'née': 14440, 'jaws': 14441, 'unruly': 14442, 'commonplace': 14443, 'maharashtra': 14444, 'baseless': 14445, 'inexhaustible': 14446, 'riddle': 14447, 'teasers': 14448, 'puns': 14449, 'gatherings': 14450, 'mettle': 14451, 'provence': 14452, '44': 14453, 'karim': 14454, '‘black': 14455, 'pirandello': 14456, 'waller': 14457, 'grind': 14458, 'innovator': 14459, 'paypal': 14460, 'shadowed': 14461, \"clever'\": 14462, 'ogden': 14463, 'ansari': 14464, 'kishimoto': 14465, 'sauce': 14466, 'tubman': 14467, 'noor': 14468, 'valentina': 14469, 'swollen': 14470, 'paste': 14471, 'uprising': 14472, 'friendless': 14473, 'confidantes': 14474, 'unreal': 14475, 'superlatives': 14476, 'rejoice': 14477, 'dinacharya': 14478, 'healer': 14479, 'advani': 14480, 'rollins': 14481, 'dragonfly': 14482, 'clans': 14483, 'busiest': 14484, 'casualty': 14485, '‘who': 14486, 'caricature': 14487, 'gags': 14488, 'unrelated': 14489, 'tubes': 14490, 'disaffected': 14491, 'remorse': 14492, 'doctrine': 14493, 'smashed': 14494, 'lined': 14495, 'sears': 14496, 'loot': 14497, 'camilla': 14498, 'cantankerous': 14499, 'ft': 14500, 'transcendental': 14501, 'southernmost': 14502, 'alleys': 14503, 'fugitives': 14504, \"sacco's\": 14505, 'seymour': 14506, 'clutch': 14507, 'brushes': 14508, 'monte': 14509, 'toddlers': 14510, 'embarking': 14511, 'salvador': 14512, 'upset': 14513, 'sails': 14514, 'intertwine': 14515, 'expense': 14516, 'surveying': 14517, 'massage': 14518, 'rejuvenate': 14519, 'unease': 14520, 'regulate': 14521, 'threshold': 14522, 'mcgraw': 14523, 'bayesian': 14524, 'pessimistic': 14525, \"chomsky's\": 14526, 'emphasize': 14527, 'moustache': 14528, 'notoriously': 14529, 'primitives': 14530, 'parallelism': 14531, 'maintainable': 14532, 'stitches': 14533, 'diy': 14534, 'crusoe': 14535, 'kaplan': 14536, 'adichie': 14537, \"summer's\": 14538, 'sums': 14539, 'imparts': 14540, 'commercials': 14541, 'miracles': 14542, 'turtles': 14543, 'glossop': 14544, 'idly': 14545, 'jasmine': 14546, 'bounce': 14547, 'scintillating': 14548, 'wallace': 14549, 'tingling': 14550, 'mccord': 14551, 'retention': 14552, 'patternmaking': 14553, 'pangs': 14554, 'protein': 14555, 'madhav': 14556, 'cep': 14557, 'fidelity': 14558, 'roxane': 14559, 'jdk': 14560, 'constructors': 14561, 'controlling': 14562, 'kavalier': 14563, 'craze': 14564, 'blankets': 14565, 'fundamentalist': 14566, 'territorial': 14567, '808': 14568, '1949': 14569, 'orwell’s': 14570, 'winston': 14571, 'webs': 14572, 'sixes': 14573, 'instinctively': 14574, 'snitch': 14575, 'pashtun': 14576, 'smoothing': 14577, 'wreaking': 14578, 'traffickers': 14579, 'orbit': 14580, 'manipulative': 14581, 'stanislavski': 14582, 'inward': 14583, 'ashoka': 14584, 'crafty': 14585, 'indigenous': 14586, 'transcended': 14587, '1913': 14588, 'ahmann': 14589, 'inevera': 14590, 'fragility': 14591, 'hello': 14592, 'fae': 14593, 'superhuman': 14594, 'recovers': 14595, 'snout': 14596, 'vernon': 14597, 'theorists': 14598, 'ought': 14599, 'neo': 14600, 'carey': 14601, 'carroll': 14602, 'volunteers': 14603, 'subordinate': 14604, 'reuben': 14605, 'speckled': 14606, 'foggy': 14607, 'unarmed': 14608, 'directional': 14609, 'homecoming': 14610, 'yuval': 14611, 'harari': 14612, 'elevate': 14613, 'parameters': 14614, 'stalwarts': 14615, 'saran': 14616, 'pyaasa': 14617, 'rehman': 14618, 'ambience': 14619, 'slain': 14620, 'fearful': 14621, 'suárez': 14622, 'tenure': 14623, \"kapoor's\": 14624, 'idols': 14625, 'salmon': 14626, 'goodness': 14627, 'notch': 14628, 'burmese': 14629, 'troy’s': 14630, 'missionary': 14631, 'alcoholic': 14632, 'commandos': 14633, 'bagchi': 14634, 'workforce': 14635, 'employers': 14636, 'breakout': 14637, \"sisters'\": 14638, \"hannah's\": 14639, 'sticks': 14640, 'sagas': 14641, 'avalanche': 14642, 'coulthard': 14643, 'register': 14644, 'dining': 14645, 'donaldson': 14646, 'revolt': 14647, 'ranged': 14648, 'soulmates': 14649, 'sussaina': 14650, 'drudan': 14651, 'meteor': 14652, 'vivian': 14653, 'turbulence': 14654, 'sweets': 14655, 'npr': 14656, 'bonded': 14657, \"red's\": 14658, 'socialite': 14659, 'pathologist': 14660, 'unify': 14661, 'phoenix': 14662, 'washes': 14663, 'heel': 14664, 'athena': 14665, \"'big\": 14666, 'romanticism': 14667, 'reconstructs': 14668, 'mentions': 14669, 'schoolmaster': 14670, 'disillusionment': 14671, 'panda': 14672, 'sly': 14673, 'laird': 14674, 'baron': 14675, 'jumbled': 14676, 'holger': 14677, 'creamer': 14678, 'roderick': 14679, 'faults': 14680, 'worn': 14681, 'huts': 14682, \"baker's\": 14683, 'md': 14684, 'revise': 14685, 'visualizations': 14686, 'slice': 14687, \"norgay's\": 14688, 'blessed': 14689, 'stale': 14690, 'prophet’s': 14691, '9th': 14692, 'simulated': 14693, 'collar': 14694, 'yacht': 14695, 'heartrending': 14696, 'sidekicks': 14697, 'bergkamp': 14698, 'pisenti': 14699, 'adulation': 14700, 'fickle': 14701, 'er': 14702, 'pipeline': 14703, 'newsroom': 14704, 'rallies': 14705, 'clanton': 14706, 'clockwork': 14707, 'cautionary': 14708, \"'not\": 14709, \"generation'\": 14710, 'hardy’s': 14711, 'complicates': 14712, 'milligan': 14713, 'culminated': 14714, 'awoke': 14715, \"billy's\": 14716, 'stirred': 14717, 'caleb': 14718, 'perfected': 14719, 'impregnable': 14720, 'novel’': 14721, 'forcibly': 14722, 'krakow': 14723, 'originals': 14724, 'heartbreakingly': 14725, \"jenny's\": 14726, 'melinda': 14727, 'superstardom': 14728, 'sholay': 14729, 'chupke': 14730, 'psychotic': 14731, 'hues': 14732, \"korede's\": 14733, 'ayoola': 14734, 'korede': 14735, 'fads': 14736, 'cults': 14737, 'fashions': 14738, 'believer': 14739, 'immortalized': 14740, 'raven': 14741, 'strata': 14742, 'utilities': 14743, 'jessie': 14744, 'flipping': 14745, 'penalty': 14746, 'dumb': 14747, 'sunk': 14748, \"for'\": 14749, 'sarcasm': 14750, 'hadi': 14751, 'scavenger': 14752, 'oddball': 14753, 'commendable': 14754, 'cuisines': 14755, 'cinnamon': 14756, 'thrives': 14757, 'refreshed': 14758, 'appraisal': 14759, 'aisle': 14760, 'evenings': 14761, '1904': 14762, 'ko': 14763, 'beacon': 14764, 'clearing': 14765, 'brainwashed': 14766, 'shorten': 14767, 'marathoning': 14768, 'hydration': 14769, 'speeding': 14770, 'pools': 14771, 'sliced': 14772, 'iniesta': 14773, 'daytripper': 14774, 'olivias': 14775, 'dominguez': 14776, 'oblivious': 14777, 'niro': 14778, 'ballon': 14779, 'davis': 14780, 'klaka': 14781, 'prem': 14782, 'mongolia': 14783, 'influencer': 14784, 'auschwitz': 14785, 'locks': 14786, 'hiroshima': 14787, \"'stranger’\": 14788, 'admirer': 14789, 'intruder': 14790, 'frightful': 14791, 'necklace': 14792, 'cornwell': 14793, 'fearlessly': 14794, \"'are\": 14795, 'raced': 14796, 'undo': 14797, 'backstreet': 14798, 'pubs': 14799, 'libby': 14800, 'dior': 14801, 'zimmerman': 14802, 'batch': 14803, 'garth': 14804, 'lanterns': 14805, 'broker': 14806, 'rows': 14807, 'casually': 14808, 'climo': 14809, 'yeti': 14810, 'dodgy': 14811, 'powerless': 14812, 'mansions': 14813, 'predominantly': 14814, 'overachieving': 14815, 'varnam': 14816, 'amply': 14817, 'fanon': 14818, 'degradation': 14819, '61': 14820, 'listened': 14821, 'anomaly': 14822, 'stubbornness': 14823, 'gerhard': 14824, 'progressions': 14825, 'sangh': 14826, 'overseas': 14827, 'bodily': 14828, 'arranges': 14829, \"'compulsive\": 14830, 'vicki': 14831, 'nicci': 14832, \"'jane\": 14833, 'exploitation': 14834, 'ripper': 14835, 'hijacking': 14836, 'hatch': 14837, 'continuum': 14838, 'blitz': 14839, 'rallison': 14840, 'christophe': 14841, 'dunes': 14842, 'beckett': 14843, 'nameless': 14844, \"'marvellous'\": 14845, 'she’ll': 14846, '1897': 14847, 'lance': 14848, 'copperplate': 14849, 'guevara': 14850, 'guerrilla': 14851, 'tulsi': 14852, 'infuriating': 14853, 'hilary': 14854, 'rookie': 14855, 'irrational': 14856, 'existential': 14857, 'unchecked': 14858, 'elspeth': 14859, 'windmills': 14860, 'll': 14861, 'engels': 14862, 'futuristic': 14863, 'tablet': 14864, 'assessing': 14865, 'hellmuth': 14866, 'playscript': 14867, 'grapples': 14868, 'albus': 14869, 'subtly': 14870, 'unnerving': 14871, 'jenks': 14872, 'cooch': 14873, 'behar': 14874, 'dietician': 14875, 'indulge': 14876, 'anthem': 14877, 'yelena': 14878, 'ons': 14879, 'plug': 14880, 'rigour': 14881, 'tracker': 14882, 'outskirts': 14883, 'educationists': 14884, 'shekhar': 14885, 'specification': 14886, 'attends': 14887, \"brother's\": 14888, 'wittiest': 14889, 'commons': 14890, 'headings': 14891, 'bruised': 14892, \"universe's\": 14893, 'cooperate': 14894, 'stout': 14895, 'royer': 14896, 'thankfully': 14897, 'backroom': 14898, 'pastime': 14899, 'smithsonian': 14900, 'marv': 14901, 'pine': 14902, 'criminally': 14903, 'battista': 14904, 'ludicrous': 14905, '‘read': 14906, 'lalmohan': 14907, 'jatayu': 14908, \"tendulkar's\": 14909, 'choking': 14910, 'aristocrat': 14911, 'mayday': 14912, 'sceptical': 14913, 'cannes': 14914, 'hopefully': 14915, 'banana': 14916, 'gauge': 14917, 'incarnation': 14918, 'cap': 14919, 'robs': 14920, 'binodini': 14921, 'likeness': 14922, 'toned': 14923, 'render': 14924, 'optioned': 14925, 'life…': 14926, 'nationality': 14927, 'swat': 14928, 'cattle': 14929, 'forgives': 14930, 'nationalists': 14931, \"wood's\": 14932, 'impacted': 14933, 'amendments': 14934, 'spontaneously': 14935, 'packet': 14936, 'opinionated': 14937, 'darth': 14938, 'vader': 14939, 'retells': 14940, \"bowden's\": 14941, '100th': 14942, 'mutant': 14943, 'cannon': 14944, 'carter’s': 14945, 'buyer': 14946, 'starving': 14947, 'export': 14948, 'deluded': 14949, 'kip': 14950, \"off'\": 14951, \"'weeks\": 14952, 'weirdness': 14953, 'stunt': 14954, 'rosy': 14955, 'unquenchable': 14956, 'subscribers': 14957, 'malaise': 14958, 'tonic': 14959, 'excluded': 14960, 'blockbustus': 14961, 'demented': 14962, '17th': 14963, 'cassius': 14964, '14th': 14965, 'nombeko': 14966, 'correlations': 14967, 'whiz': 14968, 'vedantic': 14969, 'juncture': 14970, 'saul': 14971, 'supremely': 14972, 'graves': 14973, 'forster': 14974, 'simulation': 14975, 'ultraman': 14976, 'punishing': 14977, 'wisest': 14978, 'bonds': 14979, 'hunter’s': 14980, 'life—and': 14981, 'eater': 14982, 'corbett’s': 14983, \"strike's\": 14984, 'sanctum': 14985, 'axl': 14986, 'booze': 14987, 'hotbed': 14988, 'symbiotic': 14989, 'diligence': 14990, 'callen': 14991, 'freakonomics': 14992, 'wisely': 14993, 'malloy': 14994, 'chuckle': 14995, 'ambulance': 14996, 'bigot': 14997, 'christian’s': 14998, 'maladies': 14999, 'uncanny': 15000, 'luggage': 15001, 'miniaturists': 15002, 'satan': 15003, 'authoritarianism': 15004, 'acceptable': 15005, 'geologist': 15006, 'mazes': 15007, 'aandhi': 15008, 'textual': 15009, 'biking': 15010, 'responsibilities': 15011, 'covenant': 15012, 'artifact': 15013, 'photojournalist': 15014, 'coordination': 15015, 'patiently': 15016, \"toriyama's\": 15017, 'rooney': 15018, \"dev's\": 15019, 'worried': 15020, 'lulu': 15021, 'nefarious': 15022, 'sheridan': 15023, 'shoya': 15024, 'devising': 15025, 'blame': 15026, 'mongol': 15027, 'dealings': 15028, '68': 15029, 'wattpad': 15030, 'singleton': 15031, 'smell': 15032, \"'just\": 15033, 'daevabad': 15034, 'sabaa': 15035, 'embraces': 15036, 'blackmailer': 15037, 'unconnected': 15038, 'valhalla': 15039, \"degeneres's\": 15040, 'kidding': 15041, 'disappointed': 15042, 'lamb': 15043, 'flips': 15044, 'incentives': 15045, 'dis': 15046, 'chad': 15047, 'float': 15048, 'prada': 15049, 'singles': 15050, 'romp': 15051, 'brighton': 15052, 'neatly': 15053, 'demigods': 15054, 'archery': 15055, 'acknowledging': 15056, 'lullabies': 15057, 'gloria': 15058, 'steinem': 15059, 'cleese': 15060, 'flock': 15061, 'telephone': 15062, 'bhairava': 15063, 'rikkard': 15064, 'cumference': 15065, 'farsi': 15066, 'repairs': 15067, 'emergencies': 15068, 'phrasebooks': 15069, 'drained': 15070, 'farms': 15071, 'warns': 15072, 'willpower': 15073, 'timothy': 15074, 'representative': 15075, 'productively': 15076, 'treaty': 15077, 'proportion': 15078, 'frailty': 15079, 'kuririn': 15080, 'toddler': 15081, 'buzz': 15082, 'minorities': 15083, 'academicians': 15084, 'accidents': 15085, 'inflation': 15086, 'jaitley': 15087, 'uncertainties': 15088, 'conjugation': 15089, 'amar': 15090, 'travancore': 15091, 'interference': 15092, 'periodic': 15093, 'agnes': 15094, 'carriage': 15095, \"review's\": 15096, 'stationery': 15097, 'menus': 15098, '30th': 15099, 'donated': 15100, 'christina': 15101, 'damnation': 15102, 'captors': 15103, '“if': 15104, 'ronald': 15105, 'sailors': 15106, 'peninsula': 15107, 'gdp': 15108, 'cohesive': 15109, 'persists': 15110, 'stared': 15111, 'unflinchingly': 15112, 'ridicule': 15113, 'chisaki': 15114, 'kai': 15115, 'challenger': 15116, 'bulgaria': 15117, 'whistle': 15118, 'damages': 15119, 'viruses': 15120, 'denial': 15121, 'jacobs': 15122, 'swamp': 15123, 'drag': 15124, 'vocals': 15125, 'formatting': 15126, 'insults': 15127, 'pointless': 15128, 'bendrix': 15129, 'rekindles': 15130, 'hires': 15131, 'wisden': 15132, 'tolerant': 15133, 'preparedness': 15134, 'tradeoffs': 15135, 'forecasts': 15136, 'jockey': 15137, 'hughes': 15138, 'releasing': 15139, 'moods': 15140, 'pixar': 15141, 'parisian': 15142, 'shelby': 15143, 'antiquity': 15144, 'witherspoon': 15145, 'refers': 15146, 'vitangelo': 15147, 'unremarkable': 15148, \"don's\": 15149, 'numerals': 15150, 'cad': 15151, 'kait': 15152, 'studded': 15153, 'gal': 15154, 'syndrome': 15155, 'storyboard': 15156, 'probable': 15157, 'ryder': 15158, 'hodges': 15159, 'betray': 15160, 'carson': 15161, 'id': 15162, 'edevane': 15163, 'policymaking': 15164, 'overhaul': 15165, 'protima': 15166, 'uproar': 15167, '1912': 15168, 'motivations': 15169, 'bloodbath': 15170, 'jude': 15171, 'lend': 15172, 'dabney': 15173, \"'anti\": 15174, \"guru'\": 15175, 'kari': 15176, 'arson': 15177, 'hadler': 15178, '41': 15179, 'ta': 15180, 'nehisi': 15181, 'kakutani': 15182, 'rendering': 15183, 'factual': 15184, 'bimal': 15185, 'zombie': 15186, \"examiner's\": 15187, 'respiratory': 15188, '2000s': 15189, 'fervent': 15190, 'conquerors': 15191, 'contracted': 15192, 'vicks': 15193, 'profusely': 15194, 'brodie': 15195, 'reallocation': 15196, 'goode': 15197, 'embroidery': 15198, 'obliterate': 15199, 'transferred': 15200, 'mamet': 15201, 'macy': 15202, 'dependable': 15203, 'hobbyists': 15204, 'sleeve': 15205, 'sermon': 15206, \"php's\": 15207, 'cookies': 15208, 'wraps': 15209, 'spontaneous': 15210, 'usages': 15211, 'fooling': 15212, 'tyson': 15213, 'hierarchies': 15214, 'tow': 15215, 'tardis': 15216, 'perveen': 15217, 'preaching': 15218, 'kaak': 15219, 'meat': 15220, \"getafix's\": 15221, 'precursor': 15222, 'kalaripayat': 15223, \"generation's\": 15224, 'behavioural': 15225, 'cube’s': 15226, 'hungarian': 15227, 'rubik': 15228, 'frustrating': 15229, 'cubing': 15230, 'participating': 15231, 'tanmay': 15232, 'countrymen': 15233, 'interpreter': 15234, \"ocean's\": 15235, 'waged': 15236, 'laini': 15237, 'oyster': 15238, 'prawns': 15239, 'fen': 15240, 'graphing': 15241, 'captains': 15242, 'vittorio': 15243, 'shockingly': 15244, 'centrifuges': 15245, 'uranium': 15246, 'replacing': 15247, 'firm’s': 15248, 'registry': 15249, 'shinrin': 15250, 'yoku': 15251, 'li': 15252, 'literate': 15253, 'terminal': 15254, 'orchestrated': 15255, 'musically': 15256, 'displaying': 15257, 'mcgrath': 15258, 'brushing': 15259, 'maturity': 15260, 'competency': 15261, 'traumas': 15262, 'kathy': 15263, 'genetically': 15264, 'malevolent': 15265, 'berry': 15266, 'contacted': 15267, 'misconceptions': 15268, 'youíve': 15269, 'quantitative': 15270, 'judgments': 15271, \"arnold's\": 15272, 'smersh’s': 15273, 'paymasters’': 15274, 'baccarat': 15275, 'outplay': 15276, 'eaux': 15277, 'vladimir': 15278, 'gum': 15279, 'constellations': 15280, 'atari': 15281, 'gruelling': 15282, 'determining': 15283, 'mack': 15284, 'astbury': 15285, 'chariot': 15286, 'enchantment': 15287, 'freshness': 15288, \"'hooked\": 15289, 'products’': 15290, 'nir': 15291, 'eyal': 15292, \"'hook\": 15293, 'snatches': 15294, 'havelock': 15295, 'facilitating': 15296, 'sharpe': 15297, 'brahmaputra': 15298, '30s': 15299, 'deus': 15300, 'gautama': 15301, 'inn': 15302, 'umpires': 15303, 'recorders': 15304, 'panavision': 15305, 'vantage': 15306, 'checklists': 15307, 'infosys': 15308, 'corporates': 15309, 'steep': 15310, 'interval': 15311, 'estimation': 15312, 'fulfillment': 15313, 'oasis': 15314, 'fairies': 15315, 'binge': 15316, 'kommandant': 15317, 'obstinate': 15318, 'wage': 15319, 'advertisers': 15320, 'barrage': 15321, 'sadistic': 15322, 'coca': 15323, 'renders': 15324, 'grieving': 15325, 'carla': 15326, 'timer': 15327, 'ks2': 15328, 'tantalising': 15329, 'recreational': 15330, 'curling': 15331, 'gps': 15332, 'logger': 15333, 'binary': 15334, 'tor': 15335, '‘hidden': 15336, 'treasure’': 15337, 'inhospitable': 15338, 'peshawar': 15339, 'advising': 15340, 'distillation': 15341, 'tranquil': 15342, '‘on': 15343, 'nariman': 15344, 'salim': 15345, 'flick': 15346, 'legitimacy': 15347, 'legislature': 15348, 'concert': 15349, 'jrd': 15350, 'griffiths': 15351, 'wacky': 15352, 'psychoanalysis': 15353, 'compound': 15354, 'rushes': 15355, 'adversarial': 15356, 'unsupervised': 15357, 'celtic': 15358, 'manifest': 15359, 'amoruso': 15360, 'checking': 15361, 'dunham': 15362, 'grasped': 15363, 'unveil': 15364, 'acoustic': 15365, 'mouths': 15366, 'leafwing': 15367, 'mansur': 15368, 'sharmila': 15369, 'supper': 15370, 'evoking': 15371, 'endpapers': 15372, 'bookmark': 15373, 'neurologist': 15374, 'cerebral': 15375, 'hostility': 15376, 'panorama': 15377, 'almanac': 15378, 'continuously': 15379, 'trillions': 15380, 'arising': 15381, 'stays': 15382, 'premises': 15383, 'thereafter': 15384, 'sculptor': 15385, 'convincingly': 15386, 'kurosaki': 15387, 'pining': 15388, 'conway': 15389, 'stealthy': 15390, 'bluetooth': 15391, '802': 15392, 'wrap': 15393, 'uncut': 15394, 'swings': 15395, 'lil': 15396, 'kame': 15397, 'surgical': 15398, 'hideous': 15399, 'reilly': 15400, 'schofield': 15401, 'pounce': 15402, '1893': 15403, 'sasaki': 15404, 'metals': 15405, 'clara': 15406, 'diego': 15407, '20s': 15408, 'swarup': 15409, 'mafias': 15410, 'hirsi': 15411, \"ali's\": 15412, 'amd': 15413, \"cia's\": 15414, 'morally': 15415, 'journaling': 15416, 'wrongly': 15417, 'kenneth': 15418, 'generates': 15419, 'warlords': 15420, '1885': 15421, 'bridget': 15422, 'sikh': 15423, 'massacres': 15424, 'monitors': 15425, 'introverts': 15426, 'marzi': 15427, 'artagnan': 15428, 'musketeers': 15429, 'dickinson': 15430, 'converting': 15431, 'sociologists': 15432, 'topical': 15433, 'sonnets': 15434, 'magnet': 15435, 'angélique': 15436, 'crouch': 15437, 'pines': 15438, 'macroeconomics': 15439, 'rekindle': 15440, 'wasted': 15441, 'consumerist': 15442, \"'clever\": 15443, 'mountaineer': 15444, 'urges': 15445, 'hitman': 15446, \"more'\": 15447, \"'written\": 15448, 'nazneen': 15449, 'erudite': 15450, 'poitier': 15451, 'perceives': 15452, 'unwitting': 15453, 'penciling': 15454, 'foreshortening': 15455, 'costume': 15456, 'submitting': 15457, 'reunites': 15458, 'techno': 15459, '1944': 15460, 'descended': 15461, 'amir': 15462, 'reviewing': 15463, 'ciri': 15464, 'milanovic': 15465, 'atkinson': 15466, 'umbrella': 15467, 'foibles': 15468, 'revelled': 15469, 'modernism': 15470, 'nissim': 15471, \"dahl's\": 15472, 'amme': 15473, 'bemused': 15474, 'suffocating': 15475, 'cliffs': 15476, 'reprinted': 15477, 'heals': 15478, 'gregor': 15479, 'eilis': 15480, 'emigrate': 15481, 'leaner': 15482, 'nomadic': 15483, 'mcdougall': 15484, 'bairam': 15485, 'scaled': 15486, 'fashionable': 15487, 'chat': 15488, 'terkel': 15489, 'maisie': 15490, 'archer’s': 15491, 'loner': 15492, 'subconscious': 15493, 'daphne': 15494, 'listing': 15495, 'unbiased': 15496, 'paradoxical': 15497, 'sab': 15498, 'fried': 15499, 'supermodel': 15500, 'containerization': 15501, 'saxena': 15502, 'kalim': 15503, 'policymakers': 15504, 'mentioning': 15505, 'briefs': 15506, 'amrita': 15507, 'radiant': 15508, 'loft': 15509, 'sorry': 15510, 'unborn': 15511, 'cézanne': 15512, 'creeping': 15513, 'payton': 15514, 'kendall': 15515, 'techies': 15516, 'mcc': 15517, 'patch': 15518, 'communism': 15519, 'bereft': 15520, 'tulip': 15521, 'kaif': 15522, 'contended': 15523, 'invigorate': 15524, 'depended': 15525, 'summarizes': 15526, 'erudition': 15527, 'francesca': 15528, 'jeevan': 15529, 'weaponry': 15530, 'enmeshed': 15531, 'prevents': 15532, 'rounder': 15533, 'counted': 15534, 'underprivileged': 15535, 'nash': 15536, 'gurgeh': 15537, 'marsh': 15538, 'dalip': 15539, 'khushwant': 15540, 'episodes': 15541, 'disguise': 15542, 'victore': 15543, 'entertainingly': 15544, 'biographers': 15545, 'fib': 15546, 'drudgery': 15547, 'editable': 15548, 'zusak': 15549, \"'chambers\": 15550, 'battered': 15551, 'wuthering': 15552, 'sand': 15553, 'sinner': 15554, 'screw': 15555, 'gopeshvara': 15556, 'kamsa': 15557, 'totalitarian': 15558, 'assumption': 15559, 'shatrujeet': 15560, 'unreliable': 15561, 'madhouse': 15562, 'fallacies': 15563, 'aakash': 15564, 'unabridged': 15565, 'adler': 15566, 'sheila': 15567, 'ethos': 15568, 'transformational': 15569, 'bayek': 15570, 'durbar': 15571, 'mesmerized': 15572, 'rhymes': 15573, 'varma': 15574, 'questioned': 15575, '1935': 15576, 'history’s': 15577, 'sourced': 15578, 'innumerable': 15579, 'prototypes': 15580, 'lately': 15581, 'devon': 15582, 'redemptive': 15583, 'youngsters': 15584, 'enrolled': 15585, 'noteworthy': 15586, 'slipcase': 15587, 'reigns': 15588, 'roffe': 15589, 'nightly': 15590, 'prompting': 15591, 'gcse': 15592, '“dan”': 15593, 'sectarian': 15594, 'bewildered': 15595, 'consultancy': 15596, 'characteristically': 15597, 'outlined': 15598, 'permission': 15599, 'salon': 15600, 'pathos': 15601, 'sindhi': 15602, 'malini': 15603, 'jonathon': 15604, 'outlets': 15605, 'ilyich': 15606, 'bicycle': 15607, 'wooden’s': 15608, 'actfl': 15609, 'interspersed': 15610, 'der': 15611, 'toriyama': 15612, 'chunks': 15613, 'tighter': 15614, 'skipping': 15615, 'fittest': 15616, 'strumming': 15617, 'onion': 15618, 'brawn': 15619, 'parr': 15620, 'tezuka’s': 15621, 'binds': 15622, 'linwood': 15623, 'barclay': 15624, 'ellora': 15625, 'crocodile': 15626, 'reptiles': 15627, 'prasoon': 15628, 'industrious': 15629, 'contestants': 15630, 'asians': 15631, 'cmi': 15632, 'tetlock': 15633, 'superforecasters': 15634, 'superforecasting': 15635, 'outperform': 15636, 'fancy': 15637, 'mitrokhin': 15638, '215': 15639, '76': 15640, 'unveiling': 15641, \"sachin's\": 15642, 'kakalios': 15643, 'nomads': 15644, 'footy': 15645, 'coincidences': 15646, 'hellboy': 15647, 'tushar': 15648, 'kubrick': 15649, 'backpacks': 15650, \"'classic\": 15651, 'kune': 15652, 'unfulfilled': 15653, 'subtleties': 15654, 'school’s': 15655, 'overtraining': 15656, 'corridor': 15657, 'dutta': 15658, 'ref': 15659, 'rick': 15660, 'extensions': 15661, 'kasab': 15662, \"'levison\": 15663, 'breathed': 15664, \"'britain's\": 15665, 'levison': 15666, 'sameer': 15667, 'ronin': 15668, 'embroiled': 15669, 'theoatmeal': 15670, 'christ': 15671, 'let’s': 15672, 'genders': 15673, \"andy's\": 15674, 'juror': 15675, 'topol': 15676, 'physicians': 15677, 'fiske': 15678, 'suburbs': 15679, '1930': 15680, 'bavaria': 15681, 'permanently': 15682, 'vijayanagara': 15683, 'sultans': 15684, 'it\\x92s': 15685, 'isolates': 15686, 'recluse': 15687, 'homespun': 15688, 'karamchand': 15689, 'imbalance': 15690, 'feb': 15691, 'weintraub': 15692, 'pustak': 15693, 'ocd': 15694, 'reddit': 15695, \"oxford's\": 15696, 'borrowed': 15697, 'gita': 15698, '1d': 15699, \"alive'\": 15700, 'sahara': 15701, \"philosopher's\": 15702, 'sergey': 15703, 'disappoint': 15704, 'didda': 15705, 'overs': 15706, 'beefy': 15707, 'cared': 15708, 'crystals': 15709, 'godslayer': 15710, 'goods': 15711, 'undertaking': 15712, 'rooster': 15713, 'sinegard': 15714, 'nikan': 15715, 'olivier': 15716, 'wodehouse’s': 15717, 'maradona': 15718, 'glen': 15719, \"potter's\": 15720, 'scrapbook': 15721, 'wand': 15722, 'irony': 15723, 'mosse': 15724, 'weepy': 15725, 'ac': 15726, 'shogun': 15727, 'foreigners': 15728, 'shilpa': 15729, 'coutinho': 15730, 'voters': 15731, 'atticus': 15732, 'machinery': 15733, 'o’donnell': 15734, 'halfway': 15735, 'modernist': 15736, 'shonen': 15737, 'kurosawa': 15738, 'tt': 15739, 'discourses': 15740, 'promoted': 15741, 'marwaris': 15742, 'medals': 15743, 'mishal': 15744, 'gingerbread': 15745, 'gavotte': 15746, 'shardlake': 15747, '‘catch': 15748, '22’': 15749, 'yossarian': 15750, 'fades': 15751, 'laurels': 15752, 'divisive': 15753, 'backman': 15754, 'communists': 15755, 'mips': 15756, 'google’s': 15757, 'abhithi': 15758, 'tuppence': 15759, 'wellness': 15760, 'lindsay': 15761, 'toilet': 15762, 'swamps': 15763, \"clancy's\": 15764, 'recon': 15765, 'brawl': 15766, 'proto': 15767, 'ulrich': 15768, 'jar': 15769, 'streetfight': 15770, 'moghul': 15771, 'ibrahimovic': 15772, 'eluded': 15773, \"'i'm\": 15774, 'outrun': 15775, 'avant': 15776, 'nostradamus': 15777, 'kinghan': 15778, 'toppled': 15779, 'distributions': 15780, 'headlong': 15781, 'zuckerberg': 15782, 'frieda': 15783, 'hazards': 15784, 'steroids': 15785, 'infected': 15786, 'traveled': 15787, 'vedika': 15788, 'scheduled': 15789, 'fixtures': 15790, 'maxim': 15791, 'leaning': 15792, 'prosody': 15793, 'oop': 15794, 'servlets': 15795, 'jdbc': 15796, \"astrid's\": 15797, 'characterized': 15798, 'chateau': 15799, 'deity': 15800, 'monkeys': 15801, 'bafta': 15802, 'rode': 15803, 'finley': 15804, 'figg': 15805, 'conferences': 15806, 'eaten': 15807, 'importantly…': 15808, 'narayanan': 15809, 'metric': 15810, 'payne': 15811, 'plateau': 15812, 'gorges': 15813, 'austen’s': 15814, 'chaturvedi': 15815, 'migrants': 15816, 'aisha': 15817, 'sugreeva': 15818, 'nidhi': 15819, 'darden': 15820, 'ponting': 15821, 'apj': 15822, 'ने': 15823, 'vivan': 15824, 'kristen': 15825, 'fabricated': 15826, 'dye': 15827, 'risking': 15828, 'leila': 15829, 'gk': 15830, 'immersion': 15831, 'lamott': 15832, 'compiler': 15833, 'picnics': 15834, 'wedge': 15835, 'szacki': 15836, 'pcs': 15837, 'deserved': 15838, \"bo's\": 15839, 'djokovic': 15840, 'rewired': 15841, 'untouchables': 15842, 'daman': 15843, 'socialising': 15844, 'runway': 15845, \"'lonely\": 15846, 'arkadin': 15847, 'karnazes': 15848, 'neelu': 15849, 'mattie': 15850, 'nambi': 15851, 'bilbao': 15852, 'kirsch’s': 15853, 'eliza’s': 15854, 'rapp': 15855, 'tully': 15856, 'hobbit': 15857, 'naikan': 15858, 'rohingyas': 15859, 'brynjolfsson': 15860, 'kakeibo': 15861, 'sarai': 15862, 'dota': 15863, 'qigong': 15864, 'tb12': 15865, 'zoran': 15866, 'giratina': 15867, 'shapiro': 15868, 'lana': 15869, 'hiit': 15870, 'vishy': 15871, 'etl': 15872, 'gre®': 15873, 'edumanga': 15874, 'guillem': 15875, 'avani': 15876, 'kasey': 15877, 'horschig': 15878, 'champawat': 15879, 'verstegen': 15880, 'mla': 15881, 'mandarin': 15882, 'caddy': 15883, 'ajahn': 15884, 'kuzneski': 15885, 'humanism': 15886, 'dutt’s': 15887, 'sahib': 15888, 'sedition': 15889, \"aristotle's\": 15890, 'elucidates': 15891, 'pockets': 15892, 'cereals': 15893, 'roster': 15894, 'marathoner': 15895, 'dreyer': 15896, 'shin': 15897, 'seems…': 15898, 'alertness': 15899, 'recharge': 15900, '“accursed': 15901, 'unblinking': 15902, 'molay': 15903, 'dynasty…': 15904, 'outright': 15905, 'reject': 15906, 'cryptocurrency': 15907, \"field'\": 15908, 'manipulator': 15909, 'spacing': 15910, 'claw': 15911, 'whips': 15912, 'knives': 15913, \"anil's\": 15914, 'tissera': 15915, 'educted': 15916, 'riventing': 15917, 'uchiha': 15918, 'itachi': 15919, 'kant': 15920, 'cease': 15921, \"'anyone\": 15922, 'cat’': 15923, 'elevates': 15924, 'painterly': 15925, \"friendship'\": 15926, 'deficiency': 15927, \"caioli's\": 15928, 'abc': 15929, 'trillion': 15930, 'traded': 15931, 'shantanu': 15932, 'bookies': 15933, \"gentleman's\": 15934, 'ascetic': 15935, 'mutilation': 15936, 'impassivity': 15937, 'predicates': 15938, 'sensibly': 15939, 'eschew': 15940, 'deer': 15941, 'vip': 15942, 'undertaker': 15943, 'suggest': 15944, 'lew': 15945, 'nolan': 15946, 'jyotish': 15947, 'occurs': 15948, 'astrologer': 15949, '320': 15950, 'tenzing': 15951, 'bureaucrat': 15952, 'roar': 15953, 'strapped': 15954, 'roadside': 15955, 'pot': 15956, 'dishevelled': 15957, 'mangan': 15958, 'gunmen': 15959, 'hastily': 15960, \"allon's\": 15961, \"'politics\": 15962, \"language'\": 15963, 'hurts': 15964, 'sunlight': 15965, 'anticipation': 15966, 'malay': 15967, 'translates': 15968, 'untranslatable': 15969, 'reid': 15970, 'nuance': 15971, 'rees': 15972, 'down’': 15973, '70s': 15974, 'parachuting': 15975, 'greenland': 15976, 'iceland': 15977, 'dongri': 15978, 'ashwin': 15979, 'zaidi': 15980, \"gaming's\": 15981, 'franchises': 15982, 'bubbles': 15983, 'excavated': 15984, 'liberties': 15985, 'johnston': 15986, 'emptiness': 15987, 'circumstance': 15988, 'euphemisms': 15989, 'menstruation': 15990, 'ghosh': 15991, 'unbearably': 15992, 'ousted': 15993, 'automating': 15994, 'nehruvian': 15995, 'excursions': 15996, 'moderate': 15997, 'milestones': 15998, 'aldous': 15999, 'thrall': 16000, 'overpopulation': 16001, 'glad': 16002, \"time's\": 16003, 'effervescent': 16004, 'winkle': 16005, 'kaga': 16006, 'warrant': 16007, 'age’': 16008, 'communicates': 16009, 'ab': 16010, 'scoops': 16011, 'delighting': 16012, 'excelled': 16013, 'youngster': 16014, 'fliers': 16015, 'dominic': 16016, 'modestly': 16017, \"layman's\": 16018, 'exponentially': 16019, 'prevailing': 16020, 'agreements': 16021, 'capsule': 16022, 'eurasian': 16023, 'nicholson': 16024, 'rotting': 16025, 'bombers': 16026, 'hms': 16027, 'maclean’s': 16028, \"sedaris's\": 16029, 'calypso': 16030, 'vexing': 16031, 'warmest': 16032, 'casualties': 16033, 'hush': 16034, \"winner'\": 16035, 'rarity': 16036, 'haul': 16037, 'calibers': 16038, 'enforcement': 16039, 'sinden': 16040, 'breather': 16041, 'wealthier': 16042, 'informing': 16043, 'muster': 16044, 'sedentary': 16045, 'messiah': 16046, \"richard's\": 16047, 'soar': 16048, 'ageless': 16049, 'midwestern': 16050, 'posting': 16051, 'rote': 16052, 'mnemonics': 16053, 'gala': 16054, 'parsi': 16055, 'gotta': 16056, 'ropes': 16057, \"alex's\": 16058, 'fathom': 16059, 'frighteningly': 16060, 'vertigo': 16061, 'lowercase': 16062, 'enthral': 16063, 'architecting': 16064, 'cloudformation': 16065, \"us'\": 16066, 'owl': 16067, 'scout': 16068, 'scrutiny': 16069, 'renewable': 16070, 'irreplaceable': 16071, 'benefactor': 16072, 'overviews': 16073, 'agrawal': 16074, 'pihu': 16075, 'blindly': 16076, 'wheeler': 16077, 'cobra': 16078, '901': 16079, '902': 16080, 'fee': 16081, 'wrestles': 16082, 'sourcebook': 16083, 'carb': 16084, 'deprivation': 16085, 'bod': 16086, 'pear': 16087, \"beckham's\": 16088, 'cram': 16089, \"girls'\": 16090, 'embarrassments': 16091, 'brainstorming': 16092, 'ration': 16093, 'proximity': 16094, 'fallacy': 16095, 'lengthen': 16096, 'flexors': 16097, 'imbalances': 16098, 'distractions': 16099, 'harassing': 16100, '1922': 16101, \"text'\": 16102, 'joan': 16103, 'pillows': 16104, 'ornate': 16105, 'elayne': 16106, \"'seek\": 16107, 'ye': 16108, 'echoing': 16109, 'symbologist': 16110, 'sienna': 16111, 'pursuers': 16112, \"dante's\": 16113, 'vinod': 16114, 'facet': 16115, 'transliteration': 16116, 'headwords': 16117, 'lakers': 16118, \"master'\": 16119, 'goaded': 16120, 'egos': 16121, 'humanistic': 16122, 'miyazaki': 16123, 'envision': 16124, 'outshine': 16125, 'hips': 16126, 'improves': 16127, 'whys': 16128, 'pronunciations': 16129, 'oup': 16130, 'mcdonough': 16131, 'cradle': 16132, 'carpets': 16133, 'accordance': 16134, '―la': 16135, \"truby's\": 16136, 'savannah': 16137, 'splitting': 16138, \"'5\": 16139, \"2018'\": 16140, '672': 16141, 'inspite': 16142, 'lawyer’s': 16143, 'attorney’s': 16144, 'situ': 16145, 'naturalists': 16146, 'liu': 16147, 'transgender': 16148, 'ccent': 16149, 'lite': 16150, 'routers': 16151, 'subnetting': 16152, '1800s': 16153, \"patterson's\": 16154, 'flashing': 16155, 'spiralling': 16156, 'testers': 16157, 'trojans': 16158, 'trickery': 16159, 'sniff': 16160, 'immunity': 16161, 'earls': 16162, 'ngos': 16163, 'openings': 16164, 'dostoyevsky': 16165, 'sigmund': 16166, 'taped': 16167, '“it': 16168, 'failings': 16169, 'dilapidated': 16170, 'townhouse': 16171, 'rubina': 16172, \"africa's\": 16173, 'desmond': 16174, 'administer': 16175, 'concentrates': 16176, 'chasm': 16177, 'impressionable': 16178, 'resumes': 16179, \"'sarah\": 16180, 'sham': 16181, 'hotter': 16182, \"stevenson's\": 16183, 'understated': 16184, 'lodge': 16185, 'ishaan': 16186, 'elated': 16187, 'iaf': 16188, 'mig': 16189, 'mana': 16190, 'fauji': 16191, 'scorn': 16192, 'anuja': 16193, 'maahi': 16194, 'rpa': 16195, 'spreadsheets': 16196, 'profusion': 16197, 'calculate': 16198, 'pong': 16199, 'dolly': 16200, 'cox': 16201, 'chairs': 16202, 'preached': 16203, 'ing': 16204, 'soulmate': 16205, 'incarnations': 16206, '1743': 16207, 'declares': 16208, 'brianna': 16209, '“with': 16210, 'slowing': 16211, '”—booklist': 16212, '1928': 16213, 'mathematically': 16214, 'stride': 16215, 'passepartout': 16216, 'stockbroker': 16217, 'hotshot': 16218, 'softcover': 16219, 'calculator': 16220, 'formulae': 16221, 'clippings': 16222, 'churches': 16223, 'colombian': 16224, 'drummond': 16225, 'buchanan': 16226, 'scotsman': 16227, 'bump': 16228, \"topical'\": 16229, \"gripping'\": 16230, \"'addictive'\": 16231, 'terrorised': 16232, 'zebra': 16233, 'takers': 16234, 'luxor': 16235, 'ramases': 16236, 'condone': 16237, 'magnificently': 16238, 'nudge': 16239, 'sloan': 16240, 'microsoft’s': 16241, 'meditations': 16242, 'principled': 16243, 'propositions': 16244, 'explosives': 16245, 'fortified': 16246, 'infantry': 16247, 'gallantry': 16248, 'galt': 16249, 'faltering': 16250, 'moab': 16251, 'washpot': 16252, 'addictively': 16253, 'expulsion': 16254, 'accustomed': 16255, 'rrb': 16256, 'legionaries': 16257, 'mccabe': 16258, 'autopsy': 16259, 'professor’s': 16260, 'moses': 16261, 'erasing': 16262, 'unaltered': 16263, 'smeared': 16264, 'quarry': 16265, 'charted': 16266, 'dynamism': 16267, 'globalisation': 16268, 'world\\x92s': 16269, 'outset': 16270, 'marketable': 16271, 'fa': 16272, 'customary': 16273, 'tolerate': 16274, 'hopkins': 16275, 'compiles': 16276, 'ser': 16277, 'aegon': 16278, 'lune': 16279, 'harp': 16280, 'jug': 16281, 'merrily': 16282, 'ashore': 16283, 'distinguishing': 16284, 'staunch': 16285, 'campaigner': 16286, 'fiend': 16287, 'spree': 16288, 'europeans': 16289, 'rackham': 16290, 'egmont': 16291, 'menial': 16292, 'tougher': 16293, 'cranky': 16294, 'boast': 16295, 'shuffle': 16296, '1135': 16297, 'tags': 16298, 'maxims': 16299, 'westernized': 16300, 'brahmins': 16301, 'estrangement': 16302, 'vocation': 16303, 'internally': 16304, 'reconcile': 16305, 'acutely': 16306, 'hatreds': 16307, 'kuper': 16308, 'lahaul': 16309, 'spiti': 16310, 'dotted': 16311, 'monasteries': 16312, 'manohar': 16313, 'obligations': 16314, 'deshpande': 16315, 'crichton’s': 16316, 'dismiss': 16317, 'pythonic': 16318, 'shallow': 16319, 'triathlons': 16320, '10k': 16321, 'seeming': 16322, 'totality': 16323, 'tao': 16324, 'adivasi': 16325, 'underlie': 16326, 'transcendence': 16327, 'succumbing': 16328, 'fossil': 16329, 'feathered': 16330, 'shuffling': 16331, 'irvine': 16332, 'emissaries': 16333, 'rebuffed': 16334, 'supplied': 16335, 'luc': 16336, 'likewise': 16337, 'gabriel’s': 16338, '‘keeps': 16339, 'out’': 16340, 'blackcliff': 16341, 'academy’s': 16342, 'beginner’s': 16343, 'applicants': 16344, 'arihant': 16345, '1500': 16346, 'recaptures': 16347, 'parody': 16348, 'indu': 16349, 'suitably': 16350, 'rajat': 16351, 'pervades': 16352, '‘if': 16353, 'person’s': 16354, 'summiting': 16355, 'screaming': 16356, 'bolster': 16357, 'barney': 16358, 'gregory': 16359, 'kazuo': 16360, 'ishiguro': 16361, 'nagasaki': 16362, 'arid': 16363, 'flooded': 16364, 'pietersen': 16365, 'miserably': 16366, 'undersized': 16367, 'racehorse': 16368, 'seabiscuit': 16369, 'resurrection': 16370, 'wrongfully': 16371, 'joffrey': 16372, '998th': 16373, 'night’s': 16374, 'jez': 16375, 'vlog': 16376, 'rejuvenation': 16377, 'dominion': 16378, 'magadha': 16379, 'chandragupta': 16380, 'invader': 16381, 'maurya': 16382, 'merger': 16383, 'phizzwhizzing': 16384, 'cruising': 16385, 'winslet': 16386, 'walliams': 16387, 'squelchy': 16388, 'pinewood': 16389, 'twit': 16390, \"joy'\": 16391, 'endearingly': 16392, 'rocketing': 16393, 'rotten': 16394, 'enhancement': 16395, 'fiori': 16396, 'diligently': 16397, 'typewriter': 16398, 'yields': 16399, 'massively': 16400, 'timelines': 16401, 'omaha': 16402, '1966': 16403, 'proposing': 16404, 'equity': 16405, 'impressions': 16406, 'को': 16407, 'गया': 16408, 'यह': 16409, 'होने': 16410, 'fusing': 16411, 'pugnacious': 16412, 'ironies': 16413, 'ancestry': 16414, 'nizam': 16415, 'indication': 16416, 'internecine': 16417, 'dartmouth': 16418, 'sanjeev': 16419, 'sanyal': 16420, 'inexpensive': 16421, 'fahrenheit': 16422, 'montag': 16423, 'commodities': 16424, 'bland': 16425, 'mildred': 16426, 'calamity': 16427, 'cobbled': 16428, 'xavier': 16429, 'tummy': 16430, 'honcho': 16431, 'punting': 16432, 'cleans': 16433, 'plate': 16434, \"west's\": 16435, 'vasili': 16436, 'kiyan': 16437, 'rectify': 16438, 'notices': 16439, 'unconditional': 16440, 'lego®': 16441, 'minifigures': 16442, '™': 16443, 'scholastic': 16444, 'horizons': 16445, 'potency': 16446, 'hungary': 16447, 'prosecuting': 16448, 'shigeru': 16449, 'gekiga': 16450, 'severely': 16451, 'ecmascript': 16452, 'herriot': 16453, 'sorted': 16454, 'siegfried': 16455, 'elegy': 16456, \"him'\": 16457, '49': 16458, 'resurgent': 16459, 'convulsions': 16460, 'locates': 16461, 'cataclysmic': 16462, 'bullied': 16463, 'catalog': 16464, 'consequence': 16465, 'basement': 16466, 'antiques': 16467, 'unapologetic': 16468, 'retreats': 16469, 'napier': 16470, 'civic': 16471, \"gotham's\": 16472, 'miniseries': 16473, 'analogies': 16474, 'adopts': 16475, 'ravenous': 16476, 'exit': 16477, 'jumps': 16478, 'conditional': 16479, \"'you're\": 16480, 'brahmi': 16481, 'wrecked': 16482, 'jayaprakash': 16483, 'pratap': 16484, 'antagonisms': 16485, 'accelerated': 16486, 'cofounders': 16487, 'barry': 16488, 'lifestyles': 16489, 'vibrantly': 16490, 'insurgencies': 16491, 'unloved': 16492, \"rao's\": 16493, 'quits': 16494, 'realising': 16495, 'bartender': 16496, 'reacts': 16497, 'authors’': 16498, 'peacefully': 16499, 'extraordinaire': 16500, 'casinos': 16501, 'participants': 16502, 'ours': 16503, 'mcnaught': 16504, 'alexandra': 16505, 'rapturous': 16506, 'rib': 16507, 'sathya': 16508, 'indices': 16509, 'sensibility': 16510, 'marwood': 16511, 'cate': 16512, 'humiliated': 16513, 'goat': 16514, 'earthy': 16515, \"genre'\": 16516, \"must'\": 16517, 'janardan': 16518, 'simultaneous': 16519, \"handmaid's\": 16520, 'births': 16521, 'fertile': 16522, 'daddy': 16523, 'groundwork': 16524, 'algorithmic': 16525, 'transportation': 16526, 'drags': 16527, 'shroff': 16528, 'inquire': 16529, 'garrison': 16530, 'kerboodle': 16531, 'delicacies': 16532, 'gatsby': 16533, 'attentions': 16534, 'rousseau': 16535, 'innovatorsis': 16536, \"isaacson's\": 16537, \"byron's\": 16538, 'vannevar': 16539, 'licklider': 16540, 'engelbart': 16541, 'noyce': 16542, 'berners': 16543, 'clarissa': 16544, 'accomplices': 16545, 'undergraduates': 16546, 'articulates': 16547, 'unmistakable': 16548, 'fireball': 16549, 'hurtling': 16550, 'carthage': 16551, 'sieges': 16552, 'syracuse': 16553, 'cujo': 16554, 'bolt': 16555, 'ina': 16556, 'axel': 16557, 'matchless': 16558, 'evolves': 16559, 'bloodily': 16560, 'monarch': 16561, 'geneva': 16562, 'clinging': 16563, 'redress': 16564, 'vaelin': 16565, 'spiff': 16566, 'madara': 16567, 'saigon': 16568, 'viet': 16569, 'extremist': 16570, 'godless': 16571, 'iggulden': 16572, 'kaz': 16573, 'brekker': 16574, 'breached': 16575, 'crackling': 16576, 'alleged': 16577, 'kimi': 16578, 'restructured': 16579, 'slides': 16580, 'cs': 16581, 'prelude': 16582, 'radar': 16583, 'maugham': 16584, 'chitra': 16585, 'transfers': 16586, 'sovereignty': 16587, 'womanhood': 16588, 'connoisseurs': 16589, 'artistes': 16590, 'gloom': 16591, 'chequered': 16592, 'smiled': 16593, 'day’s': 16594, 'yashavant': 16595, 'kanetkar': 16596, 'reprise': 16597, 'joss': 16598, 'whedon': 16599, 'feige': 16600, 'pelican': 16601, 'retires': 16602, '1910': 16603, 'escort': 16604, 'deadpool': 16605, \"lively's\": 16606, 'claudia': 16607, \"'history\": 16608, \"claudia's\": 16609, 'untrustworthy': 16610, 'savouring': 16611, \"goes'\": 16612, \"finished'\": 16613, 'lichfield': 16614, 'oleander': 16615, 'jacaranda': 16616, 'spiderweb': 16617, 'whitbread': 16618, 'cbe': 16619, 'dbe': 16620, 'aunts': 16621, 'catalogue': 16622, \"watterson's\": 16623, 'curator': 16624, '1200': 16625, 'warlike': 16626, 'toil': 16627, 'victors': 16628, 'tract': 16629, 'strived': 16630, 'upholding': 16631, 'tenets': 16632, 'dom': 16633, 'ratio': 16634, 'coauthor': 16635, 'lest': 16636, 'devil’s': 16637, 'fanfiction': 16638, 'rainbows': 16639, 'hardin’s': 16640, 'erupts': 16641, 'tessa’s': 16642, 'dividing': 16643, 'primeval': 16644, 'shukracharya': 16645, 'injustices': 16646, 'indonesia': 16647, 'west’s': 16648, 'orwellian': 16649, 'australians': 16650, 'crucially': 16651, 'rating': 16652, 'neuroscientists': 16653, 'billionaires': 16654, 'mediocrity': 16655, '25th': 16656, 'straw': 16657, 'cannibal': 16658, 'attila': 16659, '221b': 16660, 'cigarette': 16661, 'miraculously': 16662, 'centring': 16663, 'stefanie': 16664, 'tempo': 16665, 'pretext': 16666, 'reshaping': 16667, \"yahoo's\": 16668, 'blunders': 16669, 'postmodern': 16670, 'pvt': 16671, 'adaption': 16672, 'teacher’s': 16673, 'ardra': 16674, 'erroneous': 16675, 'speculated': 16676, 'gonick’s': 16677, 'gonick': 16678, 'insurgents': 16679, 'neighbouring': 16680, 'stereotype': 16681, 'iconoclastic': 16682, 'fluorescent': 16683, 'loosely': 16684, 'tunes': 16685, 'detractors': 16686, 'raga': 16687, 'piquant': 16688, '480': 16689, 'presidencies': 16690, 'trump’s': 16691, 'scraps': 16692, 'atwood': 16693, 'farthest': 16694, \"'superb\": 16695, \"literature's\": 16696, 'colossus': 16697, 'matures': 16698, 'encroach': 16699, 'satoshi': 16700, 'bombed': 16701, 'lockwood': 16702, 'reactions': 16703, 'episode': 16704, 'impossibility': 16705, 'bovary': 16706, 'stifled': 16707, 'sentimental': 16708, 'adultery': 16709, 'lifelike': 16710, 'insisted': 16711, 'chauvinistic': 16712, 'fingering': 16713, 'skeletal': 16714, '115': 16715, 'forgot': 16716, 'incan': 16717, 'unearthed': 16718, 'carriages': 16719, 'avalanches': 16720, \"lady's\": 16721, 'caliphate': 16722, 'tanker': 16723, 'convoluted': 16724, 'squeeze': 16725, 'vertebra': 16726, 'rotation': 16727, 'obliques': 16728, 'flex': 16729, 'toes': 16730, 'rounding': 16731, 'optimized': 16732, 'skimming': 16733, '60s': 16734, 'achiever': 16735, 'yale': 16736, 'oats': 16737, 'rajpal': 16738, 'eliot': 16739, 'carrot': 16740, 'strained': 16741, 'canal': 16742, 'risky': 16743, \"cricketer's\": 16744, 'mouthful': 16745, 'jeopardize': 16746, 'melanie': 16747, '2a': 16748, 'roam': 16749, 'hoge': 16750, 'ineffective': 16751, 'calmer': 16752, 'natalie': 16753, 'abundant': 16754, 'frederic': 16755, '160': 16756, 'nypd': 16757, 'erased': 16758, 'faceless': 16759, \"brunton's\": 16760, 'yogis': 16761, 'mystics': 16762, 'brigadier': 16763, 'brigade': 16764, 'rushed': 16765, 'apathy': 16766, 'ineptitude': 16767, 'classifieds': 16768, 'advert': 16769, \"job'\": 16770, \"dawn'\": 16771, \"'accomplished\": 16772, \"paranoia'\": 16773, \"tube'\": 16774, \"ending'\": 16775, 'chills': 16776, 'blackhurst': 16777, 'circulating': 16778, 'guesses': 16779, 'remarried': 16780, 'rothschild': 16781, 'vigour': 16782, 'labeled': 16783, 'squirrels': 16784, 'elegiac': 16785, 'madhurima': 16786, 'madhu': 16787, 'homosexual': 16788, 'conspired': 16789, \"dawn's\": 16790, 'daybreak': 16791, 'mauve': 16792, 'twinkles': 16793, 'dew': 16794, 'radiates': 16795, 'sunbathers': 16796, 'slender': 16797, 'caresses': 16798, 'gnarled': 16799, 'heath': 16800, \"art's\": 16801, 'genji': 16802, 'avenue': 16803, 'chronologies': 16804, '509': 16805, 'vain': 16806, 'ss': 16807, 'erich': 16808, 'inanimate': 16809, 'satrapi': 16810, 'iran’s': 16811, 'sanctioned': 16812, 'clare’s': 16813, 'clary': 16814, 'artifices': 16815, 'shadowhunter’s': 16816, 'romancing': 16817, \"days'\": 16818, 'carrie': 16819, 'psychopaths': 16820, 'essences': 16821, 'body’s': 16822, 'lockdown': 16823, 'preparations': 16824, 'hoarding': 16825, \"'action\": 16826, 'jog': 16827, 'unresolved': 16828, 'focal': 16829, '‘real': 16830, 'fierceness': 16831, 'blooming': 16832, 'etched': 16833, 'scrutinised': 16834, 'killer’s': 16835, 'english’': 16836, 'tabby': 16837, 'purr': 16838, \"'our\": 16839, 'thumbs': 16840, 'aboutthe': 16841, 'spanned': 16842, 'maximise': 16843, 'tls': 16844, \"'refreshingly\": 16845, 'benz': 16846, 'hallie': 16847, 'stiff': 16848, 'bipin': 16849, 'mutiny': 16850, 'bai': 16851, 'specialization': 16852, 'burke': 16853, 'à': 16854, 'sa': 16855, 'que': 16856, 'il': 16857, 'sur': 16858, 'outrageously': 16859, 'abandons': 16860, 'provinces': 16861, 'heartland': 16862, 'paranoid': 16863, 'fictions': 16864, 'abominable': 16865, 'baptiste': 16866, 'immorality': 16867, 'scent': 16868, \"literature'\": 16869, \"fantasy'\": 16870, \"'witty\": 16871, \"absorbing'\": 16872, 'parsing': 16873, 'markov': 16874, 'dhaka': 16875, 'satrapies': 16876, \"shadow's\": 16877, 'philipp': 16878, 'subversion': 16879, 'supra': 16880, 'rekindled': 16881, 'cosmological': 16882, \"sense'\": 16883, 'pyramids': 16884, \"'n'\": 16885, 'preceded': 16886, 'prefaces': 16887, 'juxtaposing': 16888, 'transfixed': 16889, 'brionne': 16890, 'allard': 16891, 'certificate': 16892, 'mateland': 16893, 'welch': 16894, 'ernest': 16895, 'funfair': 16896, \"fast'\": 16897, 'exhaustively': 16898, 'gillie': 16899, 'hesitate': 16900, 'humiliating': 16901, 'imitated': 16902, 'bettered': 16903, \"marlowe's\": 16904, \"chandler's\": 16905, 'brightly': 16906, 'nightclub': 16907, '1888': 16908, \"'chandler\": 16909, \"goldman's\": 16910, 'quests': 16911, 'inconceivable': 16912, 'coward': 16913, \"garfield's\": 16914, 'encrypt': 16915, 'jurgen': 16916, 'thirsty': 16917, 'tempt': 16918, 'tussle': 16919, 'heralds': 16920, 'buck': 16921, 'subjected': 16922, 'godin': 16923, 'billboard': 16924, 'figuratively': 16925, 'kawasaki': 16926, 'faiths': 16927, 'welcomes': 16928, 'chishti': 16929, 'sufism': 16930, 'elaborately': 16931, '12th': 16932, 'prabaker': 16933, \"bombay's\": 16934, 'khader': 16935, 'mujaheddin': 16936, 'guerrillas': 16937, 'shifted': 16938, 'mata': 16939, 'landslide': 16940, 'iac': 16941, 'yogendra': 16942, 'prashant': 16943, 'goings': 16944, 'dispositions': 16945, \"peterson's\": 16946, 'dresses': 16947, 'granite': 16948, 'lent': 16949, 'tempers': 16950, 'bilal': 16951, 'wretched': 16952, 'battleground': 16953, 'pacifist': 16954, 'atonement': 16955, 'demystifying': 16956, 'sharpness': 16957, 'chains': 16958, 'olap': 16959, 'kitten': 16960, 'megahit': 16961, 'newbery': 16962, 'beverly': 16963, 'curls': 16964, 'emilie': 16965, 'blacks': 16966, 'improvised': 16967, 'cruyff': 16968, 'blossomed': 16969, 'overload': 16970, 'interprets': 16971, 'pantheon': 16972, 'patterson’s': 16973, 'splashy': 16974, 'audiobook': 16975, 'authentically': 16976, 'knuckles': 16977, 'cyberwarfare': 16978, 'bon': 16979, 'cosmology': 16980, 'kondo': 16981, 'chiaki': 16982, 'galaxies': 16983, 'ashamed': 16984, 'coldness': 16985, 'crest': 16986, 'rattlesnakes': 16987, 'pilgrims': 16988, 'unstructured': 16989, 'measurements': 16990, 'mithya': 16991, 'decoded': 16992, 'saraswati': 16993, 'shakti': 16994, 'mythologist': 16995, 'emraan': 16996, 'unsparing': 16997, 'hashmi’s': 16998, 'delirium': 16999, \"blood'\": 17000, 'scandinavian': 17001, 'nordic': 17002, 'seasonal': 17003, 'vertical': 17004, 'unfailingly': 17005, 'lopez': 17006, 'solider': 17007, 'chieftains': 17008, \"season'\": 17009, \"later'\": 17010, \"reacher'\": 17011, 'ralston': 17012, 'utah': 17013, 'breashears': 17014, 'imax': 17015, 'rendezvous': 17016, 'exhilarated': 17017, '‘einstein’': 17018, 'jokester': 17019, 'neurone': 17020, \"hope'\": 17021, 'josephine': 17022, \"programmer's\": 17023, \"dream'\": 17024, 'hop': 17025, 'excerpt': 17026, 'cemented': 17027, 'macmillan': 17028, 'blended': 17029, 'enviable': 17030, 'randomness': 17031, 'waterloo': 17032, 'vaughan': 17033, 'mv': 17034, 'consortium': 17035, 'backpropagation': 17036, 'btech': 17037, 'baring': 17038, 'frailties': 17039, 'replaces': 17040, 'riddled': 17041, 'substantive': 17042, 'antecedents': 17043, 'overloading': 17044, 'setters': 17045, 'hash': 17046, '‘': 17047, '1892': 17048, 'pompous': 17049, 'series’': 17050, 'minimalism': 17051, 'minimizing': 17052, 'harshest': 17053, 'outgoing': 17054, 'kafkaís': 17055, 'mongrel': 17056, 'eccentricities': 17057, \"bryson's\": 17058, 'confuse': 17059, 'misuse': 17060, 'trivial': 17061, 'reputable': 17062, 'changeling': 17063, 'sascha': 17064, 'sensations': 17065, 'immeasurable': 17066, 'tyrant': 17067, 'entrances': 17068, 'synopsis': 17069, 'years’': 17070, 'glokta': 17071, 'torturer': 17072, 'nobleman': 17073, 'luthar': 17074, 'ninefingers': 17075, 'northmen': 17076, 'temper': 17077, 'carbon': 17078, 'taboo': 17079, 'causal': 17080, 'snatching': 17081, 'ayodhya': 17082, 'implosion': 17083, 'disintegration': 17084, 'fuhrer': 17085, 'tsar': 17086, 'occupations': 17087, 'manifestation': 17088, '“man': 17089, 'pia': 17090, 'chromosome': 17091, 'spotting': 17092, 'cloze': 17093, 'aleesha': 17094, 'brat': 17095, 'pins': 17096, 'cross’': 17097, 'extortionist': 17098, 'whizz': 17099, 'd’shonn': 17100, 'licking': 17101, 'bruising': 17102, 'supertanker': 17103, 'cakewalk': 17104, 'bull': 17105, 'snared': 17106, 'thorn': 17107, 'contented': 17108, 'thrillingly': 17109, 'insensitive': 17110, 'lobbies': 17111, 'revamped': 17112, 'bodyweight': 17113, 'weightlifting': 17114, 'aerobics': 17115, 'overwhelmingly': 17116, 'frying': 17117, 'stadiums': 17118, '“so': 17119, 'rampaging': 17120, 'prank': 17121, 'lakshmana': 17122, 'epitome': 17123, 'unfaltering': 17124, 'mathur': 17125, 'gryffindor': 17126, 'world”': 17127, 'valyria': 17128, 'outsized': 17129, 'collector’s': 17130, 'cobb': 17131, 'hardworking': 17132, 'who’d': 17133, 'garnering': 17134, 'accorded': 17135, 'regret': 17136, 'advices': 17137, 'bridgette': 17138, 'oscars': 17139, 'sabetha': 17140, 'repartee': 17141, \"brain's\": 17142, 'hemisphere': 17143, 'reproductions': 17144, 'samaithu': 17145, 'cookery': 17146, 'pandits': 17147, 'suze': 17148, 'usb': 17149, 'faraway': 17150, 'harass': 17151, 'rabindranath': 17152, \"'dazzling'\": 17153, \"'riveting'\": 17154, 'indulged': 17155, 'surfaced': 17156, 'ats': 17157, 'hemant': 17158, 'heaviest': 17159, 'poise': 17160, 'athleticism': 17161, 'buckle': 17162, 'ibrahim': 17163, \"'utterly\": 17164, 'brothels': 17165, 'stein': 17166, 'dramatizes': 17167, 'townspeople': 17168, 'televised': 17169, 'uninterested': 17170, 'innocuous': 17171, 'academically': 17172, 'butlers': 17173, 'boxers': 17174, 'superbikes': 17175, 'motorbike': 17176, 'honda': 17177, 'darlington': 17178, 'lends': 17179, 'populists': 17180, 'müller': 17181, 'planted': 17182, 'basket': 17183, \"asterix's\": 17184, 'brutus': 17185, 'thierry': 17186, 'united’s': 17187, 'striker': 17188, \"kishimoto's\": 17189, 'carpenter': 17190, 'differentiation': 17191, 'boar': 17192, 'jin': 17193, 'wollstonecraft': 17194, 'bloated': 17195, 'tanks': 17196, 'homemade': 17197, 'robert’s': 17198, 'uncontrollably': 17199, 'jardine': 17200, 'team’s': 17201, 'comparatives': 17202, 'ebooks': 17203, 'sniffing': 17204, 'debbie': 17205, 'prudent': 17206, 'tends': 17207, 'bhattacharya': 17208, 'resurrect': 17209, 'seeker': 17210, \"'bhaswati\": 17211, 'sajan': 17212, 'misra': 17213, 'recipients': 17214, 'lanes': 17215, 'pillow': 17216, 'leveraging': 17217, 'highlander': 17218, 'marooned': 17219, 'irreconcilable': 17220, 'heartsdale': 17221, 'paediatrician': 17222, 'mortem': 17223, 'tolliver': 17224, 'crucified': 17225, \"sibyl's\": 17226, 'policing': 17227, 'robbers': 17228, 'mattered': 17229, 'realistically': 17230, 'camels': 17231, 'daft': 17232, 'flashback': 17233, 'svr': 17234, 'grenade': 17235, 'antiterrorism': 17236, 'presumption': 17237, 'familiarises': 17238, 'jaico': 17239, '‘there': 17240, 'sansa': 17241, 'pint': 17242, 'cousins': 17243, 'marti': 17244, 'manuel': 17245, 'wary': 17246, 'skeletons': 17247, 'makeshift': 17248, 'rothchild': 17249, 'chandigarh': 17250, 'polytechnic': 17251, 'rubbish': 17252, 'rubble': 17253, 'igcse®': 17254, 'belleville': 17255, 'aes': 17256, 'ipsec': 17257, 'dantes': 17258, 'empowered': 17259, \"fiction's\": 17260, 'detroit': 17261, 'zavala': 17262, 'buyers': 17263, 'family…': 17264, 'tiger’s': 17265, 'vincent': 17266, 'crunching': 17267, 'dawson': 17268, 'usefulness': 17269, 'lakh': 17270, 'temptation': 17271, \"shah's\": 17272, 'westerners': 17273, 'glands': 17274, 'nourishes': 17275, 'rejuvenates': 17276, 'botched': 17277, 'harmful': 17278, 'specialty': 17279, 'clichés': 17280, 'qualify': 17281, 'we’ll': 17282, 'mediums': 17283, 'spam': 17284, 'shepherd': 17285, 'pills': 17286, 'rize': 17287, 'demonic': 17288, \"washington's\": 17289, '51': 17290, 'influencing': 17291, 'waid': 17292, 'ridley': 17293, 'chastain': 17294, 'ebbs': 17295, 'annika': 17296, 'sawyer': 17297, 'patrik': 17298, 'erica': 17299, 'larsson': 17300, 'indulekha': 17301, 'mahal': 17302, 'patriarchal': 17303, '9608': 17304, 'feminists': 17305, 'mcconaughey': 17306, 'idris': 17307, 'elba': 17308, 'mort': 17309, 'calla': 17310, 'muktesar': 17311, 'fink': 17312, 'exemplary': 17313, \"god's\": 17314, 'woosters': 17315, 'crusades': 17316, \"'secret\": 17317, 'heather': 17318, 'flops': 17319, \"best'\": 17320, 'plein': 17321, 'stunted': 17322, 'dignified': 17323, 'profitability': 17324, 'stashed': 17325, 'crypt': 17326, 'lex': 17327, 'poignantly': 17328, 'besotted': 17329, 'damodar': 17330, 'syama': 17331, 'dislike': 17332, 'insistence': 17333, 'obsessively': 17334, 'mockingbird': 17335, 'preacher': 17336, 'maxwell’s': 17337, 'acquitted': 17338, 'defended': 17339, 'berries': 17340, 'dual': 17341, 'bipolar': 17342, 'dragging': 17343, 'seizes': 17344, 'ora': 17345, 'coded': 17346, 'default': 17347, 'it\\x92ll': 17348, 'cart': 17349, 'inputs': 17350, 'reviewers': 17351, 'unison': 17352, \"palin's\": 17353, 'gethin': 17354, 'oceania': 17355, 'rounders': 17356, 'staring': 17357, 'tampering': 17358, 'alleviate': 17359, 'contention': 17360, 'siberian': 17361, 'dedicate': 17362, 'serena': 17363, 'enlist': 17364, 'hideaway': 17365, 'guatemala': 17366, 'marbled': 17367, '325': 17368, 'launching': 17369, 'reusable': 17370, 'angelfield': 17371, 'lea': 17372, \"stanislavski's\": 17373, '150th': 17374, 'revolver': 17375, 'gossips': 17376, 'earthly': 17377, \"stories'\": 17378, 'ericsson': 17379, 'distills': 17380, 'yarrow': 17381, 'splendor': 17382, 'bitterest': 17383, 'tops': 17384, 'approachable': 17385, 'apple’s': 17386, \"gardner's\": 17387, 'trailblazing': 17388, 'dodging': 17389, '”—usa': 17390, 'bawdy': 17391, \"france's\": 17392, 'boisterous': 17393, 'livelihood': 17394, 'unemployment': 17395, 'campbell': 17396, \"'beautiful\": 17397, \"entertaining'\": 17398, 'ness': 17399, 'gameshouse': 17400, 'cartographer': 17401, 'skirmishes': 17402, 'logistical': 17403, 'him—the': 17404, 'jalsaghar': 17405, 'undertakes': 17406, 'visualisation': 17407, 'teenaged': 17408, 'frontiers': 17409, 'backlash': 17410, 'guinea': 17411, \"bach's\": 17412, 'grandin': 17413, 'incomprehensible': 17414, 'autism': 17415, \"poehler's\": 17416, 'juicy': 17417, 'aunty': 17418, '8b': 17419, '9b': 17420, '83': 17421, 'scripture': 17422, 'manoj': 17423, 'cameraman': 17424, 'rewrote': 17425, 'galvanized': 17426, 'sitter': 17427, 'enjoys': 17428, 'vikal': 17429, 'smashing': 17430, 'suarez': 17431, 'temperament': 17432, 'marched': 17433, 'bantam': 17434, \"laxman's\": 17435, 'closet': 17436, 'brida': 17437, 'allison': 17438, 'leak': 17439, 'slipping': 17440, 'suffused': 17441, 'sorrows': 17442, 'vamp': 17443, 'ignited': 17444, 'commits': 17445, 'willed': 17446, 'pakistan’s': 17447, 'ipo': 17448, 'yeine': 17449, 'vb': 17450, 'assembly': 17451, 'everlasting': 17452, 'nightingale': 17453, 'pavilion': 17454, \"scott's\": 17455, 'supercars': 17456, 'showcasing': 17457, 'excursion': 17458, 'ellis': 17459, 'missy': 17460, 'ascends': 17461, 'golds': 17462, 'worships': 17463, \"serpent's\": 17464, 'impresses': 17465, 'kanyakumari': 17466, 'swims': 17467, 'untraceable': 17468, 'fide': 17469, 'pbs': 17470, \"chicago's\": 17471, 'acquaintance': 17472, 'indicates': 17473, 'jd': 17474, 'lineup': 17475, 'again…': 17476, 'ella’s': 17477, 'destiny…': 17478, \"user's\": 17479, 'accessing': 17480, 'coetzee': 17481, 'melville': 17482, 'ahab': 17483, 'lottery': 17484, \"war'\": 17485, 'dhirubhai': 17486, 'elevating': 17487, 'acres': 17488, 'rains': 17489, 'poe': 17490, 'kersten': 17491, 'allusions': 17492, 'mirza': 17493, '“do': 17494, 'rife': 17495, 'pill': 17496, 'aurelius': 17497, 'democratized': 17498, 'normality': 17499, 'identifiable': 17500, 'scorpions': 17501, 'numpy': 17502, 'sherpa': 17503, \"rama's\": 17504, 'oligarchs': 17505, 'murdoch': 17506, 'edwards': 17507, 'myelin': 17508, 'scrupulous': 17509, 'pretending': 17510, 'tidying': 17511, 'heists': 17512, 'chubby': 17513, 'watchdogs': 17514, 'reportedly': 17515, \"liar's\": 17516, 'queenie': 17517, \"'even\": 17518, 'kindly': 17519, 'teas': 17520, \"times'\": 17521, 'exert': 17522, 'stillness': 17523, 'nonsensical': 17524, 'mcdonalds': 17525, 'mapreduce': 17526, 'complements': 17527, 'dashboards': 17528, 'brokers': 17529, 'dismay': 17530, 'traynor': 17531, 'suitors': 17532, 'seducer': 17533, 'insanity': 17534, 'nandini': 17535, 'priorities': 17536, 'camel': 17537, 'hind': 17538, 'utkarsh': 17539, 'asserts': 17540, \"'nobody\": 17541, \"marx's\": 17542, 'prankster': 17543, \"europe's\": 17544, \"poseidon's\": 17545, 'quarters': 17546, 'eighties': 17547, 'frankfurt': 17548, '“that': 17549, 'hinterlands': 17550, 'awardee': 17551, 'dabbled': 17552, 'fruitlessly': 17553, 'dharmendra': 17554, 'bubbly': 17555, 'chops': 17556, \"naruto's\": 17557, 'unmask': 17558, 'regal': 17559, 'utilization': 17560, 'duly': 17561, 'conversing': 17562, 'coached': 17563, 'welcoming': 17564, 'supervision': 17565, 'mania': 17566, 'experimented': 17567, 'obama’s': 17568, 'aflame': 17569, 'preachers': 17570, \"druid's\": 17571, 'swallowed': 17572, 'saba': 17573, 'striped': 17574, 'flicker': 17575, 'fireflies': 17576, 'aglow': 17577, 'cirque': 17578, 'rêves': 17579, 'shudder': 17580, 'valor': 17581, 'adamant': 17582, 'rehab': 17583, 'wagner': 17584, 'underpinning': 17585, 'kaufmann': 17586, 'harvesting': 17587, 'scratching': 17588, 'sane': 17589, 'cognition': 17590, 'liane': 17591, 'precarious': 17592, 'validity': 17593, \"'grips\": 17594, 'tasting': 17595, 'lecture”': 17596, 'refined': 17597, 'mellon': 17598, 'rudyard': 17599, 'kipling': 17600, 'settles': 17601, 'loki': 17602, 'kingpin': 17603, 'complimentary': 17604, 'lifetimes': 17605, 'costly': 17606, 'imaginable': 17607, 'cum': 17608, 'tripwire': 17609, 'standardized': 17610, 'pauses': 17611, 'trusteeship': 17612, 'scholes': 17613, \"fan's\": 17614, 'retire': 17615, 'rediscovers': 17616, 'who’ve': 17617, 'sheikh': 17618, 'bali': 17619, 'year’': 17620, 'tirelessly': 17621, 'sustains': 17622, 'frigid': 17623, 'gobi': 17624, 'baikal': 17625, 'xanadu': 17626, 'coo': 17627, 'eamonn': 17628, 'clementine': 17629, 'ragen': 17630, 'cupboard': 17631, 'trevanian': 17632, 'nicholai': 17633, 'accomplished—and': 17634, 'paid—assassin': 17635, 'enemy—a': 17636, 'supermonolith': 17637, 'undeniable': 17638, 'evina': 17639, 'waterfall': 17640, 'conran': 17641, 'redhead': 17642, 'swirling': 17643, 'metaphysics': 17644, 'nietzsche’s': 17645, 'guesswork': 17646, 'airey': 17647, 'mirrorless': 17648, 'admires': 17649, 'wala': 17650, 'tipped': 17651, 'acne': 17652, 'sparrow': 17653, 'awkwardness': 17654, 'expat': 17655, 'lynch’s': 17656, '“as': 17657, \"'by\": 17658, 'mira': 17659, 'bakery': 17660, 'uganda': 17661, \"place'\": 17662, 'disarray': 17663, \"model's\": 17664, 'soho': 17665, 'haute': 17666, 'couture': 17667, 'savoir': 17668, 'envisioned': 17669, 'idealized': 17670, 'middlemarch': 17671, 'novel—the': 17672, 'disrupts': 17673, 'rev': 17674, 'blackest': 17675, 'armada': 17676, \"dc's\": 17677, 'transcend': 17678, 'aquaman': 17679, 'oatmeal': 17680, 'vigorously': 17681, 'irritable': 17682, 'dishing': 17683, 'ksi': 17684, 'miniminter': 17685, 'behzinga': 17686, 'zerkaa': 17687, 'vikkstar123': 17688, 'wroetoshaw': 17689, 'tobjizzle': 17690, 'clobber': 17691, 'shames': 17692, 'smelling': 17693, \"'sweet'\": 17694, 'aroma': 17695, 'jj': 17696, 'mightiest': 17697, 'tightened': 17698, 'assembling': 17699, 'andes': 17700, 'brosh’s': 17701, 'busier': 17702, 'himmel': 17703, 'fluidity': 17704, 'ayuzawa': 17705, 'takumi': 17706, 'amusement': 17707, 'frantz': 17708, 'sartre': 17709, 'algerian': 17710, 'decolonization': 17711, '1925': 17712, 'parris': 17713, 'parting': 17714, 'atheist': 17715, \"police's\": 17716, 'apologies': 17717, 'watchful': 17718, 'konrad': 17719, 'earns': 17720, 'hobson': 17721, \"'perfect\": 17722, \"'few\": 17723, 'cheated': 17724, \"'good\": 17725, \"'fans\": 17726, 'sd': 17727, 'hooks': 17728, \"one'\": 17729, 'exhibitions': 17730, 'buffer': 17731, 'auditing': 17732, 'functionalities': 17733, 'burp': 17734, 'lastly': 17735, 'disgust': 17736, 'fanciful': 17737, \"series'\": 17738, 'courtrooms': 17739, \"australia's\": 17740, 'tester': 17741, 'employer': 17742, 'cornwell’s': 17743, 'bbc2’s': 17744, 'mercia': 17745, 'possessive': 17746, 'alienated': 17747, 'bombshell': 17748, 'andré': 17749, 'grifter': 17750, 'madly': 17751, 'julian': 17752, 'firsts': 17753, 'skinny': 17754, 'hg': 17755, 'swindle': 17756, 'commercially': 17757, 'envelopes': 17758, 'castro': 17759, \"guevara's\": 17760, 'initiate': 17761, 'meandering': 17762, 'mussoorie': 17763, 'untouchable': 17764, 'snatched': 17765, 'wallet': 17766, 'ji': 17767, 'henrietta': 17768, '‘no': 17769, 'programmatically': 17770, 'roaming': 17771, 'camus': 17772, 'exerted': 17773, 'numeric': 17774, 'dataset': 17775, 'encoding': 17776, 'repressive': 17777, 'obliterated': 17778, 'chen': 17779, 'preservation': 17780, 'decadent': 17781, 'revolutionizing': 17782, 'zita': 17783, 'intergalactic': 17784, 'snedden': 17785, 'contends': 17786, 'disenchanted': 17787, 'ponies': 17788, 'fuse': 17789, 'conclusive': 17790, 'watts': 17791, 'hating': 17792, '“tells': 17793, 'story”–and': 17794, 'bobbie': 17795, 'manson’s': 17796, '“shiloh': 17797, 'dubus’s': 17798, '“shock': 17799, 'recognition”': 17800, 'art—wonderful': 17801, \"city'\": 17802, 'problematic': 17803, 'rafe': 17804, 'rodney’s': 17805, 'intimacies': 17806, 'tattered': 17807, 'contemplate': 17808, 'applaud': 17809, '“can': 17810, 'folktales': 17811, 'laure': 17812, \"'words\": 17813, \"terms'\": 17814, 'vegetables': 17815, 'biophysicists': 17816, 'sterilisation': 17817, 'uncontaminated': 17818, \"'collect\": 17819, \"study'\": 17820, 'piedmont': 17821, 'andromeda': 17822, \"crichton's\": 17823, 'knuckled': 17824, 'seamless': 17825, 'hamburger': 17826, 'stadium': 17827, 'ramshackle': 17828, 'fiance': 17829, 'cite': 17830, 'stamps': 17831, 'krishna’s': 17832, 'debroy': 17833, 'conundrum': 17834, 'anthropology': 17835, 'pellucidar': 17836, 'stranglehold': 17837, 'relives': 17838, 'sponsors': 17839, 'myron': 17840, 'lousy': 17841, 'goldie': 17842, 'sierra': 17843, \"franco's\": 17844, 'tolls': 17845, 'transferring': 17846, 'pyotr': 17847, 'conceals': 17848, 'kathmandu': 17849, 'mnemonic': 17850, 'paradigms': 17851, 'reworking': 17852, 'barrister': 17853, 'inmate': 17854, 'vibes': 17855, 'solicitor': 17856, 'liaison': 17857, 'unrequited': 17858, 'marxism': 17859, 'listings': 17860, 'learner’s': 17861, 'narrations': 17862, 'pile': 17863, 'hopefuls': 17864, 'subscription': 17865, 'spymaster': 17866, 'brainchild': 17867, 'astronaut': 17868, 'monograph': 17869, 'gabby': 17870, 'hesitates': 17871, 'tanvir': 17872, 'naya': 17873, 'chiefs': 17874, 'sherpas': 17875, 'sceptic': 17876, 'prahlad': 17877, '‘nobody': 17878, 'darren': 17879, 'maneuvers': 17880, 'exploiting': 17881, 'mamata': 17882, 'karunanidhi': 17883, 'resurgence': 17884, 'merging': 17885, 'factories': 17886, 'oslo': 17887, 'dogma': 17888, 'disrupted': 17889, 'syldavia': 17890, 'egan': 17891, \"minister's\": 17892, 'sportsperson': 17893, 'sobers': 17894, 'icc': 17895, 'gina': 17896, 'bhavan': 17897, 'joanna': 17898, 'liners': 17899, 'voice—the': 17900, 'grabbing': 17901, 'farmland': 17902, 'mesmerised': 17903, 'toddy': 17904, 'rasiq': 17905, 'daredreamers': 17906, '‘dear': 17907, 'aphrodisiac': 17908, 'gargantuan': 17909, 'sober': 17910, 'northernmost': 17911, 'queen’s': 17912, 'kaleidoscopic': 17913, 'mckean': 17914, 'adjustments': 17915, 'ceramix': 17916, 'knockout': 17917, 'banquet': 17918, 'careless': 17919, 'arundhati': 17920, 'restructure': 17921, 'animators': 17922, 'distilling': 17923, 'ethiopia': 17924, 'truce': 17925, 'boarded': 17926, 'shackles': 17927, 'mamba': 17928, 'bryant’s': 17929, 'recruits': 17930, 'outwitted': 17931, 'adharma': 17932, 'materialistic': 17933, 'passivity': 17934, 'arjunas': 17935, 'discernment': 17936, 'mosques': 17937, 'forts': 17938, \"unit's\": 17939, 'occurring': 17940, 'noses': 17941, 'rampant': 17942, 'commenting': 17943, 'tread': 17944, 'genius’': 17945, 'simplecpp': 17946, 'limitless': 17947, 'financed': 17948, 'kilometre': 17949, 'lacked': 17950, 'deprecatory': 17951, 'oxfordshire': 17952, 'poacher': 17953, \"arthur's\": 17954, 'insignificant': 17955, 'depressing': 17956, 'crowning': 17957, 'hudson': 17958, 'hampstead': 17959, 'ronnie': 17960, 'raucous': 17961, 'unions': 17962, 'toronto': 17963, '1937': 17964, 'femme': 17965, 'dripping': 17966, 'reckon': 17967, 'canals': 17968, 'anthropologists': 17969, 'golfer': 17970, 'aulay': 17971, 'jetta': 17972, 'iris': 17973, 'frost': 17974, 'smartly': 17975, 'devilishly': 17976, 'hale': 17977, 'adrienne': 17978, 'unworldly': 17979, 'menagerie': 17980, 'leaping': 17981, \"mcdonald's\": 17982, 'honouring': 17983, 'utmost': 17984, 'dervishes': 17985, 'narrators': 17986, 'impac': 17987, 'substances': 17988, 'fractions': 17989, 'utopia': 17990, 'individualistic': 17991, 'friel': 17992, 'sprint': 17993, 'simplifies': 17994, 'burdened': 17995, 'evading': 17996, 'floats': 17997, 'halo': 17998, \"golding's\": 17999, 'virginity': 18000, \"farrell's\": 18001, 'fibonacci': 18002, 'homemaker': 18003, 'homosexuality': 18004, 'jigsaw': 18005, 'breeze': 18006, 'devotes': 18007, 'doeacc': 18008, 'jew': 18009, 'swastika': 18010, 'coincides': 18011, 'correspond': 18012, 'complexes': 18013, 'displacement': 18014, \"plane's\": 18015, 'sherwood': 18016, 'romanov': 18017, \"business'\": 18018, 'tahir': 18019, 'nahri': 18020, 'sahir': 18021, 'ludhianvi': 18022, 'hogi': 18023, 'elio': 18024, 'antonsson': 18025, 'orgperhaps': 18026, 'maesters': 18027, 'septons': 18028, 'maegi': 18029, 'others’': 18030, 'whales': 18031, 'analyzed': 18032, 'processed': 18033, 'o’': 18034, 'torso': 18035, '1872': 18036, 'freedoms': 18037, 'introversion': 18038, 'bra': 18039, 'spearhead': 18040, 'kinder': 18041, 'sewers': 18042, 'clown': 18043, 'stylists': 18044, 'solstice': 18045, 'unflagging': 18046, 'obliging': 18047, 'karou': 18048, \"mckee's\": 18049, 'maharajah': 18050, 'parrot': 18051, 'wardrobe': 18052, 'skulduggery': 18053, 'repay': 18054, 'lunches': 18055, 'wagh': 18056, 'allergies': 18057, 'hassles': 18058, '3500': 18059, 'sightseeing': 18060, 'handsomely': 18061, 'prof': 18062, 'ramachandran': 18063, 'pathbreaking': 18064, 'rae': 18065, 'aerial': 18066, 'inherently': 18067, 'curie': 18068, 'diplomats': 18069, 'liner': 18070, \"'clive\": 18071, 'aniston': 18072, 'leukemia': 18073, 'warwick': 18074, '604': 18075, 'staged': 18076, 'geet': 18077, 'anjali': 18078, 'teresa': 18079, '105': 18080, 'breckenridge': 18081, 'interpersonal': 18082, 'regaining': 18083, 'deprivations': 18084, 'reconstruction': 18085, 'decisively': 18086, 'inference': 18087, 'analytically': 18088, 'pawar': 18089, 'overlook': 18090, 'fortitude': 18091, \"goku's\": 18092, 'gigantic': 18093, 'healthiest': 18094, 'gis': 18095, 'broadening': 18096, 'inhuman': 18097, 'disabilities': 18098, 'hung': 18099, 'raghuram': 18100, 'pranab': 18101, 'kittur': 18102, 'diwan': 18103, 'brook': 18104, 'activate': 18105, 'substitution': 18106, 'reminding': 18107, 'ifemelu': 18108, 'obinze': 18109, 'globalized': 18110, 'felines': 18111, 'odie': 18112, 'engages': 18113, 'centurions': 18114, '1960': 18115, 'islington': 18116, 'embellish': 18117, 'tenderly': 18118, 'rajendra': 18119, 'debut’': 18120, 'compel': 18121, 'satanic': 18122, 'salman': 18123, 'judaism': 18124, 'cholas': 18125, 'stoicism': 18126, 'seneca': 18127, 'lincoln’s': 18128, 'filing': 18129, 'dslr': 18130, 'abstractions': 18131, 'knocked': 18132, 'aggressively': 18133, 'dawkins': 18134, 'ekonomikrisis': 18135, 'phoenician': 18136, 'togetherness': 18137, 'iteration': 18138, 'webley': 18139, 'bloodthirsty': 18140, 'stained': 18141, 'plush': 18142, 'emily’s': 18143, 'woke': 18144, 'pieced': 18145, 'preferences': 18146, 'cursive': 18147, 'it’': 18148, 'revathi': 18149, 'marvelously': 18150, 'softly': 18151, 'betrothed': 18152, \"atreus's\": 18153, 'midgard': 18154, 'vacuum': 18155, 'osama': 18156, 'speedcubing': 18157, 'quintillion': 18158, 'outfit': 18159, 'riga': 18160, 'smorgasbord': 18161, 'navajo': 18162, 'concentrate': 18163, 'athletics': 18164, 'usain': 18165, \"'everything\": 18166, 'damn': 18167, 'borrows': 18168, \"jeeves's\": 18169, 'ports': 18170, \"mad's\": 18171, 'mockery': 18172, 'parables': 18173, '—from': 18174, 'legerdemain': 18175, 'acrobatic': 18176, 'rod': 18177, 'storyboarding': 18178, 'forcefully': 18179, 'zest': 18180, 'perceive': 18181, 'kata': 18182, 'kashi': 18183, 'aryan': 18184, 'lennie': 18185, 'tucked': 18186, 'comforts': 18187, 'prentice': 18188, 'smythe': 18189, 'nal': 18190, 'reefs': 18191, 'monolithic': 18192, 'dependency': 18193, 'mitigate': 18194, 'ascham': 18195, 'disfigured': 18196, 'elisabeth': 18197, 'orion': 18198, 'sordid': 18199, \"'highly\": 18200, 'emi': 18201, 'nassim': 18202, 'fooled': 18203, 'affinity': 18204, 'saturated': 18205, 'dreamworks': 18206, 'nickelodeon': 18207, 'despised': 18208, 'patiala': 18209, 'cohesion': 18210, 'georgette': 18211, 'heyer': 18212, 'griffin': 18213, 'apparatus': 18214, 'speedy': 18215, '62': 18216, 'irving': 18217, 'ani': 18218, 'veneer': 18219, 'dictate': 18220, 'spiteful': 18221, 'fictitious': 18222, 'suicides': 18223, 'incompatible': 18224, \"woman'\": 18225, 'faux': 18226, 'astrophysics': 18227, 'formally': 18228, 'zack': 18229, \"mackendrick's\": 18230, 'unavoidable': 18231, 'intercontinental': 18232, 'astuteness': 18233, 'catalonia': 18234, 'sighting': 18235, 'dens': 18236, 'taunting': 18237, 'jaime': 18238, 'tendulkar’s': 18239, 'odis': 18240, 'predicament': 18241, 'bachelors': 18242, 'disapproving': 18243, 'helplessly': 18244, \"grandfather's\": 18245, 'cornwall': 18246, 'attic': 18247, 'pronouncements': 18248, \"subramanian's\": 18249, 'screwtape': 18250, 'permitted': 18251, 'sari': 18252, 'rumour': 18253, 'broadly': 18254, 'underlies': 18255, 'reduction': 18256, 'deductive': 18257, 'crispin': 18258, 'ghastly': 18259, 'manabu': 18260, \"lincoln's\": 18261, 'representatives': 18262, 'weirder': 18263, \"'superbly\": 18264, 'grady': 18265, 'bombarded': 18266, 'pipe': 18267, 'replied': 18268, 'parva': 18269, 'precede': 18270, 'alibi': 18271, 'sneaky': 18272, 'teasing': 18273, 'aaron': 18274, \"atmospheric'\": 18275, '4b': 18276, '“required': 18277, 'newsday': 18278, 'pivots': 18279, 'plunder': 18280, '”—michiko': 18281, '”—entertainment': 18282, 'bauman': 18283, \"professor's\": 18284, 'embattled': 18285, 'eww': 18286, 'commodity': 18287, 'technician': 18288, 'somewhat': 18289, 'diversities': 18290, 'hiragana': 18291, 'katakana': 18292, 'officer’s': 18293, '‘why': 18294, 'vivacious': 18295, 'trainee': 18296, 'lowly': 18297, 'voyages': 18298, 'thriller’': 18299, '‘high': 18300, 'tolerating': 18301, 'unchallenged': 18302, 'chikankari': 18303, 'andy’s': 18304, 'stumped': 18305, 'highways': 18306, 'simplification': 18307, 'subsystem': 18308, 'olds': 18309, 'chaol': 18310, \"continent's\": 18311, 'drool': 18312, \"night's\": 18313, 'honors': 18314, 'birmingham': 18315, 'chronologically': 18316, 'assesses': 18317, 'santino': 18318, 'freddie': 18319, 'chiwalking': 18320, 'tutelage': 18321, 'beans': 18322, 'tolstoyan': 18323, 'learnings': 18324, 'cain’s': 18325, 'modernisation': 18326, 'glide': 18327, 'kidman': 18328, \"scary'\": 18329, 'fabumouse': 18330, 'hofmekler': 18331, 'hormones': 18332, 'reasoned': 18333, 'nouvelle': 18334, 'internationalization': 18335, 'fundamentalists': 18336, 'byomkesh’s': 18337, 'horns': 18338, 'beard': 18339, 'combustion': 18340, 'congratulations': 18341, 'categorized': 18342, 'shameful': 18343, 'subtitled': 18344, 'pad': 18345, 'convictions': 18346, 'junagadh': 18347, 'firebrand': 18348, 'ziauddin': 18349, 'connector': 18350, 'venkatesh': 18351, 'harmonious': 18352, 'chips': 18353, 'peripheral': 18354, 'constructor': 18355, 'decorator': 18356, 'arose': 18357, \"'at\": 18358, 'engrossed': 18359, 'husain': 18360, 'cornish': 18361, 'renegade': 18362, 'lopes': 18363, 'holmes’': 18364, 'archenemy': 18365, 'flavors': 18366, 'roast': 18367, 'pao': 18368, 'konkani': 18369, 'weigh': 18370, 'hardscrabble': 18371, 'ferraro': 18372, 'fraudster': 18373, 'geeks': 18374, 'grossly': 18375, 'milly': 18376, 'oils': 18377, 'fixes': 18378, 'rocked': 18379, 'shourie': 18380, 'pause': 18381, 'iceberg': 18382, 'ava': 18383, 'plastics': 18384, 'bosnia': 18385, 'prioritizing': 18386, 'burqa': 18387, 'mariam': 18388, 'islamophobia': 18389, 'hypnosis': 18390, 'recalling': 18391, 'eroded': 18392, 'overdose': 18393, 'roarke': 18394, 'shrinking': 18395, 'beckons': 18396, 'unprotected': 18397, 'richardson': 18398, 'epistolary': 18399, 'gordy': 18400, 'larynx': 18401, 'manageable': 18402, 'usable': 18403, 'murals': 18404, 'badge': 18405, 'scathing': 18406, 'piety': 18407, 'mei': 18408, 'olympia': 18409, 'schwarzenegger': 18410, 'aerobic': 18411, 'benign': 18412, 'consequent': 18413, 'jared': 18414, 'sarcastic': 18415, 'alphago': 18416, 'papillon': 18417, 'penal': 18418, 'conspire': 18419, 'behest': 18420, 'announcement': 18421, 'walton': 18422, 'stately': 18423, 'anahita': 18424, 'debutante': 18425, 'publicity': 18426, 'unquestionable': 18427, 'honorable': 18428, 'miso': 18429, 'surfing': 18430, 'dislikes': 18431, 'como': 18432, \"ben's\": 18433, 'gunned': 18434, 'dil': 18435, 'ar': 18436, 'prom': 18437, 'biffen': 18438, 'thirtieth': 18439, 'mammoth': 18440, 'couches': 18441, 'melding': 18442, 'sunita': 18443, 'narain': 18444, 'int': 18445, 'aarushi': 18446, 'ncr': 18447, 'hemraj': 18448, 'figurative': 18449, 'scriptures': 18450, 'turvy': 18451, 'kahneman': 18452, 'kapilavastu': 18453, 'leesha': 18454, 'ripe': 18455, \"lord's\": 18456, 'stump': 18457, 'leica': 18458, 'nrn': 18459, 'surgeons': 18460, 'pumping': 18461, 'battery': 18462, 'reproduction': 18463, 'dissect': 18464, 'degrading': 18465, 'scavenging': 18466, 'brooms': 18467, 'breathlessly': 18468, 'colorists': 18469, 'panacea': 18470, 'tragicomix': 18471, 'vercingetorix': 18472, 'seclusion': 18473, 'cam': 18474, 'ipv4': 18475, 'intensify': 18476, 'minh': 18477, 'esme': 18478, 'seducing': 18479, 'overheard': 18480, 'hoffmann': 18481, 'nicaragua': 18482, 'crick': 18483, 'madison': 18484, 'intelligible': 18485, 'shivaji': 18486, \"titan's\": 18487, 'amplify': 18488, 'infuse': 18489, 'positioning': 18490, '122': 18491, 'maelstrom': 18492, 'wintry': 18493, 'blueprints': 18494, 'javier': 18495, 'chairperson': 18496, 'lcd': 18497, 'estabalize': 18498, 'gandhian': 18499, 'mandate': 18500, 'popularmmos': 18501, 'khan’s': 18502, 'disability': 18503, 'ca': 18504, 'monty': 18505, 'attackers': 18506, 'technically': 18507, 'fan’s': 18508, 'preconceived': 18509, 'garry': 18510, 'terrors': 18511, '1953': 18512, 'arlen’s': 18513, 'aahana': 18514, 'jocko': 18515, 'willink': 18516, 'applicability': 18517, 'gulliver': 18518, 'reinforcing': 18519, 'consciously': 18520, 'lars': 18521, 'kepler’s': 18522, 'proposition': 18523, 'storey': 18524, 'dtp': 18525, 'determiners': 18526, 'finites': 18527, 'whatsapp': 18528, 'quitting': 18529, 'tensorboard': 18530, 'gans': 18531, 'digit': 18532, 'pac': 18533, 'rnns': 18534, 'giggle': 18535, 'humming': 18536, 'publishes': 18537, 'retailers': 18538, 'lobby': 18539, 'naysayers': 18540, 'conditionals': 18541, 'git': 18542, 'liking': 18543, 'grapevine': 18544, 'ohh': 18545, 'ip': 18546, 'unblock': 18547, 'vikas': 18548, 'excels': 18549, 'organising': 18550, 'leather': 18551, 'opportunist': 18552, 'thelma': 18553, 'terminator': 18554, 'tally': 18555, 'cielis': 18556, 'dwindling': 18557, 'dedicates': 18558, 'apostrophes': 18559, 'commas': 18560, 'dashes': 18561, 'motorcycles': 18562, 'amoral': 18563, 'astronauts': 18564, 'kishan': 18565, 'implies': 18566, 'fruition': 18567, 'estates': 18568, 'disregard': 18569, 'gamblers': 18570, 'tenzin': 18571, 'pedestrians': 18572, 'boosted': 18573, 'dedicating': 18574, 'disc': 18575, 'guesstimation': 18576, 'conveniently': 18577, 'anonymously': 18578, 'antoinette': 18579, 'thapar': 18580, '‘as': 18581, 'inject': 18582, 'traverse': 18583, 'lan': 18584, 'wan': 18585, 'conclude': 18586, 'deployments': 18587, 'complains': 18588, 'mortgage': 18589, 'overlap': 18590, 'coop': 18591, 'singled': 18592, 'tenshinhan': 18593, 'gawande’s': 18594, 'closure': 18595, \"note's\": 18596, 'sinbad': 18597, 'intervenes': 18598, 'diaspora': 18599, 'intolerance': 18600, 'sidelined': 18601, 'faction': 18602, 'suns’': 18603, 'kabul': 18604, 'prescott': 18605, 'demolition': 18606, 'travails': 18607, 'infidel': 18608, 'mvp': 18609, 'fibers': 18610, 'fiber': 18611, 'yarns': 18612, 'suggestion': 18613, 'aquilla': 18614, 'serra': 18615, \"dick's\": 18616, 'kindred': 18617, 'androids': 18618, 'contemplative': 18619, 'intimidated': 18620, 'risked': 18621, 'mildly': 18622, 'pigments': 18623, 'indispensible': 18624, 'ensemble': 18625, 'psychiatry': 18626, 'phobias': 18627, 'phobic': 18628, 'pops': 18629, \"room'\": 18630, \"solomon's\": 18631, 'darcy': 18632, 'yukon': 18633, 'nayar': 18634, 'rum': 18635, 'coastguard': 18636, 'hawks': 18637, 'becall': 18638, 'grading': 18639, 'colon': 18640, 'musketeer': 18641, 'announces': 18642, 'women’s': 18643, 'cromwell': 18644, 'pico': 18645, 'mundo': 18646, 'betterment': 18647, 'breathes': 18648, 'cecelia': 18649, 'basil': 18650, 'descents': 18651, 'shops': 18652, 'traveler': 18653, 'latham': 18654, 'dessen': 18655, \"jason's\": 18656, 'ridiculed': 18657, 'duryodhana': 18658, 'dumas': 18659, 'inconvenient': 18660, \"cosmopolitan's\": 18661, 'valuing': 18662, \"'for\": 18663, 'lena': 18664, '10s': 18665, 'macho': 18666, 'cosmologist': 18667, 'extravagant': 18668, 'confessed': 18669, 'spouse': 18670, '1880': 18671, 'skyscrapers': 18672, 'hufflepuff': 18673, 'sardesai': 18674, 'sourdough': 18675, 'bake': 18676, \"farmer's\": 18677, \"'offers\": 18678, 'inserting': 18679, \"true'\": 18680, 'bangladeshi': 18681, 'buttons': 18682, 'sutra': 18683, 'typing': 18684, 'covey': 18685, 'proactive': 18686, 'dishwasher': 18687, 'cuddle': 18688, 'rumble': 18689, 'bowls': 18690, 'butter': 18691, 'belonging': 18692, 'undergone': 18693, 'matlab': 18694, 'whisper': 18695, 'splash': 18696, 'lining': 18697, 'mare': 18698, 'petite': 18699, 'tanenbaum': 18700, 'patriarch': 18701, 'ariza': 18702, 'melancholy': 18703, 'beef': 18704, 'governess': 18705, 'pretzels': 18706, 'bruno': 18707, 'tilt': 18708, 'monsoon': 18709, 'ezekiel': 18710, 'echoed': 18711, 'hypnotic': 18712, 'charter': 18713, 'danes': 18714, 'capita': 18715, 'norway': 18716, 'booth': 18717, 'perplexed': 18718, 'unquestioning': 18719, 'bacteria': 18720, 'anonymity': 18721, 'riddler': 18722, 'wednesday': 18723, 'iain': 18724, 'duchlan': 18725, 'dundas': 18726, 'charitable': 18727, 'pervade': 18728, 'superstitious': 18729, 'merriment': 18730, 'bodyguard': 18731, 'clashing': 18732, 'snowman': 18733, 'exuberance': 18734, 'destructors': 18735, 'doubters': 18736, 'fishkin': 18737, 'mohammad': 18738, 'tease': 18739, 'discerning': 18740, 'vonnegut’s': 18741, 'showman': 18742, 'packing': 18743, 'reassuring': 18744, 'bristol': 18745, 'counterfeit': 18746, 'fakir': 18747, 'surrendering': 18748, 'sita’s': 18749, 'symbolized': 18750, 'nutrients': 18751, \"kinetics'\": 18752, 'disbelief': 18753, 'oops': 18754, \"grisham's\": 18755, 'williamson': 18756, 'craftsmen': 18757, 'untiring': 18758, 'idealist': 18759, 'illiberal': 18760, 'underwhelming': 18761, 'stitched': 18762, 'debris': 18763, 'basford': 18764, 'tomie': 18765, 'defunct': 18766, 'beens': 18767, 'wannabe': 18768, 'spill': 18769, 'voiced': 18770, \"barr's\": 18771, 'tamanna': 18772, '904': 18773, 'provider': 18774, \"energy'\": 18775, \"'relentlessly\": 18776, \"wondering'\": 18777, \"'set\": 18778, 'pentagram': 18779, 'kahn': 18780, 'ambivalence': 18781, 'handbag': 18782, 'rené': 18783, 'beam': 18784, 'cogent': 18785, 'brittle': 18786, 'titu': 18787, 'gibson': 18788, 'techie': 18789, 'korra': 18790, 'zuko': 18791, \"9's\": 18792, 'donovan': 18793, 'galloway': 18794, 'sovereign': 18795, 'spawn': 18796, 'hub': 18797, 'puberty': 18798, 'menopause': 18799, 'thyroid': 18800, 'pcos': 18801, 'stress—women': 18802, 'yoga’s': 18803, 'sonakshi': 18804, 'mukerji': 18805, 'fernandes': 18806, 'tusshar': 18807, 'zoa': 18808, 'morani': 18809, 'manish': 18810, 'sridevi': 18811, 'hiring': 18812, 'clad': 18813, 'telgemeier': 18814, 'arabia': 18815, 'acknowledges': 18816, 'watershed': 18817, 'hans': 18818, 'torrents': 18819, 'redistribute': 18820, 'ornithologist': 18821, 'static': 18822, 'personalize': 18823, 'ebert': 18824, 'entertainments': 18825, 'unrivaled': 18826, 'metropolis': 18827, 'cheer': 18828, 'paves': 18829, \"evans'\": 18830, 'shelves': 18831, 'rust’s': 18832, 'aang': 18833, 'luen': 18834, 'wildfell': 18835, 'mick': 18836, 'fingerboard': 18837, 'jemima': 18838, 'javed': 18839, 'miandad': 18840, 'richards': 18841, 'shenoy': 18842, 'keenly': 18843, 'clings': 18844, 'naxalite': 18845, 'annexed': 18846, 'evade': 18847, 'stories’': 18848, 'dr1': 18849, 'bcci': 18850, 'believers': 18851, 'lord’s': 18852, 'encyclopaedic': 18853, 'unfriendly': 18854, 'matias': 18855, 'crave': 18856, 'rebuilding': 18857, 'underlined': 18858, 'rita': 18859, 'almanack': 18860, 'annually': 18861, 'dyes': 18862, 'dayal': 18863, 'ibooks': 18864, 'calorie': 18865, 'boggling': 18866, '47': 18867, 'ultrarunner': 18868, 'mysteries®': 18869, 'gladiator': 18870, 'descender': 18871, 'pits': 18872, 'blindness': 18873, 'exchanged': 18874, 'vicky': 18875, 'worthwhile': 18876, 'moll': 18877, \"krishna's\": 18878, 'despotic': 18879, 'chetan': 18880, 'devouring': 18881, 'bobcats': 18882, \"mann's\": 18883, 'crusaders': 18884, 'coldly': 18885, 'kashish': 18886, 'resolute': 18887, 'habibi': 18888, 'dodola': 18889, 'zam': 18890, 'prostitute': 18891, 'foretells': 18892, 'silicate': 18893, 'calligrapher': 18894, 'submarines': 18895, 'frankl': 18896, 'prosecuted': 18897, 'slams': 18898, 'avenged': 18899, '74': 18900, 'powerpivot': 18901, 'spinning': 18902, 'prompt': 18903, 'pinyin': 18904, 'hanzi': 18905, 'nagging': 18906, 'deterrence': 18907, 'tsavo': 18908, \"lawrence's\": 18909, 'orleans': 18910, 'navin': 18911, 'affectionately': 18912, 'diabetes': 18913, 'einstein’s': 18914, 'kher': 18915, 'spoon': 18916, 'devine': 18917, 'boyle': 18918, 'giry': 18919, 'industrialists': 18920, \"puri's\": 18921, 'barking': 18922, 'variant': 18923, 'ceremonies': 18924, 'frida': 18925, 'kahlo': 18926, 'itv': 18927, \"storytelling'\": 18928, 'presently': 18929, 'perry': 18930, 'labyrinths': 18931, 'irene': 18932, 'mardy': 18933, 'grothe': 18934, \"'she\": 18935, 'friction': 18936, 'stephens': 18937, 'whimsy': 18938, 'asynchronous': 18939, 'rxjava': 18940, '220': 18941, 'rites': 18942, 'expelled': 18943, 'allegiances': 18944, 'powell': 18945, 'huck': 18946, 'unmissable': 18947, 'lasted': 18948, 'sounding': 18949, 'substantially': 18950, 'pleasers': 18951, 'staging': 18952, 'snapshots': 18953, 'ant': 18954, 'kenway': 18955, 'boardman': 18956, 'unclimbed': 18957, \"tasker's\": 18958, 'farrarons': 18959, 'meditate': 18960, 'feyre': 18961, 'dearest': 18962, 'achingly': 18963, 'coffin': 18964, 'cinemas': 18965, 'prix': 18966, 'morse': 18967, 'aristocrats': 18968, 'janata': 18969, 'prevailed': 18970, 'samaj': 18971, 'asserting': 18972, 'measuring': 18973, 'trish': 18974, 'overweight': 18975, 'deploys': 18976, 'pleasurable': 18977, 'flashbacks': 18978, 'shelton': 18979, 'ukridge': 18980, 'vai': 18981, 'risalo': 18982, 'meher': 18983, 'dastardly': 18984, 'wilkinson': 18985, 'glow': 18986, 'hema': 18987, 'echelons': 18988, '1906': 18989, 'prototype': 18990, 'venues': 18991, 'baz': 18992, \"tolstoy's\": 18993, 'performs': 18994, 'sympathy': 18995, 'fronts': 18996, 'xhosa': 18997, 'indiscretion': 18998, 'fervently': 18999, \"'drawing\": 19000, \"vladek's\": 19001, 'grisliest': 19002, 'pawprints': 19003, \"survivor's\": 19004, 'metamaus': 19005, \"in'\": 19006, 'perrin': 19007, 'wet': 19008, \"bose's\": 19009, 'refresher': 19010, 'reimagining': 19011, 'shirdi': 19012, \"language's\": 19013, 'thames': 19014, '1927': 19015, 'shibani': 19016, 'maven': 19017, 'chbosky': 19018, 'abhishek': 19019, 'jogging': 19020, 'ranbir': 19021, 'deepika': 19022, 'padukone': 19023, 'consolidating': 19024, 'interlinked': 19025, 'buoyant': 19026, 'viceroy': 19027, 'volunteered': 19028, 'upended': 19029, 'inserts': 19030, 'fulfils': 19031, 'brutalised': 19032, 'quebec': 19033, 'for…': 19034, 'balder': 19035, 'snowbound': 19036, 'dumbledore': 19037, 'mi6': 19038, 'cartels': 19039, 'respects': 19040, 'hazel': 19041, 'inscribed': 19042, 'resemblance': 19043, 'yugoslavia': 19044, 'tabla': 19045, 'ustad': 19046, 'creatives': 19047, 'marshmallow': 19048, 'crossfit': 19049, '\\x93the': 19050, 'senate': 19051, 'mccandless': 19052, 'comers': 19053, 'eco': 19054, 'foodie': 19055, \"teachers'\": 19056, 'jade': 19057, 'cowboy': 19058, 'lonnie': 19059, 'nashik': 19060, 'michel': 19061, 'pune': 19062, 'kunal': 19063, 'circuits': 19064, 'anointed': 19065, 'graeber': 19066, 'transient': 19067, 'manoeuvre': 19068, 'hosiery': 19069, \"john's\": 19070, 'genome': 19071, \"hellboy's\": 19072, 'proverbial': 19073, 'avert': 19074, \"director's\": 19075, \"kubrick's\": 19076, 'jeet': 19077, 'angst': 19078, 'contrarian': 19079, 'bloch': 19080, 'io': 19081, 'centenary': 19082, 'metabolism': 19083, '764': 19084, 'raids': 19085, 'franz': 19086, 'craved': 19087, 'ur': 19088, 'fixer': 19089, 'balkan': 19090, 'behaviors': 19091, 'decoder': 19092, 'rude': 19093, 'compensate': 19094, 'clarice': 19095, 'simplest': 19096, 'unhelpful': 19097, \"medicine's\": 19098, 'gutenberg': 19099, 'bonatti': 19100, 'hermann': 19101, 'missteps': 19102, 'republican': 19103, \"birla's\": 19104, 'fans’': 19105, 'flavours': 19106, 'tamarind': 19107, 'raged': 19108, 'outpouring': 19109, 'ramakrishna': 19110, 'psychobiological': 19111, 'farida': 19112, 'namrata': 19113, 'arlott': 19114, 'pious': 19115, 'dough': 19116, 'olive': 19117, 'topping': 19118, 'nola': 19119, 'complementary': 19120, 'craziness': 19121, 'lunacy': 19122, 'cgl': 19123, 'bba': 19124, 'himself…': 19125, 'choke': 19126, 'fraternity': 19127, 'anupama': 19128, 'woolf': 19129, 'kanshiram': 19130, 'monroe': 19131, 'oft': 19132, 'engineered': 19133, 'astounded': 19134, \"mao's\": 19135, 'delegation': 19136, 'you′ll': 19137, 'nashville': 19138, 'oven': 19139, 'paarth': 19140, 'kee': 19141, 'gaya': 19142, 'firstpost': 19143, 'ilse': 19144, 'roughly': 19145, 'sphinx': 19146, 'stethoscope': 19147, 'dipped': 19148, 'lacking': 19149, 'ordfront': 19150, 'marquez': 19151, 'misgovernance': 19152, 'voiceless': 19153, \"fbi's\": 19154, 'civilizational': 19155, 'zoology': 19156, 'taxonomic': 19157, 'interrogation': 19158, 'debriefing': 19159, 'workable': 19160, 'treatments': 19161, 'testify': 19162, 'playlist': 19163, 'treats': 19164, 'brajesh': 19165, 'fellows': 19166, 'manjul': 19167, 'patterning': 19168, 'spooky': 19169, 'zayn': 19170, 'fatherhood': 19171, 'grammy': 19172, \"'perhaps\": 19173, 'dramatis': 19174, 'personae': 19175, \"witty'\": 19176, 'princesses': 19177, 'candide': 19178, 'brin': 19179, 'clarifying': 19180, 'magnetism': 19181, 'omnipotent': 19182, 'unwilling': 19183, 'offence': 19184, 'seated': 19185, 'abir': 19186, 'sansom': 19187, 'raj’': 19188, '1800': 19189, 'underpin': 19190, 'bronnie': 19191, 'shelters': 19192, 'asides': 19193, 'sophy': 19194, 'berkeley': 19195, 'chokshi': 19196, 'servitude': 19197, 'whereabouts': 19198, 'chung': 19199, 'unites': 19200, 'ranchi': 19201, 'do”': 19202, \"that'll\": 19203, 'century’': 19204, 'sunlit': 19205, 'faulks': 19206, 'gentleman’s': 19207, 'baxter': 19208, 'adoring': 19209, 'decrease': 19210, 'diagon': 19211, \"harry's\": 19212, 'accession': 19213, \"mole's\": 19214, 'broadway': 19215, 'flamingo': 19216, 'tumult': 19217, 'beartown': 19218, '1916': 19219, 'fatah': 19220, 'untrue': 19221, 'infidelity': 19222, 'tagore’s': 19223, 'mains': 19224, 'intentionally': 19225, 'bhopal': 19226, 'shetty': 19227, 'busts': 19228, 'yunus': 19229, 'chivalry': 19230, 'winchester': 19231, 'slight': 19232, 'enric': 19233, 'badia': 19234, 'romero': 19235, 'superstitions': 19236, 'findlay': 19237, 'courtenay': 19238, 'wilton': 19239, 'dawsey': 19240, 'utopian': 19241, 'https': 19242, 'taiju': 19243, 'restart': 19244, 'staked': 19245, '380': 19246, \"d'or\": 19247, \"ronaldo's\": 19248, 'foucault': 19249, 'overturned': 19250, 'shihan': 19251, 'aimee': 19252, 'barak': 19253, 'periodization': 19254, 'mowgli': 19255, 'shere': 19256, 'norah': 19257, 'darling': 19258, 'diva': 19259, 'lister': 19260, 'shinichi': 19261, 'logging': 19262, 'askew': 19263, 'decadence': 19264, 'doubtful': 19265, 'backdrops': 19266, 'uttermost': 19267, 'unsuccessful': 19268, 'prayed': 19269, 'chaincode': 19270, 'hallucinatory': 19271, 'grover’s': 19272, 'solvers': 19273, 'utilise': 19274, 'sauptik': 19275, 'gravett': 19276, 'electromagnetic': 19277, 'renderings': 19278, 'winged': 19279, 'matrices': 19280, 'bonaparte': 19281, 'rowling’s': 19282, 'expansions': 19283, 'simulators': 19284, 'compilers': 19285, 'abhi': 19286, 'stabbing': 19287, 'chocolates': 19288, 'pandora': 19289, 'hansaji': 19290, 'dane': 19291, 'salvo': 19292, 'logged': 19293, 'maa': 19294, 'vodka': 19295, 'empathetic': 19296, 'hosting': 19297, 'mccarter': 19298, 'reagan': 19299, 'stalwart': 19300, 'noodle': 19301, 'supermarketwala': 19302, 'appreciates': 19303, 'biter': 19304, 'cbi': 19305, 'seats': 19306, 'jrd’s': 19307, 'vizbig': 19308, 'deadlier': 19309, 'rosa': 19310, 'unreasonable': 19311, 'regeneration': 19312, 'grownup': 19313, 'fitzherberts': 19314, 'maud': 19315, 'fitzherbert': 19316, 'conscription': 19317, 'bedrooms': 19318, 'wiring': 19319, 'sadik': 19320, '1206': 19321, 'rajput': 19322, 'gamer’s': 19323, 'overwatch': 19324, 'fortnite': 19325, 'garrett': 19326, 'stampy': 19327, 'gradient': 19328, 'gurgaon': 19329, 'alphonse': 19330, 'uefa': 19331, \"champion's\": 19332, 'raikkoenen': 19333, 'occurrences': 19334, 'virtualized': 19335, 'clair': 19336, 'cappon': 19337, \"stella's\": 19338, 'rorschach': 19339, 'time’s': 19340, 'rti': 19341, 'destitute': 19342, 'newey': 19343, 'obe': 19344, 'welding': 19345, 'indy': 19346, 'andretti': 19347, 'prost': 19348, 'mika': 19349, 'hakkinen': 19350, 'vettel': 19351, 'ayrton': 19352, 'senna’s': 19353, 'synchronicity': 19354, 'marnie’s': 19355, 'abol': 19356, 'tabol': 19357, 'statesmen': 19358, 'premature': 19359, 'catalogued': 19360, 'ultrarunning': 19361, 'frantically': 19362, 'sleepover': 19363, \"french's\": 19364, 'jerald': 19365, 'decks': 19366, 'bitterly': 19367, \"tagore's\": 19368, 'reverberate': 19369, 'printers': 19370, 'sia': 19371, 'thirds': 19372, '110': 19373, 'decreasing': 19374, \"jackson's\": 19375, 'clarifies': 19376, 'resembles': 19377, 'greet': 19378, 'kirkpatrick': 19379, 'dynamo': 19380, 'ramsay': 19381, 'resonating': 19382, 'committees': 19383, 'squat': 19384, 'cornell': 19385, 'ithaca': 19386, \"lewis'\": 19387, 'meditating': 19388, 'brock': 19389, \"'more\": 19390, 'fantasia': 19391, 'rilke’s': 19392, 'clarification': 19393, 'impactful': 19394, 'mirai': 19395, 'directive': 19396, 'sloth': 19397, 'sloths': 19398, '°': 19399, 'reducing': 19400, 'igniting': 19401, 'keating': 19402, 'roguish': 19403, 'wallander': 19404, 'prescriptions': 19405, 'bastiano': 19406, 'losers': 19407, 'swinton': 19408, 'voynich': 19409, 'olympians': 19410, 'druids': 19411, 'oracle’': 19412, 'dio': 19413, 'cryptocurrencies': 19414, 'alpaydin': 19415, 'wheat': 19416, 'naam': 19417, 'burnham': 19418, 'hounded': 19419, 'ambi': 19420, 'crawls': 19421, 'trunk': 19422, 'kun': 19423, 'stalk': 19424, \"enemy's\": 19425, 'agendas': 19426, \"cartoonist's\": 19427, 'asaram': 19428, 'bapu': 19429, 'assaults': 19430, 'laundering': 19431, 'donations': 19432, 'hefty': 19433, '1886': 19434, 'comma': 19435, 'hindu–muslim': 19436, 's3': 19437, '53': 19438, \"winter's\": 19439, 'kyra': 19440, 'lgbt': 19441, 'lindqvist': 19442, 'requirement': 19443, 'begum': 19444, 'humayun': 19445, 'quiller': 19446, 'jsp': 19447, 'remoting': 19448, 'soccer\\x92s': 19449, 'nugent': 19450, 'suck': 19451, 'touchpoints': 19452, 'mae': 19453, 'persecuted': 19454, 'follett': 19455, 'peirce': 19456, 'kinney': 19457, 'shortlist': 19458, 'tana': 19459, 'tandem': 19460, 'kabat': 19461, 'holidaying': 19462, 'imperatives': 19463, 'implacable': 19464, 'winterians': 19465, 'theron': 19466, 'mather': 19467, 'justforkix': 19468, 'mesos': 19469, 'upgrade': 19470, 'leamas': 19471, 'mundt': 19472, 'skipper': 19473, 'credence': 19474, '4hana': 19475, 'nests': 19476, 'primed': 19477, 'stave': 19478, 'snacks': 19479, 'mango': 19480, 'chantry': 19481, 'tippy': 19482, 'lambdas': 19483, 'multicore': 19484, 'leafa': 19485, 'orchard': 19486, 'astrov': 19487, 'squash': 19488, 'unbeaten': 19489, 'westinghouse': 19490, \"love'\": 19491, 'ryle': 19492, 'kylie': 19493, 'fest': 19494, 'charging': 19495, 'fulfilment': 19496, 'lingers': 19497, 'scripted': 19498, 'velicovich': 19499, 'किताब': 19500, 'या': 19501, 'न': 19502, 'wellington': 19503, 'ravinder': 19504, 'dashboard': 19505, 'tanner': 19506, 'reformation': 19507, '809': 19508, 'indigo': 19509, 'palette': 19510, 'scorers': 19511, 'hawkeye': 19512, 'pa': 19513, 'cleo': 19514, 'nolan’s': 19515, 'nintendo': 19516, 'voucher': 19517, \"'when\": 19518, 'madeira': 19519, 'canary': 19520, 'ludwig': 19521, 'oprah’s': 19522, 'separatist': 19523, 'outfits': 19524, 'halberstam': 19525, 'durrell’s': 19526, 'garrincha': 19527, \"around'\": 19528, 'nano': 19529, 'tds': 19530, 'rommy': 19531, 'exonerate': 19532, 'feral': 19533, 'babu': 19534, 'hairy': 19535, 'hooligan': 19536, 'velma': 19537, 'rewire': 19538, 'ami': 19539, '1900': 19540, 'shreyasi': 19541, 'disputes': 19542, 'reshma': 19543, 'panchatantra': 19544, 'mont': 19545, 'furry': 19546, 'severian': 19547, 'sisson': 19548, 'polly': 19549, 'musashi': 19550, 'miyamoto': 19551, 'inoue': 19552, 'twentysomething': 19553, 'vested': 19554, 'fowl': 19555, 'persuaders': 19556, '‘for': 19557, \"practitioner's\": 19558, 'db2': 19559, \"o'brien\": 19560, 'align': 19561, \"'jeeves\": 19562, 'appended': 19563, 'rajneesh': 19564, 'riya’s': 19565, 'nutter': 19566, 'antichrist': 19567, 'univ': 19568, 'unrest': 19569, 'tsuru': 19570, 'mcafee': 19571, 'wordlist': 19572, 'landon': 19573, 'veeteren': 19574, 'toolbars': 19575, 'mis': 19576, 'karnatik': 19577, 'treading': 19578, 'quatermain': 19579, 'moomin': 19580, 'zamperini': 19581, 'punisher': 19582, 'rithika': 19583, 'queerness': 19584, 'ssrs': 19585, 'configmgr': 19586, 'impermanence': 19587, 'hrithik': 19588, 'gold’s': 19589, 'hoopa': 19590, 'madhur': 19591, 'moeen': 19592, 'abigail': 19593, 'hegarty': 19594, '102': 19595, 'gulbarga': 19596, 'bindiya': 19597, 'samar': 19598, 'eternia': 19599, 'hordak': 19600, 'argentine': 19601, 'miu': 19602, 'bonner': 19603, 'ukulele': 19604, 'pillsbury': 19605, \"che's\": 19606, 'prithviraj': 19607, 'vespasian': 19608, 'sneha': 19609, 'rosa\\x92s': 19610, 'jesme': 19611, 'balram': 19612, 'brahm': 19613, 'dung': 19614, 'engleby': 19615, \"'raw\": 19616, \"polo's\": 19617, 'dictated': 19618, 'hitchcock': 19619, 'khopkar': 19620, 'kaagaz': 19621, 'biwi': 19622, 'warre': 19623, 'sovereign—or': 19624, '—to': 19625, 'substituting': 19626, 'blasphemy': 19627, '1651': 19628, \"macpherson's\": 19629, 'agonising': 19630, 'marple’s': 19631, 'creme': 19632, 'pilates': 19633, 'splints': 19634, '‘i’d': 19635, 'hatha': 19636, 'sivananda': 19637, 'vedana': 19638, 'sreedharan': 19639, 'konkan': 19640, 'westernization': 19641, 'ledger': 19642, 'voting': 19643, 'payments': 19644, 'satya': 19645, 'schwab': 19646, 'struan': 19647, 'smuggler': 19648, 'praying': 19649, 'sparring': 19650, 'nourishing': 19651, 'team…or': 19652, 'shisui': 19653, 'chunin': 19654, 'croce': 19655, 'durant': 19656, 'feelgood': 19657, 'streetcat': 19658, 'unashamedly': 19659, 'ennobles': 19660, 'gentleness': 19661, 'nana’s': 19662, \"'arikawa\": 19663, \"companionship'\": 19664, 'ghibli': 19665, 'desceration': 19666, 'footing': 19667, 'ix': 19668, 'parameter': 19669, 'communicator': 19670, 'franciscans': 19671, 'licenses': 19672, 'vendors': 19673, 'unregulated': 19674, 'unwritten': 19675, 'skepticism': 19676, 'hansie': 19677, 'cronje': 19678, 'lodha': 19679, 'sreesanth': 19680, 'cairns': 19681, \"players'\": 19682, 'humourous': 19683, 'devadatta': 19684, 'vince': 19685, 'mcmahon': 19686, 'shawn': 19687, 'bookmarks': 19688, \"wwe's\": 19689, 'psychiatrists': 19690, 'aniela': 19691, 'corrected': 19692, 'handler': 19693, 'spade': 19694, \"thief's\": 19695, 'unchartend': 19696, 'simultaneoulsy': 19697, 'mute': 19698, \"hannibal's\": 19699, \"uncle's\": 19700, 'karmic': 19701, 'kilometres': 19702, 'staid': 19703, 'possessions': 19704, 'deposit': 19705, 'dabbler': 19706, 'vipassana': 19707, 'stagger': 19708, 'davenport': 19709, 'faulty': 19710, 'straka': 19711, 'shanghaied': 19712, 'disorienting': 19713, 'abrams': 19714, 'arenas': 19715, 'combustible': 19716, '‘so': 19717, 'bowman': 19718, 'inflatable': 19719, 'moonlight': 19720, 'minuet': 19721, 'adagio': 19722, 'collectively': 19723, 'vendettas': 19724, \"youtube's\": 19725, 'infusing': 19726, 'requesting': 19727, 'subscribed': 19728, 'alexandria': 19729, 'muzzled': 19730, 'coomi': 19731, 'sterilizations': 19732, 'evictions': 19733, 'wanton': 19734, 'abrsmdownloads': 19735, 'robe': 19736, 'habitation': 19737, 'decayed': 19738, \"johnston's\": 19739, 'westworld': 19740, \"'first\": 19741, 'rapidity': 19742, 'loiter': 19743, 'maia': 19744, 'unawares': 19745, 'roz': 19746, 'ames': 19747, 'patchett': 19748, 'amitav': 19749, 'cheerfully': 19750, 'confessional': 19751, 'fredrik': 19752, \"backman's\": 19753, \"joyce's\": 19754, \"simsion's\": 19755, \"nicholl's\": 19756, \"'warm\": 19757, \"'rescued\": 19758, 'grumpiest': 19759, 'joggers': 19760, \"residents'\": 19761, 'unswerving': 19762, 'disowned': 19763, 'vajpayee’s': 19764, 'shrewdest': 19765, 'indulgence': 19766, 'regulated': 19767, 'uncomfortably': 19768, 'individualism': 19769, 'manufactured': 19770, 'dickens’s': 19771, 'prisons': 19772, \"'possibly\": 19773, 'hidaka': 19774, 'alibis': 19775, 'nonoguchi': 19776, \"nonoguchi's\": 19777, 'confesses': 19778, 'ibsen': 19779, '“first': 19780, 'helmer': 19781, 'henrik': 19782, 'doll’s': 19783, '‘supremely': 19784, 'stories…murakami': 19785, '‘murakami': 19786, 'best’': 19787, 'pigeon': 19788, 'gilchrist': 19789, 'villiers': 19790, 'regards': 19791, 'organisers': 19792, 'fpvleague': 19793, 'journos': 19794, 'newbies': 19795, 'videographers': 19796, 'humoured': 19797, 'organiser': 19798, 'reconginsable': 19799, 'dji': 19800, 'multicopters': 19801, 'uavs': 19802, 'commmunity': 19803, 'ceasefire': 19804, 'prehistory': 19805, \"museum's\": 19806, 'defenceless': 19807, 'harbour': 19808, 'farnholme': 19809, 'planter': 19810, 'effen': 19811, 'gudrun': 19812, 'trader': 19813, 'siran': 19814, 'steamer': 19815, 'tnt': 19816, 'vacations': 19817, 'vertiginous': 19818, 'loathe': 19819, 'outbreaks': 19820, 'knight’s': 19821, 'wayne’s': 19822, 'grave…': 19823, 'fawkes': 19824, 'fernando': 19825, \"anecdotes'\": 19826, 'rafa': 19827, \"'greatest\": 19828, \"played'\": 19829, 'majorca': 19830, 'racquet': 19831, 'firearms': 19832, 'airsoft': 19833, 'bb': 19834, 'pellet': 19835, 'shotguns': 19836, 'visibility': 19837, \"miracle'\": 19838, 'joon': 19839, 'purview': 19840, 'prioritise': 19841, 'maximisation': 19842, 'priesthood': 19843, 'guise': 19844, 'chipped': 19845, 'consent': 19846, 'logophilia': 19847, 'simplifying': 19848, 'nusserwanji': 19849, 'nagpur': 19850, \"'riveting\": 19851, 'honnold': 19852, 'gonna': 19853, \"'we've\": 19854, \"'free\": 19855, \"yosemite's\": 19856, 'mohun': 19857, 'bagan': 19858, 'shrewdly': 19859, 'novy': 19860, 'validating': 19861, 'frontrunner': 19862, 'rds': 19863, 'cloudfront': 19864, 'dns': 19865, 'elasticity': 19866, 'shenanigans': 19867, 'principally': 19868, 'counselors': 19869, \"cook's\": 19870, 'emphasise': 19871, \"sontag's\": 19872, 'sontag': 19873, 'manufacture': 19874, 'saturn': 19875, 'specified': 19876, 'acl': 19877, 'fools': 19878, 'hindrance': 19879, \"april's\": 19880, 'arlena': 19881, 'viper': 19882, 'entitlements': 19883, 'meyers': 19884, 'daunted': 19885, 'profitably': 19886, 'moss': 19887, 'tashan': 19888, 'kilos': 19889, 'vilified': 19890, 'monitored': 19891, 'paparazzi': 19892, 'roberto': 19893, 'zidane': 19894, 'reappraisal': 19895, 'players’': 19896, 'ferdinand': 19897, 'futaro': 19898, 'chrismd': 19899, 'greasy': 19900, 'youtuber': 19901, 'troublemaker': 19902, 'violet': 19903, 'richmal': 19904, 'tousle': 19905, 'snub': 19906, 'nosed': 19907, \"sunday's\": 19908, \"independent's\": 19909, 'excoriating': 19910, 'thorax': 19911, 'coda': 19912, 'patterned': 19913, 'piercings': 19914, 'sanitation': 19915, 'myrna': 19916, 'tagged': 19917, 'hyper': 19918, 'selflessness': 19919, 'sync': 19920, \"kiki's\": 19921, 'mononoke': 19922, 'hayao': 19923, 'attendants': 19924, 'discs': 19925, 'abdel': 19926, 'braungart': 19927, 'redesign': 19928, 'plying': 19929, 'maynard': 19930, 'keynes': 19931, 'uproariously': 19932, 'wildernesses': 19933, 'borneo': 19934, 'biotech': 19935, 'ellison': 19936, 'draper': 19937, 'fundraising': 19938, \"holmes's\": 19939, 'carreyrou': 19940, 'enron': 19941, 'triangles': 19942, 'satish': 19943, 'pricing': 19944, 'providers': 19945, \"on'\": 19946, 'inscription': 19947, 'collaborators': 19948, 'trekkers': 19949, 'ecologists': 19950, \"weekly's\": 19951, \"newsweek's\": 19952, \"post's\": 19953, \"noble's\": 19954, 'matriarchal': 19955, 'inflected': 19956, 'punk': 19957, 'marjorie': 19958, 'bisexual': 19959, 'sana': 19960, 'cgi': 19961, 'sega': 19962, 'odom': 19963, 'schemers': 19964, 'mysqls': 19965, 'failover': 19966, 'querying': 19967, 'backups': 19968, 'fuzzers': 19969, 'seitz': 19970, 'reversing': 19971, 'debugger': 19972, 'enterpriseójust': 19973, 'netflixóbut': 19974, 'dynamite': 19975, 'ashenden': 19976, 'donned': 19977, 'disheartened': 19978, 'moong': 19979, 'wholesome': 19980, 'administrations': 19981, 'immutable': 19982, 'babel': 19983, 'dialects': 19984, 'sensualist': 19985, 'tortures': 19986, 'smerdyakov': 19987, '1881': 19988, 'nikolai': 19989, 'oates': 19990, '‘little': 19991, 'miranda’s': 19992, 'cookbooks': 19993, 'ammunition': 19994, 'tantalizingly': 19995, 'womb': 19996, \"'chess\": 19997, 'averbakh': 19998, 'coordinating': 19999, \"endings'\": 20000, '6b': 20001, 'iqbal': 20002, 'reciprocate': 20003, 'paddy': 20004, 'tutu': 20005, 'paragliding': 20006, \"arctic's\": 20007, 'valet': 20008, 'herb': 20009, '1908': 20010, 'sentient': 20011, 'humanoid': 20012, 'bulleted': 20013, \"palin'\": 20014, \"fey's\": 20015, 'halfhearted': 20016, 'pelham': 20017, 'appetites': 20018, 'reputations': 20019, 'agreeable': 20020, 'boyish': 20021, \"before'\": 20022, 'stevenson': 20023, 'enchant': 20024, 'balfour': 20025, 'castaway': 20026, 'daredevil': 20027, 'makoto': 20028, 'shinkai': 20029, 'midori': 20030, 'motohashi': 20031, 'rainswept': 20032, 'takao': 20033, 'yukari': 20034, 'cobbling': 20035, 'bristling': 20036, 'replies': 20037, 'solemnly': 20038, \"'or\": 20039, 'swoop': 20040, 'grins': 20041, 'mukti': 20042, 'bahini': 20043, 'faujdaar': 20044, 'chakkahera': 20045, 'gnat': 20046, \"'sabre\": 20047, \"slayer'\": 20048, 'wreaked': 20049, 'sabre': 20050, 'squadrons': 20051, 'flanked': 20052, 'raks': 20053, 'caribou': 20054, 'gnatties': 20055, 'gana': 20056, 'shaanu': 20057, 'dadyseth': 20058, 'wax': 20059, \"chauhan's\": 20060, 'akriti': 20061, 'residency': 20062, 'uipath': 20063, 'bot': 20064, '7a': 20065, 'walkenbach': 20066, 'badami': 20067, 'reigned': 20068, 'encode': 20069, 'scissors': 20070, 'afternoons': 20071, 'alderton': 20072, 'raving': 20073, \"'bourne\": 20074, 'dualism': 20075, 'sticking': 20076, \"hilarious'\": 20077, 'pedro': 20078, 'amenities': 20079, 'subsidies': 20080, 'chitrakoot': 20081, 'warana': 20082, 'effecting': 20083, 'quaker': 20084, 'thankful': 20085, 'suspecting': 20086, 'allahabad': 20087, 'alumnus': 20088, 'kids\\x92': 20089, '\\xadadventure': 20090, 'auteur': 20091, 'tearjerker': 20092, 'setups': 20093, 'hurried': 20094, 'economical': 20095, 'sea’': 20096, 'baltic': 20097, 'duplicity': 20098, 'avarice': 20099, 'connoisseur': 20100, '‘yes': 20101, 'holders': 20102, 'sudhir': 20103, 'tastemaker': 20104, 'journeying': 20105, 'tuscan': 20106, 'copenhagen': 20107, 'gridlock': 20108, 'lounge': 20109, 'embarrassment': 20110, \"ideas'\": 20111, 'overrun': 20112, 'gladly': 20113, 'it’ll': 20114, 'distinguishes': 20115, 'renee': 20116, 'gracefully': 20117, 'dispatches': 20118, 'flashes': 20119, 'blacksmith': 20120, \"can'\": 20121, \"darkness'\": 20122, 'joanne': 20123, \"'brilliantly\": 20124, \"achievement'\": 20125, \"unsettling'\": 20126, 'anansi': 20127, 'mara': 20128, 'dividends': 20129, 'bonuses': 20130, 'reignite': 20131, 'tamose': 20132, 'enemy’s': 20133, 'utteric': 20134, 'taita’s': 20135, 'utteric’s': 20136, 'forsake': 20137, 'sparta': 20138, 'ajanta': 20139, 'revivalist': 20140, 'boyette': 20141, 'cheerleader': 20142, 'prosecutors': 20143, 'faecs': 20144, 'brainy': 20145, 'partnerships': 20146, 'excite': 20147, 'hyams': 20148, 'gl': 20149, 'batra': 20150, 'capt': 20151, 'param': 20152, 'vir': 20153, 'retake': 20154, 'businesswoman': 20155, 'transcontinental': 20156, 'expounds': 20157, 'objectivism': 20158, 'beatings': 20159, 'probation': 20160, 'cadre': 20161, 'elt': 20162, 'crowe': 20163, 'archaeologist’s': 20164, 'cascading': 20165, 'childhood—the': 20166, 'sa’eed': 20167, 'mustafa’s': 20168, 'man—whom': 20169, 'wife—in': 20170, 'defilement': 20171, 'bantrys': 20172, 'cheeks': 20173, 'charred': 20174, 'tongues': 20175, 'wag': 20176, 'underpinnings': 20177, 'bendini': 20178, 'lambert': 20179, 'mercurial': 20180, 'busby': 20181, 'devils': 20182, 'machiavellian': 20183, 'motivator': 20184, 'mourinho\\x92s': 20185, 'billed': 20186, 'that\\x92s': 20187, 'influx': 20188, 'wistful': 20189, 'reds': 20190, 'fergie\\x92s': 20191, 'encapsulating': 20192, 'isometric': 20193, 'alouette': 20194, 'chiapanecas': 20195, 'cockles': 20196, 'mussels': 20197, 'wenceslas': 20198, 'greensleeves': 20199, 'jericho': 20200, 'jingle': 20201, 'kum': 20202, 'yah': 20203, 'largo': 20204, 'dvorak': 20205, \"lavender's\": 20206, \"marine's\": 20207, 'mio': 20208, 'raisins': 20209, 'almonds': 20210, \"rockin'\": 20211, 'scarborough': 20212, 'shoo': 20213, 'tisket': 20214, 'tasket': 20215, \"pele's\": 20216, 'walling': 20217, 'dreading': 20218, 'wrightsburg': 20219, 'fanaticism': 20220, 'helicopters': 20221, 'pals': 20222, 'snot': 20223, 'sneaks': 20224, 'patrice': 20225, 'tickle': 20226, 'balzac': 20227, 'ovid': 20228, 'sappho': 20229, 'aatish': 20230, 'benares': 20231, 'insistent': 20232, 'ganges': 20233, 'brutalities': 20234, 'brazilians': 20235, 'italians': 20236, 'lamas': 20237, 'prodding': 20238, \"cameron's\": 20239, \"thing'\": 20240, 'racecourse': 20241, 'backtracking': 20242, 'theorems': 20243, 'delineated': 20244, 'deshpande’s': 20245, 'pythonista': 20246, 'attentive': 20247, 'yasso': 20248, 'conceivable': 20249, '5ks': 20250, 'yasso’s': 20251, 'antarctica': 20252, \"'growth\": 20253, 'unholy': 20254, 'ideologue': 20255, 'contractor': 20256, 'lurch': 20257, 'rohit': 20258, 'thwarting': 20259, 'natya': 20260, 'manch': 20261, 'janam': 20262, 'athenian': 20263, 'eros': 20264, 'sublimation': 20265, 'love—as': 20266, 'draupadi': 20267, 'pledge': 20268, 'resonates': 20269, 'climbing’s': 20270, '“reaching': 20271, 'heating': 20272, 'bags': 20273, \"mallory's\": 20274, '–determined': 20275, 'moroccan': 20276, 'martel’s': 20277, 'aao': 20278, 'accountancy': 20279, 'giovanni': 20280, 'plagiarism': 20281, 'granddaughter': 20282, 'hutchinson': 20283, 'jolts': 20284, 'etsuko': 20285, 'sachiko': 20286, 'vagrancy': 20287, 'bacon’s': 20288, 'summarizing': 20289, 'passion—and': 20290, 'avenging': 20291, 'vowing': 20292, 'unsophisticated': 20293, 'chairmanship': 20294, 'fragment': 20295, 'punitive': 20296, 'revolutionised': 20297, 'nimble': 20298, 'mbe': 20299, \"tourists'\": 20300, 'mussolini': 20301, '‘richly': 20302, 'engrossing’': 20303, \"stark's\": 20304, 'gareth': 20305, 'confirmation': 20306, 'taker': 20307, 'esoteric': 20308, 'mantras': 20309, 'mogodorians': 20310, 'christens': 20311, 'seem…': 20312, \"chogyal's\": 20313, 'lhendup': 20314, 'dorji': 20315, 'alignments': 20316, 'elevator': 20317, 'willy': 20318, 'wonka': 20319, 'knids': 20320, 'audiobooks': 20321, 'soundeffects': 20322, 'sob': 20323, 'adore': 20324, 'abundantly': 20325, 'inducted': 20326, \"evan's\": 20327, 'reorganized': 20328, 'sapui5': 20329, 'diagnostics': 20330, 'upgrading': 20331, 'dynpro': 20332, 'ap': 20333, 'prerequisite': 20334, '116': 20335, 'typed': 20336, 'typewritten': 20337, 'chandrachud': 20338, 'religiously': 20339, 'devangari': 20340, 'observant': 20341, 'stiglitz': 20342, 'heretical': 20343, 'compilations': 20344, 'handily': 20345, 'formatted': 20346, 'hathaway': 20347, 'gates’s': 20348, 'heartbroken': 20349, 'relieve': 20350, 'loggerheads': 20351, 'hunk': 20352, 'cupid': 20353, 'crumpling': 20354, 'razed': 20355, 'शब्द': 20356, 'शब्दों': 20357, 'किया': 20358, 'जो': 20359, 'गए': 20360, 'लिए': 20361, 'नयी': 20362, 'से': 20363, 'नहीं': 20364, 'caricatured': 20365, 'misgivings': 20366, 'militias': 20367, \"sardar's\": 20368, 'meghnad': 20369, 'sardhar': 20370, \"'iron\": 20371, 'onerous': 20372, 'nadiad': 20373, 'bowing': 20374, 'bardoli': 20375, 'kheda': 20376, 'satyagrahas': 20377, 'chagrin': 20378, 'copious': 20379, 'vemsani': 20380, 'shawnee': 20381, 'indic': 20382, 'civilisational': 20383, 'obdurate': 20384, 'liquidated': 20385, 'liquidating': 20386, 'aloofness': 20387, 'elitism': 20388, 'plebiscites': 20389, 'egalitarianism': 20390, 'unheeded': 20391, 'unifier': 20392, 'govindarajan': 20393, 'coxe': 20394, 'vallabhai': 20395, \"axworthy's\": 20396, 'nadir': 20397, \"crowley's\": 20398, '1453': 20399, 'retd': 20400, 'weaker': 20401, 'mukherji': 20402, 'vikramjit': 20403, 'ub': 20404, 'lengen': 20405, 'endeavor': 20406, 'siting': 20407, 'cactus': 20408, 'bamboo': 20409, 'bradbury’s': 20410, '451': 20411, 'fireman': 20412, 'clarisse': 20413, 'chatter': 20414, 'pilfering': 20415, 'assassinating': 20416, 'vermeil': 20417, 'seismic': 20418, 'terse': 20419, 'pestering': 20420, 'odieand': 20421, 'fattening': 20422, 'puss': 20423, 'gq': 20424, 'taleniekov': 20425, 'politburo': 20426, 'cancelled': 20427, 'widower': 20428, 'boutique': 20429, 'knob': 20430, '©2014': 20431, 'fuels': 20432, 'hustlers': 20433, 'taiwan': 20434, \"turgenev's\": 20435, 'arkady': 20436, 'criticisms': 20437, 'delusions': 20438, 'preys': 20439, 'airs': 20440, 'bronze': 20441, 'statues': 20442, 'uis': 20443, 'walmart': 20444, 'ui': 20445, 'advik': 20446, 'conquers': 20447, 'unabashed': 20448, 'heavenly': 20449, 'rihanna': 20450, 'farnon': 20451, 'centennial': 20452, 'reissues': 20453, 'commemorative': 20454, 'holden': 20455, 'caulfield': 20456, 'amis': 20457, \"was'\": 20458, \"now'\": 20459, 'folders': 20460, 'renaming': 20461, 'underscored': 20462, 'reelected': 20463, 'urvashi': 20464, 'butalia’s': 20465, 'exhume': 20466, 'history—children': 20467, 'untouchables—were': 20468, 'thich': 20469, 'nhat': 20470, 'hanh': 20471, 'mann’s': 20472, 'character’s': 20473, 'slapstick': 20474, 'drunkard': 20475, 'absentee': 20476, 'ajvide': 20477, 'oreilly': 20478, 'sagan': 20479, 'reconciling': 20480, 'antagonists': 20481, 'stingo': 20482, 'inexperienced': 20483, 'leafy': 20484, 'currencies': 20485, 'qualification': 20486, 'elucidate': 20487, 'thieving': 20488, 'tuples': 20489, 'windy': 20490, 'attracts': 20491, 'untouchability': 20492, '1353': 20493, 'lamped': 20494, 'alvin': 20495, 'telephones': 20496, 'childlike': 20497, \"annie's\": 20498, 'brewed': 20499, 'reminders': 20500, 'empathic': 20501, 'cattail': 20502, 'mistrusted': 20503, 'janpath': 20504, 'sanjana’s': 20505, 'accidently': 20506, 'punish': 20507, 'bait': 20508, 'simran': 20509, 'firenze': 20510, 'borgias': 20511, '”—parents': 20512, 'misbehavior': 20513, 'tears—without': 20514, '“d”': 20515, 'reprimand': 20516, 'redirect': 20517, 'philosophy—and': 20518, 'development—and': 20519, 'child—no': 20520, 'behavior—while': 20521, 'tantrum': 20522, 'make—and': 20523, 'siegel': 20524, '“wow': 20525, '”—lawrence': 20526, 'attimabbe': 20527, 'isc': 20528, 'exploitative': 20529, 'janda': 20530, 'frequencies': 20531, 'denying': 20532, 'ensnared': 20533, 'thrawn': 20534, 'ghostly': 20535, 'warships': 20536, 'hungers': 20537, 'ange': 20538, 'kerry': 20539, 'peels': 20540, 'silences': 20541, 'tickling': 20542, 'mirth': 20543, 'admitted': 20544, 'discomfort': 20545, 'gutted': 20546, 'monastic': 20547, 'spilling': 20548, 'theresa': 20549, 'prettier': 20550, 'man…': 20551, 'mayle': 20552, 'gastronomic': 20553, 'finer': 20554, 'provençal': 20555, \"delightful'\": 20556, \"'engaging\": 20557, 'texan': 20558, 'hon': 20559, \"'yes\": 20560, 'bathed': 20561, 'sequeira': 20562, 'movim': 20563, 'splendidly': 20564, 'rotterdam': 20565, 'teepees': 20566, 'totems': 20567, 'gobbling': 20568, \"answers'\": 20569, 'handmaids': 20570, 'prohibited': 20571, 'renée': 20572, 'cisterns': 20573, 'confirming': 20574, 'lodger': 20575, 'danger…': 20576, 'war’s': 20577, 'stems': 20578, 'righteousness': 20579, \"rome's\": 20580, 'natives': 20581, 'patrols': 20582, 'tantri': 20583, 'mantri': 20584, 'forays': 20585, 'imagination—and': 20586, 'abodes': 20587, '1700s': 20588, 'extolled': 20589, 'macfarlane': 20590, 'enveloping': 20591, 'dispose': 20592, 'zumthor': 20593, 'leis': 20594, 'hotting': 20595, \"haddock's\": 20596, 'archimedes': 20597, 'carthaginian': 20598, 'hanno': 20599, 'diverted': 20600, \"scarecrows'\": 20601, 'glowing': 20602, 'rake': 20603, 'synonym': 20604, 'appointing': 20605, \"leadership'\": 20606, 'virtuosic': 20607, 'doyenne': 20608, 'fitzgeralds': 20609, 'kennedys': 20610, 'economically': 20611, '1558': 20612, 'willard': 20613, 'precariously': 20614, 'pitches': 20615, 'tyrants': 20616, 'brexit': 20617, 'mutually': 20618, 'catalysing': 20619, 'preoccupations': 20620, 'reva': 20621, 'sfx': 20622, 'compatriots': 20623, 'cong': 20624, 'issa': 20625, 'annabel': 20626, 'deportation': 20627, 'brue': 20628, 'genos': 20629, 'cyborg': 20630, 'damned': 20631, 'cale': 20632, 'phillip': 20633, 'grinning': 20634, 'wager': 20635, 'draped': 20636, 'prince’s': 20637, 'sister’s': 20638, 'kasim': 20639, 'kasim’s': 20640, 'matrimonial': 20641, 'rejections': 20642, 'touka': 20643, 'reinforcements': 20644, 'students’': 20645, 'adopters': 20646, 'readability': 20647, 'multiverse': 20648, 'supermen': 20649, 'harsher': 20650, 'dead’': 20651, 'arrayed': 20652, '–enough': 20653, 'exterminate': 20654, \"maugham's\": 20655, 'phonemes': 20656, 'phoneme': 20657, \"comet's\": 20658, 'naps': 20659, 'wowzer': 20660, 'sundaresan': 20661, \"them'\": 20662, 'divakaruni': 20663, 'mehrunnisa': 20664, \"jahangir's\": 20665, 'danilo': 20666, 'biloxi': 20667, 'shrey': 20668, 'mccready': 20669, \"sam's\": 20670, 'futile': 20671, 'fruitful': 20672, 'nargis': 20673, 'checkmate': 20674, 'defenders': 20675, 'probed': 20676, 'attacker': 20677, 'extras': 20678, 'colliding': 20679, 'lumet': 20680, 'katharine': 20681, 'hepburn': 20682, 'newman': 20683, 'preprocessor': 20684, 'kicit': 20685, 'dini’s': 20686, 'penning': 20687, 'toon': 20688, 'hampered': 20689, 'chivvying': 20690, 'light—not': 20691, 'recklessly': 20692, '1828': 20693, 'caucasus': 20694, 'merc': 20695, 'tootsie': 20696, 'steaming': 20697, 'orthodox': 20698, 'intensifies': 20699, 'groceries': 20700, 'imdb': 20701, 'personalization': 20702, 'pogo': 20703, 'oliphant': 20704, 'jurchen': 20705, 'shifus': 20706, 'jiaxing': 20707, 'holmwood': 20708, 'disagree': 20709, 'wades': 20710, 'distortion': 20711, 'bairstow': 20712, 'keel': 20713, 'awesomeness': 20714, 'dude': 20715, \"todd's\": 20716, \"internet's\": 20717, 'hessa': 20718, 'forges': 20719, 'pilger': 20720, 'terror’': 20721, 'carmine': 20722, 'airbnb': 20723, 'disproportionately': 20724, 'louder': 20725, 'childbirth': 20726, 'incur': 20727, 'eradicated': 20728, 'reversed': 20729, 'hiv': 20730, 'favours': 20731, 'prioritized': 20732, 'occupying': 20733, 'civilised': 20734, 'pelagia': 20735, 'rumpelstiltskin': 20736, 'fleece': 20737, 'archetypal': 20738, 'bloodstains': 20739, '1890': 20740, 'cw': 20741, 'appendixes': 20742, 'optometrist': 20743, 'dresden': 20744, 'pilgrim’s': 20745, 'artist’': 20746, 'coaxed': 20747, 'racket': 20748, 'crib': 20749, 'resented': 20750, 'unstinting': 20751, 'barbra': 20752, 'streisand': 20753, \"agassi's\": 20754, 'revati': 20755, 'charmer': 20756, 'lacan': 20757, \"lacan's\": 20758, 'brained': 20759, 'benicio': 20760, 'toro': 20761, 'jointly': 20762, 'crosswords': 20763, 'atala': 20764, 'hera': 20765, 'dwai': 20766, 'ethereal': 20767, 'concoction': 20768, 'book”': 20769, 'kelvin': 20770, 'linus': 20771, 'pauling': 20772, 'hoyle': 20773, 'haste': 20774, 'incorrectly': 20775, '“thoughtful': 20776, '—new': 20777, 'hazarika': 20778, 'divisions': 20779, 'assimilation': 20780, 'combatants': 20781, '1856': 20782, 'neon': 20783, 'harnessed': 20784, 'niagara': 20785, '‘ahead': 20786, 'demille': 20787, 'godsend': 20788, 'pied': 20789, 'corrupting': 20790, \"pancham's\": 20791, \"rd's\": 20792, 'chetwynd’s': 20793, \"guin's\": 20794, 'ged': 20795, 'drown': 20796, \"icon'\": 20797, 'disastrously': 20798, 'apart…': 20799, 'dutifully': 20800, 'mer': 20801, 'bountiful': 20802, 'properity': 20803, 'tropic': 20804, \"kon's\": 20805, 'kon': 20806, 'whiling': 20807, 'commute': 20808, 'throats': 20809, 'technologically': 20810, 'malaya': 20811, 'outcry': 20812, 'femininity': 20813, 'tactile': 20814, 'arpeggio': 20815, 'cadence': 20816, 'harmonizing': 20817, 'accelerating': 20818, 'abdomen': 20819, 'braves': 20820, 'finders': 20821, 'petrol': 20822, 'marlinspike': 20823, 'aspires': 20824, 'merchantman': 20825, 'devilish': 20826, 'oceanic': 20827, 'treacherously': 20828, 'backwards': 20829, 'elbows': 20830, 'openers': 20831, 'bottles': 20832, 'width': 20833, 'lowering': 20834, 'straighten': 20835, 'hunchback': 20836, 'ceiling': 20837, 'abs': 20838, 'tapped': 20839, 'internet’s': 20840, 'thunderous': 20841, 'embroideries': 20842, 'iim': 20843, 'nevada': 20844, 'horribly': 20845, 'chaucer': 20846, 'milton': 20847, 'munch': 20848, 'intake': 20849, 'buzzing': 20850, 'mayekar': 20851, 'surgeries': 20852, 'implants': 20853, 'gums': 20854, \"patients'\": 20855, 'chawl': 20856, 'ahi': 20857, 'samim’s': 20858, 'acp': 20859, 'rathore': 20860, 'h2': 20861, 'lt': 20862, \"bedtime'\": 20863, '1c': 20864, 'dank': 20865, 'hariharan': 20866, 'hurling': 20867, 'nervousness': 20868, 'shyness': 20869, 'garland': 20870, 'machina': 20871, 'portman': 20872, 'intangible': 20873, 'delavier': 20874, 'segmented': 20875, 'whitebeard': 20876, 'rundown': 20877, 'fingerprints': 20878, 'mecca': 20879, 'blunder': 20880, 'dalvis': 20881, 'participant': 20882, 'issuing': 20883, 'unprepared': 20884, 'grelle': 20885, 'theboys': 20886, \"margot's\": 20887, 'pressingly': 20888, 'collegeand': 20889, 'margot': 20890, 'boyshe': 20891, 'loves—behind': 20892, 'differentthings': 20893, 'fervour': 20894, 'chestnut': 20895, 'deodar': 20896, '93': 20897, 'shopkeepers': 20898, \"ruchir's\": 20899, 'courtly': 20900, 'tyler’s': 20901, 'wheel’s': 20902, 'ratcheting': 20903, 'defenses': 20904, 'remarque': 20905, 'orphans': 20906, '“best': 20907, 'year”': 20908, 'marjane’s': 20909, 'mila': 20910, 'furstova': 20911, 'coldplay’s': 20912, 'apophis': 20913, 'chant': 20914, 'predecessors': 20915, 'undeclared': 20916, 'antennae': 20917, 'prop': 20918, 'accordingly': 20919, 'galactic': 20920, 'doubled': 20921, 'skyward': 20922, \"sanderson's\": 20923, \"packed'\": 20924, \"store'\": 20925, 'av': 20926, 'foolproof': 20927, \"performer's\": 20928, 'patter': 20929, 'formulating': 20930, \"war's\": 20931, \"kaur's\": 20932, 'vitriol': 20933, 'crisscrosses': 20934, 'kaur’s': 20935, 'beget': 20936, 'kalvitis': 20937, 'obstetrician': 20938, 'netaji': 20939, 'farmstead': 20940, 'spilt': 20941, 'carré': 20942, '4mk’s': 20943, 'questionnaire': 20944, 'whatsoever': 20945, 'pascal': 20946, 'kittens': 20947, 'meee': 20948, 'purrrr': 20949, 'mctat': 20950, 'busker': 20951, 'intoanimated': 20952, 'byrne': 20953, 'thakkar': 20954, 'kentaro': 20955, 'toyama': 20956, 'utopians': 20957, 'heresy': 20958, 'augustine': 20959, 'motherly': 20960, 'quiche': 20961, 'faucet': 20962, 'bananas': 20963, 'ecuador': 20964, 'sewage': 20965, 'bose’s': 20966, 'psychoanalyst': 20967, 'crazed': 20968, 'est': 20969, 'tous': 20970, 'bien': 20971, 'indications': 20972, 'cathcart': 20973, 'theologians': 20974, 'hereafter': 20975, 'hippo': 20976, 'musings': 20977, \"pamuk's\": 20978, 'flickering': 20979, 'bloodlust': 20980, 'personages': 20981, 'grenouille': 20982, 'decay': 20983, 'finite': 20984, 'pushdown': 20985, 'entailing': 20986, 'impostor': 20987, 'debauchery': 20988, 'aint': 20989, 'westward': 20990, 'cormac': 20991, 'barbarous': 20992, 'tennessean': 20993, 'scalps': 20994, 'sanctions': 20995, 'disruption': 20996, 'idolatry': 20997, 'pythagorean': 20998, '527': 20999, 'geometrical': 21000, 'onlooker': 21001, \"'common\": 21002, 'signficance': 21003, 'orbits': 21004, 'correlating': 21005, 'tessellations': 21006, 'tile': 21007, 'affirm': 21008, 'kya': 21009, 'lump': 21010, 'pharaohs': 21011, 'lads': 21012, '145': 21013, 'granta': 21014, 'manoeuvres': 21015, 'conjure': 21016, 'wagon': 21017, 'curio': 21018, 'obituaries': 21019, 'hoardings': 21020, \"singin'\": 21021, 'brevity': 21022, \"ma's\": 21023, 'capitalists': 21024, 'westover': 21025, 'hadn’t': 21026, 'registered': 21027, 'tub': 21028, '‘evil': 21029, 'suewellyn': 21030, 'troubleshooter': 21031, 'moveable': 21032, \"magician's\": 21033, '5000': 21034, 'willa': 21035, 'intrusion': 21036, 'respectability': 21037, 'elliot': 21038, 'solomons': 21039, 'virgo': 21040, 'zodiac': 21041, 'thanatos': 21042, 'stonehenge': 21043, 'cushy': 21044, 'stranger’s': 21045, 'grievously': 21046, 'whitechapel': 21047, 'edged': 21048, 'californian': 21049, \"detective's\": 21050, 'dashiell': 21051, \"hammett's\": 21052, \"attain'\": 21053, 'auster': 21054, 'equate': 21055, 'natures': 21056, 'rooting': 21057, 'foretaste': 21058, 'n5': 21059, 'n4': 21060, 'kana': 21061, 'kanji': 21062, 'signatures': 21063, 'decrypted': 21064, 'sped': 21065, 'overthrew': 21066, 'sulked': 21067, 'dusting': 21068, 'epitomizes': 21069, 'fatigued': 21070, 'stairway': 21071, 'gush': 21072, 'downfalls': 21073, 'goblin': 21074, 'curses': 21075, 'mineralogy': 21076, 'climatology': 21077, 'lunar': 21078, \"cartoons'\": 21079, \"fishburne's\": 21080, 'interim': 21081, 'cmo': 21082, 'hoteltonight': 21083, 'nestlé': 21084, 'ixl': 21085, 'kronos': 21086, 'handley': 21087, 'marketingprofs': 21088, 'btw': 21089, 'canva': 21090, 'odden': 21091, 'toprank': 21092, 'nawaz': 21093, 'muinuddin': 21094, 'hut': 21095, 'nc': 21096, 'aap’s': 21097, 'crores': 21098, 'stance': 21099, 'aam': 21100, 'party’s': 21101, 'yadav': 21102, 'undoing': 21103, 'immaculate': 21104, 'schoolfriend': 21105, 'deewan': 21106, 'apples': 21107, \"tyler's\": 21108, 'panini': 21109, 'computerized': 21110, 'eons': 21111, 'parodies': 21112, 'sexless': 21113, 'gavros': 21114, 'lizzie': 21115, 'blazes': 21116, 'pavitte': 21117, 'cleary': 21118, 'kindergarten': 21119, 'beezus': 21120, 'recess': 21121, 'pest': 21122, 'franco': 21123, '168': 21124, '169': 21125, 'reopened': 21126, \"bible'\": 21127, 'sinners': 21128, 'sportspeople': 21129, 'listener': 21130, 'jeffery': 21131, 'deaver': 21132, \"'vivid\": 21133, 'warden': 21134, 'saboteur': 21135, 'shears': 21136, '1895': 21137, 'bardhan': 21138, \"kondo's\": 21139, 'extinct': 21140, 'vale': 21141, 'matis’s': 21142, 'canada—a': 21143, 'overprotected': 21144, 'semester—a': 21145, 'college’s': 21146, '“conflict': 21147, 'mediation”': 21148, 'numbness': 21149, 'again—and': 21150, 'tidytext': 21151, 'ggraph': 21152, 'shankara': 21153, 'presume': 21154, 'conveyed': 21155, 'kidney': 21156, 'siddiqui': 21157, 'chillies': 21158, 'bard': 21159, 'balochistan': 21160, 'pertain': 21161, 'abortions': 21162, 'utilitarianism': 21163, 'categorical': 21164, 'lip': 21165, 'kierkegaard': 21166, \"ulf's\": 21167, 'enlightens': 21168, \"'captivating\": 21169, 'lomong': 21170, 'histionix': 21171, 'codfix': 21172, 'pickup': 21173, 'lawton': 21174, 'reply': 21175, 'surendranath': 21176, 'strains': 21177, 'hallucinating': 21178, 'snapping': 21179, 'hacked': 21180, 'windswept': 21181, 'vow': 21182, 'outweigh': 21183, 'courtesan': 21184, 'undisguised': 21185, 'quelling': 21186, 'flexing': 21187, 'sagarika': 21188, \"ghose's\": 21189, \"oracle's\": 21190, 'rasmussen': 21191, 'initialization': 21192, 'euphoria': 21193, 'bagan’s': 21194, 'regiment': 21195, '‘mastering': 21196, 'history’': 21197, 'pinborough': 21198, 'kernick': 21199, 'cracker': 21200, \"friggin'\": 21201, 'kavanagh': 21202, 'butterfly': 21203, '24601': 21204, 'javert': 21205, 'fantine': 21206, '1830': 21207, 'denny’s': 21208, 'babakov': 21209, 'stalin': 21210, 'despicable': 21211, 'constituent': 21212, 'neuro': 21213, 'differential': 21214, 'theakston': 21215, 'peculier': 21216, 'hardcastle': 21217, 'aiden': 21218, 'blackheath': 21219, \"problems'\": 21220, 'locus': 21221, 'radioactive': 21222, 'foreseen': 21223, 'prying': 21224, 'implanted': 21225, \"trick'\": 21226, 'generators': 21227, 'compressing': 21228, 'generator': 21229, '—why': 21230, 'priestley': 21231, 'texting': 21232, 'intentional': 21233, 'drain': 21234, 'washi': 21235, 'crafters': 21236, 'multitude': 21237, 'subscribe': 21238, 'individual’s': 21239, 'maddening': 21240, 'absolutes': 21241, 'compliment': 21242, 'blatant': 21243, 'flagrant': 21244, 'consulted': 21245, 'obsidian': 21246, 'disdain': 21247, 'influencers': 21248, 'brunson': 21249, 'boise': 21250, 'kansa': 21251, 'valour': 21252, 'lic': 21253, 'fencing': 21254, 'pathetic': 21255, 'imply': 21256, 'judea': 21257, 'dana': 21258, \"pearl's\": 21259, 'balkans': 21260, 'woos': 21261, 'matured': 21262, 'silverware': 21263, 'chancellor': 21264, 'osho’s': 21265, 'flowering': 21266, 'pokes': 21267, 'house”': 21268, 'cackling': 21269, \"'she’s\": 21270, 'me”': 21271, 'star’s': 21272, 'filmfare': 21273, 'unusually': 21274, 'mammal': 21275, 'ampersand': 21276, 'clancy’s': 21277, 'mikhail': 21278, 'filitov': 21279, 'contractions': 21280, 'airship': 21281, 'exchanging': 21282, 'unsaid’': 21283, 'bagged': 21284, 'talismanic': 21285, 'materialism': 21286, \"'eat\": 21287, 'puppet': 21288, 'scant': 21289, 'lauren’s': 21290, 'hart’s': 21291, 'qualifies': 21292, 'grossed': 21293, 'fluffy': 21294, 'chefs': 21295, 'ringing': 21296, 'workbook—now': 21297, 'updated—from': 21298, 'field—and': 21299, 'material—including': 21300, 'brokeback': 21301, 'mountain—the': 21302, 'screenwriter’s': 21303, 'salable': 21304, 'built•': 21305, 'model—the': 21306, 'paradigm—that': 21307, 'use•': 21308, 'life•': 21309, 'pro•': 21310, 'acthere': 21311, 'turn—all': 21312, 'earmarked': 21313, 'flyer': 21314, 'dishy': 21315, \"'operational'\": 21316, 'maester': 21317, 'coldest': 21318, 'yorkers': 21319, 'vellum': 21320, 'archaeology': 21321, 'garnered': 21322, 'innocently': 21323, 'them—if': 21324, 'sanctity': 21325, 'sculptural': 21326, 'kanban': 21327, 'rxjs': 21328, 'router': 21329, 'swagger': 21330, 'circleci': 21331, 'provision': 21332, 'youtubers': 21333, 'whiskers': 21334, 'entertainers': 21335, 'grammys': 21336, 'jingles': 21337, 'keenness': 21338, 'directorial': 21339, \"rahman's\": 21340, 'purge': 21341, 'locke’s': 21342, 'woo': 21343, 'smells': 21344, 'sf': 21345, '“red': 21346, 'erikson': 21347, '”—fantasy': 21348, 'grandiose': 21349, '“right': 21350, '”—patrick': 21351, 'proficient': 21352, 'alphabetic': 21353, 'kitchens': 21354, 'adherence': 21355, 'utensils': 21356, 'rasam': 21357, 'specialities': 21358, 'moar': 21359, \"body's\": 21360, \"opponent's\": 21361, 'pandita': 21362, 'agitated': 21363, 'militants': 21364, 'rukmani—fierce': 21365, 'ways—meets': 21366, 'rukmani': 21367, 'choppy': 21368, 'simar': 21369, 'malhotra': 21370, 'embed': 21371, '190': 21372, 'bitch': 21373, \"conflict'\": 21374, \"'devastating'\": 21375, \"'ingenious'\": 21376, \"'compulsive'\": 21377, \"'elegant'\": 21378, \"'sexy'\": 21379, \"'enthralling'\": 21380, \"'original'\": 21381, \"'creepy'\": 21382, \"'satisfying'\": 21383, \"'superior'\": 21384, 'instyle': 21385, 'rental': 21386, 'karkare': 21387, 'sensing': 21388, 'parademon': 21389, 'warzone': 21390, 'naughtiest': 21391, 'lala': 21392, 'salem': 21393, 'hooligans': 21394, 'goon': 21395, 'scrutinized': 21396, 'baths': 21397, 'pyjamas': 21398, 'gertrude': 21399, 'innovatory': 21400, 'musa': 21401, 'truth”': 21402, 'ponza': 21403, 'pirandello’s': 21404, 'whimper': 21405, 'uttering': 21406, 'smartness': 21407, \"waller's\": 21408, 'reggie': 21409, 'deadline': 21410, 'vaccine': 21411, 'indifference': 21412, 'afflicted': 21413, 'geopolitical': 21414, 'frontlines': 21415, 'musk’s': 21416, 'pennsylvania': 21417, \"'deliriously\": 21418, 'severed': 21419, 'alarmed': 21420, 'fiendishly': 21421, 'brooker': 21422, 'solidify': 21423, 'tierra': 21424, 'fuego': 21425, 'socratic': 21426, 'transatlantic': 21427, 'nesta’s': 21428, 'impersonating': 21429, 'pretentious': 21430, 'crackle': 21431, 'motorbikes': 21432, '1910s': 21433, 'ducati': 21434, 'davidson': 21435, 'hinting': 21436, 'redbook': 21437, 'popsugar': 21438, '“no': 21439, 'heartwrenching': 21440, 'criticizes': 21441, 'populist': 21442, 'jan': 21443, 'wrestle': 21444, 'hatching': 21445, 'reassurance': 21446, 'henry’s': 21447, 'philippe': 21448, 'artbook': 21449, 'pupils': 21450, 'barrel': 21451, \"pankhurst's\": 21452, 'pankhurst': 21453, 'qiu': 21454, 'boudicca': 21455, 'pocahontas': 21456, 'inayat': 21457, 'tereshkova': 21458, 'sayyida': 21459, 'hurra': 21460, 'hatshepsut': 21461, 'lalita': 21462, 'bluish': 21463, 'sandal': 21464, 'shastri’s': 21465, 'kgb’s': 21466, 'cia’s': 21467, 'anuj': 21468, 'dhar': 21469, 'dhar’s': 21470, 'declassification': 21471, 'kalpana': 21472, 'warsaw': 21473, 'fists': 21474, 'candlelight': 21475, 'yamini': 21476, 'revolts': 21477, 'sobbing': 21478, 'nayudu': 21479, 'palwankar': 21480, 'baloo': 21481, 'trickiest': 21482, 'constructions': 21483, 'bibliophiles': 21484, 'eugenie': 21485, 'cherish': 21486, 'misty': 21487, \"'dr\": 21488, 'vocalists': 21489, 'banaras': 21490, 'delved': 21491, 'vd': 21492, 'emeritus': 21493, 'assisted': 21494, 'vidya': 21495, 'shree': 21496, 'vaidya': 21497, 'backlashes': 21498, \"teenager's\": 21499, \"opencv's\": 21500, 'starz': 21501, 'gabaldon’s': 21502, 'voyager': 21503, '“history': 21504, '”—new': 21505, 'confirm': 21506, 'cincinnati': 21507, 'raptures': 21508, 'steel’s': 21509, 'browning': 21510, 'immerses': 21511, 'stephanie': 21512, '‘riveting’': 21513, '‘stupendous’': 21514, '‘brilliant’': 21515, 'averted': 21516, '‘filled': 21517, 'pacifism': 21518, 'cows': 21519, 'giraffes': 21520, 'hammond': 21521, 'presenters': 21522, 'cobol': 21523, 'tales’': 21524, 'stormborn': 21525, 'bayern': 21526, 'confidential': 21527, 'enfant': 21528, 'resonant': 21529, 'adorned': 21530, 'consumes': 21531, 'stripping': 21532, 'mobs': 21533, 'delaney': 21534, \"everest's\": 21535, 'col': 21536, 'sheltering': 21537, \"learner's\": 21538, 'csikszentmihalyi': 21539, 'astray': 21540, 'whim': 21541, 'domineering': 21542, 'thump': 21543, 'plasterers': 21544, 'joiners': 21545, 'butchers': 21546, 'trampoline': 21547, 'grammarians': 21548, 'mispronounce': 21549, 'relied': 21550, 'lexicographer': 21551, 'etymologist': 21552, 'squalid': 21553, 'swaths': 21554, 'flashpoint': 21555, '111': 21556, 'palestinian': 21557, 'immersing': 21558, 'fluidly': 21559, 'schoolchildren': 21560, 'sheikhs': 21561, 'thrusts': 21562, 'tinseltown': 21563, '0500': 21564, '0522': 21565, 'airports': 21566, 'workplaces': 21567, 'bedtime': 21568, 'ssl': 21569, 'pki': 21570, 'hashes': 21571, 'kerberos': 21572, 'esp': 21573, 'curves': 21574, 'cristo': 21575, 'providence': 21576, \"dumas'\": 21577, 'embodied': 21578, 'implication': 21579, 'chapman': 21580, \"newlywed's\": 21581, 'glacier': 21582, 'excavation': 21583, \"football'\": 21584, 'malabar': 21585, 'tradesmen': 21586, 'devonshire': 21587, 'stepfather’s': 21588, 'harden': 21589, 'celibate': 21590, 'scoundrel': 21591, 'stammering': 21592, 'viscount': 21593, 'berserk': 21594, 'amassing': 21595, 'mcdermott': 21596, 'simeon': 21597, 'friends’': 21598, 'bumping': 21599, 'persuading': 21600, 'byword': 21601, 'absurdist': 21602, '“regrow”': 21603, 'osteoporosis': 21604, '“hitting”': 21605, 'detoxify': 21606, 'nourish': 21607, 'cultivating': 21608, 'nei': 21609, 'mantak': 21610, 'chia': 21611, 'revives': 21612, 'shukla': 21613, 'dismayed': 21614, '‘essential': 21615, 'ptsd': 21616, 'esl': 21617, 'oman': 21618, 'qualifications': 21619, 'emailing': 21620, 'exploratory': 21621, '“sean': 21622, 'howe’s': 21623, 'weirdoes': 21624, 'world…that': 21625, 'frosting': 21626, '—jonathan': 21627, 'forayed': 21628, 'violated': 21629, 'neoliberalism': 21630, 'tablebases': 21631, 'enjoyably': 21632, 'mohenjodaro': 21633, 'layla': 21634, 'fuller': 21635, 'leech': 21636, 'stoker': 21637, 'dracula': 21638, 'goís': 21639, 'runtime': 21640, 'queuing': 21641, 'capitol': 21642, 'withers': 21643, 'step—writing': 21644, 'webcomics—like': 21645, 'thompson—lend': 21646, '“pro': 21647, 'tips”': 21648, 'oxygenator': 21649, 'suffocate': 21650, 'reclaimer': 21651, 'hab': 21652, 'starve': 21653, 'yeah': 21654, \"weir's\": 21655, 'artemis': 21656, 'heaves': 21657, 'hedström': 21658, 'falck': 21659, 'fjällbacka': 21660, 'bishop': 21661, 'anitha': 21662, 'eroding': 21663, 'firstly': 21664, 'secondly': 21665, 'innovated': 21666, 'marble': 21667, 'yamuna': 21668, 'stead': 21669, 'americanah': 21670, 'tedx': 21671, 'tinged': 21672, 'pipal': 21673, \"eater'\": 21674, 'pond': 21675, 'yammer': 21676, 'reef': 21677, \"annexe'\": 21678, 'newbie': 21679, 'optimist': 21680, 'nicholls': 21681, \"urquhart's\": 21682, 'schemed': 21683, 'tiring': 21684, 'everyman': 21685, 'apology': 21686, 'paralyzing': 21687, 'interacts': 21688, 'bahadur’s': 21689, 'indrani': 21690, 'cloistered': 21691, 'patty': 21692, 'bye': 21693, 'trenchard': 21694, 'unknowingly': 21695, 'dart': 21696, 'fullness': 21697, 'contouring': 21698, 'plunkett': 21699, 'engraved': 21700, 'atom': 21701, 'conceit': 21702, 'rashtriya': 21703, 'swayamsevak': 21704, 'mukhopadhyay': 21705, 'sarsanghchalak': 21706, 'protesting': 21707, 'nehru’s': 21708, '‘are': 21709, 'upadhyaya': 21710, 'propounded': 21711, 'thackeray': 21712, 'india—and': 21713, 'ifurious': 21714, 'cep’s': 21715, 'grann': 21716, \"alabama's\": 21717, 'reverend’s': 21718, 'mailhot': 21719, 'forwarding': 21720, 'chabon': 21721, 'aloof': 21722, 'artistically': 21723, 'escapist': 21724, 'quaint': 21725, \"thompson's\": 21726, 'hewn': 21727, 'rigorously': 21728, 'engulfing': 21729, 'miners': 21730, 'shanty': 21731, 'bushes': 21732, 'weightlifters': 21733, 'continental': 21734, 'shahid': 21735, 'hammering': 21736, 'scooping': 21737, 't20s': 21738, 'googlies': 21739, \"'boom\": 21740, \"boom'\": 21741, 'portability': 21742, 'inferiority': 21743, 'greyhound': 21744, 'wrinkles': 21745, 'joie': 21746, 'pryce': 21747, 'penetrate': 21748, 'entice': 21749, 'tightens': 21750, '‘focused': 21751, 'sinaloa': 21752, 'adán': 21753, 'barrera': 21754, 'dea': 21755, 'scourging': 21756, 'winslow': 21757, 'narcos': 21758, 'spacecraft': 21759, 'skeptics': 21760, 'batteries': 21761, 'curates': 21762, \"'word\": 21763, 'vanessa': 21764, 'emmeline': 21765, '“alpha': 21766, 'male”': 21767, 'to“zero': 21768, \"'system'\": 21769, \"'magic\": 21770, 'jester': 21771, 'deva': 21772, 'raya': 21773, 'brigands': 21774, 'assorted': 21775, 'timelessness': 21776, 'incalculable': 21777, 'endowed': 21778, 'invasions': 21779, 'ignorant': 21780, 'ericsson’s': 21781, 'moonwalking': 21782, 'conservationists': 21783, \"harm's\": 21784, 'whittier': 21785, 'cheerfulness': 21786, 'normandy': 21787, 'jardir’s': 21788, 'positing': 21789, 'shadowfever': 21790, 'o’malley': 21791, 'i’d': 21792, 'unseelie': 21793, 'tableaux': 21794, 'ryodan': 21795, 'fangs': 21796, 'moning’s': 21797, '“moning': 21798, 'heck': 21799, 'nonstop': 21800, 'threaded': 21801, '3a': 21802, \"learners'\": 21803, 'micekings': 21804, \"village's\": 21805, 'stiltonord': 21806, 'papyrus': 21807, 'deyre': 21808, 'cost…': 21809, 'behaved': 21810, 'druckerman': 21811, 'sipped': 21812, 'moneyball': 21813, 'oakland': 21814, \"a's\": 21815, 'rabbits': 21816, 'finnigan': 21817, 'bedside': 21818, 'gail': 21819, \"compelling'\": 21820, \"'terrific\": 21821, \"character'\": 21822, 'serpent': 21823, 'malorie': 21824, 'millwood': 21825, \"adventure'\": 21826, \"claudius's\": 21827, 'optio': 21828, \"cartoonists'\": 21829, 'bibhuti': 21830, 'bandopadhyay': 21831, 'chhabi': 21832, 'biswas': 21833, 'chunibala': 21834, 'indir': 21835, 'thakrun': 21836, 'bishay': 21837, 'chalachitra': 21838, 'morstan': 21839, '1891': 21840, 'iconography': 21841, 'sculpture': 21842, 'deities': 21843, 'propagation': 21844, 'variants': 21845, 'delineation': 21846, 'kalidasa': 21847, 'laymen': 21848, 'lsd': 21849, 'thoreau': 21850, 'healers': 21851, 'organism': 21852, 'counters': 21853, 'fl': 21854, 'autistic': 21855, 'suma': 21856, 'nosy': 21857, '‘great’': 21858, 'flabbergasted': 21859, 'belittle': 21860, 'unpacked': 21861, '91': 21862, 'dreads': 21863, 'plantation': 21864, 'ngo': 21865, 'keri': 21866, 'alvi': 21867, 'waheeda': 21868, 'bestseller—now': 21869, 'discovered—a': 21870, '“network': 21871, 'intelligence”': 21872, 'itself—forms': 21873, '“quantum': 21874, 'alzheimer’s': 21875, '”—dr': 21876, 'rudolph': 21877, 'tanzi': 21878, '“deepak': 21879, 'girded': 21880, 'chopra’s': 21881, '”—lisa': 21882, 'user’s': 21883, 'rossi': 21884, 'márquez': 21885, 'mph': 21886, 'mediterranean': 21887, 'cabaret': 21888, 'scrawled': 21889, 'telegrams': 21890, 'nacional': 21891, 'cementing': 21892, \"liverpool's\": 21893, 'mottled': 21894, '187': 21895, 'interpol': 21896, 'suave': 21897, 'weisz': 21898, 'saoirse': 21899, 'ronan': 21900, 'monk’s': 21901, 'rinpoche’s': 21902, 'helen’s': 21903, 'pinto’s': 21904, 'parentage': 21905, 'grisham’s': 21906, 'phelan': 21907, 'disinheritance': 21908, 'infuriates': 21909, 'abuser': 21910, \"o'riley\": 21911, 'lowry': 21912, 'pahalgam': 21913, 'nirbhay': 21914, 'subroto': 21915, 'duology': 21916, 'intro': 21917, 'catalyzed': 21918, 'ascend': 21919, 'badassery': 21920, 'buddy': 21921, 'haddish': 21922, 'sidesplitting': 21923, 'mascot': 21924, 'accommodate': 21925, 'transmit': 21926, 'schoolgirl': 21927, 'ax': 21928, 'unforgettably': 21929, 'sumptuously': 21930, 'roach': 21931, 'dads': 21932, 'bodywork': 21933, 'thinner': 21934, \"experience'\": 21935, 'deidre': 21936, \"'pierce\": 21937, 'ender': 21938, 'katniss': 21939, \"darrow'\": 21940, 'sigler': 21941, 'helldiver': 21942, 'pledges': 21943, 'procures': 21944, 'confluence': 21945, \"anika's\": 21946, 'gambits': 21947, 'baliah': 21948, 'inhumanity': 21949, \"max's\": 21950, 'approve': 21951, 'creditors': 21952, 'trask': 21953, 'unrecognisable': 21954, 'pugh': 21955, 'fitzpatrick': 21956, 'tabular': 21957, 'praeger': 21958, 'petridis': 21959, 'profiled': 21960, 'hospitality': 21961, 'cruso': 21962, 'truthfully': 21963, 'intricacy': 21964, 'whaling': 21965, 'handicapped': 21966, 'adventurer’s': 21967, 'whaler’s': 21968, 'complimented': 21969, 'impressionists': 21970, \"'glorious\": 21971, \"'unknown\": 21972, \"soldier'\": 21973, 'reopening': 21974, 'kincaid': 21975, 'ached': 21976, 'resisted': 21977, 'attribute': 21978, 'disconnected': 21979, 'maupassant': 21980, 'crucifixion': 21981, '‘to': 21982, 'purloining': 21983, 'totleigh': 21984, 'spode': 21985, 'scrupulously': 21986, 'shamings': 21987, 'transgression': 21988, 'outrage': 21989, 'jeered': 21990, 'demonized': 21991, 'shaming': 21992, 'ruining': 21993, 'downtrodden': 21994, \"fishermen's\": 21995, 'auditoriums': 21996, 'moulded': 21997, 'kerela': 21998, 'unsurprisingly': 21999, 'biograpy': 22000, 'complain': 22001, 'joyfully': 22002, 'holistically': 22003, 'employability': 22004, 'ipython': 22005, 'jupyter': 22006, 'wes': 22007, 'matplotlib': 22008, \"tenzing's\": 22009, 'jamling': 22010, 'collided': 22011, \"'amma\": 22012, 'shivas': 22013, \"hanuman's\": 22014, 'heres': 22015, 'reacheda': 22016, 'liege': 22017, 'ravanaand': 22018, 'uprooted': 22019, 'ramas': 22020, 'uswhat': 22021, 'pies': 22022, 'exported': 22023, '185': 22024, 'boardrooms': 22025, 'gill': 22026, 'neurology': 22027, 'catalyze': 22028, 'nourished': 22029, 'neighboring': 22030, 'islamabad': 22031, 'doorbell': 22032, 'yiddish': 22033, 'lexicon': 22034, 'rituparna': 22035, 'swindled': 22036, 'axios': 22037, \"pennsylvania's\": 22038, 'jho': 22039, 'gall': 22040, 'siphoned': 22041, 'champagne': 22042, 'dubs': 22043, 'homing': 22044, 'quirkiest': 22045, 'idiotic': 22046, 'minis': 22047, 'enright': 22048, 'crabtree': 22049, 'reformers': 22050, 'wrest': 22051, \"club's\": 22052, \"footballers'\": 22053, 'helmets': 22054, 'illogical': 22055, 'snarky': 22056, 'beams': 22057, 'hadoopís': 22058, 'datastores': 22059, 'saboteurs': 22060, 'yuuki': 22061, \"paper's\": 22062, 'padgitt': 22063, 'defendant': 22064, 'jurors': 22065, 'paroled': 22066, 'jailed': 22067, \"'every\": 22068, 'cleverest': 22069, 'stationed': 22070, 'wessex': 22071, 'imbued': 22072, '1874': 22073, 'restores': 22074, 'morgan’s': 22075, 'permeate': 22076, 'masterminded': 22077, 'rapes': 22078, 'splintered': 22079, 'c11': 22080, 'directives': 22081, 'unpacks': 22082, 'malaga': 22083, \"nandini's\": 22084, 'unashamed': 22085, 'uneventful': 22086, \"annabelle's\": 22087, \"hell's\": 22088, 'kerbouchard': 22089, 'steppes': 22090, 'treacheries': 22091, 'princess’s': 22092, 'l’amour’s': 22093, 'baileys': 22094, 'ondaatje': 22095, 'glows': 22096, \"mastery'\": 22097, 'mcewan': 22098, 'leyson': 22099, 'sadism': 22100, 'amon': 22101, \"leyson's\": 22102, \"schindler's\": 22103, 'rigours': 22104, 'torturous': 22105, 'frenetic': 22106, 'exercising': 22107, \"'most\": 22108, 'helpfully': 22109, 'enthusiasms': 22110, 'posses': 22111, \"ever'\": 22112, 'fiat’': 22113, 'jat': 22114, 'patthar': 22115, 'gaon': 22116, 'desh': 22117, 'yaadon': 22118, 'baaraat': 22119, 'pratiggya': 22120, 'dharam': 22121, 'satyakam': 22122, 'betaab': 22123, 'ghayal': 22124, 'sincerely': 22125, 'slipcased': 22126, \"carlin's\": 22127, 'hyperion': 22128, 'pork': 22129, 'napalm': 22130, 'veins': 22131, \"asylum's\": 22132, 'frothing': 22133, 'strenght': 22134, \"actor's\": 22135, 'festive': 22136, 'snowboarding': 22137, 'melts': 22138, 'pandavas': 22139, 'kauravas': 22140, 'interpenetration': 22141, 'ulster': 22142, 'constabulary': 22143, 'dirtiest': 22144, 'vary': 22145, 'xl': 22146, 'xs': 22147, 'paddy’s': 22148, 'royals': 22149, \"'model\": 22150, \"papers'\": 22151, 'ugc': 22152, '”―osho': 22153, 'dominated―not': 22154, 'femi': 22155, 'gloves': 22156, \"this'll\": 22157, \"ayoola's\": 22158, 'menfolk': 22159, 'daguerreotype': 22160, 'portait': 22161, 'unhinged': 22162, 'somethings': 22163, 'saloon': 22164, 'gypsies': 22165, 'bearded': 22166, 'respectful': 22167, 'compromising': 22168, 'bashir': 22169, 'nightfall': 22170, 'volition': 22171, 'outward': 22172, 'grokking': 22173, 'epub': 22174, \"island's\": 22175, \"harris's\": 22176, 'detachment': 22177, 'undivided': 22178, 'superintelligence': 22179, 'tripwires': 22180, 'indirect': 22181, 'mcraney': 22182, 'subsumed': 22183, 'arousal': 22184, \"'staggeringly\": 22185, \"compassionate'\": 22186, 'magal': 22187, 'pickle': 22188, 'patent': 22189, 'rambunctious': 22190, 'spoofs': 22191, 'rakesh': 22192, 'remotely': 22193, \"'extraordinary\": 22194, 'klay': 22195, 'redeployment': 22196, \"hadi's\": 22197, 'whatsitsname': 22198, 'blackly': 22199, 'thumping': 22200, \"chest'\": 22201, \"breathless'\": 22202, \"'cancel\": 22203, 'yards': 22204, \"freeman's\": 22205, 'montaged': 22206, 'viewfinder': 22207, 'university’s': 22208, 'pancreatic': 22209, \"taylor's\": 22210, 'deadlock': 22211, 'amadeus': 22212, 'sinks': 22213, 'mona': 22214, \"english'\": 22215, 'hermit': 22216, \"story's\": 22217, 'weston': 22218, 'rehearsals': 22219, 'divulge': 22220, 'defection': 22221, 'bellies': 22222, 'departed': 22223, 'zoroastrian': 22224, 'preta': 22225, 'jagannath': 22226, 'pakwaan': 22227, 'angsty': 22228, 'tangential': 22229, 'hijinks': 22230, 'nailbiting': 22231, '240': 22232, \"kailas's\": 22233, 'interrogate': 22234, 'andrés': 22235, 'masia': 22236, 'barca': 22237, 'propelling': 22238, 'bras’': 22239, 'issue—and': 22240, 'existence—one': 22241, 'fullest—because': 22242, 'fabio': 22243, 'feather': 22244, 'pressfield': 22245, 'day’': 22246, 'bled': 22247, 'applauded': 22248, 'punctuated': 22249, 'brendan': 22250, 'mtv': 22251, 'ikea': 22252, 'audition': 22253, 'barbecue': 22254, 'sprechen': 22255, 'sie': 22256, \"hugo's\": 22257, 'corrine': 22258, 'mirage': 22259, \"corrine's\": 22260, 'heart…': 22261, 'lynsay': 22262, 'maclean': 22263, 'nihilism': 22264, 'philosopher’s': 22265, 'controversially': 22266, \"viewer's\": 22267, 'slr': 22268, \"gatcum's\": 22269, 'garvey': 22270, '’marry': 22271, \"'happy\": 22272, 'griping': 22273, \"'’marry\": 22274, 'stimulation': 22275, 'appreciable': 22276, 'lovecraft': 22277, 'duvet': 22278, 'writers—their': 22279, 'sink': 22280, 'cobblestoned': 22281, 'luxembourg': 22282, 'titiksha': 22283, 'navoneel': 22284, 'ringo': 22285, 'popularize': 22286, \"all'\": 22287, \"'does\": 22288, \"'37\": 22289, \"son'\": 22290, 'cakes': 22291, \"cherry's\": 22292, 'patents': 22293, 'massacred': 22294, 'obsessives': 22295, 'misfit': 22296, 'faire': 22297, \"tuchman's\": 22298, 'tuchman': 22299, \"'magnificent'\": 22300, 'pyjama': 22301, 'freelancing': 22302, \"eliot's\": 22303, 'eliot’s': 22304, 'railroads': 22305, 'town’s': 22306, 'raffles': 22307, 'assembles': 22308, 'hearst': 22309, \"yeti's\": 22310, 'betwixt': 22311, 'bowels': 22312, 'surender': 22313, 'ahimsa': 22314, 'natsuru': 22315, 'upend': 22316, 'unopened': 22317, 'overlooking': 22318, 'bosphorus': 22319, 'secret—she': 22320, 'cedric': 22321, 'atatala': 22322, 'tala': 22323, 'pancha': 22324, 'nadai': 22325, 'pada': 22326, 'janyaraga': 22327, 'kramas': 22328, 'choicest': 22329, 'varanams': 22330, 'sangeeta': 22331, 'paadam': 22332, 'carnatic': 22333, \"fanon's\": 22334, 'colonization': 22335, 'inflicted': 22336, \"clanton's\": 22337, \"ambassador's\": 22338, 'despatches': 22339, 'broadcaster': 22340, \"4's\": 22341, 'germany’s': 22342, 'scrutinizing': 22343, 'gerhard’s': 22344, 'idiot’s': 22345, 'curtains': 22346, '‘painfully': 22347, 'funniness': 22348, '97': 22349, \"kay's\": 22350, 'nhs': 22351, \"daisy's\": 22352, 'bouncy': 22353, 'lied': 22354, \"'suspense\": 22355, \"end'\": 22356, 'nuala': 22357, 'ellwood': 22358, 'jensen': 22359, 'marshals': 22360, \"corry's\": 22361, \"go'\": 22362, 'metasploit': 22363, 'wireshark': 22364, 'pentesting': 22365, 'pentest': 22366, 'carreidas': 22367, 'tapioca': 22368, 'carnival': 22369, 'galileo': 22370, 'packer': 22371, \"'world\": 22372, 'chappell': 22373, 'nebula': 22374, 'neuromancer': 22375, 'banishing': 22376, 'unthinkably': 22377, 'humankind’s': 22378, '1s': 22379, 'partying': 22380, 'weirdest': 22381, 'converts': 22382, 'benazir': 22383, 'bhutto': 22384, 'khar': 22385, 'pathologically': 22386, 'pyongyang': 22387, 'shenzhen': 22388, 'titular': 22389, 'manhunts': 22390, 'susceptible': 22391, 'kobo': 22392, 'tenuous': 22393, 'rifleman': 22394, 'bizarrely': 22395, 'hustle': 22396, 'yorke': 22397, 'fellowes': 22398, \"'terrifyingly\": 22399, \"argument'\": 22400, \"trade'\": 22401, 'bowker': 22402, 'blackpool': 22403, 'ernie': 22404, 'hellogiggles': 22405, 'detour': 22406, 'dipping': 22407, 'rekha': 22408, 'madoff': 22409, 'retouch': 22410, 'calligraphers': 22411, 'fidel': 22412, 'soderbergh': 22413, '‘che': 22414, 'goons': 22415, 'outing': 22416, \"'these\": 22417, 'mccall': 22418, '“most': 22419, 'tobacco': 22420, 'book’': 22421, 'mantel': 22422, 'lives’': 22423, 'supercharge': 22424, 'visualizing': 22425, 'lemonade': 22426, 'succumbs': 22427, 'lucia': 22428, 'procedural': 22429, 'involuntary': 22430, \"maggie's\": 22431, 'skewer': 22432, 'stacking': 22433, \"lenin's\": 22434, 'subvert': 22435, 'proletarian': 22436, 'offspring': 22437, 'ipod': 22438, 'interfere': 22439, 'issacson': 22440, \"zita's\": 22441, 'esque': 22442, 'marx’s': 22443, 'tribesmen': 22444, 'holographic': 22445, 'congressman': 22446, 'bri': 22447, 'med': 22448, 'internship': 22449, 'bluff': 22450, 'overworked': 22451, \"'special\": 22452, \"edition'\": 22453, 'diagnoses': 22454, 'unforeseeable': 22455, 'enveloped': 22456, 'gayatri': 22457, 'hoyden': 22458, \"'pink\": 22459, 'schein': 22460, 'differs': 22461, 'rodney': 22462, 'caradec': 22463, 'grasslands': 22464, \"'don’t\": 22465, 'weight’': 22466, 'calories': 22467, 'rhi': 22468, 'kodaikanal': 22469, 'dawned': 22470, 'springboks': 22471, \"zealand's\": 22472, 'reprieve': 22473, 'ixia': 22474, \"butterfly's\": 22475, 'emergencies—from': 22476, 'yesterday’s': 22477, 'responder': 22478, 'caballero': 22479, 'toppling': 22480, 'interventions': 22481, 'rhimes': 22482, 'nerdy': 22483, 'shu': 22484, 'malo': 22485, 'doerr': 22486, 'apparel': 22487, 'herbs': 22488, 'comprehensiveness': 22489, 'ramsar': 22490, 'affluent': 22491, 'acharya': 22492, \"roam's\": 22493, 'licensed': 22494, 'cumulative': 22495, 'denim': 22496, \"television's\": 22497, 'galvanize': 22498, 'bulb': 22499, \"storyteller's\": 22500, 'annotation': 22501, 'behavioral': 22502, 'scanner': 22503, 'rationality': 22504, 'selfishness': 22505, 'regulation': 22506, 'compartment': 22507, 'frye': 22508, 'macintosh': 22509, 'ery': 22510, 'cereal': 22511, 'drifted': 22512, 'paycheques': 22513, 'overblown': 22514, 'sepp': 22515, 'blatter': 22516, 'bolitar': 22517, 'crystalline': 22518, 'rodriguez': 22519, 'times’': 22520, 'knausgaard': 22521, 'freighted': 22522, 'dyamiting': 22523, 'comradeship': 22524, \"salander's\": 22525, 'sensuously': 22526, 'manifestos': 22527, 'treat’': 22528, 'kedarnath': 22529, \"napoleon's\": 22530, 'tintoretto': 22531, 'ir': 22532, 'wankhede': 22533, 'chants': 22534, \"d'souza\": 22535, 'hysteria': 22536, 'outspokenness': 22537, 'puncture': 22538, 'samanth': 22539, \"sartre's\": 22540, \"firm's\": 22541, 'accusation': 22542, 'affords': 22543, 'traverses': 22544, 'enthused': 22545, 'foils': 22546, 'explosions': 22547, 'barnstorming': 22548, 'souped': 22549, 'insisting': 22550, \"eldritch's\": 22551, 'virginal': 22552, 'filial': 22553, 'obsessional': 22554, 'love…': 22555, 'hamster': 22556, 'argentina': 22557, 'sheffield': 22558, 'nayak': 22559, 'kawakami': 22560, 'yoshimoto': 22561, 'cramped': 22562, 'macleod': 22563, 'diluting': 22564, 'glasgow': 22565, \"azure's\": 22566, \"microsoft's\": 22567, 'modify': 22568, 'powershell': 22569, 'heuristic': 22570, 'targeting': 22571, 'dictates': 22572, 'isis’s': 22573, 'astor': 22574, 'grendel': 22575, 'allegedly': 22576, \"grendel's\": 22577, 'submerged': 22578, 'gupta’s': 22579, 'habib': 22580, 'tanvir’s': 22581, 'evades': 22582, 'oedipus': 22583, \"maharaja's\": 22584, 'maharaj': 22585, 'maughan': 22586, 'pastels': 22587, 'winter’s': 22588, 'dennett': 22589, 'met—from': 22590, 'modernization': 22591, 'subtext': 22592, 'faultlines': 22593, 'tuttle': 22594, 'knuckle': 22595, 'cyclist': 22596, 'recreate': 22597, \"'nothing\": 22598, 'sarmad': 22599, 'mehar': 22600, 'raf': 22601, 'goldfinger': 22602, 'levine': 22603, 'counterattacks': 22604, 'agreeing': 22605, 'polls': 22606, 'ministership': 22607, 'rifkin': 22608, 'usher': 22609, 'hierarchical': 22610, 'traction': 22611, 'connecticut': 22612, 'recruiters': 22613, 'jihadist': 22614, 'nigh': 22615, 'polly’s': 22616, 'brightens': 22617, 'orators': 22618, 'rouse': 22619, 'perish': 22620, 'boils': 22621, '“we': 22622, \"jarrett's\": 22623, 'leia': 22624, 'videogame': 22625, '1789': 22626, 'cobblestone': 22627, 'arno': 22628, 'élise': 22629, \"waugh's\": 22630, 'reignited': 22631, 'naming': 22632, 'unauthorised': 22633, 'shaky': 22634, 'kook': 22635, 'eleventendulkar': 22636, 'vibhushan': 22637, 'portals': 22638, \"sensation'\": 22639, \"mackintosh'\": 22640, 'winkler': 22641, 'decapitated': 22642, 'unstoppably': 22643, 'inundated': 22644, 'handyman': 22645, 'out…': 22646, 'being—and': 22647, 'writing—funny': 22648, 'sitcom': 22649, 'greeting': 22650, 'professionals—from': 22651, 'brillstein': 22652, 'titus': 22653, 'fields—stand': 22654, 'essays—and': 22655, '“authentic”': 22656, 'level—making': 22657, 'money—by': 22658, 'banning': 22659, 'exports': 22660, '2050': 22661, 'detached': 22662, 'splinter': 22663, 'chromeria': 22664, 'teia': 22665, 'karris': 22666, 'ascendant': 22667, \"'brent\": 22668, \"work'\": 22669, 'ostensibly': 22670, 'khasak': 22671, 'elders': 22672, 'intrude': 22673, 'karma': 22674, 'portent': 22675, 'scorned': 22676, 'astray…in': 22677, '1860': 22678, 'tenants': 22679, \"gauls'\": 22680, 'liquor': 22681, 'baratheon': 22682, 'catelyn': 22683, 'unnatural': 22684, 'relegated': 22685, 'arryn': 22686, 'cersei': 22687, 'cyrus': 22688, 'silliness': 22689, 'indefatigable': 22690, 'manly': 22691, 'epoque': 22692, 'libellus': 22693, 'foresees': 22694, 'snag': 22695, 'armorica': 22696, 'newsmonger': 22697, 'confoundtheirpolitix': 22698, 'goscinny': 22699, 'uderzo': 22700, 'ferri': 22701, 'detention': 22702, \"knight's\": 22703, 'cavilleri': 22704, 'expresses': 22705, 'normans': 22706, 'toad': 22707, 'mayeki': 22708, 'township': 22709, 'missiles': 22710, 'voluntarily': 22711, 'baroness': 22712, 'orissa': 22713, 'naxalites': 22714, 'hurdle': 22715, 'batched': 22716, 'proceed': 22717, 'bough': 22718, 'captained': 22719, 'seedy': 22720, 'exceedingly': 22721, 'perverse': 22722, 'bellow': 22723, 'militarized': 22724, 'attaché': 22725, 'uv': 22726, 'infra': 22727, 'confrontations': 22728, 'woes': 22729, 'stallybrass': 22730, 'kermode': 22731, \"'that\": 22732, '1879': 22733, 'dias': 22734, 'mosaic': 22735, 'read…': 22736, 'page’': 22737, 'down…': 22738, \"bird's\": 22739, 'harlow': 22740, 'vaidyanathan': 22741, '84': 22742, 'favors': 22743, 'vere': 22744, 'martha’s': 22745, 'vineyard': 22746, 'watertight': 22747, 'veres': 22748, 'raconteur': 22749, 'kumaon—corbett’s': 22750, 'stomping': 22751, 'mokameh': 22752, 'ghat': 22753, '‘kunwar': 22754, 'singh’': 22755, 'kaladhungi': 22756, '‘putli': 22757, 'kalwa’': 22758, 'bullock': 22759, 'uncle’s': 22760, 'prowls': 22761, '‘adventures': 22762, 'magog’': 22763, 'scruffy': 22764, \"akbar's\": 22765, 'disinterested': 22766, 'parvati': 22767, 'fielder': 22768, \"'hugely\": 22769, \"yet'\": 22770, 'excessive…but': 22771, 'dangling': 22772, 'pouty': 22773, '“slash”': 22774, 'bowie’s': 22775, 'stoke': 22776, 'iggy': 22777, 'bmx': 22778, 'rose…and': 22779, 'n’': 22780, 'builiding': 22781, 'imploded': 22782, '–from': 22783, 'demo': 22784, 'cupertino': 22785, 'decisiveness': 22786, 'bodine': 22787, \"bodine's\": 22788, \"everything's\": 22789, 'haider': 22790, 'affiliate': 22791, \"campfire's\": 22792, 'servicemen': 22793, \"'ass\": 22794, \"fatale'\": 22795, \"'60's\": 22796, \"sybex's\": 22797, 'buffs': 22798, 'unused': 22799, \"berry's\": 22800, 'gretchen': 22801, 'rubin': 22802, 'epiphany': 22803, 'dehradun': 22804, 'reacquaints': 22805, 'pensive': 22806, 'stamp': 22807, 'gramophone': 22808, 'bazaars': 22809, 'broadcasting': 22810, 'reflexes': 22811, 'hockey': 22812, 'silas': 22813, 'prima': 22814, 'pursuer': 22815, 'panicked': 22816, 'overton': 22817, '“fresh': 22818, 'alamgir': 22819, '1658': 22820, '1707': 22821, 'hater': 22822, 'zealot': 22823, 'peddled': 22824, 'strove': 22825, 'truschke': 22826, 'james’': 22827, 'disciplined': 22828, 'penthouse': 22829, 'dispel': 22830, 'skips': 22831, 'loser': 22832, 'solver': 22833, 'durrell': 22834, 'bookworm': 22835, 'durrells': 22836, 'announcing': 22837, 'freight': 22838, 'wormholes': 22839, 'pierre': 22840, '1591': 22841, 'murat': 22842, '‘when': 22843, 'bipan': 22844, 'specimens': 22845, 'kara': 22846, 'doppelganger': 22847, 'crawler': 22848, 'cortana': 22849, '117': 22850, 'forerunners': 22851, 'reawakened': 22852, 'bouts': 22853, 'greeks': 22854, 'pythagoras': 22855, 'leibniz': 22856, 'hogan’s': 22857, 'mi5': 22858, 'preston': 22859, 'verify': 22860, 'berg': 22861, \"'don't\": 22862, 'referencing': 22863, 'ruggieri': 22864, 'repugnant': 22865, '1820s': 22866, 'charise': 22867, 'elopes': 22868, 'burleton': 22869, 'pier': 22870, 'westmoreland': 22871, 'crates': 22872, 'variation': 22873, 'dialect': 22874, 'connaître': 22875, 'bitwise': 22876, 'srivastava': 22877, 'msc': 22878, 'bully': 22879, 'shoko': 22880, 'hatbox': 22881, 'mailed': 22882, 'revisionist': 22883, 'mystically': 22884, 'navagrahas': 22885, \"malala's\": 22886, 'reconsider': 22887, 'lockhart': 22888, 'xii': 22889, 'reqest': 22890, 'meda': 22891, 'airframe': 22892, 'yore': 22893, 'chanced': 22894, 'freighter': 22895, \"'oceanography's\": 22896, \"delivers'\": 22897, 'coonts': 22898, 'brass—': 22899, '—conjuring': 22900, 'nahri’s': 22901, 'daevabad—and': 22902, 'ali’s': 22903, 'marid—the': 22904, 'spirits—have': 22905, \"daevabad's\": 22906, 'brews': 22907, 'daur': 22908, 'phir': 22909, 'hosted': 22910, 'dhruvi': 22911, 'kishore': 22912, 'weed': 22913, 'earthquakes': 22914, 'whirlpools': 22915, 'riffs': 22916, 'idealised': 22917, 'fragmentation': 22918, 'cynicism': 22919, 'sinek': 22920, 'eadweard': 22921, 'muybridge': 22922, 'sutras': 22923, 'attachment': 22924, 'ugh': 22925, 'pajamas': 22926, 'awkwardnesses': 22927, 'helbig': 22928, 'correll': 22929, 'arcane': 22930, 'philanthropic': 22931, 'capacities': 22932, 'endeavors': 22933, 'impatient': 22934, 'cripples': 22935, 'plummet': 22936, 'derry': 22937, 'coils': 22938, 'yachts': 22939, 'flats': 22940, 'flaming': 22941, 'shakespearean': 22942, 'sunburned': 22943, 'friendliest': 22944, 'driest': 22945, 'curiousity': 22946, 'harbors': 22947, 'crocodiles': 22948, 'riptides': 22949, 'extroverted': 22950, 'beaming': 22951, \"love's\": 22952, \"leav's\": 22953, 'errand': 22954, 'talons': 22955, 'mckinley': 22956, 'fetish': 22957, 'pepper': 22958, 'frederica': 22959, \"miriam's\": 22960, 'glistening': 22961, \"yoga's\": 22962, 'bhairva': 22963, 'chielf': 22964, 'whosemoralsarelastix': 22965, 'fairground': 22966, 'eleastic': 22967, 'chanel': 22968, 'catwalk': 22969, 'dissatisfied': 22970, '1327': 22971, 'chiselled': 22972, 'aforementioned': 22973, 'citizens…': 22974, 'simblet': 22975, 'speciality': 22976, 'aus': 22977, 'nocturnal': 22978, 'neuroscientist': 22979, 'hardwired': 22980, 'boyfriends': 22981, 'kinda': 22982, 'freaks': 22983, 'surveyors': 22984, 'racers': 22985, 'multicopter': 22986, 'correspondents': 22987, 'freelancer': 22988, 'confucius': 22989, 'fused': 22990, 'principals': 22991, \"'night\": 22992, \"probe'\": 22993, 'remission': 22994, 'convinces': 22995, \"gallwey's\": 22996, 'laura’s': 22997, 'underperform': 22998, \"teams'\": 22999, 'explainer': 23000, 'tectonic': 23001, 'heated': 23002, 'coauthors': 23003, 'them”': 23004, 'salesmanship': 23005, '1936': 23006, 'drèze': 23007, 'participatory': 23008, 'inequalities': 23009, 'privileges': 23010, 'signing': 23011, 'jairam': 23012, 'ramesh': 23013, 'amnesia': 23014, '1867': 23015, 'anova': 23016, 'insulted': 23017, 'vulgar': 23018, 'india\\x92s': 23019, 'khaled': 23020, 'snipers': 23021, 'kenan': 23022, 'dragan': 23023, 'cellist': 23024, 'toying': 23025, 'genki': 23026, 'dama': 23027, 'evidenced': 23028, 'déjà': 23029, 'vu': 23030, 'peters': 23031, 'glacial': 23032, 'considerably': 23033, 'havens': 23034, 'offshore': 23035, 'evaluates': 23036, 'enacting': 23037, 'ambit': 23038, 'llb': 23039, 'gipsy’s': 23040, 'acre': 23041, 'nonviolent': 23042, 'counterpoint': 23043, 'crunch': 23044, 'infections': 23045, 'nirmala': 23046, 'sitharaman': 23047, 'sania': 23048, 'freewheeling': 23049, 'chatty': 23050, 'langenscheidt': 23051, 'declension': 23052, 'lakshmibai': 23053, 'progressing': 23054, 'direct—indirect': 23055, '2400': 23056, 'rearrangement': 23057, 'fawley': 23058, 'elliott': 23059, 'prolix': 23060, 'soothsayer': 23061, 'prophesies': 23062, 'lagos': 23063, 'mound': 23064, 'lessen': 23065, 'supersized': 23066, 'collaborated': 23067, 'cogently': 23068, 'heroics': 23069, 'waging': 23070, 'indulges': 23071, 'sweethearts': 23072, 'envisage': 23073, 'tumor': 23074, '‘p': 23075, 'you’': 23076, 'on…': 23077, '‘light': 23078, 'unnamed': 23079, 'weighty': 23080, '‘throughout': 23081, 'characterisation': 23082, 'odour': 23083, 'pyne': 23084, 'deathbed': 23085, 'justifying': 23086, 'rushdie': 23087, 'judging': 23088, 'silks': 23089, 'jewellery': 23090, 'aremis': 23091, 'undamaged': 23092, 'peach': 23093, 'doses': 23094, 'doers': 23095, 'zeno': 23096, 'cleanthes': 23097, 'musonius': 23098, 'rufus': 23099, 'inquirer': 23100, 'dispatch': 23101, 'commandeered': 23102, 'god”': 23103, 'commemorates': 23104, 'bicentennial': 23105, 'photoshoots': 23106, 'characterise': 23107, 'devolved': 23108, 'cacofonix': 23109, 'gladiators': 23110, 'maximus': 23111, 'saveer': 23112, 'larman': 23113, 'ooa': 23114, 'iterations': 23115, 'chorus': 23116, \"seinfeld's\": 23117, 'seinlanguage': 23118, 'raisinettes': 23119, 'fans—and': 23120, '—remains': 23121, 'decency': 23122, \"robinson's\": 23123, 'exercises—and': 23124, 'passé': 23125, 'schoolers—it': 23126, 'neurologists': 23127, 'therapists': 23128, 'slower': 23129, 'typing—but': 23130, 'longhand': 23131, 'reintroduces': 23132, 'hegel': 23133, 'complicating': 23134, \"hegel's\": 23135, 'women’': 23136, \"others'\": 23137, '‘number': 23138, 'hooted': 23139, 'garrick': 23140, 'awaiting': 23141, 'graceling': 23142, 'nyx': 23143, 'mountaintop': 23144, 'hassaikai': 23145, 'sivanpalli': 23146, 'carmen': 23147, 'ladin': 23148, 'repairing': 23149, \"rubik's\": 23150, 'staggeringly': 23151, 'salomon': 23152, 'eduard': 23153, 'roschmann': 23154, 'testifying': 23155, 'campaigning': 23156, 'bribery': 23157, 'staircase': 23158, 'bathroom': 23159, 'mitko': 23160, 'hustler': 23161, 'firecracker': 23162, \"medium's\": 23163, 'invisibly': 23164, \"fermat's\": 23165, 'talkers': 23166, 'logisitical': 23167, 'kotter': 23168, 'mobilize': 23169, 'tipping': 23170, 'socks': 23171, 'torrent': 23172, \"school's\": 23173, 'pauline': 23174, 'venerable': 23175, 'virtualization': 23176, 'idiots”': 23177, \"movie's\": 23178, 'punching': 23179, 'farce': 23180, 'sergio': 23181, 'r2': 23182, 'erdnase': 23183, 'palming': 23184, 'peep': 23185, 'flap': 23186, 'sy': 23187, 'mangrove': 23188, 'people—swimming': 23189, 'woodcutters': 23190, 'cleveland': 23191, 'remarks': 23192, 'argus': 23193, 'jot': 23194, 'hunches': 23195, 'skating': 23196, 'gwen': 23197, 'amused': 23198, 'slated': 23199, 'bark': 23200, 'scrabble': 23201, \"nelson's\": 23202, 'scorecards': 23203, 'retiring': 23204, 'forgiven': 23205, 'abide': 23206, 'unbreakable': 23207, 'wracked': 23208, 'orrin': 23209, 'rustlers': 23210, 'tye': 23211, 'performant': 23212, 'alerting': 23213, 'kpis': 23214, '1546': 23215, 'ounce': 23216, 'straczynski': 23217, 'kryptonite': 23218, 'kent’s': 23219, 'writers’': 23220, 'illustrators’': 23221, '‘scott': 23222, 'taleb': 23223, 'aggression': 23224, 'fertility': 23225, 'ratnam’s': 23226, 'kamal': 23227, 'credentials': 23228, 'mahmoody': 23229, \"moody's\": 23230, 'mahtob': 23231, 'appalled': 23232, 'scenic': 23233, 'shahi': 23234, 'revolutionise': 23235, 'vigna': 23236, 'middlemen': 23237, 'alastair': 23238, 'versailles': 23239, 'ravishing': 23240, 'pomeroy': 23241, 'bobo': 23242, 'impractical': 23243, '510': 23244, 'surest': 23245, 'competence': 23246, 'mourn': 23247, \"sniper's\": 23248, 'bree': 23249, 'springs': 23250, 'diagrammed': 23251, 'chernev': 23252, 'rook': 23253, 'petrosian': 23254, 'luckiest': 23255, \"lies'\": 23256, \"'loved\": 23257, 'spinner': 23258, \"corleone's\": 23259, 'giuliano': 23260, '1909': 23261, 'decomposition': 23262, 'personas': 23263, 'projections': 23264, 'manali': 23265, 'multiply': 23266, 'lobster': 23267, 'devises': 23268, \"'adorable'\": 23269, \"heartbreaking'\": 23270, 'letterforms': 23271, 'freestyle': 23272, 'uppercase': 23273, 'step–by–step': 23274, 'imperialist': 23275, 'leftist': 23276, 'vanger': 23277, 'desi': 23278, 'bitten': 23279, 'chew': 23280, 'cronin': 23281, 'ouch': 23282, '‘hearts’': 23283, 'tanya': 23284, 'srishti': 23285, 'oceania—one': 23286, 'superstates': 23287, 'genial': 23288, 'cleverness': 23289, 'totalitarianism': 23290, 'stalinist': 23291, 'survivor’s': 23292, 'semester': 23293, 'airgun': 23294, 'abyssinia': 23295, 'penrod': 23296, '‘fiction’': 23297, 'equation': 23298, \"massacre'\": 23299, 'hartsfield': 23300, 'pawns': 23301, 'miro': 23302, 'outlawed': 23303, 'ramon': 23304, 'other’s': 23305, 'cajole': 23306, 'andolan': 23307, 'kajal': 23308, 'overdue': 23309, 'hunky': 23310, 'amorous': 23311, 'riverton': 23312, 'loeanneth': 23313, 'gleaming': 23314, 'enforced': 23315, 'appraising': 23316, 'hypotheses': 23317, 'circulated': 23318, 'gst': 23319, 'letters’': 23320, 'descending': 23321, 'dissolved': 23322, '‘annihilation': 23323, 'caste’': 23324, 'stratification': 23325, 'vis': 23326, 'slit': 23327, 'skirts': 23328, 'kailash': 23329, 'nrityagram': 23330, 'timepass': 23331, 'vicarious': 23332, 'natchez': 23333, \"penn's\": 23334, '80s': 23335, 'archiving': 23336, 'kannur': 23337, 'sickening': 23338, \"area's\": 23339, 'overriding': 23340, 'hofstadter': 23341, 'sleuths': 23342, 'clicking': 23343, 'hanaoka': 23344, 'togashi': 23345, 'yasuko’s': 23346, 'commotion': 23347, 'infuriated': 23348, 'assange': 23349, 'whistleblower': 23350, 'superwoman': 23351, 'bawse': 23352, 'exudes': 23353, \"lilly's\": 23354, 'escalators': 23355, 'decluttering': 23356, 'cks': 23357, 'nottle': 23358, 'instructed': 23359, 'baleful': 23360, \"'stinker'\": 23361, 'stiffy': 23362, 'byng': 23363, \"'james\": 23364, 'dhamija': 23365, \"raaj's\": 23366, 'vodafone': 23367, \"harper's\": 23368, 'debuts': 23369, 'maslin': 23370, 'rained': 23371, 'typescripts': 23372, '4c': 23373, 'states”': 23374, 'naacp': 23375, '“race': 23376, 'falsehood': 23377, 'men—bodies': 23378, 'coates’s': 23379, 'son—and': 23380, 'readers—the': 23381, 'bracingly': 23382, '“powerful': 23383, '“eloquent': 23384, 'ellison’s': 23385, '“brilliant': 23386, '“urgent': 23387, '”—vogue': 23388, '“titanic': 23389, 'digestible': 23390, 'dour': 23391, 'thematically': 23392, \"'uncanny'\": 23393, 'ants': 23394, 'knitting': 23395, 'exterminating': 23396, 'stints': 23397, 'alleviation': 23398, 'cab': 23399, 'disgruntled': 23400, 'emigre': 23401, '1868': 23402, '‘unwell’': 23403, 'killer…': 23404, 'crumpled': 23405, \"lewis's\": 23406, \"'hilarious'\": 23407, 'broadcasts': 23408, 'interstellar': 23409, 'kovacs': 23410, 'pigeons': 23411, 'udaipur': 23412, 'dehydrated': 23413, 'sutton’s': 23414, 'sutton': 23415, 'maria’s': 23416, 'ancillary': 23417, 'lika': 23418, 'blum': 23419, 'cruelly': 23420, 'farmed': 23421, 'taplin': 23422, 'thiel': 23423, 'subordinating': 23424, 'monoculture': 23425, 'mogadorian': 23426, 'populous': 23427, 'patronage': 23428, 'terrifies': 23429, 'feverishly': 23430, \"roget's\": 23431, 'prophets': 23432, 'byways': 23433, 'mythologizes': 23434, \"'lootenant'\": 23435, \"'toosday'\": 23436, 'kellogg': 23437, 'cornflakes': 23438, 'locking': 23439, 'deepa': 23440, 'objectivity': 23441, 'dolphin': 23442, 'antica': 23443, '7b': 23444, 'sweater': 23445, 'ichi': 23446, '4”': 23447, \"snow's\": 23448, '“longclaw': 23449, 'sheath': 23450, 'mohit': 23451, 'coretta': 23452, 'nonviolence': 23453, 'connie': 23454, 'incapacitated': 23455, 'iconoclast': 23456, 'exacts': 23457, 'puzo': 23458, 'alton': 23459, 'epidemics': 23460, 'pharmaceuticals': 23461, 'energizing': 23462, 'caper': 23463, 'bystanders': 23464, 'bender': 23465, 'ducks': 23466, 'capsizing': 23467, 'canoes': 23468, 'dictatorships': 23469, 'descriptionthis': 23470, 'solutionstype': 23471, 'qos': 23472, 'postman': 23473, 'unknowable': 23474, 'meera': 23475, 'nurturing': 23476, 'shuffles': 23477, '121': 23478, 'meryl': 23479, 'streep': 23480, \"'blame\": 23481, \"'blending\": 23482, \"wickedly'\": 23483, 'famouse': 23484, 'ori': 23485, 'popularized': 23486, 'fasting': 23487, 'insulin': 23488, 'locating': 23489, 'menakshi': 23490, 'kamala': 23491, 'overtaken': 23492, 'bandyopadhyay’s': 23493, 'delhi’s': 23494, 'sadiq': 23495, 'omar': 23496, 'birdsong': 23497, \"macgregor's\": 23498, 'gorge': 23499, \"researched'\": 23500, 'vibrations': 23501, 'measurement': 23502, 'dba': 23503, 'mnc': 23504, 'henchard': 23505, 'crux': 23506, 'afloat': 23507, 'sampson': 23508, 'jaysinhdev': 23509, 'facilitated': 23510, 'dwyane': 23511, '165': 23512, 'collocation': 23513, 'recycled': 23514, 'rebelled': 23515, 'ziauddin’s': 23516, 'contraction': 23517, 'littered': 23518, 'paralyzed': 23519, 'vegan': 23520, \"cacofonix's\": 23521, 'orinjade': 23522, 'dumpster': 23523, 'amends': 23524, 'deceiving': 23525, 'cbs': 23526, 'transmitted': 23527, 'gurukkals': 23528, 'dressmakers': 23529, 'shoaib': 23530, 'literals': 23531, 'piglet': 23532, 'winnie': 23533, 'aziz': 23534, 'morris': 23535, 'nicola': 23536, 'flabby': 23537, '“speedcuber”': 23538, 'speedsolving': 23539, 'cube—think': 23540, 'pong—while': 23541, 'ernő': 23542, 'brainteaser': 23543, 'competitions—from': 23544, 'tournaments—and': 23545, 'budapest': 23546, 'tinkering': 23547, 'quasi': 23548, '“sub': 23549, \"'islam\": 23550, 'preoccupy': 23551, 'permeation': 23552, 'literalism': 23553, 'channelled': 23554, 'compassionately': 23555, 'contending': 23556, 'peaceable': 23557, 'unsure': 23558, 'westaway': 23559, 'bequest': 23560, 'prayers': 23561, 'getter': 23562, 'bahu': 23563, 'serials': 23564, 'inhabitant': 23565, 'cockerel': 23566, 'grishaverse': 23567, 'divvying': 23568, 'ketterdam': 23569, 'jurda': 23570, 'parem': 23571, \"kaz's\": 23572, 'maas': 23573, 'cashore': 23574, \"ladies'\": 23575, 'ramotswe': 23576, 'mama': 23577, 'mma': 23578, 'gentlemanly': 23579, 'matekoni': 23580, 'dizzy': 23581, 'seafood': 23582, 'goan': 23583, 'prawn': 23584, 'foray': 23585, 'quadratic': 23586, '“manifesto': 23587, 'freedom”': 23588, 'closing—fast': 23589, 'diamandis': 23590, 'category—water': 23591, 'freedom—diamandis': 23592, 'turner…but': 23593, 'fight”': 23594, 'feng': 23595, 'shui': 23596, 'rationale': 23597, 'harte': 23598, 'vastness': 23599, \"'queen\": 23600, 'shinning': 23601, 'involuntarily': 23602, 'visht': 23603, 'hachette': 23604, 'warfare—one': 23605, 'megaton': 23606, 'mystery—apparently': 23607, 'belarus': 23608, 'rebooting': 23609, 'stuxnet’s': 23610, 'bush’s': 23611, 'iran—and': 23612, '“grey': 23613, 'militaries': 23614, 'infiltrations': 23615, 'alike—and': 23616, 'zetter’s': 23617, 'infiltrating': 23618, 'invisibility': 23619, \"'art\": 23620, 'reconnect': 23621, \"li's\": 23622, 'protege': 23623, '27th': 23624, \"fabers'\": 23625, 'withdrawal': 23626, \"mentzer's\": 23627, 'sharkey': 23628, 'mage': 23629, 'clover': 23630, \"thing's\": 23631, 'soured': 23632, 'regiments': 23633, 'rudrangshu': 23634, 'contestant': 23635, 'jock': 23636, '“but': 23637, 'ebb': 23638, \"jesse's\": 23639, 'biochemistry': 23640, 'glenn': 23641, 'breast': 23642, \"qaeda's\": 23643, '158': 23644, 'astonished': 23645, 'scepticism': 23646, 'lyle': 23647, \"robb's\": 23648, \"'curious\": 23649, 'reichs': 23650, 'immari': 23651, 'envisioning': 23652, 'wastelands': 23653, \"pop's\": 23654, \"michael's\": 23655, \"'learn\": 23656, 'smallpox': 23657, 'armageddon': 23658, 'musculature': 23659, 'constructing': 23660, 'importing': 23661, 'vectors': 23662, 'kazakhstan': 23663, 'seals': 23664, 'dolls': 23665, 'himachal': 23666, 'speeds': 23667, 'doddle': 23668, 'boolean': 23669, 'nem': 23670, 'nesa': 23671, 'eligibility': 23672, 'lifters': 23673, 'broadest': 23674, 'accentuate': 23675, 'stand–in': 23676, 'minimalistic': 23677, 'uninitiated': 23678, 'once–underappreciated': 23679, \"'entertaining\": 23680, 'sven': 23681, 'beet': 23682, 'kushner': 23683, 'nicely': 23684, \"atkinson's\": 23685, 'prizewinning': 23686, 'lunchtime': 23687, 'nesting': 23688, 'prominently': 23689, \"lang's\": 23690, 'gradients': 23691, 'trpo': 23692, 'ppo': 23693, 'ddpg': 23694, 'd4pg': 23695, 'entropy': 23696, 'connect4': 23697, 'openai': 23698, 'recaptured': 23699, 'functioned': 23700, 'smoldering': 23701, 'erupted': 23702, 'shielded': 23703, 'insert': 23704, 'gecko': 23705, 'ucla': 23706, 'relieved': 23707, 'boasted': 23708, 'dustin': 23709, 'burstyn': 23710, 'stanislavsky': 23711, \"selling'\": 23712, '1800ceoread': 23713, 'model’': 23714, 'cycles’': 23715, 'techcrunch': 23716, 'enganchado': 23717, 'construir': 23718, 'productos': 23719, 'servicios': 23720, 'exitosos': 23721, 'formen': 23722, 'habitos': 23723, 'balmy': 23724, 'betrayer': 23725, 'barker': 23726, 'cinema—from': 23727, 'nasreen': 23728, 'munni': 23729, 'gora': 23730, 'lai': 23731, 'bandini': 23732, '‘dil': 23733, 'playback': 23734, 'asha': 23735, 'poop': 23736, 'commemorate': 23737, 'herein': 23738, 'thermometer': 23739, 'leakage': 23740, 'posterity': 23741, 'sticky': 23742, 'soggy': 23743, 'memento': 23744, 'tilbury': 23745, 'percy': 23746, 'pilbeam': 23747, 'brene': 23748, 'crowdfunding': 23749, 'passersby': 23750, 'interdependent': 23751, 'trafficked': 23752, 'blurring': 23753, 'moazed': 23754, 'poole': 23755, \"townsend's\": 23756, \"o'clock\": 23757, 'wept': 23758, 'shipton': 23759, \"bonington's\": 23760, 'arouse': 23761, 'harish': 23762, 'panch': 23763, 'chuli': 23764, 'environmentalist': 23765, 'pesticides': 23766, 'confl': 23767, 'noida': 23768, 'neepa': 23769, 'sheth': 23770, 'km': 23771, 'cathy': 23772, 'temptations': 23773, \"today'\": 23774, 'nomination': 23775, 'protests': 23776, 'remorseless': 23777, 'understatement': 23778, 'hysterics': 23779, 'krasia': 23780, 'precipice': 23781, 'rojer': 23782, 'duchies': 23783, 'alladiya': 23784, 'songbird': 23785, 'kesarbai': 23786, 'kerkar': 23787, 'kesar': 23788, 'richie': 23789, 'benaud': 23790, 'atherton': 23791, 'gaffes': 23792, 'stump’s': 23793, 'cinematographers': 23794, 'arriflex': 23795, 'blackmagic': 23796, 'ikonoskop': 23797, 'panasonic': 23798, 'weisscam': 23799, 'angenieux': 23800, 'fujinon': 23801, 'schneider': 23802, 'uniqoptics': 23803, 'zeiss': 23804, 'intermediates': 23805, 'stereo': 23806, 'nr': 23807, 'imbibed': 23808, 'ritu': 23809, 'charlatans': 23810, 'cheaters': 23811, 'faint': 23812, 'westaby': 23813, 'dearly': 23814, 'disassemble': 23815, 'ida': 23816, 'shellcode': 23817, 'synthesize': 23818, 'dissections': 23819, 'thorat': 23820, 'quintet': 23821, 'ugliest': 23822, 'seductively': 23823, 'sehmat': 23824, 'reminiscence': 23825, 'mart': 23826, 'bernoulli': 23827, 'heron': 23828, 'haven®': 23829, 'conscript': 23830, 'scariest': 23831, 'jacked': 23832, \"seles's\": 23833, 'steffi': 23834, 'assaulted': 23835, 'ipv6': 23836, 'firewall': 23837, 'vpn': 23838, 'sect': 23839, 'thunder': 23840, 'smuggles': 23841, \"jacob's\": 23842, \"goodread's\": 23843, 'diep': 23844, 'defective': 23845, 'steadfastly': 23846, 'tran': 23847, 'dwindles': 23848, 'plum': 23849, 'monsieur': 23850, \"exercises'\": 23851, 'governmental': 23852, 'cambodia': 23853, 'pol': 23854, 'propagandistic': 23855, 'basing': 23856, 'rupee': 23857, 'frugality': 23858, \"'attention\": 23859, \"merchants'\": 23860, 'wu': 23861, 'individualized': 23862, 'schumacher': 23863, '‘god': 23864, 'unlearn': 23865, 'bhonsle': 23866, 'chhatrapati': 23867, 'maratha': 23868, 'crowned': 23869, 'laundry': 23870, 'stylishly': 23871, \"bollywood's\": 23872, 'pacey': 23873, 'crowdsourcing': 23874, 'wayfinding': 23875, 'placemaking': 23876, 'durability': 23877, 'barest': 23878, 'revamps': 23879, 'abnormal': 23880, 'emphasised': 23881, \"clark's\": 23882, 'exaggeration': 23883, 'ratios': 23884, 'dilara': 23885, 'explicitly': 23886, 'melodic': 23887, 'rudra': 23888, 'hawaiian': 23889, 'arse': 23890, 'fondness': 23891, 'smith’s': 23892, '1776': 23893, 'imperfections': 23894, 'myriads': 23895, 'pity': 23896, \"tashi's\": 23897, 'recruitments': 23898, 'segments': 23899, 'facilitates': 23900, 'affirmative': 23901, 'slope': 23902, 'portmanteau': 23903, \"cards'\": 23904, 'killig': 23905, 'tagalog': 23906, 'nunchi': 23907, 'gauging': 23908, \"another's\": 23909, 'bomby': 23910, 'laotian': 23911, 'entreating': 23912, 'courtier': 23913, 'gurkha': 23914, 'avatari': 23915, 'khanbalik': 23916, '1294': 23917, 'shamanistic': 23918, 'bardo': 23919, 'thodol': 23920, '50–ca': 23921, 'arrianus': 23922, 'enchiridion': 23923, \"epictetus'\": 23924, 'oz': 23925, '135': 23926, 'polite': 23927, 'nagar': 23928, 'nsg': 23929, 'sandeep': 23930, 'multiway': 23931, 'pairing': 23932, 'trumper': 23933, 'pearls': 23934, 'spinners': 23935, 'vase': 23936, 'culling': 23937, 'magics': 23938, 'sharak': 23939, 'demonkind': 23940, 'breeds': 23941, 'elissa': 23942, 'fractious': 23943, 'exposures': 23944, '550': 23945, 'nishith': 23946, 'babin': 23947, 'instituted': 23948, 'echelon': 23949, 'elude': 23950, 'inventing': 23951, 'mulgan': 23952, 'foolish': 23953, 'ariston': 23954, 'keeley': 23955, 'straits': 23956, 'unforeseen': 23957, 'ranbaxy': 23958, 'bajaj': 23959, '104': 23960, \"denton's\": 23961, 'xp': 23962, 'pagemaker': 23963, 'coreldraw': 23964, 'photoshop': 23965, 'cs2': 23966, 'corrects': 23967, '9a': 23968, '10a': 23969, 'concord': 23970, 'auxiliary': 23971, 'confusable': 23972, 'berkowski': 23973, 'stain': 23974, \"tensorflow's\": 23975, 'detecting': 23976, 'autoencoders': 23977, 'cnns': 23978, 'chalkfulloflove': 23979, 'cadavers': 23980, 'astrological': 23981, 'directionless': 23982, 'json': 23983, 'yaml': 23984, 'ci': 23985, 'niharika': 23986, 'karthik': 23987, 'akshat': 23988, 'vishal': 23989, 'delilah': 23990, 'bartered': 23991, 'allegory': 23992, 'competed': 23993, 'jimi': 23994, 'hendrix': 23995, 'strum': 23996, 'kirk': 23997, 'back’': 23998, 'hives': 23999, \"wasp's\": 24000, 'hivewings': 24001, 'clearsight': 24002, 'silkwing': 24003, 'swordtail': 24004, 'sundew': 24005, 'dragonet': 24006, 'reminisce': 24007, 'nigella’s': 24008, 'dish': 24009, 'cozy': 24010, 'nemo': 24011, 'scarcely': 24012, 'modeled': 24013, 'quills': 24014, 'casablanca': 24015, 'linings': 24016, 'argo': 24017, 'lambs': 24018, 'adapting': 24019, 'punctuations': 24020, 'semicolons': 24021, 'exclamation': 24022, 'interpreted': 24023, 'schizophrenic': 24024, 'auden': 24025, \"kissinger's\": 24026, 'boyd': 24027, 'cumming': 24028, 'predates': 24029, 'abandonment': 24030, 'shaheed': 24031, 'babar': 24032, 'labourers': 24033, 'laudable': 24034, 'befitting': 24035, 'interdimensional': 24036, '1900s': 24037, 'zimmer': 24038, 'biomedical': 24039, 'alisha': 24040, 'boxer': 24041, 'footwork': 24042, 'assassinate': 24043, 'rudy': 24044, 'wearable': 24045, 'moonraker': 24046, 'tuscany': 24047, 'spaceship': 24048, 'paradoxically': 24049, \"matt's\": 24050, \"kind'\": 24051, 'edit': 24052, 'buzzword': 24053, 'deepmind': 24054, 'ghosts—he': 24055, 'hollow—a': 24056, 'soul—ichigo': 24057, 'kubo’s': 24058, 'approximation': 24059, 'knot': 24060, 'marginal': 24061, 'southeast': 24062, 'descriptionthe': 24063, 'expenses': 24064, '5g': 24065, '“my': 24066, 'lifespan': 24067, 'sacked': 24068, 'millionaires': 24069, 'martinez': 24070, 'superficial': 24071, \"grandparents'\": 24072, \"coop's\": 24073, 'injections': 24074, 'hemispheres': 24075, 'disorentation': 24076, 'arctor': 24077, 'narcotics': 24078, 'gloss': 24079, 'wilful': 24080, 'butterfingers': 24081, 'hurtles': 24082, 'butt': 24083, 'crazier': 24084, 'pompey': 24085, 'farid': 24086, 'duped': 24087, 'rajadhyaksha': 24088, 'counterculture': 24089, 'distributing': 24090, 'hd': 24091, 'untested': 24092, 'coexist': 24093, 'deformation': 24094, 'torsion': 24095, 'ironing': 24096, 'afghani': 24097, 'runner’': 24098, '“for': 24099, 'forebears': 24100, 'strait': 24101, \"swarup's\": 24102, 'propulsive': 24103, 'uninhabited': 24104, 'cunningly': 24105, 'harkness': 24106, 'dutiful': 24107, 'facades': 24108, 'namespace': 24109, 'fabrics': 24110, 'derren': 24111, \"tahir's\": 24112, 'shrike': 24113, 'assailed': 24114, 'capitalizes': 24115, 'nightbringer': 24116, 'veturius': 24117, \"elias's\": 24118, 'devotion–even': 24119, \"boss's\": 24120, 'permits': 24121, 'vio': 24122, 'body—eating': 24123, 'speculative': 24124, 'hallucinations': 24125, 'archie’s': 24126, 'spontaneity': 24127, 'teachyourself': 24128, 'prosaic': 24129, 'flexi': 24130, 'adam’s': 24131, 'kirkland': 24132, \"angelou's\": 24133, 'caged': 24134, 'liberates': 24135, 'aarav': 24136, 'dinotopia': 24137, \"books'\": 24138, \"'reading\": 24139, 'donuts': 24140, 'trunks': 24141, 'spinster': 24142, 'suri': 24143, 'affidavits': 24144, 'testified': 24145, 'kuldip': 24146, 'radcliffe': 24147, 'gaffers': 24148, 'blain': 24149, 'waveform': 24150, 'vectorscopes': 24151, 'gamma': 24152, 'encoded': 24153, 'asc': 24154, 'cdl': 24155, 'weekends’': 24156, 'correcting': 24157, 'interlopers': 24158, 'hiriluk': 24159, 'reindeer': 24160, 'chopper': 24161, 'wapol—whose': 24162, 'eats—and': 24163, 'alabasta': 24164, 'cain': 24165, 'introverted': 24166, 'doodling': 24167, 'extrovert': 24168, 'athos': 24169, 'porthos': 24170, 'aramis': 24171, 'linen': 24172, 'anne’s': 24173, 'figuring': 24174, 'astronomer': 24175, 'kepler': 24176, 'biologist': 24177, 'waldo': 24178, 'barrett': 24179, 'streamed': 24180, 'privatization': 24181, 'erosion': 24182, 'roffer': 24183, 'commandments': 24184, 'suffrage': 24185, 'zeitgeist': 24186, 'elizabethan': 24187, 'paddlers': 24188, 'conducts': 24189, 'bendingly': 24190, 'abductor': 24191, 'hazmat': 24192, \"'welcome\": 24193, 'woken': 24194, \"could've\": 24195, 'austerity': 24196, 'james’s': 24197, 'anticipate': 24198, 'ana’s': 24199, 'ideologically': 24200, 'dax': 24201, 'alexandre': 24202, 'sneaking': 24203, 'dumps': 24204, 'yashwant': 24205, 'notifications': 24206, \"asperger's\": 24207, 'explosion—the': 24208, 'usage—that': 24209, 'haver': 24210, 'lyn': 24211, 'lewin': 24212, 'doctorate': 24213, 'aghast': 24214, 'vigilant': 24215, 'catastrophically': 24216, 'bastion': 24217, 'manned': 24218, 'cena': 24219, \"alix's\": 24220, 'summed': 24221, 'neuron': 24222, '75th': 24223, 'kriminalpolizei': 24224, \"berlin's\": 24225, 'eei': 24226, 'pours': 24227, 'entanglements': 24228, 'harbours': 24229, 'undone': 24230, 'ramp': 24231, \"fowler's\": 24232, 'pathological': 24233, 'cripple': 24234, 'rajdeep': 24235, 'visa': 24236, 'cafeteria': 24237, 'anders’': 24238, 'turnip': 24239, \"reflections'\": 24240, 'carpool': 24241, 'karaoke': 24242, 'connell': 24243, \"punchy'\": 24244, 'snows': 24245, '110th': 24246, 'opted': 24247, \"lifetime'\": 24248, \"last'\": 24249, 'brazil’s': 24250, 'drank': 24251, 'kama': 24252, 'ratings': 24253, \"gaskell's\": 24254, 'moors': 24255, \"covey's\": 24256, 'indestructibles': 24257, 'paperlike': 24258, 'surrendered': 24259, 'suppressive': 24260, 'advocacy': 24261, 'pratchett': 24262, 'trimmed': 24263, 'freshest': 24264, 'carves': 24265, '‘corporatocracy’': 24266, 'colluding': 24267, 'corporatocracy': 24268, 'blowers': 24269, 'yanis': 24270, 'varoufakis': 24271, 'brownstein': 24272, \"detail'\": 24273, 'grasshopper': 24274, 'cohorts': 24275, 'been—and': 24276, 'are—some': 24277, 'gil': 24278, 'deodato': 24279, 'lau': 24280, 'covers—the': 24281, 'details—that': 24282, 'blammm': 24283, 'cornucopia': 24284, 'excelsior': 24285, 'iping': 24286, 'goggles': 24287, 'bandages': 24288, \"wells's\": 24289, 'capitals': 24290, 'typefaces': 24291, 'typographic': 24292, 'whit': 24293, \"stillman's\": 24294, 'beckinsale': 24295, 'chloë': 24296, 'sevigny': 24297, '\\xad': 24298, '2c': 24299, 'cable': 24300, 'fibre': 24301, 'congestion': 24302, 'father—a': 24303, 'constants': 24304, \"'love\": 24305, 'cholera’': 24306, 'juvenal': 24307, 'cholera': 24308, 'poisoning': 24309, 'meredith': 24310, 'portal': 24311, 'rejoin': 24312, 'mindstorms': 24313, 'papert': 24314, 'piketty': 24315, 'conferred': 24316, 'coelho’s': 24317, 'sapolsky': 24318, 'dump': 24319, 'insightfully': 24320, 'clamour': 24321, 'postwar': 24322, 'metre': 24323, 'unquiet': 24324, 'justifiably': 24325, 'kurôzu': 24326, 'constituted': 24327, 'yemen': 24328, 'scandinavians': 24329, 'hideout': 24330, 'cove': 24331, 'winn': 24332, 'hendricks': 24333, 'pekkanen': 24334, 'kubert': 24335, 'nowlan': 24336, \"bohemia'\": 24337, 'blackmailed': 24338, \"crime'\": 24339, \"haig's\": 24340, 'triumphed': 24341, 'tunnel': 24342, 'review’s': 24343, 'gloomy': 24344, 'fish’s': 24345, 'rediscovery': 24346, 'colm': 24347, 'tentative': 24348, \"'unforgettable'\": 24349, 'jewelled': 24350, 'acquaint': 24351, 'pronouncing': 24352, '57': 24353, 'regent': 24354, 'mahesh': 24355, 'insidious': 24356, 'tibetans': 24357, 'nilgiri': 24358, 'fright': 24359, 'mauritius': 24360, 'misspelled': 24361, \"couple's\": 24362, 'adhere': 24363, 'judiciously': 24364, 'metaprogramming': 24365, 'overcomes': 24366, 'moz': 24367, \"fishkin's\": 24368, 'dorm': 24369, 'fizzle': 24370, 'petit': 24371, 'ostracized': 24372, 'unmistakably': 24373, 'unremitting': 24374, 'preoccupation': 24375, 'whimsically': 24376, '“like': 24377, '”–the': 24378, '“filled': 24379, 'bookshots': 24380, 'johannesburg': 24381, 'montague': 24382, 'loading': 24383, 'mjolnir': 24384, 'climaxes': 24385, 'shylock': 24386, 'centrality': 24387, 'verity': 24388, 'appreciating': 24389, 'nick’s': 24390, 'clifton’s': 24391, 'befriend': 24392, 'flux': 24393, 'perpetrated': 24394, 'nata': 24395, 'jee': 24396, 'chapterwise': 24397, \"athletes'\": 24398, 'nutrient': 24399, 'physicists': 24400, 'lorna': 24401, 'mugs': 24402, 'chalk': 24403, 'oklahoma': 24404, 'fritz': 24405, 'convicts': 24406, 'disturb': 24407, 'dragonstone': 24408, 'daenerys’s': 24409, 'abbreviated': 24410, 'parent’s': 24411, 'blumenthal': 24412, 'saath': 24413, 'pandering': 24414, 'prides': 24415, 'misgovernance—communal': 24416, \"lynching's\": 24417, 'gau': 24418, 'rakshaks': 24419, 'rashtra': 24420, 'deleterious': 24421, 'lutetia': 24422, 'tangling': 24423, 'navishtrix': 24424, 'clovogarlix': 24425, 'indore': 24426, 'heartbeat': 24427, 'bleu': 24428, 'belcher': 24429, 'pesto': 24430, 'spanking': 24431, 'blush': 24432, 'uprisings': 24433, 'modernize': 24434, 'benefitted': 24435, 'bandaka': 24436, 'tatta': 24437, 'bandit': 24438, \"holmes'\": 24439, 'farr': 24440, 'immortalised': 24441, 'payload': 24442, 'pleases': 24443, 'unfathomable': 24444, 'terrains': 24445, 'soothes': 24446, 'shells': 24447, 'pixels': 24448, 'cruelties': 24449, 'tammy': 24450, \"'wonderful\": 24451, 'marauding': 24452, 'playthinks': 24453, 'reworked': 24454, 'topology': 24455, 'polygons': 24456, 'follies': 24457, 'derisive': 24458, 'zorah': 24459, '‘bestseller’': 24460, '38’s': 24461, 'knotty': 24462, 'hammerfest': 24463, 'retraces': 24464, 'monologues': 24465, 'guilds': 24466, 'portions': 24467, 'tremolo': 24468, 'respectfully': 24469, 'conforms': 24470, 'retrieval': 24471, 'surveil': 24472, 'discriminate': 24473, 'censor': 24474, 'cybercriminals': 24475, 'voluntary': 24476, 'goliath': 24477, 'schneier': 24478, \"lore's\": 24479, 'airey’s': 24480, 'projects–including': 24481, 'designs–he': 24482, 'scher': 24483, 'citi': 24484, 'lindon': 24485, 'fedex': 24486, 'metadesign': 24487, 'sagmeister': 24488, 'sarabhai': 24489, 'ahmedabad': 24490, 'kars': 24491, \"'suicide\": 24492, \"epidemic'\": 24493, 'headscarves': 24494, 'islamists': 24495, 'ipek': 24496, 'blanketing': 24497, 'snowfall': 24498, 'slyly': 24499, \"moment'\": 24500, 'warship': 24501, 'discoverer': 24502, 'overdrawn': 24503, 'cheering': 24504, 'thrifty': 24505, 'economising': 24506, 'lexi': 24507, \"'marriage\": 24508, \"manual'\": 24509, 'underwhelmed': 24510, \"workers'\": 24511, 'bites': 24512, 'shred': 24513, 'verlaque': 24514, 'provence’s': 24515, 'rouquet': 24516, 'donna': 24517, 'longworth’s': 24518, '—kirkus': 24519, 'subset': 24520, 'crockford': 24521, 'mingled': 24522, 'applets': 24523, \"'check\": 24524, \"yourself'\": 24525, 'skyscraper': 24526, 'catapult': 24527, 'hardening': 24528, 'equated': 24529, 'promptly': 24530, 'redeem': 24531, 'gram': 24532, 'universality': 24533, \"5's\": 24534, 'esther': 24535, 'randomized': 24536, 'rcts': 24537, 'nighteye': 24538, 'massacring': 24539, 'shuttles': 24540, 'nasrin': 24541, 'defamation': 24542, 'veneers': 24543, 'bartlett': 24544, 'nix': 24545, 'stitch': 24546, 'iansiti': 24547, 'lakhani': 24548, 'ravenclaw': 24549, 'slytherin': 24550, 'solos': 24551, 'promo': 24552, 'sway': 24553, 'wages': 24554, 'bennet': 24555, 'soldiering': 24556, 'gryf': 24557, 'worshippers': 24558, 'backstory': 24559, 'schema': 24560, 'caravan': 24561, \"race'\": 24562, 'robustness': 24563, 'cables': 24564, 'vlan': 24565, 'groupings': 24566, 'advertises': 24567, 'invisibles': 24568, 'kang': 24569, \"'impressive\": 24570, 'kershaw': 24571, \"come'\": 24572, 'borrowing': 24573, 'dependencies': 24574, 'katara': 24575, 'impasse': 24576, 'kuei': 24577, 'sokka': 24578, 'toph': 24579, 'metalbending': 24580, 'firebenders': 24581, 'venue': 24582, 'arthritis': 24583, '‘into': 24584, 'sandford': 24585, 'shenoy’s': 24586, 'sensed': 24587, 'ticketing': 24588, 'artfully': 24589, 'nme': 24590, 'sprinting': 24591, 'devour': 24592, 'bingo': 24593, 'inability': 24594, 'crawling': 24595, 'ftp': 24596, 'ukrainian': 24597, 'brainteasers': 24598, 'clothed': 24599, 'redefined': 24600, 'unfurls': 24601, 'devastate': 24602, 'feck': 24603, 'perfuction': 24604, 'kohli’s': 24605, 'points—this': 24606, 'firmament': 24607, 'cricket—the': 24608, 'beater': 24609, 'testimonial': 24610, 'mustang': 24611, 'bader': 24612, 'ginsburg—“a': 24613, 'opinion”': 24614, 'harper’s': 24615, '“showcases': 24616, 'ginsburg’s': 24617, 'range”': 24618, 'hartnett': 24619, 'discourse”': 24620, 'lighthouse': 24621, 'stillbirth': 24622, 'thesis': 24623, \"'high\": 24624, \"scorer's\": 24625, \"choice'\": 24626, '40th': 24627, '204': 24628, 'computes': 24629, \"franklin's\": 24630, 'flintstone': 24631, 'spongebob': 24632, 'squarepants': 24633, 'mushu': 24634, 'mulan—plus': 24635, 'novels—make': 24636, 'artscape': 24637, 'ephemeral': 24638, 'bharati': 24639, 'florentyna': 24640, 'cse': 24641, 'blendshapes': 24642, 'edgeloops': 24643, 'stretchy': 24644, 'tina’s': 24645, 'routledgetextbooks': 24646, 'ohailey': 24647, \"told'\": 24648, \"ambitious'\": 24649, \"'extraordinary'\": 24650, 'darts': 24651, 'wayfarer': 24652, 'hyperspace': 24653, 'endangering': 24654, 'carbs': 24655, 'matriarch': 24656, 'grandmasters': 24657, 'springboard': 24658, 'junkies': 24659, 'stacy': 24660, \"caroline's\": 24661, 'unraveling': 24662, 'shivering': 24663, 'holt': 24664, 'winding': 24665, 'jurek’s': 24666, '‘probably': 24667, 'brontë': 24668, 'villette': 24669, 'eyre': 24670, 'squall': 24671, \"mourinho's\": 24672, 'lemire': 24673, '“you': 24674, 'unaccounted': 24675, 'me’': 24676, 'obtaining': 24677, 'incest': 24678, 'redeemed': 24679, \"'whispering\": 24680, 'horn': 24681, 'manhunt': 24682, \"kamsa's\": 24683, 'epitomized': 24684, 'mojo': 24685, 'crease': 24686, 'devaloka': 24687, 'patala': 24688, 'nath': 24689, 'meru': 24690, 'footpath': 24691, 'moose': 24692, 'motel': 24693, 'exemplify': 24694, 'burchell': 24695, 'differentiated': 24696, 'toles': 24697, 'denialism': 24698, 'unconscionable': 24699, 'partisanship': 24700, 'denialists': 24701, \"toles's\": 24702, 'enlivens': 24703, 'affleck': 24704, 'radishes': 24705, 'snail': 24706, 'dmitri': 24707, 'dostoevsky’s': 24708, \"o's\": 24709, 'uninhibited': 24710, 'underpinned': 24711, 'penchant': 24712, 'aneri': 24713, 'disagreement': 24714, 'blowpipe': 24715, 'subdivided': 24716, 'synopses': 24717, 'bibek': 24718, 'rangoon': 24719, 'indecent': 24720, 'lectured': 24721, 'actor’s': 24722, 'martini': 24723, 'quarterly': 24724, 'gauld': 24725, 'heti': 24726, 'fringe': 24727, 'spurred': 24728, 'pegasus': 24729, 'antonio’s': 24730, 'persisted': 24731, 'nautilus': 24732, \"snider's\": 24733, 'gordian': 24734, 'porch': 24735, 'brandy': 24736, 'sate': 24737, 'bondage': 24738, 'maeve': 24739, 'overseeing': 24740, '1898': 24741, 'kilimanjaro': 24742, 'airplane': 24743, 'carney': 24744, 'surfer': 24745, 'resided': 24746, 'relativity': 24747, 'astronomical': 24748, 'particles': 24749, 'introspect': 24750, 'gracious': 24751, 'maha': 24752, 'madhavpur': 24753, 'inhabitable': 24754, 'subjugate': 24755, 'yuga': 24756, 'impulses': 24757, \"else's\": 24758, \"carroll's\": 24759, '1882': 24760, \"'incredibly\": 24761, '70bc': 24762, 'medjay': 24763, 'siwa': 24764, \"medjay's\": 24765, \"capital's\": 24766, 'incompetent': 24767, 'contour': 24768, 'mimic': 24769, 'recurrence': 24770, 'atheism': 24771, 'confining': 24772, 'hollows': 24773, 'laypersons': 24774, 'dcu': 24775, 'crimea': 24776, 'clutching': 24777, 'gilgamesh': 24778, 'nirvana': 24779, 'renunciation': 24780, 'attainment': 24781, \"gray's\": 24782, \"'astonishingly\": 24783, \"good'\": 24784, \"hooked'\": 24785, \"'extraordinarily\": 24786, 'raphael': 24787, \"goosebumps'\": 24788, \"'portrayed\": 24789, \"movie'\": 24790, 'guises': 24791, \"vitality'\": 24792, \"dobbs'\": 24793, 'memorably': 24794, 'olsen': 24795, 'soulful': 24796, 'stimulus': 24797, 'seamy': 24798, 'prospecting': 24799, 'dubbelosix': 24800, 'oxymoronica': 24801, 'viva': 24802, 'similes': 24803, '140': 24804, 'aranya': 24805, 'flashy': 24806, 'multiplayer': 24807, 'mores': 24808, 'teeters': 24809, 'mimicking': 24810, '666': 24811, 'niggling': 24812, 'multisensory': 24813, 'swordfish': 24814, 'euwe': 24815, 'brighten': 24816, 'smog': 24817, 'archi': 24818, 'tecture': 24819, 'kalki': 24820, 'leverages': 24821, 'aha': 24822, 'moviegoers': 24823, 'admittedly': 24824, 'regrettably': 24825, 'shied': 24826, \"jackie's\": 24827, 'bloopers': 24828, \"samantha's\": 24829, 'slade': 24830, 'geospatial': 24831, 'perplexing': 24832, 'figurehead': 24833, 'interns': 24834, \"angela's\": 24835, 'strip’s': 24836, 'to1996': 24837, 'syndicated': 24838, 'syndication': 24839, 'huckleberry': 24840, 'abby': 24841, 'rearranging': 24842, 'prevail': 24843, 'platter': 24844, 'lebanon': 24845, 'mentalism': 24846, 'blindfold': 24847, 'annemann': 24848, \"magic's\": 24849, \"roffe's\": 24850, 'strifes': 24851, 'opposes': 24852, 'diffident': 24853, 'installed': 24854, 'spellbound': 24855, 'wool': 24856, 'homestead': 24857, 'bonington': 24858, '1976': 24859, \"boardman's\": 24860, 'exultation': 24861, 'helper': 24862, 'ks': 24863, 'aunties': 24864, 'psychopathic': 24865, 'mans': 24866, 'stylistic': 24867, 'dictum': 24868, 'ceases': 24869, 'complacency': 24870, 'incarcerated': 24871, 'stupidity': 24872, 'cowardice': 24873, 'interestingly': 24874, 'gleick': 24875, 'pbs’s': 24876, 'impetuous': 24877, 'dunne': 24878, \"nick's\": 24879, 'demystified': 24880, 'republicanism': 24881, \"right's\": 24882, 'sarvarkar': 24883, 'bankim': 24884, 'aurobindo': 24885, 'conservatism': 24886, 'participates': 24887, \"'top\": 24888, 'dollors': 24889, 'ramadorai': 24890, 'inescapable': 24891, 'feature’s': 24892, '‘circus’': 24893, 'sexist': 24894, 'suppandi': 24895, 'kalia': 24896, 'birthright': 24897, 'saxon': 24898, '10b': 24899, 'diver': 24900, 'hoyt': 24901, \"diver's\": 24902, 'periodical': 24903, 'spotlights': 24904, 'prohías': 24905, 'cartoonnetwork': 24906, 'slapping': 24907, 'neurobics': 24908, 'anytime': 24909, 'offbeat': 24910, 'heinrichs': 24911, 'pentagon': 24912, 'comfy': 24913, \"latif's\": 24914, 'interlocking': 24915, 'masala': 24916, '‘angels': 24917, 'demons’': 24918, 'vetra': 24919, 'cern': 24920, 'cardinals': 24921, 'persistently': 24922, 'overpower': 24923, 'scientist’s': 24924, 'vittoria': 24925, 'vetra’s': 24926, 'nucleon': 24927, 'russia’s': 24928, 'contrasts': 24929, 'hereby': 24930, 'kapur': 24931, 'cleanse': 24932, 'lisp': 24933, 'familiarizes': 24934, 'undiscovered': 24935, 'farmelo': 24936, 'simon’s': 24937, 'watford': 24938, 'magicks': 24939, 'attachments': 24940, \"ilyich's\": 24941, 'thurber': 24942, 'punishable': 24943, 'subsisting': 24944, 'caterpillars': 24945, 'anticipates': 24946, 'riou': 24947, 'fay': 24948, 'africa…': 24949, 'reeking': 24950, 'mothballs': 24951, 'electocuted': 24952, '‘17': 24953, 'kilmorden': 24954, 'castle’': 24955, 'senseless': 24956, 'cassie': 24957, 'assimilate': 24958, 'lstm': 24959, 'hassle': 24960, 'seq': 24961, 'tighten': 24962, '‘all': 24963, '4a': 24964, \"'read'\": 24965, 'teachyourselflanguages': 24966, 'inhabits': 24967, 'mackensie': 24968, 'oktoberfest': 24969, 'ein': 24970, 'viel': 24971, 'converse': 24972, 'boulevard': 24973, \"'taut\": 24974, \"memory'\": 24975, 'watering': 24976, 'invitational': 24977, 'akruti': 24978, \"akruti's\": 24979, 'copycat': 24980, 'timebreaker': 24981, \"jerome's\": 24982, 'terrier': 24983, 'tabloids': 24984, 'crazes': 24985, 'agarwal': 24986, 'forman': 24987, 'rite': 24988, 'marker': 24989, 'sonam': 24990, 'whisperer': 24991, 'barnaby': 24992, 'bagenda': 24993, 'here—a': 24994, 'rayner': 24995, 'godforsaken': 24996, 'vega': 24997, 'lives…as': 24998, 'motorsport': 24999, 'cataclysm': 25000, 'com’s': 25001, 'reshaped': 25002, 'gimmicks': 25003, '60’s': 25004, 'seinen': 25005, 'landowners': 25006, 'erode': 25007, 'sire': 25008, 'troubles—the': 25009, 'gasping': 25010, \"'meticulously\": 25011, 'gerritsen': 25012, 'margrave': 25013, 'lusted': 25014, 'auditioning': 25015, \"jill's\": 25016, 'reshuffles': 25017, 'gellert': 25018, 'scamander': 25019, \"grindelwald's\": 25020, 'minalima': 25021, 'nods': 25022, 'him…': 25023, 'association’s': 25024, 'frisson': 25025, 'ga': 25026, '88': 25027, 'pranay': 25028, 'mammals': 25029, 'rakeysh': 25030, 'omprakash': 25031, 'rang': 25032, 'unchanged': 25033, '—entertainment': 25034, 'colenso': 25035, \"ricky's\": 25036, 'serbia': 25037, 'cupcake': 25038, 'seema': 25039, 'aparna': 25040, 'deciphering': 25041, 'mummies': 25042, 'solidarity': 25043, 'samir': 25044, \"megha's\": 25045, 'rill': 25046, 'foss': 25047, 'grata': 25048, 'fiji': 25049, 'dear…': 25050, 'discord': 25051, 'gratification': 25052, 'mischel': 25053, 'implicit': 25054, 'layperson': 25055, \"tetlock's\": 25056, 'funded': 25057, 'demonstrable': 25058, 'froning': 25059, 'competes': 25060, 'sandwiches': 25061, \"kgb's\": 25062, 'tranche': 25063, \"empire'\": 25064, \"'stunning\": 25065, \"scale'\": 25066, \"'headline\": 25067, \"originator'\": 25068, 'miscellaneum': 25069, 'provocatively': 25070, 'mccandless’s': 25071, 'asheville': 25072, 'dhabi': 25073, 'splashed': 25074, 'hinterland': 25075, 'rovers': 25076, 'endeared': 25077, 'ramnath': 25078, 'hayden': 25079, 'algeria': 25080, 'valentine': 25081, 'montglane': 25082, 'reassembles': 25083, 'scatter': 25084, 'webbing': 25085, \"krypton's\": 25086, 'newtonian': 25087, 'redeeming': 25088, \"obelix's\": 25089, 'mistletoe': 25090, 'suu': 25091, 'kyi': 25092, 'spilled': 25093, 'bachchan': 25094, 'chapra': 25095, 'who’ll': 25096, 'juice': 25097, 'morphogenetic': 25098, 'phylocode': 25099, 'quarkonium': 25100, 'telescope': 25101, 'molecular': 25102, 'genomics': 25103, 'reprinting': 25104, \"intelligence'\": 25105, 'skewed': 25106, 'lawbreakers': 25107, 'mumtaz': 25108, 'jahanara': 25109, 'zenana': 25110, 'zenda': 25111, 'rassendyll': 25112, 'ruritania': 25113, '1894': 25114, 'rupert': 25115, 'emphasises': 25116, 'tremensdelirius': 25117, 'nonverbal': 25118, 'penetrates': 25119, 'backpacking': 25120, 'footwear': 25121, 'rosnovski': 25122, 'tapan': 25123, 'dasgupta': 25124, \"'moving\": 25125, \"picture'\": 25126, 'wong': 25127, 'reclaims': 25128, 'ghaznavi': 25129, 'ushering': 25130, 'enums': 25131, 'serialization': 25132, 'util': 25133, 'herzog': 25134, 'higdon': 25135, 'insomnia': 25136, 'creedish': 25137, 'imparting': 25138, 'nascent': 25139, 'scriptwriter': 25140, 'rangoonwalla': 25141, \"lutyens'\": 25142, 'connaught': 25143, 'dispenses': 25144, 'fragmented': 25145, 'dillon': 25146, 'ripples': 25147, 'kirstie': 25148, 'cafe': 25149, 'tihar': 25150, 'mcsa': 25151, 'declassified': 25152, \"pontryagin's\": 25153, 'fittes': 25154, 'planck': 25155, 'lamps': 25156, 'disquieting': 25157, 'renu': 25158, 'roleplaying': 25159, 'player’s': 25160, 'dmg': 25161, 'tavern': 25162, 'assign': 25163, 'birchwood': 25164, 'ringleaders': 25165, 'arming': 25166, 'dispatching': 25167, 'kasab’s': 25168, 'ranulph': 25169, 'fiennes': 25170, 'paratrooper': 25171, 'herders': 25172, 'shamans': 25173, 'shagun': 25174, 'reincarnation': 25175, 'gorazde': 25176, \"'fixer'\": 25177, \"neven's\": 25178, 'sehar': 25179, 'mayan': 25180, 'accommodations': 25181, '365': 25182, 'louisa': 25183, 'temperamental': 25184, 'starling': 25185, 'solicit': 25186, \"cannibal'\": 25187, 'focussing': 25188, 'johann': 25189, 'chorales': 25190, \"adams'\": 25191, 'demolished': 25192, 'bypass': 25193, \"'where\": 25194, 'hideously': 25195, 'gown': 25196, 'fenchurch': 25197, 'specialising': 25198, \"jennifer's\": 25199, 'indicate': 25200, 'priestly': 25201, 'impersonal': 25202, 'paternalistic': 25203, 'digitized': 25204, 'incurable': 25205, 'majors': 25206, 'kashyap': 25207, 'werelions': 25208, 'burdens': 25209, \"bonatti's\": 25210, 'marshall': 25211, \"relations'\": 25212, 'gelber': 25213, 'schwartz': 25214, \"gruffalo's\": 25215, 'berendt’s': 25216, 'encountering': 25217, 'ethnicity': 25218, 'whispered': 25219, 'ineffectiveness': 25220, 'raju': 25221, 'activates': 25222, 'yama': 25223, 'lilies': 25224, 'porcupines': 25225, 'ag': 25226, 'sukhdev': 25227, 'leanings': 25228, 'vohra': 25229, 'pulses': 25230, \"cutter's\": 25231, 'bergin': 25232, 'relished': 25233, 'gleefully': 25234, 'vindhyas—temple': 25235, 'dosais': 25236, 'appams—are': 25237, 'rajmohan': 25238, 'cultures—kannada': 25239, 'cultures—kodava': 25240, 'tulu': 25241, 'indigenous—that': 25242, 'sultanates': 25243, 'ahmadnagar': 25244, 'golconda': 25245, 'nayakas': 25246, 'haidar': 25247, 'arcot': 25248, 'holdings': 25249, 'monarchy—queen': 25250, '‘empress': 25251, 'india’—and': 25252, 'citizenry': 25253, 'rajas': 25254, 'mahajana': 25255, 'dravida': 25256, 'kazhagam': 25257, 'besant': 25258, 'rajagopalachari': 25259, 'ramasami': 25260, 'naicker': 25261, 'varadarajulu': 25262, 'kamaraj': 25263, 'annadurai': 25264, 'kamaladevi': 25265, 'namboodiripad': 25266, 'potti': 25267, 'sriramulu': 25268, 'lines—tamil': 25269, 'karunakaran': 25270, 'hegde': 25271, 'subbulakshmi': 25272, 'ananthamurthy': 25273, 'muthulakshmi': 25274, 'bromwich': 25275, 'kerr': 25276, 'lowell': 25277, 'gorey': 25278, 'beastly': 25279, \"'media\": 25280, \"'soft\": 25281, \"cash'\": 25282, 'plath': 25283, 'suppers': 25284, 'cauliflower': 25285, 'cashew': 25286, 'traybake': 25287, 'fennel': 25288, 'thai': 25289, 'noodles': 25290, 'lemon': 25291, 'pavlova': 25292, 'cookie': 25293, 'pots': 25294, 'toasty': 25295, 'granola': 25296, 'nisha': 25297, \"'beat\": 25298, \"clock'\": 25299, 'westcliff': 25300, 'secretaries': 25301, \"administration's\": 25302, 'quebert': 25303, 'kellergan': 25304, 'hatterr': 25305, 'gypsy': 25306, 'seriousness': 25307, 'hali': 25308, 'librarian': 25309, 'edition’': 25310, 'presumably': 25311, 'gentlemen': 25312, 'trailblazers': 25313, 'cwe': 25314, 'discount': 25315, 'trigonometry': 25316, 'ordinate': 25317, 'soneji': 25318, 'censorship': 25319, 'teju': 25320, 'vips': 25321, \"'cj'\": 25322, 'venerated': 25323, 'annihilate': 25324, 'opportunistic': 25325, 'mayawati': 25326, 'produnova': 25327, 'dipa': 25328, 'karmakar’s': 25329, 'gymnastics': 25330, 'dipa’s': 25331, 'tripura': 25332, 'outpost': 25333, 'gilot': 25334, 'giacometti': 25335, 'swindlers': 25336, 'hannay': 25337, \"kaplan's\": 25338, 'jong': 25339, 'swans': 25340, 'austere': 25341, 'copying': 25342, 'ghanta': 25343, 'all–in–one': 25344, 'keyword': 25345, 'beerus': 25346, 'saiyan': 25347, \"drablow's\": 25348, 'kipps': 25349, 'eel': 25350, 'townsfolk': 25351, \"death'\": 25352, 'elvis': 25353, 'presley': 25354, 'sinatra': 25355, 'counseling': 25356, 'hughes’': 25357, 'fc': 25358, 'queue': 25359, 'haggle': 25360, 'dwarves': 25361, 'rivia': 25362, 'ccc': 25363, 'mcq’s': 25364, 'upasna': 25365, 'sikhaane': 25366, 'tatha': 25367, 'rachana': 25368, 'vaaky': 25369, \"course'\": 25370, 'bigotry': 25371, \"'raises\": 25372, 'recapture': 25373, 'nara': 25374, 'pim': 25375, 'vinyl': 25376, 'pea': 25377, 'misunderstanding': 25378, \"'book\": 25379, 'gwenda': 25380, 'ilk': 25381, 'bleakest': 25382, 'laboratories': 25383, 'unequal': 25384, 'succor': 25385, 'gainsborough': 25386, 'hals': 25387, 'rembrandt': 25388, \"anyone's\": 25389, 'eastward': 25390, 'underlines': 25391, 'prolonged': 25392, '1000awesomethings': 25393, 'brandishing': 25394, 'revolvers': 25395, 'master’': 25396, 'juventus': 25397, 'explosively': 25398, \"d'urbervilles\": 25399, '2025': 25400, \"sen's\": 25401, 'shortfalls': 25402, 'extraterrestrial': 25403, 'you—and': 25404, 'sam’s': 25405, '“falkoff': 25406, 'kumud': 25407, 'bedding': 25408, 'suitor': 25409, 'bhagwaan': 25410, '‘our': 25411, 'palettes': 25412, 'approval': 25413, \"'really\": 25414, 'dial': 25415, 'fulton': 25416, 'snodde': 25417, 'theatres': 25418, 'backstage': 25419, 'fortnight': 25420, '8000': 25421, 'tutoring': 25422, 'imaginatively': 25423, 'mötley': 25424, 'crüe': 25425, 'mansoor': 25426, \"pataudi's\": 25427, \"soha's\": 25428, 'balliol': 25429, 'forthrightness': 25430, \"wit'\": 25431, \"'tender\": 25432, \"'her\": 25433, \"famous'\": 25434, \"beings'\": 25435, \"'moves\": 25436, \"moments'\": 25437, 'unselfconscious': 25438, \"wise'\": 25439, 'shabana': 25440, 'azmi': 25441, \"'soha\": 25442, 'bhawana': 25443, 'somaya': 25444, \"detours'\": 25445, 'hoskote': 25446, 'konkona': 25447, 'starry': 25448, \"company'\": 25449, 'depreciating': 25450, 'choudry': 25451, 'shying': 25452, 'manicures': 25453, \"bachchan's\": 25454, 'shatrughan': 25455, 'dia': 25456, 'befuddling': 25457, 'tinted': 25458, 'cappuccino': 25459, 'bae': 25460, 'novel—a': 25461, 'barmy': 25462, 'weren’t': 25463, 'assemblies': 25464, 'ravan': 25465, 'lakshman': 25466, \"ravan's\": 25467, 'ramayan': 25468, 'nishi': 25469, 'siddharth': 25470, 'gleeful': 25471, 'satiric': 25472, 'brandt': 25473, \"founders'\": 25474, 'heartbreaker': 25475, 'meds': 25476, \"molly's\": 25477, 'idaho': 25478, 'papercraft': 25479, 'representational': 25480, 'tomoko': 25481, 'replacement': 25482, 'govind': 25483, 'avenues': 25484, 'khans': 25485, \"thrillers'\": 25486, 'sati': 25487, 'lesley': 25488, 'hazleton': 25489, 'nontechnical': 25490, 'checker': 25491, 'commences': 25492, 'declines': 25493, 'sexism': 25494, \"'stilton'\": 25495, 'cheesewright': 25496, \"mukherjee's\": 25497, \"not'\": 25498, 'rollercoaster’': 25499, '‘confirms': 25500, 'nationalistic': 25501, '‘indian': 25502, 'arthashastra': 25503, 'rishabh': 25504, \"'people\": 25505, \"skills'\": 25506, 'coles': 25507, 'beauvoir': 25508, 'goalkeeper': 25509, 'palliative': 25510, 'nearing': 25511, 'is—and': 25512, 'sojourn': 25513, \"aitken's\": 25514, 'acerbic': 25515, '261': 25516, 'svms': 25517, \"guide's\": 25518, \"'laini\": 25519, 'combusted': 25520, \"dreams'\": 25521, 'roshani': 25522, \"queen'5s\": 25523, \"lazlo's\": 25524, 'arrests': 25525, 'sufficiency': 25526, 'stokes': 25527, 'consonant': 25528, '2018’': 25529, 'aced': 25530, 'keju': 25531, 'rin’s': 25532, 'shamanism': 25533, 'psychoactive': 25534, 'nikara': 25535, 'mugen': 25536, 'away…': 25537, 'bons': 25538, 'entendres': 25539, 'tms': 25540, 'beards': 25541, 'bakes': 25542, 'rag': 25543, 'cliffhangers': 25544, 'insist': 25545, \"chung's\": 25546, 'selfie': 25547, '”—lucy': 25548, 'kalanithi': 25549, '“belongs': 25550, 'kalanithi’s': 25551, '”—bill': 25552, 'disapproval': 25553, '“blessing': 25554, 'tacitly': 25555, 'subscribing': 25556, '“can’t': 25557, 'tumors': 25558, 'populates': 25559, 'retinue': 25560, 'gripping—she’s': 25561, 'didion': 25562, '”—glennon': 25563, 'existentialist': 25564, \"conscience'\": 25565, 'jethmalani': 25566, 'parliamentarians': 25567, \"'wodehouse\": 25568, \"tonic'\": 25569, '‘cheaper': 25570, 'valium’': 25571, '‘relief': 25572, 'raginess': 25573, 'sour’': 25574, '‘mr': 25575, 'bask': 25576, 'wooster’s': 25577, 'haplessness': 25578, 'raconteurs': 25579, \"'without\": 25580, 'imitable': 25581, 'straddle': 25582, 'mavericks': 25583, 'meazza': 25584, 'hagi': 25585, 'cory': 25586, 'levitz': 25587, 'alchemists': 25588, 'robes': 25589, 'gringotts': 25590, 'ollivanders': 25591, 'strategically': 25592, 'victimhood': 25593, 'madhya': 25594, \"leicester's\": 25595, 'badges': 25596, 'fuentes': 25597, 'underpaid': 25598, 'lefevre': 25599, 'liv': 25600, 'halston': 25601, \"painting's\": 25602, \"liv's\": 25603, \"'moyes\": 25604, \"adore'\": 25605, 'stoppingly': 25606, \"moyes'\": 25607, 'animosity': 25608, 'debunking': 25609, 'debunks': 25610, 'scheherazade': 25611, 'doniger’s': 25612, 'chiefly': 25613, 'gist': 25614, 'imparted': 25615, 'misplaced': 25616, 'ornamentation': 25617, 'returned…': 25618, 'traceless': 25619, 'collaborates': 25620, 'locally': 25621, 'tasty': 25622, 'elaboration': 25623, 'chutneys': 25624, 'amalgamation': 25625, 'bona': 25626, 'minuscule': 25627, \"yunus's\": 25628, 'homelessness': 25629, 'hurriedly': 25630, \"saadiq's\": 25631, 'nestled': 25632, 'cling': 25633, 'strategists': 25634, 'massimo': 25635, 'bierut': 25636, 'enthusiastically': 25637, 'administered': 25638, \"conway's\": 25639, 'izuku': 25640, 'ramsden': 25641, 'trepidation': 25642, 'joker’': 25643, 'princess’': 25644, 'pawn’': 25645, 'specializing': 25646, \"man'\": 25647, 'ceaseless': 25648, 'declan': 25649, 'kiberd': 25650, 'frees': 25651, 'senku': 25652, \"edinburgh's\": 25653, 'prabhudesai': 25654, 'composure': 25655, 'siobhan': 25656, '170mph': 25657, 'spewing': 25658, 'boxset': 25659, 'vandal': 25660, 'inspect': 25661, \"foucault's\": 25662, 'phew': 25663, 'substitutes': 25664, 'goju': 25665, 'ryu': 25666, 'kig': 25667, 'thickens': 25668, 'residential': 25669, 'sinclair': 25670, 'ariel': 25671, 'sharon': 25672, 'ehud': 25673, 'suez': 25674, '2b': 25675, 'spiced': 25676, 'carlo': 25677, 'periodized': 25678, 'verified': 25679, 'cohn': 25680, \"rowell's\": 25681, 'bray': 25682, 'dealers': 25683, 'delineate': 25684, 'sacking': 25685, 'hungryalists': 25686, 'treaties': 25687, 'plucked': 25688, \"other'\": 25689, \"lydia's\": 25690, \"'devastating\": 25691, 'violinist': 25692, 'interacting': 25693, 'localization': 25694, 'dumping': 25695, 'emphatic': 25696, 'humanities': 25697, 'oss': 25698, 'lamentation': 25699, \"henry's\": 25700, \"shardlake's\": 25701, 'cotterstoke': 25702, 'printer': 25703, 'opportunists': 25704, 'bui': 25705, \"sui's\": 25706, '1884': 25707, \"divakaruni's\": 25708, 'bock': 25709, 'addenda': 25710, 'primacy': 25711, 'steeping': 25712, \"kurosawa's\": 25713, 'bytime': 25714, 'omniscient': 25715, 'bombardier': 25716, '256th': 25717, 'squadron': 25718, 'aircrafts': 25719, 'rationally': 25720, 'waiver': 25721, \"heller's\": 25722, 'back”': 25723, 'indianapolis': 25724, 'ranji': 25725, 'machu': 25726, 'picchu': 25727, 'puma': 25728, 'bares': 25729, \"maestro's\": 25730, 'candidness': 25731, 'golang': 25732, 'dreaming—a': 25733, \"morpheus'\": 25734, 'realm—be': 25735, 'masse': 25736, 'dissemination': 25737, \"rusty's\": 25738, 'besiege': 25739, 'gardener': 25740, 'hellbent': 25741, 'gregg': 25742, 'laxmans': 25743, 'pardon': 25744, 'saudis': 25745, 'aruna': 25746, 'chakravarti': 25747, 'talukdar': 25748, \"powerful'\": 25749, \"uk's\": 25750, 'hothead': 25751, \"cricketers'\": 25752, 'cancellation': 25753, 'denomination': 25754, 'deposited': 25755, 'fotedar': 25756, 'peking': 25757, '8a': 25758, 'icily': 25759, 'myshkin': 25760, 'swimsuit': 25761, 'mcdaniels': 25762, 'lifes': 25763, 'reinvigorated': 25764, 'superstition': 25765, 'undergoing': 25766, 'trimester': 25767, 'salvi': 25768, 'gynaecology': 25769, 'mistiqarts': 25770, 'petrovic': 25771, 'paneled': 25772, 'chibis': 25773, 'proletarians': 25774, 'probabilistic': 25775, 'spim': 25776, 'assembler': 25777, 'ia': 25778, 'pentium': 25779, 'cpu2000': 25780, 'web99': 25781, 'eembc': 25782, 'opteron': 25783, 'hdl': 25784, 'abhimanyu': 25785, 'beresford': 25786, 'blunt’s': 25787, 'leapt': 25788, 'sunningdale': 25789, 'dynamically': 25790, 'shanay': 25791, 'orchid': 25792, 'lehar': 25793, 'doe': 25794, \"'essential\": 25795, 'pleading': 25796, 'grundrisse': 25797, 'yui': 25798, 'sachi': 25799, 'swig': 25800, 'chhi': 25801, 'sun’': 25802, 'riot': 25803, \"adichie's\": 25804, 'aliebn': 25805, 'bees': 25806, 'you”': 25807, 'ks1': 25808, 'maldives': 25809, \"singapore's\": 25810, 'kuan': 25811, 'yew': 25812, 'infrastructural': 25813, 'vigor': 25814, 'stove': 25815, 'nirad': 25816, 'misinterpreted': 25817, 'augment': 25818, 'domestication': 25819, 'correspondences': 25820, 'liquid': 25821, 'elena': 25822, 'tita': 25823, 'rantz': 25824, 'eyes’': 25825, 'laskin': 25826, 'deafo': 25827, 'telep': 25828, 'dander': 25829, 'istaking': 25830, 'ruffian': 25831, 'purrfect': 25832, 'inheritor': 25833, 'alva': 25834, 'serbian': 25835, 'transformer': 25836, 'kaiô': 25837, 'amplifying': 25838, 'berger’s': 25839, 'lifetime’s': 25840, \"barker's\": 25841, 'mythicality': 25842, 'scares': 25843, 'likelihood': 25844, 'ne': 25845, 'vocab': 25846, 'scaachi': 25847, 'liberalisation': 25848, 'inﬂuence': 25849, 'swaps': 25850, 'landlord': 25851, 'greenwood': 25852, \"plath's\": 25853, 'errant': 25854, 'drill': 25855, 'janette': 25856, 'bikers': 25857, 'kaling': 25858, 'obedient': 25859, 'errands': 25860, 'brutalized': 25861, 'priti': 25862, \"baby's\": 25863, 'fighterz': 25864, 'nier': 25865, 'gamers': 25866, 'wanderings': 25867, 'confiscated': 25868, 'syllables': 25869, 'doubling': 25870, 'norwegian': 25871, 'winona': 25872, 'backwater': 25873, 'beauchamp': 25874, 'underlay': 25875, 'searingly': 25876, 'dramatisation': 25877, 'authorised': 25878, 'hotakainen': 25879, 'nark': 25880, 'outposts': 25881, 'shredding': 25882, 'gilmore': 25883, \"reece's\": 25884, 'jumpy': 25885, 'faraday': 25886, 'suppress': 25887, 'expounded': 25888, 'burner': 25889, 'ambiguous': 25890, 'frustrate': 25891, 'manageability': 25892, 'workloads': 25893, 'skillful': 25894, 'rene': 25895, 'angles': 25896, 'enslavement': 25897, 'borne': 25898, 'veritable': 25899, 'veidt': 25900, 'clerres': 25901, 'liveships': 25902, 'paragon': 25903, 'justine': 25904, 'alex’s': 25905, 'instructing': 25906, 'utilitarian': 25907, \"steel's\": 25908, 'decodes': 25909, 'honk': 25910, \"lottie's\": 25911, 'remark': 25912, 'mired': 25913, 'rodin': 25914, 'you\\x92ll': 25915, 'savaging': 25916, 'elwin’s': 25917, 'barnabas': 25918, 'americas—coins': 25919, 'gallows': 25920, 'anchoring': 25921, 'waterlilies': 25922, 'jérôme': 25923, 'vladimir’s': 25924, 'craves': 25925, 'acker': 25926, 'stanislaw': 25927, 'lem': 25928, 'ramanujan': 25929, 'essayists': 25930, 'wada': 25931, 'cobains': 25932, 'heavier': 25933, \"nina's\": 25934, 'instinctive': 25935, 'nostredame': 25936, 'wil': 25937, \"nostradamus'\": 25938, 'favoured': 25939, 'nanami': 25940, 'yokai': 25941, 'aradhana': 25942, 'sonar': 25943, 'kella': 25944, 'gaadi': 25945, 'gandhiji': 25946, 'khair': 25947, 'flown': 25948, 'fazes': 25949, 'chats': 25950, 'odin': 25951, 'nothingness': 25952, 'jimenez': 25953, 'peripherals': 25954, '98': 25955, 'comically': 25956, 'frenzied': 25957, 'soreness': 25958, 'interrelated': 25959, \"'shirley\": 25960, \"unforgettable'\": 25961, 'communalism': 25962, 'bagieu': 25963, 'goodnight': 25964, \"scholastic's\": 25965, 'kepri': 25966, 'excel′s': 25967, 'median': 25968, \"'extremely\": 25969, 'wrongful': 25970, 'ton': 25971, 'alverstoke': 25972, 'fork': 25973, 'anchors': 25974, 'hillside': 25975, 'bleeding': 25976, 'rangers': 25977, 'fali': 25978, 'carbide': 25979, 'blissfully': 25980, 'joost': 25981, 'elffers': 25982, 'squabbling': 25983, 'clausewitz': 25984, 'tzu': 25985, 'barnum': 25986, 'victimized': 25987, 'kanye': 25988, 'amga': 25989, 'belay': 25990, 'repel': 25991, 'elaborated': 25992, \"bindra's\": 25993, 'colonna': 25994, 'francaise': 25995, 'lifter': 25996, 'paleo': 25997, 'perceiving': 25998, 'uc': 25999, \"'someone\": 26000, 'vibiana': 26001, 'traditionalists': 26002, 'orchestrating': 26003, 'vms': 26004, 'paas': 26005, 'iaas': 26006, 'dbas': 26007, 'playbooks': 26008, 'ec2': 26009, 'annawadi': 26010, 'watergate': 26011, 'boo’s': 26012, 'pumped': 26013, 'docked': 26014, 'nyc': 26015, 'integrationist': 26016, 'nonwhite': 26017, '“malcolm': 26018, 'x’s': 26019, 'unadorned': 26020, '”—barack': 26021, '“extraordinary': 26022, '”—spike': 26023, 'afro': 26024, '”—i': 26025, \"council's\": 26026, 'hillary': 26027, 'rodham': 26028, \"clinton's\": 26029, 'courteney': 26030, 'parched': 26031, 'ascetics': 26032, 'coauthored': 26033, \"macy's\": 26034, 'straying': 26035, 'dal': 26036, 'ashford’s': 26037, 'texture': 26038, '“hits': 26039, 'electroshock': 26040, 'therapy”': 26041, 'rainer': 26042, 'andersen’s': 26043, 'adulting': 26044, 'dissolve': 26045, 'unhealthy': 26046, 'cooler': 26047, 'calmness': 26048, 'constitutions': 26049, 'soundly': 26050, 'somnath': 26051, 'germs': 26052, 'edwin': 26053, 'j2se': 26054, 'limiting': 26055, 'resolves': 26056, 'economix': 26057, 'anomalies': 26058, 'costumed': 26059, 'lothar': 26060, 'irby': 26061, 'wainwright': 26062, 'choma': 26063, 'ox': 26064, 'chewing': 26065, 'bridgerton': 26066, \"kate's\": 26067, \"'two\": 26068, 'goswami': 26069, 'tulsidas': 26070, 'hymns': 26071, \"'mahaviri'\": 26072, 'citations': 26073, 'deewaar': 26074, \"hai'\": 26075, 'migrations': 26076, \"deewaar's\": 26077, 'slashed': 26078, 'creek': 26079, 'kjo': 26080, 'unsuitable': 26081, 'reminisces': 26082, 'veiled': 26083, 'ayushi': 26084, \"murch's\": 26085, 'markers': 26086, \"bruce's\": 26087, '667': 26088, '675': 26089, '501': 26090, '508': 26091, 'ennui': 26092, 'derby': 26093, 'skates': 26094, 'puffin': 26095, 'ce': 26096, \"kalidasa's\": 26097, 'vaynerchuk': 26098, 'newer': 26099, 'woodland': 26100, 'lamp': 26101, 'graysons': 26102, 'vanzetti': 26103, 'collegiate': 26104, 'customizable': 26105, 'mural': 26106, 'countess': 26107, 'toolbox': 26108, 'fishy': 26109, 'infirmofpurpus': 26110, 'herring': 26111, 'concussed': 26112, 'cuff': 26113, '‘sql': 26114, 'kao': 26115, 'flaw': 26116, 'hazardous': 26117, 'thicker': 26118, \"'jai\": 26119, 'slumdog': 26120, 'actuality': 26121, 'hammered': 26122, 'howling': 26123, 'quaffle': 26124, 'bludgers': 26125, 'hilla': 26126, 'lalli': 26127, 'junji': 26128, 'ito': 26129, 'charity': 26130, 'truimph': 26131, 'phantomhive': 26132, 'tracy’s': 26133, \"'but\": 26134, \"'jean\": 26135, 'prioritised': 26136, 'kennedy’s': 26137, 'clip': 26138, 'logistic': 26139, 'word2vec': 26140, 'staves': 26141, 'tomboy': 26142, 'omit': 26143, 'breakup': 26144, 'illnesses': 26145, 'feckless': 26146, 'denouement': 26147, \"masterpiece'\": 26148, 'skids': 26149, 'hitching': 26150, 'killercoming': 26151, 'cloudwatch': 26152, 'registration': 26153, 'comicbooks': 26154, '50s': 26155, 'asrtists': 26156, 'jams': 26157, \"delisle's\": 26158, 'résumés': 26159, 'contemplates': 26160, 'stranger’': 26161, 'prevails': 26162, 'gamer': 26163, 'hippie': 26164, 'unplanned': 26165, 'irrepressibly': 26166, 'infallible': 26167, 'oversharing': 26168, 'overshare': 26169, \"rosie's\": 26170, 'unfiltered': 26171, 'tended': 26172, 'courting': 26173, 'patrons': 26174, 'botany': 26175, \"'haunting\": 26176, 'bharara': 26177, 'anaga': 26178, 'perfecting': 26179, 'gouache': 26180, 'heightened': 26181, 'briefing': 26182, 'kell': 26183, 'litigators': 26184, 'zinc': 26185, 'cholesterol': 26186, 'pundit': 26187, 'pundits': 26188, 'dumbest': 26189, 'carmichael': 26190, \"old's\": 26191, 'everywhere’': 26192, 'formalizes': 26193, 'futureproofing': 26194, 'pharus': 26195, 'underline': 26196, 'fedoriv': 26197, 'freytag': 26198, 'bedow': 26199, 'renwick': 26200, 'ico': 26201, 'lundgren': 26202, 'fatima': 26203, 'bubblee': 26204, 'carrick': 26205, 'giggs': 26206, 'worsens': 26207, 'assessed': 26208, 'diets': 26209, 'rumor': 26210, 'catatonic': 26211, 'nominations': 26212, 'bá': 26213, '‘diary': 26214, 'kid’': 26215, 'worse…': 26216, 'chad’s': 26217, \"'rankin's\": 26218, 'denise': 26219, 'donati': 26220, 'headmaster': 26221, '1870': 26222, 'zinn': 26223, 'avinash': 26224, 'undulating': 26225, 'libido': 26226, 'placid': 26227, 'thread…': 26228, 'sumerian': 26229, 'smmm': 26230, 'wom': 26231, 'mci': 26232, 'blogging': 26233, 'pinterest': 26234, 'buscema': 26235, 'supercomputer': 26236, 'fulcrum': 26237, 'automaton': 26238, \"turk'\": 26239, \"turing's\": 26240, 'strategize': 26241, 'fliss': 26242, 'behaves': 26243, 'extravagance': 26244, 'cordell': 26245, 'instantaneous': 26246, \"vikings'\": 26247, \"framework's\": 26248, 'carre': 26249, 'ashe': 26250, 'daggers': 26251, 'allocation': 26252, 'accommodated': 26253, 'sharer': 26254, 'green’s': 26255, 'sourcing': 26256, 'marshal': 26257, 'ashecliffe': 26258, \"'epic\": 26259, 'kinnaur': 26260, 'abra': 26261, 'cody': 26262, '500c': 26263, 'steely': 26264, 'securely': 26265, 'horan': 26266, 'tomlinson': 26267, 'us’': 26268, 'izmat': 26269, 'guantanamo': 26270, 'arabs': 26271, 'poles': 26272, 'dhows': 26273, 'khyber': 26274, 'yangtze': 26275, 'altitudes': 26276, 'flashpoints': 26277, 'cheats': 26278, 'atlantean': 26279, 'parrish': 26280, 'confidante': 26281, 'undermine': 26282, 'certificates': 26283, 'licences': 26284, 'ethnographic': 26285, 'vegetable': 26286, 'categorised': 26287, 'pom’s': 26288, 'brothel': 26289, 'settlements': 26290, 'vague': 26291, 'tiggy': 26292, 'puddle': 26293, 'merchandise': 26294, 'ginger': 26295, 'poopypants': 26296, 'tinkletrousers': 26297, 'kohinoor': 26298, 'bahía': 26299, 'midlife': 26300, 'him—and': 26301, 'lagoon': 26302, 'vaughn': 26303, 'usagi': 26304, 'vrmmorpg': 26305, \"bentley's\": 26306, 'houston': 26307, 'adiga': 26308, 'labourer': 26309, '555': 26310, 'currents': 26311, 'battlegrounds': 26312, 'drifts': 26313, 'vana': 26314, 'asura': 26315, 'fraternal': 26316, 'vanara': 26317, 'grande': 26318, 'alafair': 26319, 'sabine': 26320, '290': 26321, 'rusbridger': 26322, 'mislead': 26323, '114': 26324, \"babylon's\": 26325, 'boone': 26326, 'bagwan': 26327, \"splunk's\": 26328, 'fez': 26329, 'priyanka': 26330, 'pri': 26331, 'nepotism': 26332, 'ellington': 26333, 'depleted': 26334, 'tutors': 26335, 'novak': 26336, 'rulebook': 26337, 'positioned': 26338, 'brie': 26339, \"traveler's\": 26340, 'morale': 26341, 'rameswaram': 26342, 'pike': 26343, 'bano': 26344, 'municipal': 26345, 'custodial': 26346, 'zia': 26347, 'military’s': 26348, 'बदल': 26349, 'हैं।': 26350, 'गाँवों': 26351, 'इस': 26352, 'बदलाव': 26353, 'हमारे': 26354, 'तकनीक': 26355, 'तकलीफ': 26356, 'दी': 26357, 'छोटे': 26358, 'पड़ताल': 26359, 'बनने': 26360, 'बदलने': 26361, 'ticks': 26362, 'ferryman': 26363, 'abercrombie': 26364, 'hanged': 26365, 'gedge': 26366, 'candle': 26367, 'specialize': 26368, 'epstein': 26369, 'stuntman': 26370, 'infertility': 26371, 'sass': 26372, 'misinformed': 26373, 'narcissistic': 26374, 'weingarten': 26375, 'godbole': 26376, 'tightrope': 26377, 'insides': 26378, 'wholesale': 26379, 'disney·pixar': 26380, 'vagrants': 26381, 'barsati': 26382, 'saurav': 26383, 'devapriya': 26384, 'emis': 26385, 'swivel': 26386, 'firang': 26387, 'jostle': 26388, 'thuggish': 26389, 'hurtle': 26390, 'wanderlust': 26391, 'dazzlingly': 26392, 'submissive': 26393, 'fredric': 26394, 'trailing': 26395, 'ishmael': 26396, 'risc': 26397, \"'new'\": 26398, \"'tom\": 26399, 'nagas': 26400, 'vegetarianism': 26401, 'eurystheus': 26402, 'hobbyist': 26403, 'bugs': 26404, 'whatís': 26405, 'marketeers': 26406, 'swati': 26407, 'troll': 26408, 'confrontational': 26409, 'tireless': 26410, 'death…': 26411, 'urmilla': 26412, 'pema': 26413, 'atish': 26414, 'demeanour': 26415, \"keyes'\": 26416, 'wisecracks': 26417, \"'find\": 26418, \"'myself\": 26419, 'troublemakers': 26420, 'teetering': 26421, \"'girl\": 26422, \"cried'\": 26423, \"'keyes\": 26424, 'rediscovering': 26425, \"herself'\": 26426, \"'fabulously\": 26427, \"form'\": 26428, 'turners': 26429, 'vujicic': 26430, 'supple': 26431, \"davies's\": 26432, 'baronville': 26433, 'outback': 26434, 'rangarajan': 26435, 'districts': 26436, 'brows': 26437, 'morpurgo': 26438, 'fieldwork': 26439, \"application's\": 26440, 'bottlenecks': 26441, 'bean': 26442, 'schiffs': 26443, 'avik': 26444, \"ludwig's\": 26445, 'bunker': 26446, 'shortage': 26447, '180': 26448, 'dentists': 26449, 'uptight': 26450, 'sneaker': 26451, 'robotham': 26452, \"robotham's\": 26453, 'ache': 26454, 'unvarnished': 26455, \"archie's\": 26456, 'revolutionising': 26457, 'veterinary': 26458, 'intersections': 26459, 'curfewed': 26460, 'lifesaving': 26461, '1832': 26462, 'toiling': 26463, 'showbiz': 26464, 'authoritarian': 26465, 'benoy': 26466, \"jack's\": 26467, \"morton's\": 26468, 'sweltering': 26469, 'suffolk': 26470, 'dorothy': 26471, 'vivien': 26472, 'fatefully': 26473, 'munari': 26474, \"clavell's\": 26475, 'blackthorne': 26476, \"blackthorne's\": 26477, 'vpc': 26478, 'leoni': 26479, 'meld': 26480, 'accents': 26481, 'franklin’s': 26482, 'sculpting': 26483, 'frcr': 26484, 'amassed': 26485, 'pga': 26486, 'infatuation': 26487, 'simmering': 26488, 'beware': 26489, 'coated': 26490, 'measles': 26491, 'penick': 26492, 'golfing': 26493, 'gauri': 26494, 'nature’s': 26495, 'hebditch': 26496, 'ack': 26497, 'sociopaths': 26498, 'geena': 26499, 'nissa': 26500, 'kits': 26501, '340': 26502, 'vouchers': 26503, 'guideline': 26504, 'excise': 26505, 'gandolph': 26506, 'muriel': 26507, \"county's\": 26508, 'emphasized': 26509, 'devlok': 26510, 'tropes': 26511, '‘mihir': 26512, \"hiccup's\": 26513, 'profiler': 26514, 'tippoo': 26515, 'hakeswill': 26516, 'cornerstone': 26517, 'manjrekar': 26518, 'gamestorming': 26519, 'noticeably': 26520, 'grander': 26521, 'dicharry': 26522, 'clinics': 26523, 'bel': 26524, 'hollow': 26525, 'spooked': 26526, 'noises': 26527, 'jitters': 26528, 'soraya': 26529, 'credo': 26530, 'situates': 26531, 'laboring': 26532, 'insulting': 26533, 'chandraki': 26534, 'easter': 26535, 'amadeo': 26536, 'laing': 26537, 'quentin': 26538, \"golwalkar's\": 26539, 'gung': 26540, 'immemorial': 26541, 'wilde’s': 26542, 'merlin': 26543, 'dogzombies': 26544, 'ijeawele': 26545, 'saujani': 26546, 'epically': 26547, 'mead': 26548, 'irascible': 26549, 'elicit': 26550, \"hobbe's\": 26551, 'terrorizes': 26552, 'screwing': 26553, 'gooshy': 26554, 'slimy': 26555, 'bleecch': 26556, 'dum': 26557, 'phonology': 26558, 'sprinkled': 26559, 'altercation': 26560, 'gervase': 26561, 'seahawks': 26562, 'soda': 26563, '“fat': 26564, 'shay’s': 26565, 'mit’s': 26566, 'khali': 26567, 'ivanhoe': 26568, 'provost': 26569, 'glimmer': 26570, 'ripping': 26571, 'horne': 26572, 'osmond': 26573, 'kilometers': 26574, 'tel': 26575, 'aviv': 26576, 'ivry': 26577, '–where': 26578, 'enthusiastic': 26579, 'tripadvisor': 26580, \"traveller's\": 26581, 'fairfax': 26582, 'scarlett’s': 26583, 'gopalan': 26584, 'takeaways': 26585, 'silos': 26586, 'diggity': 26587, 'thoughout': 26588, 'asleep': 26589, 'mysooru': 26590, 'chimera': 26591, 'guild': 26592, 'conciliator': 26593, 'carbohydrate': 26594, 'facto': 26595, 'ghostwriter': 26596, 'cowriter': 26597, 'tripathi': 26598, 'meluha’': 26599, 'nagas’': 26600, 'vayuputras’': 26601, 'chandravanshis': 26602, 'accusing': 26603, 'rashad': 26604, 'contesting': 26605, '4500': 26606, 'giza': 26607, 'reputedly': 26608, 'bestowing': 26609, 'booby': 26610, 'jsps': 26611, 'demos': 26612, 'liar': 26613, 'takezo': 26614, 'octopus': 26615, 'rashtrapati': 26616, 'kilter': 26617, 'kassandra': 26618, \"unique'\": 26619, \"games'\": 26620, 'wells': 26621, \"inspector's\": 26622, 'maldivian': 26623, 'capitalises': 26624, 'obligingly': 26625, 'unpick': 26626, 'cryogenic': 26627, 'faked': 26628, 'springtime': 26629, 'dunstable': 26630, 'pongo': 26631, 'adaptability': 26632, 'appexchange': 26633, 'gauze': 26634, 'prep’s': 26635, 'adaptive': 26636, 'packard': 26637, \"disappointed'\": 26638, 'symbology': 26639, 'iconology': 26640, 'evening’s': 26641, 'kirsch': 26642, 'museum’s': 26643, 'ambra': 26644, '‘dan': 26645, 'cliffhanger’': 26646, 'for’': 26647, 'provide’': 26648, 'factivity': 26649, 'elitist': 26650, 'fizzing': 26651, 'intersects': 26652, 'fandom': 26653, 'mirk': 26654, 'rowell’s': 26655, 'stevenson’s': 26656, 'nimona': 26657, 'tenka’ichi': 26658, '“strongest': 26659, 'krabbé': 26660, 'kitsch': 26661, 'kremlin’s': 26662, 'kumbh': 26663, 'mela': 26664, 'density': 26665, 'pgecet': 26666, 'lyrebird': 26667, \"reviewer's\": 26668, 'sizeable': 26669, 'afzal': 26670, 'conjugated': 26671, 'sweeney': 26672, 'propels': 26673, 'kirloskar': 26674, 'life—all': 26675, 'lapd': 26676, \"da'quan\": 26677, \"side'\": 26678, 'satan’s': 26679, 'warlock': 26680, 'nervegear': 26681, 'elation': 26682, 'aincrad': 26683, 'tumble': 26684, \"elizabeth's\": 26685, 'saina': 26686, 'avatars': 26687, 'nigma': 26688, 'bilbo': 26689, 'wenzel': 26690, 'gia': 26691, '63': 26692, 'tolkachev': 26693, 'bio': 26694, 'fei': 26695, 'gale': 26696, 'rumored': 26697, 'memorise': 26698, 'ncert': 26699, 'accomplishing': 26700, 'nel': 26701, '2e': 26702, 'neanderthals': 26703, 'gatherers': 26704, 'erasmus': 26705, 'spate': 26706, 'nikki’s': 26707, 'hornet’s': 26708, 'hilarity': 26709, 'deadpan': 26710, 'allen’s': 26711, 'chaozu': 26712, 'brauer': 26713, 'entrancing': 26714, 'dime': 26715, 'lasso': 26716, 'pashu': 26717, 'paws': 26718, 'cooling': 26719, 'claud': 26720, 'scarpetta': 26721, 'ijaazat': 26722, 'carys': 26723, 'wires': 26724, 'cleansing': 26725, 'rohingya': 26726, 'culpability': 26727, 'clergy': 26728, 'fests': 26729, 'mehboob': 26730, 'metres': 26731, 'insufficient': 26732, 'chopin’s': 26733, 'ranthambhore': 26734, 'evelyn’s': 26735, 'austrian': 26736, 'pamphlet': 26737, 'manheim': 26738, '“oh': 26739, 'lavanya’s': 26740, 'disclose': 26741, 'wesley': 26742, 'godspawn': 26743, 'clapham': 26744, \"'climate\": 26745, 'molecules': 26746, 'pert': 26747, 'larousse': 26748, 'suzanna': 26749, '108': 26750, 'commune': 26751, 'bhagwans': 26752, 'swoosh': 26753, 'lazarus': 26754, 'kalarippayattu': 26755, 'interdisciplinary': 26756, 'ggplot2': 26757, 'trylle': 26758, 'outcaste': 26759, 'egri': 26760, 'zlatan’s': 26761, 'immigrant’s': 26762, 'zadie': 26763, 'ipa': 26764, 'patriots': 26765, 'brady’s': 26766, 'slipped': 26767, 'palestinians': 26768, \"'arthashastra'\": 26769, 'environmentally': 26770, 'dyed': 26771, 'history”': 26772, 'ripley’s': 26773, 'thangka': 26774, 'pouring': 26775, 'servelets': 26776, 'canwell': 26777, 'dickenson': 26778, '1971’s': 26779, 'extra”': 26780, 'contrasted': 26781, 'midas': 26782, 'affair”': 26783, '“murder': 26784, 'misanthrope”': 26785, 'counterbalanced': 26786, 'capers': 26787, 'sang': 26788, \"pip's\": 26789, 'impede': 26790, \"erasmus's\": 26791, 'vista': 26792, 'ordination': 26793, 'bricks': 26794, 'isi': 26795, 'angelina': 26796, 'jolie': 26797, 'delinquent': 26798, 'judeo': 26799, 'mari': 26800, 'wander': 26801, 'dryer': 26802, 'sociocultural': 26803, 'pant': 26804, 'paramedic': 26805, \"'uncannies'\": 26806, 'rstudio': 26807, 'grolemund': 26808, 'davidowitz': 26809, 'handheld': 26810, \"punisher's\": 26811, \"castle's\": 26812, 'platoon': 26813, 'dapper': 26814, \"miss'\": 26815, 'elin': 26816, 'hilderbrand': 26817, 'conforming': 26818, 'kimball': 26819, 'netter': 26820, \"christian's\": 26821, 'warhol': 26822, 'crunchyroll': 26823, 'circa': 26824, 'ameer': 26825, 'learned®': 26826, 'holmes’s': 26827, 'lodgings': 26828, \"malaysia's\": 26829, 'palau': 26830, 'sofas': 26831, 'videotape': 26832, 'physiques': 26833, \"yuko's\": 26834, 'blume': 26835, 'grammaticaux': 26836, 'competences': 26837, \"d'activites\": 26838, 'epreuves': 26839, 'pedagogique': 26840, 'roasted': 26841, 'rickshaw': 26842, 'meatballs': 26843, \"uhtred's\": 26844, 'audit': 26845, 'scamander’s': 26846, 'igarashi': 26847, 'saturdays': 26848, 'bespectacled': 26849, 'yamamoto': 26850, 'averages': 26851, 'coefficients': 26852, 'ramen': 26853, \"rui's\": 26854, 'ohmsha': 26855, 'pitcher': 26856, '2016•': 26857, 'season•': 26858, 'balague': 26859, 'galáctico': 26860, 'balagué': 26861, \"people'\": 26862, 'jens': 26863, 'muller': 26864, 'fiat': 26865, 'life—a': 26866, 'thula': 26867, 'restraint': 26868, '1864': 26869, 'dwyer': 26870, 'parallelize': 26871, 'interoperability': 26872, 'mangalyaan—india’s': 26873, 'isro’s': 26874, 'queensbury': 26875, 'adil': 26876, \"arlen's\": 26877, 'crimes…': 26878, 'jamsetji’s': 26879, 'banners': 26880, 'bases': 26881, 'faccio': 26882, 'hislop': 26883, 'noakes': 26884, 'bercaw': 26885, 'determinants': 26886, 'rhetorical': 26887, 'kleon': 26888, 'ironwoods': 26889, 'afghans': 26890, 'rut': 26891, 'destruct': 26892, 'multiculturalism': 26893, \"merkel's\": 26894, 'repatriation': 26895, 'fixation': 26896, 'scandinavia': 26897, 'lampedusa': 26898, 'ideo': 26899, '‘creative': 26900, 'susskind': 26901, 'peppa': 26902, 'bode': 26903, \"bode's\": 26904, 'they’d': 26905, 'marah': 26906, \"murad's\": 26907, 'amal': 26908, 'yazidi': 26909, 'whopping': 26910, 'mayada': 26911, 'ejb': 26912, '810': 26913, '813': 26914, 'kullu': 26915, 'jatakas': 26916, \"nugent's\": 26917, 'tiers': 26918, 'sneha’s': 26919, 'heartbreaks': 26920, 'peck': 26921, 'satyagraha': 26922, 'boynton': 26923, 'stanhope': 26924, 'jax': 26925, 'ws': 26926, 'centre’': 26927, 'revolutionist’s': 26928, 'premo': 26929, 'scrooge\\x92s': 26930, 'cisa': 26931, 'auditor': 26932, 'azam': 26933, '•make': 26934, '5013': 26935, 'bancroft': 26936, 'viki': 26937, 'france’s': 26938, 'maxine': 26939, 'auroville': 26940, '“rejuvenate': 26941, 'zamas': 26942, 'notre': 26943, 'truckload': 26944, 'buddhists': 26945, 'cinema—like': 26946, 'orson': 26947, 'welles': 26948, 'mizoguchi': 26949, 'jancso': 26950, 'ophuls—for': 26951, 'films—pyaasa': 26952, 'melancholic': 26953, 'medium—acting': 26954, 'rhythm—that': 26955, 'grain': 26956, 'businessman…': 26957, 'fortescue': 26958, '‘counting': 26959, 'house’': 26960, 'rhyme…': 26961, 'zestful': 26962, 'unequalled': 26963, 'workload': 26964, 'tranquillity': 26965, 'linnet': 26966, 'ridgeway': 26967, 'outburst': 26968, 'setting’': 26969, 'karmayogi': 26970, 'technocrat': 26971, 'workday': 26972, \"sreedharan's\": 26973, 'sanskriti': 26974, 'refuted': 26975, \"'blockchain\": 26976, 'bamboozling': 26977, 'cryptographically': 26978, 'tapscott': 26979, \"'spectacular\": 26980, 'expansiveness': 26981, \"profundity'\": 26982, \"'iconic'\": 26983, \"innovator's\": 26984, \"'occasionally\": 26985, '—four': 26986, 'arts—kung': 26987, 'taijiquan—shaolin': 26988, 'taijiquan': 26989, 'fu—contrasting': 26990, 'wudang': 26991, 'xingyi': 26992, 'tanglangquan': 26993, 'mantis': 26994, 'objectives—setting': 26995, 'fu—inheritance': 26996, 'application—the': 26997, 'practice—relieving': 26998, 'animals—understanding': 26999, 'set—how': 27000, 'fu—the': 27001, 'qin': 27002, 'na': 27003, 'strategies—using': 27004, 'tactic': 27005, 'weapons—staffs': 27006, 'chi—the': 27007, 'lohan': 27008, 'abdominal': 27009, 'zen—culitvating': 27010, 'bodhidharma': 27011, 'taoism': 27012, 'enlightenment—attaining': 27013, 'kents': 27014, \"kent's\": 27015, 'spectacles': 27016, 'covent': 27017, 'genin': 27018, 'anbu—itachi': 27019, 'ahead…': 27020, 'philosophers—plato': 27021, 'spinoza': 27022, 'voltaire': 27023, 'schopenhauer': 27024, 'bergson': 27025, 'santayana': 27026, 'dewey—the': 27027, 'durant’s': 27028, 'dazzle': 27029, 'watersones': 27030, \"'lynne\": 27031, '‘bewitching': 27032, 'handholding': 27033, 'yoursap': 27034, 'contextualized': 27035, 'paraguayan': 27036, 'smuggle': 27037, 'multinationals': 27038, 'unregistered': 27039, 'kiosks': 27040, 'neuwirth': 27041, 'strategy—along': 27042, 'perceptions—neuwirth': 27043, 'woolmer': 27044, 'suspending': 27045, 'erring': 27046, 'srinivasan': 27047, 'hamid': 27048, 'cassim': 27049, 'tinku': 27050, 'mandi': 27051, \"'friends'\": 27052, 'patchy': 27053, 'kapilvastu': 27054, 'uruvela': 27055, 'ananda': 27056, 'ajatasattu': 27057, 'jetavana': 27058, 'tributes': 27059, \"undertaker's\": 27060, 'bret': 27061, 'foreward': 27062, 'topps': 27063, \"'mania\": 27064, 'fanhood': 27065, 'jaffé': 27066, \"jung's\": 27067, 'sermones': 27068, 'mortuos': 27069, 'preceding': 27070, 'on—a': 27071, 'person—the': 27072, '“case': 27073, 'notes”': 27074, 'macdonald’s': 27075, 'staccato': 27076, 'defouw': 27077, 'svoboda': 27078, 'bellamy': 27079, 'readers’': 27080, 'murasaki': 27081, 'enfield': 27082, 'thunderbird': 27083, 'waiters': 27084, 'mechanics―': 27085, 'ordained': 27086, 'acquires': 27087, 'wheelie’s': 27088, 'unfazed': 27089, 'receptionists': 27090, 'unkarmic': 27091, 'sceptics': 27092, 'unkempt': 27093, 'tenzing’s': 27094, \"monk's\": 27095, 'confessedly': 27096, 'charan': 27097, 'mankins': 27098, 'indecisiveness': 27099, 'evasions': 27100, 'schizophrenia': 27101, \"'language\": 27102, 'slovenly': 27103, 'misleading': 27104, 'theseus': 27105, 'grad': 27106, 'dorst': 27107, 'enmeshing': 27108, \"dorst's\": 27109, 'traveller’s': 27110, 'goya': 27111, 'pisanzapra': 27112, \"banana'\": 27113, 'turntable': 27114, '“daisy': 27115, '‘once': 27116, 'haze': 27117, 'freefall': 27118, 'dehydration': 27119, 'unassisted': 27120, 'icebergs': 27121, 'labrador': 27122, 'für': 27123, 'elise': 27124, 'bagatelle': 27125, 'sonatas\\x97such': 27126, 'cantabile': 27127, '8\\x97as': 27128, 'symphonies': 27129, 'pianists': 27130, 'mp3s': 27131, 'byculla': 27132, 'chota': 27133, 'gawli': 27134, 'naik': 27135, 'narrativizing': 27136, 'ages…': 27137, '391': 27138, 'storehouse': 27139, 'stronghold…': 27140, 'ranata': 27141, 'emotive': 27142, 'silhouette': 27143, 'mournful': 27144, \"'sunset'\": 27145, \"'darkness'\": 27146, \"light'\": 27147, 'stupor': 27148, 'hatred—as': 27149, 'wolfe': 27150, 'chast': 27151, 'odenkirk': 27152, 'maron': 27153, 'crabapple': 27154, 'munro': 27155, 'eckstein': 27156, '‘secular’': 27157, 'bordered': 27158, '‘bangladeshi': 27159, 'foreigners’': 27160, 'nellie': 27161, 'love–hate': 27162, 'feeder': 27163, \"huxley's\": 27164, 'khatri': 27165, 'lagheri': 27166, 'rajesh’s': 27167, \"gq's\": 27168, \"'nerd\": 27169, 'singlehandedly': 27170, \"biz's\": 27171, 'tupman': 27172, 'snodgrass': 27173, '1836–37': 27174, 'debtors’': 27175, 'chesterton': 27176, '“before': 27177, 'thundering': 27178, 'clamorous': 27179, 'inns': 27180, 'swaggering': 27181, \"britain'\": 27182, 'kunihiko': 27183, 'kyochiro': 27184, \"hidaka's\": 27185, '9780751336818': 27186, 'akumatized': 27187, 'embody': 27188, 'akuma': 27189, \"'ab\": 27190, 'armoury': 27191, 'paddles': 27192, 'flicks': 27193, 'wanderers': 27194, 'newlands': 27195, \"ab's\": 27196, 'warmbaths': 27197, 'pyrotechnics': 27198, 'deflect': 27199, 'india–pakistan': 27200, 'deteriorated': 27201, 'postulates': 27202, 'recontextualizes': 27203, 'enriches': 27204, 'unenduring': 27205, 'misanthropic': 27206, 'lounging': 27207, \"'til\": 27208, 'snort': 27209, 'betrays': 27210, 'detest': 27211, 'hauntings': 27212, 'batman’s': 27213, 'crimefighters': 27214, 'tynion': 27215, 'seeley': 27216, 'fabok': 27217, 'pasarin': 27218, 'guéra': 27219, 'pistols': 27220, 'rut—we': 27221, 'incentive': 27222, 'potato—movement': 27223, 'activities—be': 27224, 'picnic—and': 27225, 'airspace': 27226, 'cornfields': 27227, 'illinois': 27228, 'propeller': 27229, 'biplane': 27230, 'itinerant': 27231, 'barnstormer': 27232, 'shimoda': 27233, 'wrenches': 27234, 'livingston': 27235, 'messiahs': 27236, 'avail': 27237, 'teething': 27238, 'matthan': 27239, 'cautioning': 27240, 'harms': 27241, 'benefiting': 27242, 'estd': 27243, 'memorising': 27244, \"logophilia's\": 27245, 'paedagogy': 27246, 'sophisticating': 27247, 'experiential': 27248, 'philia': 27249, 'navsari': 27250, 'hydel': 27251, \"tatas'\": 27252, \"'aren't\": 27253, \"solo'\": 27254, 'pitons': 27255, \"soloing'\": 27256, 'sendero': 27257, 'luminoso': 27258, 'matte': 27259, '124': 27260, \"'brazil\": 27261, \"asia'\": 27262, \"'novy\": 27263, 'nayeemuddin': 27264, 'dronacharya': 27265, 'validate': 27266, 'caches': 27267, 'balancers': 27268, 'rdbms': 27269, 'postgres': 27270, 'coupling': 27271, 'stateless': 27272, 'opsworks': 27273, 'beanstalk': 27274, 'jalebis': 27275, '870': 27276, 'pictography': 27277, 'ideography': 27278, 'ojibway': 27279, 'unimaginative': 27280, 'tripled': 27281, 'kahney': 27282, \"'insatiability\": 27283, \"eye'\": 27284, 'idealize': 27285, 'omnipresent': 27286, \"'sontag\": 27287, \"appetites'the\": 27288, \"ourselves'washington\": 27289, \"subject'new\": 27290, \"agrawal's\": 27291, 'congratulate': 27292, 'awl': 27293, 'immature': 27294, 'banalities': 27295, 'none’': 27296, 'trenches”': 27297, '“try': 27298, 'coupon': 27299, 'rosabeth': 27300, 'kanter': 27301, 'tweak': 27302, \"customers'\": 27303, '‘rujuta': 27304, 'karishma': 27305, 'sonali': 27306, 'secret―you': 27307, 'unbidden': 27308, 'bebo’s': 27309, 'want―including': 27310, 'parathas―and': 27311, 'stars―now': 27312, 'captaining': 27313, 'acrimonious': 27314, 'posse': 27315, 'newshounds': 27316, 'beckham’s': 27317, 'figo': 27318, 'eriksson': 27319, 'midterm': 27320, \"quintuplets'\": 27321, \"futaro's\": 27322, 'arounds': 27323, 'sausage': 27324, \"chris's\": 27325, 'proudest': 27326, 'weakest': 27327, 'work—until': 27328, \"ockham's\": 27329, 'havealso': 27330, 'indexed': 27331, 'elaborations': 27332, 'endorphins': 27333, 'reliever': 27334, 'downside': 27335, 'quadriceps': 27336, 'neglects': 27337, 'remainder': 27338, 'taxed': 27339, 'smoother': 27340, 'bott': 27341, \"crompton's\": 27342, 'sherry': 27343, 'petersik': 27344, 'borawski': 27345, 'bookcases': 27346, 'rugs': 27347, 'repaint': 27348, 'placements': 27349, 'sterility': 27350, 'aftercare': 27351, 'piercer': 27352, 'vidra': 27353, 'rn': 27354, 'edd': 27355, 'sifts': 27356, \"sanjay's\": 27357, 'hindsight': 27358, \"'zen\": 27359, 'jest': 27360, 'sportswriters': 27361, \"preacher's\": 27362, 'knicks': 27363, 'rodman': 27364, \"'uncoachable'\": 27365, 'wellsprings': 27366, 'palgrave': 27367, 'xiv': 27368, 'victimised': 27369, 'swaying': 27370, 'icebreakers': 27371, '“walk': 27372, 'shame”': 27373, 'airline’s': 27374, '“technical': 27375, 'snag”': 27376, \"how's\": 27377, 'breezing': 27378, 'piparaiya': 27379, 'printable': 27380, 'lookup': 27381, 'oddballs': 27382, 'sheik': 27383, 'razek': 27384, 'recycling': 27385, 'recycle”': 27386, '“cradle': 27387, 'grave”': 27388, 'minimising': 27389, 'pahari': 27390, 'predetermined': 27391, 'enumerates': 27392, 'tannen': 27393, 'conger': 27394, \"stakeholders'\": 27395, 'flatulence': 27396, 'minaret': 27397, 'hyenas': 27398, 'loo': 27399, 'sunrises': 27400, 'uzbekistan': 27401, 'pamirs': 27402, 'punctuating': 27403, 'droll': 27404, 'phoolan': 27405, 'veerappan': 27406, 'prisoners’': 27407, 'deodhar': 27408, 'monopolistic': 27409, 'oligarchy': 27410, \"'hands\": 27411, 'ghnpca': 27412, \"thrillist's\": 27413, \"powell's\": 27414, \"1900's\": 27415, 'deco': 27416, 'monstress': 27417, 'vona': 27418, \"liu's\": 27419, 'northstar': 27420, 'glaad': 27421, 'takeda': 27422, 'niigata': 27423, 'wendell': 27424, 'customization': 27425, 'mentoring': 27426, 'schemas': 27427, 'innodb': 27428, 'partitioned': 27429, 'cpus': 27430, 'disks': 27431, 'greeted': 27432, 'tweaking': 27433, 'debuggers': 27434, 'emulators': 27435, 'fuzz': 27436, 'encrypted': 27437, 'pydbg': 27438, 'sulley': 27439, 'idapython': 27440, 'pyemu': 27441, 'handiwork': 27442, \"oakshott's\": 27443, 'altamont': 27444, 'cornwallis': 27445, \"coronet'\": 27446, \"creator's\": 27447, 'surpasses': 27448, '605': 27449, 'ulsan': 27450, 'mutate': 27451, 'mcwhorter': 27452, 'hidebound': 27453, 'pidgins': 27454, 'creoles': 27455, 'nonstandard': 27456, 'mcduff': 27457, 'landowner': 27458, 'mitya': 27459, 'parricide': 27460, 'rifts': 27461, 'mikhailovich': 27462, '1821': 27463, '1849': 27464, \"gogol's\": 27465, 'fluctuations': 27466, \"karamazov'\": 27467, \"'dostoyevsky\": 27468, 'windfalls': 27469, 'famous…': 27470, 'cells’': 27471, 'selp': 27472, 'helf': 27473, '“leaked”': 27474, 'diarrhe': 27475, 'khichdi': 27476, 'divinity—he': 27477, 'trudy': 27478, 'banal': 27479, \"trudy's\": 27480, 'middlegame': 27481, 'positional': 27482, 'chessplayer': 27483, \"'comprehensive\": 27484, '5b': 27485, \"'how'\": 27486, \"'from'\": 27487, \"'soon'\": 27488, \"'street'\": 27489, 'wyk': 27490, 'bouma': 27491, 'archbishop': 27492, 'indict': 27493, 'guillotine': 27494, 'bouquet': 27495, 'quay': 27496, 'querulous': 27497, 'europol’s': 27498, 'gesicht': 27499, 'murders—the': 27500, 'communication—including': 27501, 'mail—and': 27502, 'read—and': 27503, 'marquess': 27504, 'lusty': 27505, \"'move\": 27506, \"day'\": 27507, \"'several\": 27508, 'connives': 27509, 'leans': 27510, \"akriti's\": 27511, \"siddhant's\": 27512, 'scraping': 27513, '6c': 27514, '6a': 27515, \"walkenbach's\": 27516, 'aihole': 27517, 'pattadakal': 27518, 'mahakuta': 27519, 'malprabha': 27520, 'chalukyas': 27521, '6th–8th': 27522, 'chalukya': 27523, 'playable': 27524, 'yahtzee': 27525, 'kids—you': 27526, \"interior'\": 27527, \"'turning\": 27528, 'dee': 27529, \"tori's\": 27530, \"'relatable\": 27531, \"bombs'\": 27532, 'dissonant': 27533, 'shinier': 27534, \"'identifiable\": 27535, 'giovanna': 27536, 'roisin': 27537, 'meaney': 27538, \"'sure\": 27539, \"thirties'\": 27540, \"deception'\": 27541, \"'sublime\": 27542, \"observed'\": 27543, 'incinerates': 27544, \"'injected\": 27545, \"'well\": 27546, 'sheerluxe': 27547, 'similarity': 27548, 'weighs': 27549, '1778': 27550, 'washington’s': 27551, 'jamie’s': 27552, 'frasers': 27553, '“another': 27554, '”—library': 27555, 'juggles': 27556, 'sizable': 27557, 'plotlines': 27558, 'bedtimes': 27559, \"universal'\": 27560, \"'letters\": 27561, \"daughter'\": 27562, 'mussourie': 27563, \"'phileas\": 27564, 'superfluous': 27565, 'wagers': 27566, '‘ghosts’': 27567, 'favoritess': 27568, 'bun': 27569, '‘respect': 27570, 'breakfast’': 27571, '‘uncle': 27572, 'audaciously': 27573, 'kurva': 27574, 'blindsided': 27575, 'wife’s': 27576, 'chaaya': 27577, 'broking': 27578, 'birdman': 27579, 'literature—a': 27580, 'dphil': 27581, 'mukund': 27582, \"tata's\": 27583, 'custodian': 27584, 'franta': 27585, 'sensation—so': 27586, 'paramhansa': 27587, 'yogananda': 27588, 'yogananda’s': 27589, 'kriya': 27590, 'gorakhpur': 27591, 'bogotá': 27592, 'livesandshows': 27593, 'neighbourhoods': 27594, \"answer'\": 27595, 'experimentalist': 27596, 'highlanders': 27597, 'disconcerting': 27598, 'courageous―or': 27599, 'enticing―woman': 27600, 'bonny': 27601, 'lass': 27602, 'nunnery': 27603, 'bridgman': 27604, \"bridgman's\": 27605, 'repackages': 27606, 'dorks': 27607, \"bff's\": 27608, \"nikki's\": 27609, 'dork': 27610, 'crumbly': 27611, 'akihiko': 27612, \"kayaba's\": 27613, 'vrmmo': 27614, 'rapier': 27615, 'aria': 27616, 'rondo': 27617, 'interrogated': 27618, 'tightening': 27619, \"'constantly\": 27620, \"'theres\": 27621, \"'something\": 27622, \"'takes\": 27623, \"sinister'\": 27624, \"'frightening'\": 27625, \"'terrific'\": 27626, \"done'\": 27627, 'koch': 27628, \"'claustrophobic\": 27629, \"scrutiny'\": 27630, \"downstairs'\": 27631, 'grippling': 27632, 'imogen': 27633, 'masai': 27634, 'siphoning': 27635, \"shareholders'\": 27636, \"bankers'\": 27637, 'misidentify': 27638, 'mariana': 27639, 'mazzucato': 27640, \"'egypt\": 27641, 'mortally': 27642, 'tamose’s': 27643, 'dashed': 27644, 'egypt’s': 27645, 'worthier': 27646, 'reclaimed': 27647, 'goodbooks': 27648, 'propping': 27649, 'lives—the': 27650, '“hit': 27651, 'refresh”': 27652, 'immigrating': 27653, 'ceo—one': 27654, 'ballmer': 27655, 'soul—transforming': 27656, '“ideas': 27657, '“empathy': 27658, 'deliberative': 27659, 'improvement—for': 27660, 'prepositional': 27661, 'congolese': 27662, 'awoken': 27663, 'haggard': 27664, 'brigades': 27665, 'regulars': 27666, 'downplayed': 27667, 'infiltrators': 27668, 'operationally': 27669, 'intruders': 27670, 'objectivist': 27671, \"'stop\": 27672, \"'men\": 27673, 'dagny': 27674, 'taggart': 27675, 'alisa': 27676, 'rosenbaum': 27677, 'emigrated': 27678, 'espoused': 27679, 'eminence': 27680, \"bitterly'\": 27681, \"'atlas\": 27682, \"happiness'\": 27683, 'greenspan': 27684, 'bruisingly': 27685, 'carnal': 27686, 'violation': 27687, 'omnibuses': 27688, 'spines': 27689, 'koreans': 27690, \"beginners'\": 27691, 'delirious': 27692, 'mummify': 27693, 'body—while': 27694, 'decimating': 27695, 'mystery…': 27696, 'mcdeere': 27697, 'poured': 27698, 'exceeded': 27699, \"mitch's\": 27700, 'hate\\x85': 27701, 'football\\x92s': 27702, 'club\\x92s': 27703, 'schemer': 27704, 'man\\x92s': 27705, '\\x93manager': 27706, 'lout\\x94': 27707, 'manager\\x92s': 27708, '\\x93once': 27709, '\\x94can': 27710, 'recognisably': 27711, 'showdowns': 27712, 'season\\x92s': 27713, 'kirby\\x92s': 27714, 'ferguson\\x92s': 27715, 'united\\x92s': 27716, 'gaal': 27717, 'finesta': 27718, 'republik': 27719, 'mancunia': 27720, 'blogthe': 27721, 'fanbuy': 27722, 'fanpraise': 27723, 'europea': 27724, 'reviewerthe': 27725, 'amazonpraise': 27726, 'couragegreat': 27727, 'reviewsexcellent': 27728, \"lesson's\": 27729, 'kowalchyk': 27730, 'rhody': 27731, 'auld': 27732, 'syne': 27733, 'bandleader': 27734, 'wynton': 27735, 'marsalis': 27736, 'dueling': 27737, 'harmonics': 27738, 'liza': 27739, 'lullaby': 27740, 'waltzing': 27741, 'lose…': 27742, 'iima': 27743, 'enrapturing': 27744, 'deference': 27745, 'pennies': 27746, 'baru': 27747, 'knighthood': 27748, 'lowliest': 27749, 'pilkey’s': 27750, 'storiesis': 27751, 'hindusince': 27752, 'tazuna': 27753, 'bridgebuilder': 27754, 'superpowered': 27755, 'garo': 27756, 'cranks': 27757, 'bandhopadhyay': 27758, 'rejoiced': 27759, 'ason': 27760, 'sired': 27761, 'manobi—the': 27762, 'manobi': 27763, 'girls’': 27764, 'lawnmageddon': 27765, 'seer': 27766, 'shibabawa': 27767, 'homogenizing': 27768, 'bathers': 27769, \"taseer's\": 27770, 'calculation': 27771, 'glaciers': 27772, 'mulkila': 27773, 'rakshasini': 27774, 'barsi': 27775, 'nullah': 27776, 'bhoot': 27777, 'chandrataal': 27778, 'joginis': 27779, 'gills': 27780, \"write'\": 27781, 'residing': 27782, 'stifle': 27783, 'celaena': 27784, 'sardothien': 27785, \"celaena's\": 27786, \"ceo'\": 27787, \"'person\": 27788, 'thoughtprovoking': 27789, 'karumanchi': 27790, 'recursions': 27791, 'careermonk': 27792, 'pretense': 27793, 'other—every': 27794, 'honolulu': 27795, 'microbiology': 27796, 'nanigen': 27797, 'microtechnologies': 27798, 'edge…': 27799, \"gordon's\": 27800, 'passives': 27801, 'scratched': 27802, \"coworkers'\": 27803, 'backported': 27804, 'mariatta': 27805, 'wijaya': 27806, 'belderbos': 27807, 'felder': 27808, 'bart': 27809, 'ultramarathons': 27810, 'particularities': 27811, 'success—': 27812, '825': 27813, 'aphorisms—from': 27814, 'filmmaking—all': 27815, 'energize': 27816, 'principles—including': 27817, 'human—including': 27818, 'existence—health': 27819, 'achievement—work': 27820, 'artists—art': 27821, 'liberation—conditioning': 27822, 'becoming—self': 27823, 'actualization': 27824, 'principles—yin': 27825, 'giver': 27826, 'truth—he': 27827, '—bruce': 27828, 'nexus': 27829, 'paymaster': 27830, 'unrevealed': 27831, 'posturing': 27832, 'dichotomies': 27833, \"peoples'\": 27834, 'grime': 27835, \"janam's\": 27836, '—elizabeth': 27837, 'today—or': 27838, 'so—than': 27839, 'reframes': 27840, 'war—her': 27841, 'neverending': 27842, 'all—never': 27843, 'proportions—one': 27844, \"'thrilling\": 27845, 'godzilla': 27846, 'triassic': 27847, 'cretaceous': 27848, 'paleontologist': 27849, 'tyrannosaurs': 27850, 'carnivores': 27851, 'raptor': 27852, 'impassively': 27853, 'crusted': 27854, 'groan': 27855, 'sandpaper': 27856, 'wisps': 27857, 'mists': 27858, \"irvine's\": 27859, 'pillai’s': 27860, 'terminologies': 27861, '1348': 27862, \"boccaccio's\": 27863, '—boing': 27864, 'boing': 27865, 'reza': 27866, 'farazmand': 27867, 'skewers': 27868, 'reuniting': 27869, 'phone—and': 27870, 'shitty': 27871, 'shit': 27872, 'banter': 27873, \"indu's\": 27874, \"hutchinson's\": 27875, 'science’s': 27876, 'frogs’': 27877, 'lactic': 27878, 'electrode': 27879, 'subliminal': 27880, 'baldessari': 27881, 'lorca': 27882, 'dicorcia': 27883, 'meiselas': 27884, 'soth': 27885, 'polaroid': 27886, 'slack': 27887, 'fulford': 27888, 'halpern': 27889, 'scholasticism': 27890, '“friendship”': 27891, '“truth': 27892, 'glib': 27893, 'expositions': 27894, 'incline': 27895, 'dream—and': 27896, 'talent—for': 27897, 'dressmaker': 27898, 'irishman': 27899, 'society—she': 27900, \"flynn's\": 27901, 'milborne': 27902, 'unsteadiness': 27903, 'sheringham': 27904, 'wantage': 27905, 'slumped': 27906, 'rjr': 27907, 'nabisco': 27908, 'inconsistent': 27909, 'stodgy': 27910, 'departmental': 27911, 'divesting': 27912, 'unneeded': 27913, 'gooder': 27914, 'disconsolate': 27915, 'maguire': 27916, 'mishandled': 27917, 'floundering': 27918, \"underdog's\": 27919, 'manpower': 27920, 'bale': 27921, 'ronaldinho': 27922, 'mesut': 27923, 'oezil': 27924, 'tekkers': 27925, 'f2playlikeapro': 27926, 'salutation': 27927, 'prana': 27928, 'purification': 27929, 'bija': 27930, 'physiological': 27931, 'therapeutic': 27932, '326': 27933, 'nandas': 27934, 'embers': 27935, 'tamers': 27936, 'moriya': 27937, 'magadhan': 27938, 'whoosh': 27939, 'vernicious': 27940, 'vindictive': 27941, 'gizzard': 27942, 'nell': 27943, 'zink': 27944, 'armistead': 27945, 'maupin': 27946, \"'charming\": 27947, 'languid': 27948, \"arthur'\": 27949, \"'marvellously\": 27950, 'shteyngart': 27951, \"'bedazzling\": 27952, \"wonderful'\": 27953, 'haslett': 27954, 'despatched': 27955, 'sap’s': 27956, 'brf': 27957, 'brfplus': 27958, 'archivelink': 27959, 'management—processes': 27960, 'bloomed': 27961, 'weasel': 27962, 'gob': 27963, 'smacking': 27964, 'bonanza': 27965, 'inscriptions': 27966, 'terri': 27967, 'libenson': 27968, 'emmie': 27969, 'cheery': 27970, \"emmie's\": 27971, \"stiglitz's\": 27972, 'mismanagement': 27973, \"urgently'\": 27974, 'hutton': 27975, \"'stiglitz\": 27976, 'ruffled': 27977, \"establishment'\": 27978, \"keynes'\": 27979, 'blackburn': 27980, \"carlton's\": 27981, \"ramsey's\": 27982, \"linklater's\": 27983, 'memorabilia': 27984, 'remarkable—': 27985, 'investor—nor': 27986, 'buf\\xadfett’s': 27987, 'writ\\xading': 27988, 'accomplishments—and': 27989, 'occa\\xadsional': 27990, 'arti\\xadcle': 27991, '1977’s': 27992, 'inf': 27993, 'lation': 27994, 'swindles': 27995, 'tobias’s': 27996, '“letters': 27997, 'berk\\xadshire': 27998, 'shareholder': 27999, '“avoiding': 28000, '“enough': 28001, 'work’s': 28002, 'under\\xadstanding': 28003, 'cetmol': 28004, 'suturing': 28005, 'transplantation': 28006, 'hugs': 28007, 'thrice': 28008, 'hrudi': 28009, 'calumniated': 28010, 'gynecologist': 28011, 'inebriated': 28012, 'hrishikesh': 28013, 'wrecking': 28014, 'hrish': 28015, 'mended': 28016, 'प्रस्तुत': 28017, 'कोष': 28018, 'बहु': 28019, 'प्रचलित': 28020, 'जमा': 28021, 'अच्छी': 28022, 'हिंदी': 28023, 'आवश्यक': 28024, 'अंग': 28025, 'बन': 28026, 'जिनकी': 28027, 'सहाय्यतासे': 28028, 'साहित्य': 28029, 'अच्छा': 28030, 'दन्यांन': 28031, 'प्राप्त': 28032, 'जा': 28033, 'सकता': 28034, 'विशेष': 28035, 'ध्यान': 28036, 'रखा': 28037, 'कि': 28038, 'सभी': 28039, 'जरूरी': 28040, 'इसमें': 28041, 'अवश्य': 28042, 'जाएँ': 28043, 'आज': 28044, 'पीढ़ी': 28045, 'उन': 28046, 'पाठकों': 28047, 'बहुत': 28048, 'उपयोगी': 28049, 'मुहावरों': 28050, 'शायरी': 28051, 'सुनकर': 28052, 'दिलचस्पी': 28053, 'इजहार': 28054, 'तो': 28055, 'करते': 28056, 'लेकिन': 28057, 'भाषा': 28058, 'अपरिचित': 28059, 'कारण': 28060, 'उसका': 28061, 'आनंद': 28062, 'उठा': 28063, 'पाते': 28064, 'sisal': 28065, 'septic': 28066, 'humid': 28067, 'temporate': 28068, 'firemen': 28069, '“family': 28070, 'mindless': 28071, 'awed': 28072, 'candlelit': 28073, 'antibes': 28074, 'recoiling': 28075, 'corsican': 28076, 'terminate': 28077, 'erotica': 28078, 'handcuffs': 28079, 'luncheons': 28080, 'psychosexual': 28081, 'camlin': 28082, 'freshman': 28083, 'eds': 28084, 'rowdy': 28085, 'brothersseries': 28086, 'disasterfor': 28087, 'madison’s': 28088, 'financially': 28089, 'batman™': 28090, 'superman™': 28091, 'woman™': 28092, 'wb': 28093, 's14': 28094, 'top—twice': 28095, 'chuan': 28096, '“i’ve': 28097, 'waitzkin’s': 28098, 'climactic': 28099, 'competitor’s': 28100, 'petrovich': 28101, 'nihilist': 28102, 'bazarov': 28103, \"arkady's\": 28104, 'landowning': 28105, 'tongs': 28106, 'aresure': 28107, 'deary': 28108, 'grippando': 28109, 'everglades': 28110, 'dated': 28111, 'abe’s': 28112, 'fast—but': 28113, 'today–a': 28114, 'semiautobiographical': 28115, 'worldwar': 28116, \"mizuki's\": 28117, 'albumaward': 28118, 'nonnonba': 28119, 'angoulême': 28120, 'kyokujitsu': 28121, 'sho': 28122, 'shiju': 28123, 'hosho': 28124, 'kodanshamanga': 28125, 'sakaiminato': 28126, 'road―a': 28127, 'kitaro': 28128, 'characters―and': 28129, 'porcello': 28130, 'reloads': 28131, 'reactís': 28132, 'potnis': 28133, 'pansy': 28134, 'rihana': 28135, 'repeats': 28136, \"rihana's\": 28137, 'farmer’s': 28138, 'fangled': 28139, 'calves': 28140, 'magnus': 28141, 'folder': 28142, \"6x6''\": 28143, \"father'\": 28144, \"mayor's\": 28145, \"buttigieg's\": 28146, 'chasten': 28147, 'glezman': 28148, 'reels': 28149, 'revitalized': 28150, 'flyover': 28151, 'despaired': 28152, 'recur': 28153, 'herr': 28154, 'friedemann': 28155, 'gladius': 28156, 'dei': 28157, 'puritanical': 28158, 'centerpiece': 28159, 'dreary': 28160, 'wets': 28161, \"lindqvist's\": 28162, 'easy—and': 28163, 'pace—building': 28164, 'pivottable': 28165, 'views—even': 28166, 'vista®': 28167, 'ereference—plus': 28168, 'medias': 28169, 'oreillys': 28170, 'booktech': 28171, 'repurposed': 28172, 'baptized': 28173, 'irritatingly': 28174, 'disarm': 28175, 'her—until': 28176, 'terrorized': 28177, 'discredit': 28178, 'councilman': 28179, 'destroyer': 28180, 'southerner': 28181, 'supplicant': 28182, 'thatís': 28183, 'blockchainóideal': 28184, 'exquisiteness': 28185, 'snowcapped': 28186, 'magnify': 28187, 'olpc': 28188, 'raspberry': 28189, 'briggs': 28190, 'ravens': 28191, 'dissected': 28192, '—a': 28193, 'tkinter': 28194, 'elusiveness': 28195, 'bulldozes': 28196, 'grate': 28197, 'vishwanath': 28198, 'toffler’s': 28199, 'inhabit—the': 28200, 'supermarkets': 28201, '“info': 28202, 'wars”': 28203, 'arise—not': 28204, '“fast”': 28205, '“slow': 28206, 'wariness': 28207, '”—newsday': 28208, 'trimble': 28209, 'right—even': 28210, 'womanly': 28211, 'drinkers': 28212, 'nalebuff': 28213, 'sweetened': 28214, '0000': 28215, 'drugstores': 28216, 'mouseford': 28217, 'delarattand': 28218, 'film’s': 28219, 'delarattas': 28220, 'snobby': 28221, '—can’t': 28222, 'sethi': 28223, 'exceeds': 28224, 'animus': 28225, 'desmond’s': 28226, 'deliverance': 28227, 'cesare': 28228, 'borgia': 28229, 'wearer': 28230, '200th': 28231, 'cardrooms': 28232, 'newness': 28233, 'pokersnowie': 28234, 'piosolver': 28235, 'polarized': 28236, 'overbets': 28237, 'townsende': 28238, 'republic’s': 28239, 'lando': 28240, 'calrissian': 28241, 'council—only': 28242, 'tetons': 28243, 'valley—a': 28244, 'impish': 28245, 'deepti': 28246, 'moushumi': 28247, \"actors'\": 28248, 'guffaws': 28249, \"saran's\": 28250, 'seyyed': 28251, 'hossein': 28252, 'nasr': 28253, \"'ruth\": 28254, \"dugdall's\": 28255, \"wilks'\": 28256, 'hatcher': 28257, 'remorseful': 28258, 'champion’s': 28259, '“inside': 28260, 'head”': 28261, 'faced—including': 28262, 'confidant—and': 28263, 'pete’s': 28264, 'sampras': 28265, 'seemes': 28266, '—jon': 28267, 'wertheim': 28268, '‘theo': 28269, 'dexter’': 28270, 'theo’s': 28271, 'theresa’s': 28272, 'farmhouse': 28273, 'lubéron': 28274, 'boules': 28275, \"appreciative'\": 28276, \"'stylish\": 28277, 'ethiopian': 28278, \"mussolini's\": 28279, \"'scalpel\": 28280, \"sonata'\": 28281, \"'its\": 28282, 'lynn': 28283, 'coldstream': 28284, 'celia': 28285, 'brayfield': 28286, 'turquoise': 28287, 'systems—the': 28288, 'vehicular': 28289, 'transportation—that': 28290, 'longship': 28291, \"atwood's\": 28292, 'handmaiden': 28293, 'ileadean': 28294, 'evaded': 28295, \"hunters'\": 28296, 'defends': 28297, 'dogmas': 28298, 'sycophancy': 28299, 'alveston': 28300, 'cottage…': 28301, '‘mrs': 28302, 'mcginty’s': 28303, '‘down': 28304, 'tasteless': 28305, 'mcginty': 28306, 'pitifully': 28307, 'and—something': 28308, 'imagined—against': 28309, 'srinath': 28310, 'raghavan': 28311, 'raghavan’s': 28312, 'substantiated': 28313, 'pgdm': 28314, 'degeneracy': 28315, 'blatantly': 28316, 'commonalities': 28317, 'aggregate': 28318, 'deletes': 28319, '‘left': 28320, 'overs’': 28321, 'knowhow': 28322, 'scarrow': 28323, 'britannia': 28324, 'eagles': 28325, \"cato's\": 28326, 'cement': 28327, 'passable': 28328, 'snowstorms': 28329, \"macro's\": 28330, 'vicinity': 28331, 'thinning': 28332, \"soldier's\": 28333, 'legate': 28334, 'quintatus': 28335, 'lair': 28336, 'hujli': 28337, 'coconuts': 28338, \"shelley's\": 28339, 'diplomat’s': 28340, 'mustn’t': 28341, 'discovered…': 28342, 'daydreaming': 28343, '‘supposing': 28344, 'all…': 28345, 'harbus': 28346, \"hbs's\": 28347, 'forthinking': 28348, \"visitor's\": 28349, 'lanscape': 28350, 'topographical': 28351, 'canton': 28352, 'graubunden': 28353, 'thematizing': 28354, \"pirate's\": 28355, 'sirius': 28356, '213': 28357, 'quintus': 28358, 'quintus’': 28359, 'aurelia': 28360, 'unnameable': 28361, 'camber': 28362, 'rabid': 28363, 'mynah': 28364, 'mo': 28365, 'donaldson’s': 28366, 'scarecrows': 28367, 'o’barley': 28368, 'o’hay': 28369, 'reginald': 28370, 'antonym': 28371, 'oaths': 28372, 'prospector': 28373, 'afrikaaner': 28374, 'extravagent': 28375, 'ninetieth': 28376, 'toasts': 28377, 'ambition…': 28378, 'sorna': 28379, 'alpiran': 28380, \"'engrossing'\": 28381, \"'powerful'\": 28382, 'sffworld': 28383, \"'compelling'\": 28384, \"wolf's\": 28385, 'draconis': 28386, 'memoria': 28387, 'to—it’s': 28388, 'rad': 28389, 'endanger': 28390, 'secreted': 28391, \"client's\": 28392, \"issa's\": 28393, 'incongruous': 28394, 'frères': 28395, 'scenting': 28396, 'prickles': 28397, 'crabby': 28398, 'mosquito': 28399, 'muscly': 28400, 'meathead': 28401, 'shotover': 28402, 'scarp': 28403, 'occupants': 28404, \"sanctuary's\": 28405, 'scablands': 28406, \"pullman's\": 28407, 'umberto': 28408, \"eco's\": 28409, 'dazed': 28410, \"myself'\": 28411, \"'tremendous\": 28412, \"momentum'\": 28413, 'covetable': 28414, 'sharpshooter': 28415, 'wraith': 28416, 'heartrender': 28417, 'throttle': 28418, 'diamonds…': 28419, 'zhamair': 28420, 'sauveterre’s': 28421, 'fling—always': 28422, 'surrenders': 28423, 'sauveterre': 28424, 'quagmire': 28425, 'feedbacks': 28426, \"'click'\": 28427, '‘click’': 28428, 'nishio': 28429, 'skyrocket': 28430, 'phindia': 28431, 'rajibmall': 28432, 'mc': 28433, 'jurgens': 28434, 'flashpoint…the': 28435, 'convergence…and': 28436, 'kent—its': 28437, 'distrusted—and': 28438, 'crusading': 28439, 'behind—and': 28440, \"'indu\": 28441, 'reconstructed': 28442, 'jagat': 28443, 'gosisni': 28444, 'combats': 28445, 'junta': 28446, 'khurram': 28447, 'bestows': 28448, 'odi’s': 28449, 'paraguay': 28450, 'complained': 28451, 'gravestone': 28452, 'cemetry': 28453, 'lanigan': 28454, 'chandni': 28455, 'chowk': 28456, 'deb’s': 28457, 'deceiver': 28458, \"service's\": 28459, 'mandarins': 28460, 'controllable': 28461, 'genhis': 28462, 'pankratin': 28463, 'qaddafi': 28464, 'marginalised': 28465, 'archived': 28466, 'yasir': 28467, 'abbasi': 28468, 'mehdi': 28469, 'jaidev': 28470, 'balraj': 28471, 'sahni': 28472, 'introspecting': 28473, 'flowchart': 28474, 'chessboard': 28475, \"fischer's\": 28476, 'displacing': 28477, 'limitation': 28478, 'hurob': 28479, 'cor': 28480, 'contritum': 28481, 'dobbs': 28482, \"schneier's\": 28483, 'shostack': 28484, 'validated': 28485, 'discern': 28486, 'shootout—involving': 28487, 'taxis—in': 28488, 'york’s': 28489, 'honchos': 28490, 'verdict—and': 28491, 'pacino—lumet': 28492, '‘let': 28493, 'c’': 28494, 'vjti': 28495, 'kanpur': 28496, \"'microsoft\": 28497, \"profession'\": 28498, 'kset': 28499, 'unfading': 28500, 'didactic': 28501, \"druids'\": 28502, 'carnutes': 28503, 'asterixian': 28504, 'webbed': 28505, 'smattering': 28506, 'episodic': 28507, 'disﬁgure': 28508, 'gordon’s': 28509, 'christa': 28510, 'faust': 28511, 'hilly': 28512, 'darjeeling': 28513, 'thukpa': 28514, 'aama': 28515, 'grandfather’s': 28516, 'purdah': 28517, '‘liberal’': 28518, 'claustrophobia': 28519, 'unsentimental': 28520, 'descriptionrecommendation': 28521, 'fnished': 28522, 'fltering': 28523, 'watter': 28524, 'peanuts': 28525, 'krazy': 28526, 'doonesbury': 28527, 'borgman': 28528, 'steadman': 28529, \"exhibition's\": 28530, 'exaggerated': 28531, 'contestations': 28532, 'mobilization': 28533, \"'work\": 28534, \"progress'\": 28535, 'majoritarian': 28536, 'distortions': 28537, '‘bluey’': 28538, 'jonny’s': 28539, 'stinson’s': 28540, 'code—the': 28541, 'resembling': 28542, '“bro”': 28543, '“unspoken”': 28544, 'hos': 28545, 'sidebros': 28546, 'stinson': 28547, 'dudes': 28548, 'duh': 28549, 'series—newly': 28550, 'inpenetrable': 28551, 'core—and': 28552, 'tragedy—will': 28553, 'hardin—any': 28554, 'tessa—and': 28555, 'life—away': 28556, 'him—not': 28557, 'for—hardin': 28558, \"mist'\": 28559, 'gramma': 28560, 'hug': 28561, 'bloodied': 28562, 'unbowed': 28563, 'council’s': 28564, 'deceitful': 28565, 'councilors': 28566, 'kalidasa’s': 28567, 'signaling': 28568, 'suharto’s': 28569, 'seizure': 28570, '“global': 28571, 'economy”': 28572, 'indonesians': 28573, 'bank’s': 28574, '“model': 28575, '‘war': 28576, 'subjugation': 28577, 'persuasion―from': 28578, '“soft”': 28579, 'skill―it': 28580, 'irreplaceable―earning': 28581, 'airbnb’s': 28582, 'aristotle’s': 28583, 'valuable―not': 28584, 'age―the': 28585, 'economy―you': 28586, 'exceptionality': 28587, 'stingy': 28588, 'expenditures': 28589, 'incidence': 28590, 'sujatha': 28591, 'congratulatory': 28592, 'glinting': 28593, \"tragedy'\": 28594, 'corelli': 28595, 'cephalonia': 28596, 'ostracised': 28597, 'bestial': 28598, \"'louis\": 28599, 'bernières': 28600, \"taste'\": 28601, 'poseidon': 28602, 'dodgeball': 28603, 'vendetti': 28604, 'futaki': 28605, 'tamàs': 28606, 'gàspàr': 28607, '“there’s': 28608, 'colourless': 28609, 'skein': 28610, 'isolate': 28611, '”sherlock': 28612, 'watson—the': 28613, 'era—and': 28614, 'fosters': 28615, 'bonding': 28616, 'amplitude': 28617, 'baseband': 28618, 'roses…': 28619, 'polarised': 28620, \"mayer's\": 28621, \"renta's\": 28622, 'dotcom': 28623, 'idolising': 28624, \"loeb's\": 28625, 'carlson': 28626, 'psychoanalytical': 28627, 'theorist': 28628, 'lacanian': 28629, 'freudianism': 28630, \"'mirror\": 28631, \"phase'\": 28632, 'gonzo': 28633, 'addle': 28634, 'gilliam': 28635, 'depp': 28636, 'pty': 28637, 'handbooks': 28638, 'ratnasagar': 28639, 'aakasha': 28640, 'gandharvas': 28641, 'apsaras': 28642, 'slayer': 28643, 'impervious': 28644, '“scholarly': 28645, 'fields—but': 28646, 'darwin’s': 28647, 'gravely': 28648, 'miscalculated': 28649, 'astrophysicist': 28650, '“big': 28651, 'bang”': 28652, 'objections': 28653, 'disproven': 28654, 'universe—and': 28655, 'livio': 28656, 'luminously': 28657, 'itself”': 28658, 'written”': 28659, 'scientists—and': 28660, 'sanjoy': 28661, 'hazarika’s': 28662, '‘different’': 28663, '‘idea': 28664, 'india’': 28665, 'swamped': 28666, '‘north': 28667, 'easterner’': 28668, 'critiques': 28669, '‘bangladeshi’': 28670, '‘race': 28671, 'discrimination’': 28672, 'fells': 28673, '”—scientific': 28674, 'tesla’s': 28675, 'channeling': 28676, 'telegraphy': 28677, 'turbines': 28678, 'inventor’s': 28679, '“wizard': 28680, 'seifer': 28681, '”—nelson': 28682, 'science—likely': 28683, '”—american': 28684, '“vivid': 28685, \"way—it's\": 28686, \"greats'\": 28687, \"soul'\": 28688, 'atuan': 28689, 'tehanu': 28690, 'goatherd': 28691, 'gont': 28692, 'roke': 28693, 'archmage': 28694, 'sparrowhawk': 28695, 'tenar': 28696, 'ratty': 28697, \"trailblazer'\": 28698, 'mieville': 28699, 'defected': 28700, 'onset': 28701, 'downtime': 28702, 'commutes': 28703, \"ezio's\": 28704, 'niccolo': 28705, 'defenseless': 28706, 'stormed': 28707, 'unstoppable—and': 28708, 'ina’s': 28709, 'banality': 28710, \"flaubert's\": 28711, 'erotically': 28712, 'flaubert': 28713, \"c'est\": 28714, 'moi': 28715, 'flaubert’s': 28716, 'torrid': 28717, 'wall’s': 28718, \"bovary's\": 28719, 'michèle': 28720, 'penguin’s': 28721, 'partisan': 28722, 'unpartisan': 28723, 'politicised': 28724, 'frank’s': 28725, '11761': 28726, 'accommodates': 28727, '5754': 28728, '5743': 28729, '760': 28730, 'multilateral': 28731, 'preventive': 28732, 'buttocks': 28733, 'capitalisation': 28734, '‘preparing': 28735, 'standardised': 28736, 'test’': 28737, \"dog's\": 28738, 'veterinarian': 28739, 'fogle': 28740, 'rascar': 28741, 'capac': 28742, 'doctored': 28743, 'gipsy': 28744, 'forewarning': 28745, \"gipsy's\": 28746, 'thera': 28747, 'weight—maybe': 28748, 'rotate': 28749, 'squeezing': 28750, 'pelvic': 28751, 'gluteals': 28752, 'supine': 28753, 'extensors': 28754, 'hipwidth': 28755, 'sitz': 28756, 'articulation': 28757, 'latissimus': 28758, 'dorsi': 28759, '“nicholas': 28760, '”—michael': 28761, 'agger': 28762, '“is': 28763, 'crystallized': 28764, 'net’s': 28765, 'bounties': 28766, '“tools': 28767, 'mind”—from': 28768, 'computer—carr': 28769, 'merzenich': 28770, 'kandel': 28771, 'reroute': 28772, 'mcluhan': 28773, 'ethic—a': 28774, 'consumption—and': 28775, 'sparkles': 28776, 'vignettes—friedrich': 28777, 'locomotive—even': 28778, 'plumbs': 28779, \"marjane's\": 28780, 'pepsico': 28781, 'cfo': 28782, 'tropicana': 28783, 'beverage': 28784, \"nooyi's\": 28785, 'nanoparticles': 28786, 'reproducing': 28787, 'chaucerian': 28788, '500–1340': 28789, 'dryden': 28790, 'wordsworth': 28791, 'tennyson': 28792, 'hairpin': 28793, 'spoil': 28794, 'symmetry': 28795, 'sandesh': 28796, 'lightened': 28797, 'assuaged': 28798, 'dentistry': 28799, 'swanky': 28800, 'bandstand': 28801, 'bandra': 28802, 'prescribes': 28803, \"'provocative\": 28804, \"turns'\": 28805, 'gisele': 28806, \"'marrs\": 28807, \"terrifying'\": 28808, \"'has\": 28809, \"'ball'\": 28810, \"'shop'\": 28811, \"'toy'\": 28812, 'species—the': 28813, 'beckoning': 28814, 'war—the': 28815, 'ltte': 28816, 'silvam': 28817, 'toeic': 28818, '’a': 28819, 'masterpiece’': 28820, 'gundill': 28821, 'biceps': 28822, 'forearms': 28823, 'counterstrike': 28824, 'admirals': 28825, 'facedown': 28826, 'beheading': 28827, 'alsace': 28828, 'ramana': 28829, 'maharishi': 28830, 'nefa': 28831, 'cordial': 28832, 'traitorously': 28833, 'brij': 28834, 'kaul': 28835, 'underfed': 28836, 'harshly': 28837, 'natraj': 28838, 'belagere': 28839, \"fanatic's\": 28840, \"andersen's\": 28841, \"brosh's\": 28842, \"lawson's\": 28843, '‘living': 28844, 'friends—the': 28845, 'chir': 28846, 'hornbills': 28847, 'langurs': 28848, 'mynahs': 28849, 'jays—there’s': 28850, \"next'\": 28851, 'nudges': 28852, 'impotent': 28853, 'negligible': 28854, 'jovial': 28855, \"pandey's\": 28856, '“likely': 28857, 'come”': 28858, 'novel—and': 28859, 'royall': 28860, 'glossaries': 28861, 'biles': 28862, 'skateboarding': 28863, 'mcgee': 28864, 'cashes': 28865, 'iowa': 28866, 'too…': 28867, 'persevering': 28868, 'barracks': 28869, 'liberation—or': 28870, 'own—then': 28871, 'comix': 28872, 'satrapi’s': 28873, 'shah’s': 28874, 'marxists': 28875, 'dethroned': 28876, 'whippings': 28877, 'corcoran': 28878, 'cindy': 28879, 'cashman': 28880, 'distill': 28881, 'commitments': 28882, 'gag': 28883, 'kanes': 28884, 'robin—a': 28885, 'superpowers—or': 28886, 'gotham’s': 28887, 'dc’s': 28888, '•includes': 28889, 'regenerative': 28890, '•reveals': 28891, 'fountain': 28892, 'swallowing': 28893, 'propped': 28894, 'rickards': 28895, 'reliquify': 28896, 'heeds': 28897, \"rickards'\": 28898, 'you’d': 28899, '‘grand': 28900, 'dude’': 28901, 'surer': 28902, 'hugard': 28903, 'fredrick': 28904, 'braue': 28905, 'crimps': 28906, 'prearranged': 28907, 'zingone': 28908, 'shakedown': 28909, 'misdirection': 28910, 'verbally': 28911, 'ideational': 28912, 'palimpsest': 28913, \"rider's\": 28914, '“accidental”': 28915, \"'everest\": 28916, \"dots'\": 28917, 'overjoyed': 28918, 'pram': 28919, 'guillam': 28920, '‘4mk’': 28921, 'trinkets': 28922, '‘spoken': 28923, 'enquiries': 28924, 'extempore': 28925, 'alikes': 28926, 'hubs': 28927, 'boons': 28928, 'comm': 28929, 'unications': 28930, \"money's\": 28931, 'placate': 28932, 'lain': 28933, 'parthesh': 28934, 'revolve': 28935, \"candidate's\": 28936, 'characteristics—fathers’': 28937, 'maulana': 28938, 'azad’s': 28939, 'blackswan': 28940, \"recommendation'\": 28941, \"comprehensive'\": 28942, 'cabinets': 28943, 'evangelize': 28944, 'waldorf': 28945, 'reinvigorates': 28946, 'zealots': 28947, 'awuah': 28948, \"ghana's\": 28949, 'sreenivasa': 28950, 'edney': 28951, 'bateman': 28952, 'suzy': 28953, \"mom's\": 28954, 'fajitas': 28955, 'baking': 28956, \"suzy's\": 28957, \"hallie's\": 28958, 'teary': 28959, \"members'\": 28960, 'radiators': 28961, 'clang': 28962, 'manholes': 28963, 'ascher': 28964, 'subways': 28965, 'chandra’s': 28966, 'mangal': 28967, 'pandey’s': 28968, 'laxmi': 28969, 'subash': 28970, \"psychiatrist's\": 28971, 'stevens…': 28972, 'teri': 28973, 'washburn': 28974, 'paranoiac': 28975, 'winner…if': 28976, 'expressly': 28977, 'conjugations': 28978, 'cette': 28979, 'édition': 28980, 'entièrement': 28981, 'refondue': 28982, 'actualisée': 28983, 'découvrez': 28984, 'dictionnaire': 28985, 'fois': 28986, 'complet': 28987, 'économique': 28988, 'génération': 28989, 'facile': 28990, 'consulter': 28991, 'jamais': 28992, 'tout': 28993, 'offrant': 28994, 'utilisable': 28995, 'contexte': 28996, 'scolaire': 28997, 'aussi': 28998, \"qu'en\": 28999, 'chez': 29000, 'soi': 29001, 'ou': 29002, 'précieuses': 29003, 'difficultés': 29004, 'deux': 29005, 'langues': 29006, 'prononciation': 29007, 'conjugaison': 29008, 'tournures': 29009, 'idiomatiques': 29010, 'contribuent': 29011, 'platypus': 29012, 'psychotherapists': 29013, 'wiseguys': 29014, 'heidegger': 29015, 'pearly': 29016, \"klein's\": 29017, 'epicurus': 29018, 'wrecks': 29019, 'punctual': 29020, \"execution'\": 29021, 'blackguards': 29022, 'misanthropy': 29023, 'wickedness': 29024, 'disgusted': 29025, \"début'\": 29026, \"'unlike\": 29027, 'figaro': 29028, 'ferociously': 29029, 'grammars': 29030, 'undecidability': 29031, 'bund': 29032, 'bhawal': 29033, 'zamindar': 29034, 'denounced': 29035, 'unwound': 29036, 'chatterjee’s': 29037, \"'bhawal\": 29038, 'sannyasi': 29039, \"case'—among\": 29040, 'disputes—has': 29041, 'squandered': 29042, 'wights': 29043, \"weeks's\": 29044, 'ponraj': 29045, 'grassroots': 29046, 'disparity': 29047, 'antagonism': 29048, 'befits': 29049, 'dominos': 29050, 'keema': 29051, 'dopyaaza': 29052, '‘hungry': 29053, '‘kitna': 29054, 'deti': 29055, '‘kya': 29056, 'karte': 29057, 'hain': 29058, 'toothpaste': 29059, 'mascots': 29060, 'unbusiness': 29061, 'rans': 29062, 'rhampsinitus': 29063, 'thievery': 29064, 'hardbacks': 29065, \"novella'\": 29066, 'swiftness': 29067, \"savour'\": 29068, \"imagine'\": 29069, 'repeating': 29070, 'firework': 29071, \"'feel\": 29072, 'hollywood’s': 29073, 'envelops': 29074, \"amul's\": 29075, 'namo': 29076, 'polka': 29077, 'potted': 29078, '192': 29079, 'not—especially': 29080, 'media—with': 29081, 'fewest': 29082, 'rupi': 29083, 'concepts—and': 29084, 'behemoth': 29085, 'totaled': 29086, '248': 29087, 'magnates': 29088, 'vengenance': 29089, 'retumed': 29090, \"brionne's\": 29091, 'allards': 29092, 'bamb': 29093, 'beevor': 29094, 'stibbe': 29095, 'apples…': 29096, 'hallowe’en': 29097, 'bobbing': 29098, 'presence’': 29099, 'murderer…': 29100, 'caterpillar': 29101, \"cousin's\": 29102, '280': 29103, \"ge's\": 29104, \"businessman's\": 29105, \"ernest's\": 29106, 'seán': 29107, 'madox': 29108, 'djinni': 29109, \"bartimaeus's\": 29110, 'samarkand': 29111, \"stroud's\": 29112, 'antagonist': 29113, 'trewlove': 29114, 'door—or': 29115, 'alleyway': 29116, 'tavern—gillie': 29117, 'distractingly': 29118, 'recuperating': 29119, 'gillie’s': 29120, 'woman—albeit': 29121, 'one—is': 29122, 'pièce': 29123, 'résistance': 29124, 'thornley’s': 29125, 'comb': 29126, 'philosophising': 29127, 'entanglement': 29128, 'sternwood': 29129, 'tarnished': 29130, 'crooner': 29131, 'epitaph': 29132, \"hawks'\": 29133, 'bacall': 29134, \"'raymond\": 29135, 'slumming': 29136, 'angelos': 29137, \"presence'\": 29138, 'celluloid': 29139, 'westley': 29140, 'buttercup': 29141, 'florin': 29142, 'beefs': 29143, 'evenn': 29144, 'jonn': 29145, 'barnyard': 29146, 'ciphers—algorithms': 29147, 'transposition': 29148, 'vigenère': 29149, 'gibberish': 29150, 'encrypts': 29151, 'decrypts': 29152, 'affine': 29153, 'isues': 29154, 'rotor': 29155, 'muttski': 29156, 'chilidog': 29157, 'setback': 29158, 'stimulant': 29159, 'borussia': 29160, 'dortmund': 29161, 'bundesliga': 29162, 'wooed': 29163, 'kop': 29164, 'tactician': 29165, \"logic's\": 29166, 'goblins': 29167, \"maidens'\": 29168, 'hatchling': 29169, 'fanged': 29170, 'prospectors': 29171, 'forefathers': 29172, 'oceanography': 29173, 'palaeontology': 29174, 'volcanology': 29175, 'geochemistry': 29176, 'petrology': 29177, 'stratigraphic': 29178, 'martian': 29179, 'clientele': 29180, 'anteiku': 29181, 'yoshimura': 29182, \"café's\": 29183, 'harboring': 29184, 'ajmer': 29185, 'gharib': 29186, 'khwaja': 29187, 'signpost': 29188, 'asia—one': 29189, 'supersedes': 29190, 'doctrines—and': 29191, 'qawwali': 29192, 'recitals': 29193, 'example—his': 29194, 'caliph—an': 29195, 'oeuvre': 29196, 'giant—rich': 29197, 'photographs—ajmer': 29198, '“bharat': 29199, 'jai”': 29200, 'aadmi': 29201, 'unspooling': 29202, 'troughs': 29203, 'crests': 29204, 'indestructible': 29205, 'teeter': 29206, 'party—born': 29207, 'movement—get': 29208, 'wrongdoing': 29209, 'accounted': 29210, 'ignoble': 29211, 'all—from': 29212, 'undisclosed': 29213, 'india—from': 29214, 'kejriwal—to': 29215, 'india—of': 29216, 'aap—the': 29217, 'emphatically': 29218, 'reinvesting': 29219, 'uninventive': 29220, 'safeena': 29221, 'bhat': 29222, 'ahanagar': 29223, 'amity': 29224, 'safeena’s': 29225, '‘collateral': 29226, 'damage’': 29227, 'sinful': 29228, 'belongingness': 29229, 'stylized': 29230, 'dimily': 29231, 'gels': 29232, 'paninian': 29233, 'annabeth': 29234, 'ecommerce': 29235, 'crm': 29236, 'chaptered': 29237, 'baddies': 29238, '7x10': 29239, 'eren': 29240, 'childish': 29241, 'greek…and': 29242, 'montgomery’s': 29243, 'lizzie…yet': 29244, 'lizzie’s': 29245, 'name…': 29246, 'milo': 29247, \"klimt's\": 29248, 'querkles': 29249, \"classmate's\": 29250, 'boingy': 29251, 'bussi': 29252, 'lyse': 29253, 'crédule': 29254, 'duc': 29255, 'demotion': 29256, \"rebus's\": 29257, \"saints'\": 29258, 'referendum': 29259, \"'sports\": 29260, \"2016'\": 29261, 'illusive': 29262, 'englishspeaking': 29263, 'allencompassing': 29264, 'anglosaxon': 29265, 'biographically': 29266, 'allegories': 29267, 'twentiethcentury': 29268, 'epochs': 29269, 'clinton’s': 29270, 'gulp': 29271, '‘needless': 29272, '‘missing’': 29273, 'quaid': 29274, \"'relentless\": 29275, 'hiaasen': 29276, 'meltzer': 29277, 'helluva': 29278, \"'marry\": 29279, 'chernow': 29280, 'kwai': 29281, 'pows': 29282, 'siam': 29283, \"nicholson's\": 29284, 'endows': 29285, 'gustave': 29286, 'penrose': 29287, 'feasibility': 29288, 'cluttered': 29289, 'konmari': 29290, 'blockbusting': 29291, 'silge': 29292, 'dplyr': 29293, 'documentís': 29294, 'widyr': 29295, 'usenet': 29296, '‘myth': 29297, 'mythology’': 29298, 'conceptualized': 29299, 'phallic': 29300, 'seers': 29301, 'serpents': 29302, 'decanted': 29303, \"hashmi's\": 29304, 'stabilized': 29305, \"ayaan's\": 29306, 'typecast': 29307, 'chemotherapy': 29308, 'claustrophobic': 29309, \"scenario's\": 29310, 'caretakers': 29311, 'chalking': 29312, 'kisser': 29313, 'euthanasia': 29314, \"bentham's\": 29315, \"mill's\": 29316, 'refinement': 29317, 'immanual': 29318, \"kant's\": 29319, \"‘telos'\": 29320, \"rawl's\": 29321, 'sandinavian': 29322, 'malmo': 29323, 'spa': 29324, \"sweden's\": 29325, 'vulpine': 29326, 'marten': 29327, 'affective': 29328, 'anaesthetist': 29329, 'blomquist': 29330, 'uniformed': 29331, 'svensson': 29332, 'confuses': 29333, 'freeclimber': 29334, 'caldwell': 29335, 'toehold': 29336, \"'caldwell's\": 29337, \"vision'\": 29338, \"honest'\": 29339, \"enriches'\": 29340, 'schoolwork': 29341, 'conceive': 29342, 'sponsor': 29343, \"lomong's\": 29344, 'series—from': 29345, 'abbi': 29346, 'glines—about': 29347, 'ashby': 29348, 'carleton’s': 29349, 'father—so': 29350, 'own—or': 29351, 'go…': 29352, 'dasgupta’s': 29353, 'collation': 29354, 'yielded': 29355, 'propagated': 29356, 'slid': 29357, 'trapping': 29358, 'hypothermic': 29359, 'hydrated': 29360, 'braking': 29361, 'amputating': 29362, 'rappell': 29363, 'hikers': 29364, 'tourniqued': 29365, 'whir': 29366, 'limb': 29367, 'macgillivray': 29368, 'broughton': 29369, 'coburn': 29370, 'humbled': 29371, \"prince's\": 29372, 'aargau': 29373, 'cinderella': 29374, 'sera’s': 29375, \"augustus's\": 29376, 'sera': 29377, 'rolf': 29378, 'scoping': 29379, \"'india’s\": 29380, 'u17': 29381, 'connotations': 29382, 'ifa': 29383, 'parsees': 29384, 'abstracted': 29385, \"'books\": 29386, \"'thirteen\": 29387, 'brookmyre': 29388, 'preclude': 29389, \"'i've\": 29390, \"'quite\": 29391, \"'tore\": 29392, 'sisterson': 29393, \"'wow\": 29394, \"'fantastically\": 29395, 'susi': 29396, 'holliday': 29397, \"'guilty\": 29398, 'cass': 29399, 'palce': 29400, 'fluctuation': 29401, 'mightier': 29402, \"buckingham's\": 29403, 'anatoly': 29404, \"babakov's\": 29405, 'fenwick': 29406, \"giles's\": 29407, \"farthing's\": 29408, 'hybridization': 29409, 'artmap': 29410, 'associative': 29411, 'demonstrative': 29412, 'desirous': 29413, 'gosford': 29414, 'groundhog': 29415, 'systems—gate': 29416, \"murdock's\": 29417, 'elfrise': 29418, 'swanston': 29419, 'humbleness': 29420, 'displace': 29421, 'moralist': 29422, \"elfride's\": 29423, 'dalziel': 29424, 'indexing': 29425, 'prematurely': 29426, 'trescothick’s': 29427, \"gooch's\": 29428, \"'personal\": 29429, '07': 29430, 'microfilm': 29431, 'surgically': 29432, 'amnesiac': 29433, 'scarf': 29434, 'parlor': 29435, \"prediction'\": 29436, \"vanish'\": 29437, 'illusionist': 29438, 'standpoints': 29439, 'str': 29440, \"spain's\": 29441, 'annexures': 29442, 'getters': 29443, 'hanoi': 29444, 'recapitulation': 29445, 'ppts': 29446, 'see—because': 29447, '“somebody”': 29448, 'spoof': 29449, 'masterpiece—which': 29450, 'doings': 29451, 'pooter': 29452, 'lupin—has': 29453, '‘71': 29454, 'alessandro': 29455, 'volta': 29456, 'avogadro': 29457, 'celsius': 29458, 'ampere': 29459, 'antonie': 29460, 'leeuwenhoek': 29461, 'newport': 29462, 'maximises': 29463, 'towell': 29464, '9781405393560': 29465, 'handmade': 29466, 'resemble': 29467, 'paper—at': 29468, '6x6': 29469, 'confluent': 29470, 'controller': 29471, 'rejecting': 29472, '‘will': 29473, 'power’': 29474, 'comforted': 29475, 'radicals': 29476, 'possessives': 29477, 'inedible': 29478, 'uneatable': 29479, 'concurrently': 29480, 'flounder': 29481, 'differentiate': 29482, \"many'\": 29483, 'shards': 29484, 'feehan': 29485, '“rehabilitation”—the': 29486, 'erasure': 29487, 'was…both': 29488, 'butchered': 29489, 'packmate': 29490, 'identities—or': 29491, 'temptation…': 29492, 'paterson': 29493, 'webinar': 29494, 'julzb': 29495, 'nunez': 29496, 'funnels': 29497, 'mes': 29498, 'rakshasa': 29499, 'kalanemi': 29500, 'vanquish': 29501, 'councillor': 29502, 'yadavas': 29503, 'tit': 29504, 'torturing': 29505, 'blot': 29506, 'bethod': 29507, 'bayaz': 29508, 'glotka': 29509, 'causation': 29510, 'invoked': 29511, 'dioxide': 29512, 'holiest': 29513, 'hermitage': 29514, \"heart's\": 29515, 'abducts': 29516, \"ravana's\": 29517, 'semblance': 29518, 'sanguine': 29519, 'igor': 29520, \"komarov's\": 29521, 'authenticated': 29522, \"alliance's\": 29523, 'trilateral': 29524, 'cleave': 29525, 'caioli’s': 29526, 'relhok': 29527, \"relhok's\": 29528, \"luna's\": 29529, 'debating': 29530, 'institutionalized': 29531, 'affiliations': 29532, 'view―showing': 29533, '“other”': 29534, 'thigns': 29535, 'proxemics': 29536, 'reltions': 29537, '—chicago': 29538, '—richard': 29539, 'neutra': 29540, 'guerra': 29541, 'brown—the': 29542, 'domes': 29543, 'dushanbe': 29544, 'filitov’s': 29545, 'safety…': 29546, 'baseline': 29547, 'forehands': 29548, 'backhands': 29549, 'volleys': 29550, 'volleyer': 29551, 'baseliner': 29552, 'modernizing': 29553, 'patronyms': 29554, 'thakur': 29555, 'clanmate': 29556, 'kakashi’s': 29557, 'coldhearted': 29558, 'request’': 29559, 'miniskirts': 29560, 'discotheque': 29561, 'blackberry': 29562, 'chatting': 29563, '‘you’re': 29564, '‘sorry': 29565, 'type’': 29566, '‘that’s': 29567, 'met’': 29568, '‘few': 29569, 'achievers’': 29570, \"merry'\": 29571, \"'greed\": 29572, \"addiction'\": 29573, 'spiritualism': 29574, \"morals'\": 29575, 'virtuous': 29576, 'beetle': 29577, 'booted': 29578, 'ashrams': 29579, 'shovel': 29580, 'reclaiming': 29581, 'rerelease': 29582, '“she”': 29583, 'yeller': 29584, 'belts': 29585, 'pans': 29586, 'jk': 29587, 'droppa': 29588, 'sanji': 29589, 'katakuri': 29590, 'clocks…': 29591, 'backfired': 29592, 'thesiger': 29593, '‘seven': 29594, 'dials’': 29595, 'significance…': 29596, 'sanjeevani': 29597, 'martells': 29598, 'dorne': 29599, 'euron': 29600, 'crow’s': 29601, 'samwell': 29602, 'tarly': 29603, 'babe': 29604, 'engulf': 29605, 'hyrule': 29606, 'historia': 29607, 'indepth': 29608, 'sprite': 29609, 'crisscrossing': 29610, 'monkey’s': 29611, 'paw’': 29612, 'man’': 29613, 'cobweb’': 29614, 'catechism': 29615, 'doubilet': 29616, 'abell': 29617, 'stanfield': 29618, 'brandenburg': 29619, 'field—sometimes': 29620, 'leah': 29621, 'bendavid': 29622, 'photographers’': 29623, 'themes—wildlife': 29624, 'inconsiderate—and': 29625, 'provocative—completely': 29626, 'infuriating—creature': 29627, 'hookup': 29628, 'chloe’s': 29629, 'rules—or': 29630, 'tby789—and': 29631, 'sites—beautiful': 29632, 'religiosity': 29633, 'superordinate': 29634, 'precincts': 29635, 'authoritatively': 29636, 'descriptionangular': 29637, 'backend': 29638, 'nginx': 29639, 'node': 29640, 'frontend': 29641, 'danisnotonfire': 29642, 'coincidentally': 29643, 'viewer': 29644, 'vloggers': 29645, 'lester’s': 29646, 'maestros': 29647, 'flare': 29648, 'genre”': 29649, 'physiker': 29650, 'bondsmage': 29651, 'sorcery': 29652, 'body—though': 29653, 'jean’s': 29654, 'imploring—and': 29655, 'bondsmage’s': 29656, 'sabetha—or': 29657, '“fast': 29658, '”—wired': 29659, '”—sf': 29660, 'revu': 29661, '“lynch': 29662, 'paper—the': 29663, '“grand': 29664, 'grandiloquent': 29665, 'draftsmanship': 29666, '”—locus': 29667, 'plasticity': 29668, \"floppy's\": 29669, 'spellers': 29670, 'spoons': 29671, 'appliances': 29672, 'idli': 29673, 'dosai': 29674, 'sambar': 29675, 'kuzhambu': 29676, 'pongal': 29677, 'murukku': 29678, 'jangiri': 29679, 'pachadis': 29680, 'maga―the': 29681, 'fending': 29682, 'attacker―and': 29683, 'maga―and': 29684, 'counterattack': 29685, 'kravist': 29686, '‘azaadi’': 29687, 'clots': 29688, 'supplementing': 29689, 'outbursts': 29690, \"zhou's\": 29691, 'zhou': 29692, \"'exhilarating\": 29693, \"semtex'\": 29694, 'kabuliwala': 29695, 'wares': 29696, \"hawkins's\": 29697, \"train'\": 29698, 'buzziest': 29699, 'folgate': 29700, 'abides': 29701, 'allegation': 29702, 'stenographers': 29703, 'telgi': 29704, 'peeps': 29705, 'howlers': 29706, 'gritters': 29707, 'recited': 29708, 'stumpers': 29709, 'liven': 29710, 'cheetah': 29711, 'comaneci': 29712, \"comaneci's\": 29713, 'pare': 29714, 'barge': 29715, 'seine': 29716, 'perdu': 29717, \"'literary\": 29718, \"apothecary'\": 29719, 'nursed': 29720, 'rue': 29721, 'montagnard': 29722, 'unmoor': 29723, 'piranha': 29724, 'chhota': 29725, 'haji': 29726, 'mastan': 29727, 'varadarajan': 29728, 'mudaliar': 29729, 'mafia’s': 29730, 'dawood’s': 29731, 'unreported': 29732, '‘headley': 29733, 'i’': 29734, 'blasts’': 29735, \"fascinating'\": 29736, 'toulouse': 29737, 'lautrec': 29738, 'sitwell': 29739, 'gershwin': 29740, 'gin': 29741, 'currey': 29742, 'amphetamines': 29743, 'headstand': 29744, \"shaw's\": 29745, 'bucolic': 29746, 'campion': 29747, 'headquartered': 29748, 'musty': 29749, 'hitchcockian': 29750, '9780241317631': 29751, 'incepted': 29752, 'roli': 29753, 'vaccinating': 29754, 'vaccinate': 29755, 'healthworkers': 29756, 'trudging': 29757, 'solarcity': 29758, 'freakishly': 29759, 'disintegrated': 29760, 'ashlee': 29761, 'brooker’s': 29762, 'ephemera': 29763, '‘brooker': 29764, 'creep': 29765, 'rears': 29766, 'terrifying’': 29767, \"'i'll\": 29768, \"socrates'\": 29769, '474': 29770, '347': 29771, 'phaedrus': 29772, 'protagoras': 29773, 'meno': 29774, 'timaeus': 29775, 'critias': 29776, 'theaetetus': 29777, 'gorgias': 29778, 'crocker': 29779, 'odious': 29780, 'zündapp': 29781, 'zundapp': 29782, \"italy's\": 29783, 'marques': 29784, 'farraday': 29785, 'skirted': 29786, 'darlington’s': 29787, 'parade': 29788, '–instyle': 29789, '“earth': 29790, 'shaking…you': 29791, '–cosmopolitan': 29792, \"“reid's\": 29793, '—us': 29794, 'life—named': 29795, 'pick—comes': 29796, 'emma’s': 29797, 'viktor': 29798, 'orban': 29799, \"populism's\": 29800, 'democrats': 29801, 'jalees': 29802, 'fahmida': 29803, 'jogeshwari': 29804, 'taxis': 29805, 'compulsions': 29806, 'rosenberg': 29807, 'developers—led': 29808, 'kapor—designing': 29809, 'unpredictability': 29810, 'behavior—': 29811, 'crismus': 29812, 'masquerading': 29813, 'nursemaid': 29814, '‘illuminated': 29815, 'insights’': 29816, 'thierry’s': 29817, '‘hand': 29818, 'gaul’': 29819, 'ever’': 29820, 'auclair': 29821, 'celebratory': 29822, 'rosen': 29823, 'divali': 29824, \"pupils'\": 29825, 'encyclopaedicus': 29826, \"britannicus's\": 29827, 'chawla': 29828, 'pbi': 29829, 'padmanabhan': 29830, 'her―': 29831, 'karnal': 29832, 'nasa―to': 29833, 'exodus': 29834, 'qb': 29835, 'haj': 29836, 'mitla': 29837, 'tragedy—and': 29838, \"uris's\": 29839, 'wehrmacht': 29840, 'lillibet': 29841, 'hope…and': 29842, 'nainital': 29843, 'chirpy': 29844, 'bookworms': 29845, 'hardcovers': 29846, 'tung’s': 29847, 'recognizable—making': 29848, 'nerds': 29849, 'aderik': 29850, 'rijnmter': 29851, 'salis': 29852, 'eugenieos': 29853, 'saphira': 29854, 'aderikos': 29855, 'lethargic': 29856, 'spiritless': 29857, 'practisable': 29858, 'allopathic': 29859, 'slokas': 29860, 'emanates': 29861, 'toiled': 29862, 'chandrabhushan': 29863, 'rasa': 29864, 'shastra': 29865, \"'ancient\": 29866, 'ayurvedic': 29867, 'rishis': 29868, 'partap': 29869, 'jiva': 29870, 'krishnakumar': 29871, 'pharmacy': 29872, 'coimbatore': 29873, 'fulbright': 29874, 'spinoff': 29875, 'blackwood': 29876, '“nobody': 29877, '”—lee': 29878, 'idealist—a': 29879, 'swamplands': 29880, 'beachheads': 29881, 'trinidad': 29882, 'lbp': 29883, 'orb': 29884, 'sassenach—an': 29885, '“outlander”—in': 29886, 'raiding': 29887, '“marvelous': 29888, '”—san': 29889, '“gabaldon': 29890, '“triumphant': 29891, '“unforgettable': 29892, 'embroidered': 29893, \"gabriel's\": 29894, 'francisco’s': 29895, 'wylie’s': 29896, 'unattached': 29897, 'galvanizes': 29898, \"hultgren's\": 29899, 'halftones': 29900, 'hultgren': 29901, 'avidly': 29902, \"gear's\": 29903, 'stig': 29904, 'clarkson': 29905, 'bobbed': 29906, 'willfully': 29907, \"'richard\": 29908, \"'porter\": 29909, 'marbles': 29910, 'morphology': 29911, 'fortran': 29912, \"citizen's\": 29913, 'wrongdoers': 29914, 'meara': 29915, \"o'dwyer\": 29916, 'branna': 29917, 'iona': 29918, \"'access\": 29919, \"perarnau's\": 29920, \"astonishing'\": 29921, \"'write\": 29922, \"like'\": 29923, 'catalan': 29924, 'matchdays': 29925, 'sabbatical': 29926, \"bayern's\": 29927, 'reprogramme': 29928, 'arjen': 29929, 'robben': 29930, 'neuer': 29931, 'lahm': 29932, 'thiago': 29933, 'alcantara': 29934, 'bastian': 29935, 'schweinsteiger': 29936, 'ayla': 29937, 'cro': 29938, 'magnon': 29939, 'neanderthal': 29940, \"clan's\": 29941, 'iza': 29942, 'creb': 29943, 'ebulent': 29944, 'bongo': 29945, \"feynman's\": 29946, 'typographer': 29947, 'bringhurst': 29948, 'prowling': 29949, 'confusions': 29950, 'inflamed': 29951, 'doughty': 29952, 'kohl': 29953, 'bangles': 29954, 'darvish': 29955, 'beeran': 29956, 'meiral': 29957, 'chaitanya': 29958, 'pierces': 29959, \"desire'\": 29960, 'devastates': 29961, 'creepers': 29962, 'tracey': 29963, 'keeps…': 29964, \"cordina's\": 29965, 'highness': 29966, 'cordina': 29967, 'macgee': 29968, 'caine': 29969, \"camilla's\": 29970, 'irritation': 29971, 'secret…': 29972, 'ratcliffe': 29973, 'shouldered': 29974, 'horrify': 29975, 'mihaly': 29976, 'chore': 29977, 'milli': 29978, 'bajwa': 29979, 'luckless': 29980, 'swipes': 29981, 'singhania': 29982, 'unconquerable': 29983, 'newcago': 29984, 'borough': 29985, 'regalia': 29986, 'oppressed': 29987, 'despot': 29988, 'firefight': 29989, 'footnote': 29990, 'coldblooded': 29991, 'goražde': 29992, 'nee': 29993, 'celebs': 29994, 'microsite': 29995, 'aspirational': 29996, 'emboldens': 29997, 'plusses': 29998, 'minuses': 29999, 'styluses': 30000, 'bathtime': 30001, 'handshake': 30002, 'v4': 30003, 'v5': 30004, 'ah': 30005, 'ike': 30006, 'pem': 30007, 'mime': 30008, 'pgp': 30009, 'elliptic': 30010, 'canterbury': 30011, 'miscarriage': 30012, \"hero's\": 30013, 'hubristic': 30014, '1846': 30015, \"quirk's\": 30016, \"owner's\": 30017, 'quarreling': 30018, 'techy': 30019, 'waterways': 30020, 'deepwater': 30021, 'lochs': 30022, \"researching'\": 30023, 'tantalisingly': 30024, \"'agent'\": 30025, \"'mr\": 30026, \"'super\": 30027, \"agent'\": 30028, \"jon's\": 30029, 'arching': 30030, 'brokering': 30031, 'evangeline': 30032, 'jenner': 30033, 'sinfully': 30034, 'strength—and': 30035, \"miura's\": 30036, '‘deadly': 30037, 'punch’': 30038, 'michaela': 30039, 'arundel': 30040, '‘sacred': 30041, 'sword’': 30042, 'begins…': 30043, \"sanders'\": 30044, 'floated': 30045, 'disclosure': 30046, 'contradicts': 30047, 'policyexamples': 30048, 'nontraditional': 30049, 'unheralded': 30050, 'participated': 30051, 'disco': 30052, 'rats': 30053, 'sulking': 30054, 'kundan': 30055, 'jaane': 30056, 'bhi': 30057, 'yaaro': 30058, 'humours': 30059, 'nonathletic': 30060, 'exhaust': 30061, '“steel': 30062, 'body”': 30063, 'medication': 30064, 'distorted': 30065, 'gadre': 30066, 'abhay': 30067, 'proliferation': 30068, 'dissenting': 30069, 'afflicting': 30070, 'vox': 30071, 'reality’': 30072, 'watchers': 30073, 'become’': 30074, 'trialled': 30075, 'jaron': 30076, 'lanier': 30077, '‘vivid': 30078, 'extraordinary’': 30079, 'modules―listening': 30080, 'labeling': 30081, 'intomcgraw': 30082, 'sureyou’re': 30083, 'you’retaking': 30084, 'englishlanguageuniversity': 30085, 'foremployment': 30086, 'multinationalcorporation': 30087, 'bookgives': 30088, 'andstrategies': 30089, 'yourpreparation': 30090, 'program―and': 30091, 'youneed': 30092, 'sorrenson': 30093, 'examinersince': 30094, 'ukraine': 30095, 'fromthe': 30096, 'graphite': 30097, 'stairways': 30098, 'rectangular': 30099, \"'hi\": 30100, 'resurging': 30101, 'forecast': 30102, 'redundancy': 30103, 'embellishment': 30104, 'endgames': 30105, 'lamprecht': 30106, 'osbourne': 30107, '3800': 30108, 'meluhha': 30109, 'bloodstone': 30110, 'shakari': 30111, 'iaf’s': 30112, 'redder': 30113, 'gouts': 30114, 'trickled': 30115, 'lids': 30116, 'pouches': 30117, 'gorged': 30118, 'repletion': 30119, 'bram': 30120, 'exponents': 30121, 'vampirism': 30122, 'tractable': 30123, 'buday': 30124, 'tooling': 30125, 'goroutines': 30126, 'dunning': 30127, 'upstream': 30128, 'mapr': 30129, 'flink': 30130, 'geo': 30131, 'masonic': 30132, 'amherst': 30133, 'exeter': 30134, 'risk—fundamental': 30135, 'strips—containing': 30136, 'chews': 30137, 'waist': 30138, 'indivisible': 30139, '‘radhakrishna’': 30140, 'eponymous': 30141, '‘radha’': 30142, 'petulant': 30143, 'capri': 30144, \"'star\": 30145, \"water'\": 30146, 'nerezza': 30147, 'lackberg’s': 30148, 'murkier': 30149, 'midst…': 30150, 'daemons': 30151, \"harkness's\": 30152, 'enchant\\xading': 30153, 'clairmont': 30154, \"matthew's\": 30155, 'ashmole': 30156, '782': 30157, 'knowl\\xadedge': 30158, 'rocker': 30159, 'salsa': 30160, 'arranger': 30161, 'funk': 30162, \"devasia's\": 30163, 'smoothness': 30164, 'tharu': 30165, 'surinamboothiri': 30166, 'devasia': 30167, \"austen's\": 30168, \"'accidental'\": 30169, \"'flawed'\": 30170, 'chandumenon': 30171, 'neral': 30172, '\\x93write': 30173, 'don\\x92t': 30174, 'mausoleum': 30175, \"stepson's\": 30176, 'rivaled': 30177, '1611': 30178, 'imprisioned': 30179, 'orientalist': 30180, \"nur's\": 30181, 'fairer': 30182, 'truer': 30183, 'differently…’': 30184, '“feminism”': 30185, 'odetta': 30186, \"roland's\": 30187, 'ueland': 30188, 'sandburg': 30189, \"village'\": 30190, \"'kunwar\": 30191, \"singh'\": 30192, 'pani': 30193, 'talla': 30194, \"rudraprayag'\": 30195, \"'robin'\": 30196, \"'sultana'\": 30197, \"'having\": 30198, 'overture': 30199, 'sellable': 30200, 'advertise': 30201, 'procter': 30202, '233': 30203, 'vissa': 30204, 'vissen': 30205, 'nicky': 30206, 'paulina': 30207, \"nottle's\": 30208, 'newts': 30209, 'stampede': 30210, 'chum': 30211, 'tuppy': 30212, \"madeline's\": 30213, 'bulging': 30214, 'cd5': 30215, 'equals': 30216, \"relished'\": 30217, \"coaster'\": 30218, 'invigoration': 30219, 'norm': 30220, 'pleasantly': 30221, 'impaired': 30222, 'corrigan': 30223, 'improvisatory': 30224, 'gingerly': 30225, 'deports': 30226, \"1890's\": 30227, \"1980's\": 30228, 'disliked': 30229, 'spidey': 30230, \"webhead's\": 30231, 'mizoram': 30232, 'akhil': 30233, 'carlington': 30234, 'norden': 30235, 'ipsita': 30236, 'standbys': 30237, 'nestles': 30238, \"mohune's\": 30239, 'befall': 30240, 'dorset': 30241, \"'moon\": 30242, \"fleet'\": 30243, 'skylines': 30244, 'hinging': 30245, 'ome': 30246, 'ribbing': 30247, 'smallville': 30248, 'kansas—or': 30249, 'matter…': 30250, 'millar': 30251, 'kilian': 30252, 'sangh—or': 30253, 'rss—the': 30254, 'nilanjan': 30255, 'keshav': 30256, 'baliram': 30257, 'hedgewar': 30258, '‘cocaine’': 30259, 'vinayak': 30260, 'savarkar': 30261, 'sadashiv': 30262, 'golwalkar': 30263, '‘guruji’': 30264, '‘hermit': 30265, 'ideologue’': 30266, 'balasaheb': 30267, 'deoras': 30268, 'pracharak': 30269, '‘communist’': 30270, 'rss—‘it': 30271, 'debatable': 30272, 'deendayal': 30273, '‘philosophy’': 30274, 'vijaya': 30275, 'raje': 30276, 'scindia': 30277, 'bal': 30278, 'organisation’s': 30279, 'multicultural': 30280, 'rss’s': 30281, 'politics—the': 30282, 'bjp’s': 30283, \"'astounding'\": 30284, \"sledgehammer'\": 30285, 'reservation': 30286, 'hospitalised': 30287, 'terese': 30288, \"mailhot's\": 30289, 'reconciliation': 30290, 'melded': 30291, \"ofer's\": 30292, \"'notifiers'\": 30293, 'galilee': 30294, 'avram': 30295, 'netbeans': 30296, 'encapsulation': 30297, '“towering': 30298, 'swash': 30299, 'buckling': 30300, 'chabon’s': 30301, '“magnum': 30302, 'opus”': 30303, 'cyan': 30304, 'magenta': 30305, 'winterscape': 30306, 'linework': 30307, 'brushstrokes': 30308, 'secularism': 30309, '568': 30310, 'venezuelan': 30311, 'yanomami': 30312, 'descendents': 30313, 'copacabana': 30314, 'ipanema': 30315, 'paolo': 30316, 'supermodels': 30317, 'gauchos': 30318, 'pantanal': 30319, 'iguaçu': 30320, 'language—and': 30321, 'challenge\\x97did': 30322, 'kris\\x92s': 30323, 'athlete\\x97from': 30324, 'superstate': 30325, 'ambivalent': 30326, 'sharapova': 30327, 'bollettieri': 30328, \"sharapova's\": 30329, 'chernobyl': 30330, \"living'\": 30331, 'infiltrates': 30332, 'insulate': 30333, 'reframe': 30334, 'subtracting': 30335, 'vivre': 30336, \"paradox'\": 30337, \"valley's\": 30338, 'automotive': 30339, 'dock': 30340, 'multiplanetary': 30341, 'postings': 30342, \"portrait'\": 30343, \"companies'\": 30344, 'liaisons': 30345, 'redgrave': 30346, 'colman': 30347, 'adeline': 30348, \"angelfield's\": 30349, 'vida': 30350, \"margaret's\": 30351, \"if'\": 30352, 'juvenile': 30353, 'dies…': 30354, 'rehabilitiation': 30355, 'delinquents': 30356, 'serrocold': 30357, 'gilbrandsen': 30358, 'gilbrandsen’s': 30359, 'ashtadiggajas': 30360, 'bhuvana': 30361, 'vijayam': 30362, 'vijayanagar': 30363, '1509': 30364, '1529': 30365, 'samudra': 30366, 'vardhana': 30367, 'admirable': 30368, 'shrewdness': 30369, '“fix': 30370, 'problems–the': 30371, 'way”': 30372, 'purveys': 30373, 'moralistic': 30374, 'sagaciously': 30375, 'enthuses': 30376, 'amuses': 30377, 'cusses': 30378, 'bharata': 30379, 'khanda': 30380, '”—seth': 30381, 'linchpin': 30382, '“anyone': 30383, '”—fortune': 30384, 'virtuosos': 30385, 'mavens': 30386, '”—dan': 30387, 'coyle': 30388, '“ericsson’s': 30389, '”—joshua': 30390, 'deathless': 30391, 'siddhas': 30392, 'agastyar': 30393, 'boganathar': 30394, \"'18\": 30395, 'siddha': 30396, \"tradition'\": 30397, 'babaji': 30398, \"yogananda's\": 30399, 'tundras': 30400, 'primordial': 30401, 'familiar—lions': 30402, 'bears—and': 30403, 'compelling—and': 30404, 'threatened—species': 30405, 'sourtempered': 30406, \"pollyanna's\": 30407, 'summarising': 30408, 'coreling': 30409, 'willingly': 30410, 'enlivened': 30411, 'kid’s': 30412, 'wellillustrated': 30413, 'moning': 30414, 'awc—after': 30415, '“mega”': 30416, 'rules—and': 30417, 'pandemonium': 30418, 'liabilities': 30419, 'dani’s': 30420, 'ex–best': 30421, 'mackayla': 30422, 'jayne': 30423, 'encased': 30424, 'dublin’s': 30425, 'blanketed': 30426, 'hoarfrost': 30427, 'club’s': 30428, 'tracks—and': 30429, 'bargains': 30430, 'dublin—before': 30431, 'darkfever': 30432, 'bloodfever': 30433, 'faefever': 30434, 'dreamfever': 30435, 'feverborn': 30436, 'feversong': 30437, '”—rt': 30438, 'textured': 30439, '”─kirkus': 30440, 'needle': 30441, \"'me'\": 30442, \"'tea'\": 30443, \"'bed'\": 30444, \"'give'\": 30445, '3b': 30446, 'parachuted': 30447, 'mountainside': 30448, 'interrogators': 30449, 'onlyfeatures': 30450, 'mouseum': 30451, \"mouseum's\": 30452, 'yikes': 30453, 'geese': 30454, 'braised': 30455, 'leeks': 30456, 'diaper': 30457, 'investigate—and': 30458, 'bébé': 30459, '—forbes': 30460, 'beane': 30461, 'knowledge—insights': 30462, 'discard': 30463, 'occupational': 30464, 'gerbils': 30465, 'nears': 30466, 'haig': 30467, 'honeyman': 30468, \"masterful'\": 30469, 'dashner': 30470, 'pfd': 30471, '84k': 30472, \"finish'\": 30473, 'blackman': 30474, \"'kiran\": 30475, 'hargrave': 30476, 'wasteland': 30477, 'kiran': 30478, \"hargrave's\": 30479, 'makings': 30480, 'rache': 30481, 'pips': 30482, 'ritchie': 30483, 'judo': 30484, 'godhead': 30485, 'iconometry': 30486, 'mudras': 30487, 'pithas': 30488, 'sects': 30489, 'saivism': 30490, 'vaisnavism': 30491, 'saktism': 30492, 'mosque': 30493, '8x8': 30494, 'storybook': 30495, 'slinging': 30496, 'cpii': 30497, \"'reminds\": 30498, \"universe'\": 30499, 'psychedelic': 30500, 'psilocybin': 30501, 'dmt': 30502, 'forwardas': 30503, 'pollan': 30504, 'psychedelics': 30505, 'fascinatedby': 30506, \"since'\": 30507, 'schama': 30508, 'wildflowers': 30509, 'wildflower': 30510, 'efficacy': 30511, 'thyself': 30512, 'repertory': 30513, 'honeysuckle': 30514, 'relieves': 30515, 'homesickness': 30516, 'beech': 30517, 'aggravation': 30518, 'mimulus': 30519, 'payable': 30520, 'livestock': 30521, 'autism—because': 30522, \"sunny'\": 30523, \"'private'\": 30524, \"'room'\": 30525, \"'dressing'\": 30526, 'gobind': 30527, 'exiting': 30528, 'medallist': 30529, 'amu': 30530, 'aligarh': 30531, 'trustee': 30532, 'kalpavriksha': 30533, 'fecund': 30534, 'chaperone': 30535, 'aar': 30536, 'theatricality': 30537, 'artificiality': 30538, 'donning': 30539, 'bibi': 30540, 'kotha': 30541, \"lila's\": 30542, '‘learn': 30543, 'valentino': 30544, 'motorsports': 30545, 'motogp': 30546, 'marc’s': 30547, '175': 30548, 'fished': 30549, 'kowloon': 30550, \"uruguay's\": 30551, 'montevideo': 30552, 'merseyside': 30553, 'kenny': 30554, 'dalglish': 30555, 'evra': 30556, 'branislav': 30557, 'ivanovic': 30558, 'giorgi': 30559, 'chiellini': 30560, 'derided': 30561, 'scrapping': 30562, 'overstep': 30563, 'unceremoniously': 30564, 'thefts': 30565, 'raided': 30566, 'warehouses': 30567, 'complicit': 30568, 'jilted': 30569, 'looters': 30570, 'pillaging': 30571, \"sebold's\": 30572, 'wahlberg': 30573, 'sarandon': 30574, 'mentors': 30575, 'garwood': 30576, 'yongey': 30577, 'mingyur': 30578, 'rinpoche': 30579, 'saare': 30580, 'zamaane': 30581, 'gham': 30582, 'dawaa': 30583, 'hoon': 30584, 'adhikaar': 30585, 'helen—nicknamed': 30586, '‘h': 30587, 'bomb’': 30588, 'career—continues': 30589, 'improbably': 30590, '‘cabarets’': 30591, 'feeling’': 30592, '‘highly': 30593, 'writte': 30594, 'entrepreneur’—frontline': 30595, 'mindtree': 30596, 'jobs’—business': 30597, 'entrepreneurs’': 30598, '—sahara': 30599, 'entrepreneurs’—': 30600, 'scratch’': 30601, '—free': 30602, 'darr': 30603, 'triptych': 30604, 'jemisin': 30605, 'dreamblood': 30606, 'qbasic': 30607, 'quickbasic': 30608, 'cis': 30609, 'vo': 30610, 'hultgten': 30611, 'foxes': 30612, 'kangaroos': 30613, 'realign': 30614, \"sincero's\": 30615, 'sweden’s': 30616, 'lull': 30617, 'sandman”': 30618, 'neighborhoods': 30619, 'money—as': 30620, 'mitzvah': 30621, 'woman—to': 30622, 'funny—it’s': 30623, 'is—humble': 30624, 'clientelistic': 30625, 'accommodating': 30626, 'pradeep': 30627, 'chhibber': 30628, 'verma': 30629, 'statism': 30630, 'majorities': 30631, 'lacrosse': 30632, '‘cat’': 30633, 'upjohn': 30634, 'victim…': 30635, 'raises—questions': 30636, 'alpinist': 30637, \"viesturs's\": 30638, 'mums': 30639, 'f1': 30640, 'gullwing': 30641, 'changers': 30642, 'lamborghini': 30643, 'bugatti': 30644, 'pagani': 30645, 'mclaren': 30646, 'porsche': 30647, 'bonnets': 30648, \"'fine\": 30649, 'mccomb': 30650, \"community's\": 30651, '12k': 30652, 'affliction': 30653, '‘martin': 30654, 'millions’': 30655, 'robb’s': 30656, 'book3': 30657, 'uncontrollable': 30658, \"merdan's\": 30659, 'ita': 30660, 'overhears': 30661, 'sparky': 30662, 'wisecrack': 30663, \"harney's\": 30664, 'retracing': 30665, 'dibs': 30666, 'narsiah': 30667, 'vangapally': 30668, 'karimnagar': 30669, 'telangana': 30670, 'acquiescence': 30671, 'internalization': 30672, 'satyanarayana': 30673, 'simplicitywith': 30674, 'bachelorette': 30675, 'jett': 30676, 'diahann': 30677, 'donnell': 30678, 'mariska': 30679, 'careening': 30680, 'automakers': 30681, 'bailout': 30682, 'taxpayer': 30683, 'lackluster': 30684, 'dys\\xadfunctional': 30685, 'backstabbing': 30686, 'family—america’s': 30687, 'dynasty—could': 30688, 'great\\xadest': 30689, 'comebacks': 30690, 'automaker': 30691, 'meet\\xadings': 30692, 'ford’s': 30693, \"excel's\": 30694, \"data'\": 30695, 'deere': 30696, 'reimagines': 30697, \"defoe's\": 30698, 'mutinous': 30699, 'tongueless': 30700, \"cruso's\": 30701, 'discouragement': 30702, \"gogh's\": 30703, 'tenured': 30704, 'logics': 30705, 'sectioned': 30706, \"'peter's\": 30707, 'schuster': 30708, 'edict': 30709, 'highland': 30710, 'against—an': 30711, \"kincaid's\": 30712, 'brazenly': 30713, 'quelled': 30714, 'isinclude': 30715, 'disparities': 30716, 'autopilot': 30717, 'saroyan': 30718, 'o’connor': 30719, 'common—the': 30720, 'theologian': 30721, 'ahmad': 30722, 'language’': 30723, 'schott': 30724, '”’': 30725, 'hatches': 30726, 'droopy': 30727, 'unappealing': 30728, 'rageiness': 30729, 'wodehouse’': 30730, 'responsibly': 30731, 'byzantium': 30732, 'east’s': 30733, 'indignation': 30734, 'jauntiness': 30735, 'dalrymple’s': 30736, 'accumulates': 30737, 'book…from': 30738, 'cri': 30739, 'coeur': 30740, 'resist’―philip': 30741, 'marsden': 30742, 'psc': 30743, 'bhms': 30744, 'bachelor’s': 30745, 'mckinney': 30746, 'groupby': 30747, 'avoiders': 30748, 'jaunty': 30749, \"gambler's\": 30750, 'rebranded': 30751, '193': 30752, \"clubs'\": 30753, 'globalised': 30754, \"tottenham's\": 30755, 'arsène': 30756, 'kroenke': 30757, 'txiki': 30758, 'begiristain': 30759, 'ham': 30760, 'leicester': 30761, 'aston': 30762, 'bustand': 30763, 'tapping': 30764, 'hotbeds—from': 30765, 'upstate': 30766, 'york—coyle': 30767, 'ignition': 30768, 'commitment—call': 30769, 'passion—born': 30770, '“talent': 30771, 'whisperers”': 30772, 'microscopic': 30773, 'michelangelo’s': 30774, 'jordan’s': 30775, 'lings’': 30776, 'sira': 30777, 'acknowledgment': 30778, 'seerate': 30779, 'tradecraft': 30780, 'ghostman': 30781, 'hennessy': 30782, 'hospice': 30783, 'confessing': 30784, 'sadisitc': 30785, 'kharkov': 30786, 'grigori': 30787, 'bulganov': 30788, 'moscow—and': 30789, 'repays': 30790, 'trait': 30791, 'brutes': 30792, 'hideouts': 30793, 'haves': 30794, 'mimics': 30795, 'bognor': 30796, 'regis': 30797, 'wondrously': 30798, 'bewilderments': 30799, 'styron': 30800, 'languishes': 30801, \"bollygarchs'\": 30802, 'encrusted': 30803, 'endemic': 30804, 'answersanswers': 30805, 'reallifelore': 30806, 'ecosystems': 30807, \"trump's\": 30808, 'finland': 30809, 'hiveís': 30810, 'dialectóhiveqlóto': 30811, 'filesystem': 30812, 'petabytes': 30813, 'tablesóand': 30814, 'grouping': 30815, 'udfs': 30816, 'amazonís': 30817, 'unremarked': 30818, 'intef': 30819, \"intef's\": 30820, 'lostris': 30821, 'commanded': 30822, 'pow': 30823, 'trans': 30824, 'alaskan': 30825, 'enemy…': 30826, \"'he's\": 30827, \"there'\": 30828, 'kazumasa': 30829, 'kanto': 30830, \"colleagues'\": 30831, \"yuuki's\": 30832, 'unconquered': 30833, 'proprietary': 30834, 'jughead': 30835, \"'bollygarchs'\": 30836, 'languish': 30837, 'megacities': 30838, 'terrace': 30839, 'bankrupt': 30840, 'prospered': 30841, \"'what's\": 30842, 'eh': 30843, 'orgy': 30844, 'delinquency': 30845, 'freewill': 30846, \"'nadsat'\": 30847, 'slang': 30848, 'gruesomely': 30849, 'excitements': 30850, 'demobilization': 30851, 'nazareth': 30852, 'orchestra': 30853, 'bathsheba': 30854, 'everdene': 30855, 'weatherbury': 30856, 'boldwood': 30857, 'unsettles': 30858, 'rosemarie': 30859, 'body—a': 30860, 'personality—the': 30861, 'history—bringing': 30862, 'drugstore': 30863, 'adalana': 30864, '“used”': 30865, '“keeper': 30866, 'pain”': 30867, 'shocker': 30868, 'rivet': 30869, '”—flora': 30870, 'rheta': 30871, 'schreiber': 30872, '”—los': 30873, '”—cosmopolitan': 30874, 'programs…without': 30875, 'neverbeen': 30876, 'programs–and': 30877, 'program’s': 30878, 'computer’s': 30879, 'tsarist': 30880, \"'new\": 30881, 'bertil': 30882, 'lintner': 30883, \"wadhwani's\": 30884, 'gaurav': 30885, 'ajai': 30886, 'mirchandani': 30887, \"ajai's\": 30888, 'dhanani': 30889, \"'sur'\": 30890, \"'aaina'\": 30891, 'balearic': 30892, 'podar': 30893, 'sensibilities': 30894, 'incognito': 30895, 'dehaven': 30896, \"jonathan's\": 30897, 'surfacing': 30898, 'funimation': 30899, 'frontier—the': 30900, 'byzantine': 30901, 'molds': 30902, 'is…': 30903, 'obito': 30904, 'leib': 30905, 'lezjon': 30906, 'goeth': 30907, 'plaszow': 30908, 'schindler': 30909, 'rancour': 30910, 'boxis': 30911, 'bulge': 30912, 'forties': 30913, '‘tasks’': 30914, 'currency’': 30915, '‘rich’': 30916, 'treadmill': 30917, 'firmness': 30918, \"quarterfinalist'\": 30919, 'tire': 30920, \"arvind's\": 30921, \"communicate'\": 30922, 'whetted': 30923, 'cuddy': 30924, 'procrastinator': 30925, 'freeloader': 30926, 'panama': 30927, 'artcic': 30928, 'robber': 30929, \"'powerfully\": 30930, \"attention'\": 30931, \"'melinda\": 30932, \"gates's\": 30933, \"arms'\": 30934, \"courage'\": 30935, 'brené': 30936, 'melinda’s': 30937, 'contraceptives': 30938, 'inequity': 30939, 'magnus’s': 30940, 'hyperventilate': 30941, 'poppy’s': 30942, 'phone’s': 30943, 'roxton': 30944, 'wading': 30945, 'putty': 30946, 'scenes…': 30947, 'rockies': 30948, \"meg's\": 30949, 'airwaves': 30950, 'buckled': 30951, 'confidences': 30952, 'drenching': 30953, 'intercuts': 30954, 'vyasa': 30955, '1600': 30956, 'classic…': 30957, 'back…': 30958, 'confessor': 30959, 'givingadequate': 30960, 'gofrom': 30961, 'tenyears': 30962, 'celebworkouts': 30963, 'kirsten': 30964, 'upton': 30965, 'transitioned': 30966, 'breakaways': 30967, 'thebarefootcoach': 30968, 'busking': 30969, 'beg': 30970, 'alive…': 30971, 'malts': 30972, 'retrospective': 30973, \"'descriptive\": 30974, \"material'\": 30975, \"'short\": 30976, \"'points\": 30977, \"revision'\": 30978, \"'objective\": 30979, \"questions'\": 30980, 'upward': 30981, 'slet': 30982, 'mackay': 30983, 'neurosis': 30984, \"'tulipomania'\": 30985, \"stone's\": 30986, 'speechwriter': 30987, 'speechwriters': 30988, 'correspondents’': 30989, '“state': 30990, 'obamaworld': 30991, 'humorists’': 30992, 'classiest': 30993, '“people': 30994, 'boss’s': 30995, 'championed': 30996, 'outlive': 30997, 'mohawks': 30998, '“seafoodetarian”': 30999, \"books—mcsorley's\": 31000, \"gould's\": 31001, 'secret—that': 31002, 'offhand': 31003, 'uncollected': 31004, 'unsuspected': 31005, 'odder': 31006, 'citizens—as': 31007, 'coopersmith': 31008, 'émigrée': 31009, 'quaestor': 31010, 'vexatius': 31011, 'sinustitis': 31012, \"governor's\": 31013, 'helvetia': 31014, 'safes': 31015, 'hourglasses': 31016, 'yodelling': 31017, 'helvetian': 31018, 'occupies': 31019, \"moon'\": 31020, 'interchangeably': 31021, \"knuth's\": 31022, 'algorithmically': 31023, 'tiro': 31024, 'verres': 31025, 'cicero': 31026, 'orator': 31027, 'imperium': 31028, \"tiro's\": 31029, 'persia—a': 31030, 'known—are': 31031, 'format—each': 31032, 'squaw': 31033, 'ski': 31034, 'cleverer': 31035, 'survivable': 31036, 'detonation': 31037, 'oracles': 31038, 'genies': 31039, 'singletons': 31040, 'endowment': 31041, 'normativity': 31042, 'convergence': 31043, 'emulation': 31044, 'couplings': 31045, 'malthusian': 31046, 'forbiddingly': 31047, \"bostrom's\": 31048, 'reconceptualization': 31049, \"nate's\": 31050, 'sloppiness': 31051, 'mendelssohn': 31052, 'derham': 31053, 'tallis': 31054, 'handel': 31055, 'ae': 31056, 'toolsin': 31057, 'plugins': 31058, 'outsmarting': 31059, 'enclothed': 31060, 'deindividuation': 31061, 'misattribution': 31062, '“worth': 31063, 'it”': 31064, 'assigning': 31065, 'gratefulness': 31066, 'unintentionally': 31067, 'aadmis': 31068, 'parading': 31069, 'sues': 31070, 'jhunjhunwala': 31071, \"stockbroker's\": 31072, \"'fake\": 31073, \"jhunjhunwala'\": 31074, 'wakeman': 31075, 'fiancee': 31076, 'lia': 31077, 'uniforms': 31078, 'honourable': 31079, 'source–': 31080, 'grinders': 31081, 'marryam': 31082, 'reshii': 31083, 'zesty': 31084, 'guntur': 31085, 'chilli': 31086, \"'true'\": 31087, 'hillsides': 31088, 'cardamom': 31089, 'mashhad': 31090, 'pulsates': 31091, 'jubilant': 31092, 'presumptive': 31093, '“achieving': 31094, 'dreams”': 31095, 'ct': 31096, 'auditorium': 31097, 'zaslow': 31098, 'theirs’': 31099, '“time': 31100, 'have…': 31101, '“bobby': 31102, 'nest—if': 31103, '”—ernest': 31104, 'cline': 31105, 'stuck—depressed': 31106, 'mom’s': 31107, 'aisles': 31108, 'flynn’s': 31109, 'strolling': 31110, 'accosted': 31111, 'escapade': 31112, 'renews': 31113, \"'pondoro'\": 31114, \"'taylor\": 31115, \"factor'\": 31116, 'ballistics': 31117, 'jacketed': 31118, 'cupronickel': 31119, 'tubed': 31120, 'softpoint': 31121, 'cartridges': 31122, 'hungering': 31123, 'supremo': 31124, 'reprimanded': 31125, \"smallwood's\": 31126, 'clefs': 31127, \"'brush\": 31128, 'punishments': 31129, 'hyeonseo': 31130, \"hyeonseo's\": 31131, 'scarlatti': 31132, 'taper': 31133, 'aww': 31134, 'hori': 31135, 'miyamura': 31136, 'wrists': 31137, 'sequenced': 31138, 'rested': 31139, \"hanon's\": 31140, 'midi': 31141, '5715': 31142, \"bullet'\": 31143, 'otherworldy': 31144, 'tanned': 31145, 'condom': 31146, 'walnuts': 31147, 'nosing': 31148, 'hobie': 31149, 'reappearance': 31150, 'payphone': 31151, 'feeney': 31152, \"'value'\": 31153, \"barcelona's\": 31154, 'remarked': 31155, 'midfield': 31156, 'liga': 31157, 'maestro': 31158, \"'intermediate\": 31159, '“enthralling': 31160, 'turner”': 31161, 'statham': 31162, 'vigilantes': 31163, 'dispense': 31164, 'justice—no': 31165, 'd’or': 31166, 'madrid’s': 31167, 'death’': 31168, 'rakhi': 31169, 'lovey': 31170, 'succumbed': 31171, 'celebral': 31172, 'depriving': 31173, '‘biggest': 31174, 'support’': 31175, 'lokapally': 31176, 'virat’s': 31177, 'intimates': 31178, 'bcci’s': 31179, '‘international': 31180, 'icc’s': 31181, '‘odi': 31182, 'sportspro': 31183, '‘second': 31184, 'incidences': 31185, 'altruism': 31186, 'charities': 31187, 'pluck': 31188, 'willow': 31189, 'insouciant': 31190, 'louisville': 31191, 'kentucky': 31192, 'listers': 31193, 'skechers': 31194, 'shazam': 31195, '9gag': 31196, 'dein': 31197, 'jurkovac': 31198, 'peas': 31199, 'ranta': 31200, 'studio71': 31201, 'fullscreen': 31202, 'techstars': 31203, 'skogmo': 31204, 'jukin': 31205, 'jashni': 31206, 'raintree': 31207, 'vid': 31208, 'deutsch': 31209, 'ja': 31210, 'ale': 31211, 'bierfest': 31212, \"defence's\": 31213, 'tattooist': 31214, 'jarmond': 31215, \"vel'\": 31216, \"d'hiv'\": 31217, 'roundup': 31218, 'tatiana': 31219, 'rosnay': 31220, 'laird’s': 31221, 'highlander—and': 31222, 'sands’': 31223, 'buchanan’s': 31224, 'brawny': 31225, 'emerge—it’s': 31226, 'ensnare': 31227, 'flattery': 31228, 'hilt': 31229, 'kin—and': 31230, 'evina’s': 31231, \"nietzsche's\": 31232, '“will': 31233, 'epistemological': 31234, 'reception': 31235, 'snapper': 31236, 'masterclasses': 31237, 'bespoke': 31238, 'anker': 31239, 'macnicol': 31240, 'ovenden': 31241, 'dixie': 31242, 'duckett': 31243, 'architechture': 31244, 'janie': 31245, 'retouching': 31246, 'frankfurter': 31247, 'interchangeable': 31248, 'reto': 31249, 'bulbasaur': 31250, 'poliwhirl': 31251, 'dagon': 31252, 'cthulhu': 31253, 'herbert': 31254, 'reanimator': 31255, 'menaces': 31256, 'page…': 31257, 'cullen': 31258, 'mullet': 31259, 'excruciatingly': 31260, 'mortification': 31261, 'emo': 31262, 'embarrassingly': 31263, \"chloe's\": 31264, 'lincolnshire': 31265, \"'bristling\": 31266, \"crafted'\": 31267, 'transplanted': 31268, 'skiing': 31269, 'suspiciously': 31270, 'lithe': 31271, 'commissioning': 31272, 'temptress': 31273, 'embodies': 31274, 'vagueness': 31275, 'abruptness': 31276, 'complicacies': 31277, 'chakraborty’s': 31278, \"lynch's\": 31279, 'mind”': 31280, 'bestseller’s': 31281, 'diminish': 31282, 'parasitic': 31283, 'infection': 31284, \"'libby\": 31285, \"fund'\": 31286, 'diondra': 31287, 'workmanship': 31288, 'coherence': 31289, \"conflict's\": 31290, 'precipitous': 31291, 'stilwell': 31292, 'midlands': 31293, 'complexly': 31294, 'dorothea': 31295, 'lydgate': 31296, 'spendthrift': 31297, 'vincy': 31298, 'equilibrium—will': 31299, 'ladislaw': 31300, 'dorothea’s': 31301, 'casaubon': 31302, 'superbeings': 31303, 'earths': 31304, 'supergirl': 31305, 'witherspoon’s': 31306, '‘prima': 31307, 'turners’': 31308, 'beginning…': 31309, '…what': 31310, 'make…': 31311, 'sylvie': 31312, \"'left\": 31313, 'sprinkle': 31314, 'sohal': 31315, 'pathak': 31316, 'kaise': 31317, 'bani': 31318, 'rallied': 31319, 'tricoloured': 31320, 'banner': 31321, 'protes': 31322, 'birtish': 31323, 'batons': 31324, 'bonfires': 31325, 'dandi': 31326, 'tyst': 31327, 'thsi': 31328, 'sth': 31329, 'estory': 31330, 'sayagraha': 31331, 'swaraj': 31332, 'nanao': 31333, 'grader': 31334, 'suzumura': 31335, 'hookey': 31336, 'rickety': 31337, 'underway': 31338, \"idol's\": 31339, 'linguist': 31340, 'contrived': 31341, 'seething': 31342, \"'hyperbole\": 31343, \"half'\": 31344, \"cake'\": 31345, '“learning': 31346, 'experiences”': 31347, \"scope'\": 31348, \"zusak's\": 31349, 'refracted': 31350, 'melancholy–or': 31351, 'hüzün–': 31352, 'istanbullus': 31353, 'painters–both': 31354, 'foreign–who': 31355, 'joyce’s': 31356, 'pamuk’s': 31357, 'colonialist': 31358, 'farrington': 31359, \"'third\": 31360, 'martinique': 31361, 'essayist': 31362, 'psychopathology': 31363, \"said's\": 31364, 'orientalism': 31365, \"racism'\": 31366, 'turkmenistan': 31367, \"'funnies'\": 31368, \"ambassadors'\": 31369, 'valedictories': 31370, \"'parting\": 31371, 'unbuttoned': 31372, 'indiscreet': 31373, \"bbc's\": 31374, \"'sensational\": 31375, \"code'\": 31376, 'assure': 31377, 'nutcase': 31378, \"'six\": 31379, \"four'\": 31380, 'yokoyama': 31381, 'compositional': 31382, 'orchestration': 31383, 'oversize': 31384, 'trim': 31385, 'shridhar': 31386, 'damle': 31387, 'mend': 31388, 'instagrams': 31389, 'cumbria': 31390, 'brusque': 31391, \"'romantic\": 31392, \"refreshing'\": 31393, 'mhairi': 31394, \"kinsella'\": 31395, \"corry'\": 31396, \"vicki's\": 31397, 'epilepsy': 31398, \"riddance'\": 31399, 'suggesting': 31400, \"jewell's\": 31401, \"hunter's\": 31402, 'fearsomely': 31403, 'portrayals': 31404, \"liaisons'\": 31405, 'lj': 31406, \"'psychological\": 31407, 'sykes': 31408, 'furnivall': 31409, 'hamer': 31410, 'cotterell': 31411, 'fortin': 31412, 'sanjida': 31413, 'elgar': 31414, 'katerina': 31415, 'visvakarma': 31416, 'nmap': 31417, 'pentesters': 31418, 'pwning': 31419, 'ows': 31420, 'fiddling': 31421, 'defned': 31422, 'nethunter': 31423, 'jexboss': 31424, 'patator': 31425, 'airoscript': 31426, 'aeroplane': 31427, 'rastapopoulos': 31428, \"tapioca's\": 31429, 'hq': 31430, 'forgers': 31431, 'cliffhanger': 31432, 'eastwood’s': 31433, 'rooftops': 31434, 'legend”': 31435, 'shaitan': 31436, 'devil”': 31437, 'war—including': 31438, 'teammates—and': 31439, 'taya': 31440, 'kyle’s': 31441, 'gamow': 31442, 'originator': 31443, 'curvature': 31444, 'greig': 31445, 'haigh': 31446, \"haigh's\": 31447, 'warlight': 31448, \"slaughter's\": 31449, 'ritually': 31450, 'masterpiece—a': 31451, 'matrix—until': 31452, 'cyberspace': 31453, 'shotgun': 31454, 'upped': 31455, 'ante': 31456, 'future—a': 31457, 'nation…': 31458, 'durrani': 31459, 'balked': 31460, \"tehmina's\": 31461, 'ré': 31462, 'handcuffed': 31463, 'andré’s': 31464, 'attests': 31465, 'pared': 31466, 'muted': 31467, \"mitnick's\": 31468, 'desperado': 31469, 'adage': 31470, 'rifling': 31471, 'irate': 31472, 'irs': 31473, '“basically': 31474, 'roger’s': 31475, 'friend—someone': 31476, 'yanks': 31477, 'just…disappears': 31478, 'wait…what': 31479, 'sloane’s': 31480, 'her—and': 31481, 'porter—who': 31482, 'um…': 31483, '—includes': 31484, \"well's\": 31485, 'konnikova': 31486, 'catapulting': 31487, 'che’s': 31488, 'diaries’': 31489, '‘reminiscences': 31490, 'war’': 31491, 'diary’': 31492, '1’': 31493, '2’': 31494, 'landour': 31495, 'faints': 31496, '‘bad': 31497, 'boy’': 31498, 'bollywood—veer': 31499, 'tomar': 31500, 'incidentally': 31501, 'veer’s': 31502, 'amyra': 31503, \"poirot'\": 31504, 'tulsi’s': 31505, '“loverboy': 31506, 'investigator”': 31507, 'archrival': 31508, 'ram’s': 31509, 'hela': 31510, 'henrietta’s': 31511, '‘immortality’': 31512, 'skloot’s': 31513, 'magnificent’': 31514, 'kirill': 31515, 'eremenko': 31516, 'adeptness': 31517, 'existent': 31518, 'soto': 31519, 'lodged': 31520, 'kristiania': 31521, \"norway's\": 31522, 'growling': 31523, 'knut': 31524, \"hamsun's\": 31525, 'fante': 31526, 'flodberga': 31527, 'mannheimer': 31528, \"larsson's\": 31529, 'goulding': 31530, 'yasser': 31531, 'seirawan': 31532, 'deflection': 31533, 'dataóinto': 31534, 'zheng': 31535, 'casari': 31536, 'binning': 31537, 'grams': 31538, 'uninformative': 31539, 'featurization': 31540, 'transliterated': 31541, 'devanagri': 31542, 'bolshevik': 31543, 'rebuttal': 31544, 'krautsky': 31545, \"'opportunists'\": 31546, 'fudged': 31547, 'suppressing': 31548, \"'wage\": 31549, \"slave'\": 31550, 'proletariat': 31551, \"'bourgeoisie'\": 31552, 'bureaucracy': 31553, 'unaccountable': 31554, 'reversion': 31555, \"'dictatorship\": 31556, \"proletariat'\": 31557, 'dispossess': 31558, 'wither': 31559, 'tiananmen': 31560, 'reassigned': 31561, \"party's\": 31562, 'dismissal': 31563, 'jobs’': 31564, 'cooperated': 31565, 'perfectionism': 31566, 'chickens': 31567, 'neurotic': 31568, 'spacegirl': 31569, 'hatke': 31570, '“money': 31571, \"julie's\": 31572, 'politico': 31573, 'southwestern': 31574, 'instigated': 31575, 'plebiscite': 31576, 'recourse': 31577, 'mooted': 31578, 'equestria': 31579, 'ponyville': 31580, 'canterlot': 31581, 'montages': 31582, 'abductions': 31583, 'complication': 31584, 'fire…': 31585, 'toby—pretty': 31586, 'all—working': 31587, \"hold'em\": 31588, 'stud': 31589, 'razz': 31590, 'conservatively': 31591, 'ominously': 31592, \"thorne's\": 31593, 'theatregoers': 31594, 'playscripts': 31595, 'it–on': 31596, 'level–restores': 31597, 'memory–making': 31598, 'lyme': 31599, 'antibiotic': 31600, 'treatment–only': 31601, 'icu–bleeding': 31602, 'jaundiced': 31603, 'incoherent–and': 31604, 'illness–the': 31605, 'diagnosis–revealing': 31606, 'patient’s': 31607, 'patients’': 31608, 'moment’s': 31609, 'invests': 31610, 'amenable': 31611, 'supercomputing': 31612, 'backtest': 31613, 'positives': 31614, 'inferior―it': 31615, 'shuts': 31616, 'subordinates': 31617, 'nothing―anybody': 31618, 'desktops': 31619, 'catrin': 31620, 'thornley': 31621, 'caradec—gambler': 31622, 'wanderer': 31623, 'fortune—was': 31624, 'waterfronts': 31625, 'ship’s': 31626, 'fo’c’sle': 31627, 'gastroenterology': 31628, 'accreditation': 31629, 'misted': 31630, 'invictus': 31631, 'eastwood': 31632, 'afrikaner': 31633, 'afrikaans': 31634, 'taster': 31635, 'dusté': 31636, 'clear—': 31637, 'shareware': 31638, 'irrefutable': 31639, \"'war\": 31640, \"terror'\": 31641, 'unraveller': 31642, 'accredited': 31643, 'dissection': 31644, \"'anybody\": 31645, \"survival'\": 31646, \"intellect'\": 31647, 'hamish': 31648, 'grey’s': 31649, 'rhimes’s': 31650, 'watch”': 31651, 'publicist': 31652, 'muttered': 31653, 'shonda’s': 31654, 'yes—from': 31655, 'begun—when': 31656, '“honest': 31657, 'revelatory”': 31658, 'shondaland': 31659, 'homebody': 31660, 'new”': 31661, '460': 31662, 'laure’s': 31663, 'agoraphobic': 31664, 'radios': 31665, 'converges': 31666, 'parlance': 31667, \"ramsar's\": 31668, 'businessweek': 31669, 'cubicle': 31670, 'seminar': 31671, '•improve': 31672, '•apply': 31673, '•instantly': 31674, '•learn': 31675, 'dogeared': 31676, 'aqa': 31677, 'edexcel': 31678, 'eduqas': 31679, 'ocr': 31680, 'ks3': 31681, 'copymasters': 31682, 'mops': 31683, 'corduroy': 31684, \"pastor's\": 31685, 'surmount': 31686, 'paralysing': 31687, 'yankee': 31688, 'fundraise': 31689, 'actuate': 31690, \"'aha'\": 31691, 'home…': 31692, 'leonides': 31693, 'aristide': 31694, 'barbiturate': 31695, 'hayward': 31696, 'millionare’s': 31697, 'granddaughter…': 31698, 'abstracts': 31699, 'dois': 31700, 'locators': 31701, 'advisory': 31702, 'peruse': 31703, 'ditched': 31704, 'ramu': 31705, 'kaka': 31706, '‘raghunathan': 31707, 'instances’': 31708, '—bibek': 31709, 'indians—among': 31710, 'dispassionate': 31711, 'baffling—v': 31712, 'raghunathan': 31713, 'irrationality': 31714, 'us—whether': 31715, 'us—stand': 31716, 'fukuyama': 31717, 'snowdrift': 31718, 'pythonlearn': 31719, 'moët': 31720, 'skyrocketed': 31721, 'hedgcock': 31722, 'bones—and': 31723, 'fuji': 31724, 'verdant': 31725, '“dark': 31726, 'adapting—surviving': 31727, 'reconquering': 31728, 'sigma’s': 31729, 'enemy—the': 31730, 'guild—even': 31731, \"goldie's\": 31732, 'codirector': 31733, 'gearing': 31734, 'rourke': 31735, 'knausgaard’s': 31736, '‘perhaps': 31737, 'photocopiable': 31738, 'ded': 31739, 'lalla': 31740, 'philosophically': 31741, 'ecstatic': 31742, 'paraphrases': 31743, 'jagged': 31744, \"lalla's\": 31745, \"hoskote's\": 31746, 'that’': 31747, 'uppity': 31748, 'forthright': 31749, 'pyotr…': 31750, 'touchingly': 31751, 'taming': 31752, 'shrew': 31753, \"joyful'\": 31754, 'finigan': 31755, '‘tyler': 31756, 'effortlessness': 31757, 'art’': 31758, 'dunmore': 31759, '‘tyler’s': 31760, 'duplicate’': 31761, 'hanya': 31762, 'yanigihara': 31763, 'prelims': 31764, 'deafening': 31765, 'televisions': 31766, \"'sa\": 31767, 'showered': 31768, 'rafters': 31769, \"'through\": 31770, 'swell': 31771, 'electra': 31772, 'orestes': 31773, 'cartwright': 31774, 'belmarsh': 31775, \"danny's\": 31776, \"beth's\": 31777, 'wheels': 31778, 'whooshing': 31779, \"istanbul's\": 31780, 'topkapi': 31781, \"treat'\": 31782, \"'frightening\": 31783, 'fromsleepy': 31784, 'paignton': 31785, 'brownlow': 31786, 'antwerp': 31787, 'untangling': 31788, 'bohemian': 31789, 'girlfriend’s': 31790, 'giovanni’s': 31791, 'love’s': 31792, 'extremities': 31793, 'springboards': 31794, 'bother': 31795, 'libertarian': 31796, 'fthe': 31797, \"'el\": 31798, \"trindente'\": 31799, 'barça': 31800, '£70m': 31801, 'nou': 31802, 'educations': 31803, 'strikeforce': 31804, 'luiz': 31805, 'felipe': 31806, 'scolari': 31807, 'vicente': 31808, 'bosque': 31809, 'head—the': 31810, 'aparajito': 31811, 'charulata': 31812, 'aranyer': 31813, 'din': 31814, 'ratri': 31815, 'shatranj': 31816, 'khilari': 31817, 'ghare': 31818, 'baire': 31819, 'agantuk': 31820, 'flaherty': 31821, 'declaring': 31822, 'inclinations': 31823, 'thoroughness': 31824, 'seton': 31825, 'scenarist': 31826, 'hiromi': 31827, 'otsuichi': 31828, 'yoko': 31829, 'tawada': 31830, 'demolishes': 31831, 'pancakes': 31832, 'strolls': 31833, 'mangle': 31834, 'roofed': 31835, 'unputdowneable': 31836, 'ymca': 31837, 'gapingvoid': 31838, 'averse': 31839, \"macleod's\": 31840, \"'symbol\": 31841, \"culture'\": 31842, 'germaine': 31843, 'descriptionmicrosoft': 31844, 'subscriptions': 31845, 'writeback': 31846, '—interview': 31847, '—what': 31848, '—revelations': 31849, 'globe—brussels': 31850, 'turkey—the': 31851, 'syrian': 31852, 'insur': 31853, 'gent': 31854, 'jihadi': 31855, 'slickly': 31856, 'musab': 31857, 'zarqawi': 31858, 'are—from': 31859, 'bakr': 31860, 'baghdadi': 31861, 'baathists': 31862, 'ranks—where': 31863, 'operate—from': 31864, 'matériel': 31865, 'forces—assad’s': 31866, 'quds': 31867, 'shiite': 31868, 'militias—are': 31869, 'astor’s': 31870, \"astor's\": 31871, 'kottke': 31872, 'legg': 31873, 'wilcox': 31874, \"blowin'\": 31875, 'aveen': 31876, 'khilnani': 31877, 'germano': 31878, 'celant': 31879, 'raqs': 31880, '“must”': 31881, 'summer”': 31882, 'hinges': 31883, 'unfold—with': 31884, 'gabby’s': 31885, 'guestroom': 31886, 'agra': 31887, 'bazar': 31888, 'nazir': 31889, 'socioeconomic': 31890, 'charandas': 31891, 'chor': 31892, 'kalarin': 31893, 'chattisgarh': 31894, 'mother–son': 31895, 'hirma': 31896, \"binodini's\": 31897, 'engrossingly': 31898, 'rajahs': 31899, 'chiaroscuro': 31900, '“light': 31901, 'dark”': 31902, 'sitter’s': 31903, 'tautly': 31904, 'malkovich': 31905, \"mia's\": 31906, \"kubica's\": 31907, 'banff': 31908, 'intense…buried': 31909, '—kate': 31910, 'ncte': 31911, 'jaggi': 31912, 'vasudev': 31913, 'sadhguru': 31914, 'agnostic': 31915, 'contagious': 31916, 'kakar': 31917, 'pinto': 31918, \"missed'\": 31919, 'purohit': 31920, 'general’s': 31921, 'flatulent': 31922, 'cork': 31923, 'auric': 31924, 'stockpiling': 31925, 'goldfinger’s': 31926, 'fleming…': 31927, 'unqualified': 31928, 'imi': 31929, 'lichtenfeld': 31930, \"assailant's\": 31931, 'bragged': 31932, '‘get': 31933, 'of’': 31934, 'edgware': 31935, 'sarila': 31936, 'imbroglio': 31937, 'didi': 31938, 'kingmaker': 31939, 'clout': 31940, 'nitish': 31941, 'pedagogue': 31942, 'dravidian': 31943, 'spiraling': 31944, 'tanked': 31945, 'reordering': 31946, 'lateral': 31947, \"rifkin's\": 31948, 'transitioning': 31949, \"unrivalled'\": 31950, 'mosul': 31951, \"indispensable'\": 31952, 'aaronovitch': 31953, 'fatwa': 31954, '‘just': 31955, 'glad’': 31956, 'i’s': 31957, '“government': 31958, 'sprodj': 31959, 'flummoxed': 31960, \"krosoczka's\": 31961, 'mommy': 31962, 'kiddo': 31963, 'undisputedly': 31964, \"bradman's\": 31965, 'invincibles': 31966, \"too'\": 31967, 'rasool': 31968, 'joo': 31969, \"haleema's\": 31970, 'haleema': 31971, \"'half\": 31972, \"mother'\": 31973, 'jails': 31974, 'morgues': 31975, 'hazzah': 31976, 'kitab': 31977, '222': 31978, 'mm': 31979, 'aaz9788171510283': 31980, 'replay': 31981, 'premonitions': 31982, 'sleepwalks': 31983, 'grace’s': 31984, 'waterside': 31985, 'chipping': 31986, 'musters': 31987, 'spiking': 31988, \"crisis'\": 31989, 'us—but': 31990, 'mcraney’s': 31991, 'chapter—covering': 31992, 'helplessness': 31993, 'transparency—is': 31994, 'palghat': 31995, 'enfolds': 31996, 'travelerin': 31997, 'bewitched': 31998, 'tappers': 31999, 'shamans—takes': 32000, 'sojourner’s': 32001, 'vijayan’s': 32002, 'reboots': 32003, 'halka': 32004, 'inhumanly': 32005, 'vyom': 32006, 'skyrocketing': 32007, \"'gooseberries'\": 32008, \"kiss'\": 32009, \"volodyas'\": 32010, \"chekhov's\": 32011, 'squaronthehypotenus': 32012, \"durrell's\": 32013, \"rookwhistle's\": 32014, 'bequeathed': 32015, '£500': 32016, 'fiefdoms': 32017, 'rickon': 32018, 'wildings': 32019, 'vainglorious': 32020, 'belies': 32021, 'viserys': 32022, 'dothraki―whose': 32023, 'dawson’s': 32024, 'says—uncensored': 32025, 'miley': 32026, 'hilton': 32027, 'myselfie': 32028, 'shane’s': 32029, 'dawdle': 32030, 'sonal': 32031, 'kalra': 32032, 'pappu': 32033, 'chaddha': 32034, 'inaugurated': 32035, '1896': 32036, 'goldblatt': 32037, 'fairs': 32038, \"fool's\": 32039, 'hells': 32040, 'barret': 32041, 'unforeseeably': 32042, 'segal': 32043, 'oliver’s': 32044, 'badger': 32045, 'weasels': 32046, 'willows': 32047, 'soweto': 32048, 'housecleaner': 32049, 'merges': 32050, 'diverges': 32051, 'dismantled': 32052, \"nombeko's\": 32053, 'jonasson': 32054, 'eccentrics': 32055, 'deserter': 32056, \"sweden'\": 32057, 'maoism': 32058, 'ridicules': 32059, 'posco': 32060, 'tribals': 32061, 'eradicating': 32062, 'cherukuri': 32063, 'rajkumar': 32064, 'statistically': 32065, 'weighting': 32066, 'subsetting': 32067, 'bivariate': 32068, 'multivariate': 32069, '1670': 32070, \"'hal'\": 32071, 'zanzibar': 32072, 'nazet': 32073, 'glints': 32074, 'scents': 32075, 'graver': 32076, \"zanzibar's\": 32077, 'imperilling': 32078, \"slave's\": 32079, 'bestsellling': 32080, 'bryant―a': 32081, 'mamba”': 32082, '“mamba': 32083, 'obligation': 32084, 'granular': 32085, '2016―and': 32086, 'bernstein’s': 32087, 'edifis': 32088, \"sphinx's\": 32089, 'profane': 32090, 'reprehensible': 32091, 'deviant': 32092, 'superjerk': 32093, 'obese': 32094, 'shirted': 32095, 'stabber': 32096, 'wrong…': 32097, 'barrack': 32098, 'townhouses': 32099, 'havelis': 32100, 'samsung': 32101, 'pretension': 32102, 'cellphone': 32103, \"'photography\": 32104, 'householders': 32105, 'instability': 32106, 'effusive': 32107, \"'pseudoscholarship'\": 32108, \"chronology'\": 32109, 'proust': 32110, 'habitual': 32111, 'howards': 32112, 'hobsbawm': 32113, 'columbus': 32114, 'duhigg': 32115, '‘uplifting': 32116, 'cunnell': 32117, '‘utterly': 32118, 'compelling…this': 32119, 'heydorn': 32120, 'condition…': 32121, 'wasim': 32122, 'geary': 32123, 'burrage': 32124, 'am…': 32125, 'reader…': 32126, 'rereading’': 32127, 'jocelyne': 32128, 'quennell': 32129, 'love’': 32130, 'fospero': 32131, 'pictorially': 32132, 'invariants': 32133, 'variable': 32134, 'influential—and': 32135, 'controversial—futurist': 32136, 'civilization—reverse': 32137, 'kurzweil’s': 32138, 'costello': 32139, 'protestors': 32140, 'deniers': 32141, 'envied': 32142, 'truth…': 32143, 'anu': 32144, '420': 32145, 'threesome': 32146, 'naturedly': 32147, \"brahms's\": 32148, 'wiegenlied': 32149, 'memes': 32150, 'impressionist': 32151, 'etsy': 32152, \"trotsky's\": 32153, '435': 32154, 'lindnord': 32155, 'meatball': 32156, \"'arthur\": 32157, 'latched': 32158, 'watched…': 32159, 'shahjahan': 32160, \"'teardrop\": 32161, 'upheld': 32162, 'ranas': 32163, 'mewar': 32164, 'frivolity': 32165, 'canny': 32166, 'conscientious': 32167, 'undervalued': 32168, 'shindig': 32169, 'makarand': 32170, 'wagainkar': 32171, \"yuvraj's\": 32172, 'sundown': 32173, 'longbow': 32174, 'skinner': 32175, 'molded': 32176, 'stata': 32177, \"gladwell's\": 32178, 'murtaza': 32179, 'ryerson': 32180, 'regionomics': 32181, 'munk': 32182, 'adjunct': 32183, 'mcgill': 32184, 'manchuria': 32185, 'nelle': 32186, \"movement's\": 32187, \"malone's\": 32188, 'tortugas': 32189, 'leading—and': 32190, 'living—a': 32191, 'think—often': 32192, '—daniel': 32193, 'author—rubin': 32194, 'marshland': 32195, 'vic': 32196, 'glades': 32197, 'twinges': 32198, 'anarchist': 32199, 'evey': 32200, 'pedophile': 32201, 'deranging': 32202, 'sabotages': 32203, 'overthrown': 32204, 'mined': 32205, 'faves': 32206, 'checkup': 32207, 'merritt': 32208, 'leno': 32209, 'shulz': 32210, 'buchanans': 32211, 'clan’s': 32212, 'wife…': 32213, 'aulay’s': 32214, 'flaring': 32215, 'macneal': 32216, '1850': 32217, 'erected': 32218, 'raphaelite': 32219, 'darkening': 32220, \"panache'\": 32221, \"'dark\": 32222, 'tumbles': 32223, 'motionless': 32224, '999': 32225, \"'addictive\": 32226, 'seddon': 32227, \"sitting'\": 32228, 'hollie': 32229, '―new': 32230, 'leuyen': 32231, 'pham': 32232, 'friends―and': 32233, \"jen's\": 32234, 'group―or': 32235, \"editors'\": 32236, 'yalsa': 32237, 'empty—until': 32238, 'shapely': 32239, 'him—past': 32240, 'monogatari': 32241, 'senjogahara': 32242, 'hachikuji': 32243, 'kizumonogatari': 32244, 'crazies': 32245, 'weirdly': 32246, 'araragi': 32247, 'hanekawa': 32248, 'oshino': 32249, 'toting': 32250, 'villas': 32251, 'geckos': 32252, 'toads': 32253, \"'durrell\": 32254, \"eccentricities'\": 32255, \"spivet's\": 32256, 'spivet': 32257, 'baird': 32258, 'hallowed': 32259, 'hobo': 32260, 'rims': 32261, \"science's\": 32262, \"journey's\": 32263, 'jeunet': 32264, 'catlett': 32265, 'bonham': 32266, \"anderson's\": 32267, 'roars': 32268, 'panthers': 32269, 'curating': 32270, 'rectangles': 32271, '×': 32272, 'menseki': 32273, 'meiro': 32274, 'mazes—they': 32275, \"vampire'\": 32276, \"'daughters\": 32277, 'lynette': 32278, \"'enchantress'\": 32279, 'irrestistible': 32280, 'bewitches': 32281, \"thea's\": 32282, \"blaise's\": 32283, \"kensington's\": 32284, \"bashir's\": 32285, 'duality': 32286, 'befit': 32287, 'trainingbible': 32288, \"friel's\": 32289, 'triathlete': 32290, 'ps4': 32291, 'insomniac': 32292, 'fisk': 32293, 'altruistic': 32294, \"fisk's\": 32295, 'autumn―the': 32296, 'marines': 32297, 'slipspace': 32298, \"humans'\": 32299, 'capoeira': 32300, 'hippy': 32301, 'twonk': 32302, 'pyromaniac': 32303, 'jordana': 32304, 'submerge': 32305, 'transfinite': 32306, 'clegg': 32307, \"pugh's\": 32308, 'khwarizmi': 32309, 'cantor': 32310, 'venn': 32311, 'godel': 32312, 'mandelbrot': 32313, 'sport—fully': 32314, 'eighty—if': 32315, 'tee': 32316, 'illustrations—as': 32317, \"vorderman's\": 32318, '9781405363426': 32319, 'northouse': 32320, '“dragon': 32321, 'balls”': 32322, 'lifetime…': 32323, 'therese': 32324, 'highsmith': 32325, 'forbids': 32326, \"highsmith's\": 32327, 'blanchett': 32328, 'dacha': 32329, 'gusty': 32330, 'treaded': 32331, 'majesty': 32332, 'reconnaissance': 32333, 'father\\x92s': 32334, 'incumbent': 32335, 'one\\x85': 32336, \"shoot'\": 32337, 'turnbull': 32338, 'inscrutable': 32339, 'lawsuits': 32340, 'reimbursements': 32341, 'transpires': 32342, '22x26x6': 32343, 'bromleigh': 32344, \"charise's\": 32345, 'langford': 32346, 'inadvertent': 32347, \"westmoreland's\": 32348, 'expository': 32349, 'stylistics': 32350, 'sentential': 32351, '‘c': 32352, 'depth’': 32353, 'utilized': 32354, '310': 32355, 'deepali': 32356, \"f'gdca\": 32357, 'mjp': 32358, 'rohilkhand': 32359, 'goad': 32360, 'shouldering': 32361, 'bullying…': 32362, 'mongols': 32363, 'abolished': 32364, 'yoligov': 32365, 'humanitarian': 32366, 'philanthropists': 32367, 'pacified': 32368, 'ruin…': 32369, 'everything―wealth': 32370, 'siren': 32371, 'slows': 32372, '“central': 32373, 'nottinghamshire': 32374, \"sherrif's\": 32375, 'robinhood': 32376, 'rouble': 32377, 'sulphur': 32378, 'scuttles': 32379, 'switchback': 32380, 'coorg': 32381, 'consternation': 32382, 'purveyor': 32383, 'lucre': 32384, 'conspirator': 32385, 'unsinkable': 32386, 'kayakers': 32387, 'harbingers': 32388, 'eruptions': 32389, 'kemprecos': 32390, 'daytime': 32391, 'rug': 32392, 'unvacuumed': 32393, 'ivories': 32394, 'untickled': 32395, 'sweetly': 32396, 'amiably': 32397, 'kale': 32398, 'catwalks': 32399, 'intimidate': 32400, 'betrayers': 32401, 'morrow': 32402, \"shakespeare'\": 32403, 'swimmers': 32404, 'glides': 32405, 'lynton': 32406, 'solnit’s': 32407, 'muybridge—who': 32408, 'photographically—becomes': 32409, 'acceleration': 32410, 'solnit': 32411, 'post–civil': 32412, 'industries—hollywood': 32413, 'valley—that': 32414, 'place—outside': 32415, 'unignorable': 32416, 'businesspeople': 32417, 'gentler': 32418, 'outstrip': 32419, 'shunted': 32420, 'contaminant': 32421, 'in…': 32422, 'declined': 32423, 'shreds': 32424, \"chad's\": 32425, \"'they\": 32426, 'drains': 32427, 'pennywise': 32428, 'sullen': 32429, '‘charlie’': 32430, 'infamously': 32431, 'feltner': 32432, 'mags': 32433, 'jets': 32434, '–and': 32435, \"james'\": 32436, 'somerville': 32437, 'screams': 32438, 'parked': 32439, 'workmen': 32440, \"blood's\": 32441, 'arrivals': 32442, 'duet': 32443, 'interlude': 32444, 'translating': 32445, 'exhibiting': 32446, 'pierced': 32447, \"please'\": 32448, 'sighed': 32449, 'brimstone': 32450, \"'elsewhere'\": 32451, \"brimstone's\": 32452, 'shergill': 32453, 'shivyanka': 32454, 'devang': 32455, 'anatoli': 32456, 'boukreev': 32457, 'makalu': 32458, 'manaslu': 32459, 'buchwald': 32460, 'swipe': 32461, 'honeymoons': 32462, 'elbow': 32463, 'lettuce': 32464, 'hartford': 32465, 'courant': 32466, 'egyptologist': 32467, 'scours': 32468, 'arumbaya': 32469, '30am': 32470, 'bracelet': 32471, 'underside': 32472, 'uncharacteristically': 32473, 'coco': 32474, 'couturiers': 32475, 'millinery': 32476, 'sportswear': 32477, 'cardigan': 32478, 'lagerfeld': 32479, \"chanel's\": 32480, 'underachieve\\x97going': 32481, 'strength\\x85': 32482, 'progress\\x97and': 32483, 'kavadlo': 32484, 'calisthenics': 32485, 'strength\\x97like': 32486, 'lever\\x97they': 32487, 'baskerville': 32488, 'deciphers': 32489, 'charcoal': 32490, 'handwoven': 32491, 'yavar': 32492, 'dehghani': 32493, 'schooner': 32494, 'aground': 32495, 'jelly': 32496, 'inour': 32497, 'humanbrain': 32498, 'aphrodite': 32499, 'melting': 32500, 'roomie': 32501, \"stevie's\": 32502, 'vampyres': 32503, 'interpretive': 32504, 'dispassion': 32505, 'message’': 32506, 'com\\xadics': 32507, 'hague': 32508, \"'untouchables'\": 32509, 'malta': 32510, 'detainees': 32511, \"white's\": 32512, 'enliven': 32513, 'maira': 32514, 'kalman’s': 32515, 'rethought': 32516, 'terminally': 32517, 'gawande': 32518, \"dumplin'—now\": 32519, 'parton': 32520, 'school—only': 32521, 'righting': 32522, 'savor': 32523, '0510': 32524, '0511': 32525, '0991': 32526, 'scaffolded': 32527, 'emphasising': 32528, \"'tennis\": 32529, 'beattie': 32530, 'concentrating': 32531, 'gamesmanship': 32532, 'lapses': 32533, 'conceal…': 32534, 'concoct': 32535, 'shielding': 32536, 'victim’s': 32537, 'suspects…': 32538, '‘another': 32539, 'mousetrap’': 32540, 'katzenbach': 32541, 'amabile': 32542, 'tamara': 32543, 'erickson': 32544, 'touchy': 32545, 'constructively': 32546, \"'blueprints'\": 32547, 'biro': 32548, \"'microwaves'\": 32549, \"'bridges'\": 32550, \"'datacentres'\": 32551, 'cooled': 32552, '—this': 32553, 'enrollment': 32554, 'totaling': 32555, 'crom': 32556, '“us': 32557, \"'’how\": 32558, 'people’’': 32559, 'bestsellerand': 32560, \"'’lincoln\": 32561, 'unknown’’': 32562, 'famines': 32563, 'stagnation': 32564, 'restraints': 32565, 'secretariat': 32566, 'nationalization': 32567, 'abolition': 32568, 'purses': 32569, 'haksar’s': 32570, 'the1960s': 32571, 'esenwein': 32572, 'photoplay': 32573, 'leeds': 32574, 'computationally': 32575, 'refreshers': 32576, 'urmila': 32577, 'feministand': 32578, 'eschews': 32579, 'subverts': 32580, 'motherwit': 32581, 'ursula’s': 32582, 'stillborn': 32583, 'umbilical': 32584, 'cord': 32585, 'argyll': 32586, \"o'sullivan's\": 32587, 'cartography': 32588, 'meteorology': 32589, 'geomorphology': 32590, 'marginalization': 32591, 'rurality': 32592, 'legislations': 32593, '“international': 32594, 'rights”': 32595, 'justifications': 32596, 'divyang': 32597, 'llm': 32598, 'jurists': 32599, 'upland': 32600, 'fir': 32601, 'heeded': 32602, 'locals’': 32603, '‘there’s': 32604, 'meddles': 32605, 'beginning…’': 32606, 'morihei': 32607, 'ueshiba': 32608, 'aikido': 32609, \"musashi's\": 32610, \"tzu's\": 32611, 'anticipating': 32612, 'manisha': 32613, \"koirala's\": 32614, 'ovarian': 32615, 'oncologists': 32616, 'apprehensions': 32617, 'katha': 32618, 'cede': 32619, 'jhansi': 32620, 'repelled': 32621, 'chenamma': 32622, 'velu': 32623, 'thampi': 32624, 'kunwar': 32625, '‘assessment': 32626, 'learning’': 32627, 'conceptually': 32628, '“check': 32629, 'understanding”': 32630, '“mock': 32631, 'test”': 32632, 'mcq': 32633, 'analogy': 32634, 'rousing': 32635, 'twitter’s': 32636, 'hackers—ev': 32637, 'dorsey': 32638, 'bilton': 32639, 'kathryn': 32640, 'croft': 32641, 'marrs': 32642, 'mazey': 32643, \"coming'\": 32644, 'swarfield': 32645, \"'close\": 32646, 'broadchurch': 32647, 'turny': 32648, \"whodunnit'\": 32649, 'mel': 32650, 'sheratt': 32651, 'credulous': 32652, 'undocumented': 32653, '‘americanah’': 32654, 'martinet': 32655, \"chapter's\": 32656, 'vietnam—now': 32657, '“age': 32658, 'counterinsurgency': 32659, 'disregards': 32660, 'mocks': 32661, 'posers': 32662, 'asshole': 32663, 'bounder': 32664, 'calderon': 32665, 'embellishing': 32666, 'multiples': 32667, 'critters': 32668, 'monograms': 32669, 'notes—any': 32670, 'mccool': 32671, \"neal's\": 32672, \"something's\": 32673, 'gerry’s': 32674, 'applegate': 32675, 'initiating': 32676, 'griffith': 32677, '1025': 32678, 'bharatvarsha': 32679, 'helmed': 32680, 'srivijaya': 32681, 'ravaging': 32682, 'sangrama': 32683, 'kingship': 32684, 'luscious': 32685, 'tejpal’s': 32686, 'description’': 32687, 'beautifully’': 32688, \"tejpal's\": 32689, '‘heart': 32690, 'specialist’': 32691, 'packington': 32692, 'forlorn': 32693, 'pyne’': 32694, 'unbounded': 32695, 'allah': 32696, 'demonpossessed': 32697, 'hellfire': 32698, 'luridly': 32699, \"qur'an's\": 32700, 'unbelievers': 32701, 'licentiousness': 32702, 'inferiors': 32703, 'justifies': 32704, 'largescale': 32705, 'policymaker': 32706, 'perfumes': 32707, 'nagapattinam': 32708, 'anantha': 32709, \"empire's\": 32710, 'choja': 32711, 'shiro': 32712, 'momotaro': 32713, 'libel': 32714, 'integrity–what': 32715, '“authenticity”–whose': 32716, 'jotting': 32717, 'drawer': 32718, 'amazement': 32719, 'confederate': 32720, 'privately': 32721, '“think': 32722, 'everyman’s': 32723, 'ideas’': 32724, 'isaiah': 32725, 'historicism': 32726, 'relativism': 32727, 'irrationalism': 32728, 'hausheer': 32729, 'marr': 32730, 'unequally': 32731, \"'take\": 32732, 'discontent': 32733, 'restive': 32734, 'quotas': 32735, 'palpably': 32736, \"'india\": 32737, \"emu's\": 32738, 'catfish': 32739, 'sneezing': 32740, 'octogenarians': 32741, 'tsunamis': 32742, 'fomo': 32743, 'kavya': 32744, 'unpretentious': 32745, \"'little\": 32746, \"things'\": 32747, 'todayís': 32748, 'gothelf': 32749, 'seiden': 32750, 'deliverables': 32751, 'changeófor': 32752, 'designersí': 32753, 'agileís': 32754, \"reality'\": 32755, \"morgenstern's\": 32756, 'undisturbed': 32757, 'duckpond': 32758, 'reshapes': 32759, 'incrementally': 32760, 'vole': 32761, 'chancer': 32762, 'romaine': 32763, \"neurosurgeon's\": 32764, 'eben': 32765, 'bacterial': 32766, 'meningitis': 32767, \"alexander's\": 32768, 'inactivity': 32769, 'fabricating': 32770, 'nailing': 32771, \"'waterfall\": 32772, \"ahead'\": 32773, 'phalanx': 32774, 'wined': 32775, 'dined': 32776, 'goethe': 32777, 'schiller': 32778, 'disabling': 32779, 'unworldliness': 32780, 'pushy': 32781, 'juggle': 32782, 'unpromotable': 32783, 'harming': 32784, 'acing': 32785, '‘9': 32786, '‘choose': 32787, 'outable': 32788, 'moustaches': 32789, \"money'\": 32790, 'legitmately': 32791, \"twenties'\": 32792, 'uncannily': 32793, 'tsai': 32794, 'loudly—some': 32795, 'malice—whether': 32796, 'nines’': 32797, 'devadasis': 32798, 'breasts': 32799, '‘original': 32800, 'shouted': 32801, 'hijras': 32802, 'hijra': 32803, 'tingle': 32804, \"'n'yoni'\": 32805, 'mbejane': 32806, 'pall': 32807, 'noonday': 32808, 'natal': 32809, 'rearing': 32810, 'zulu': 32811, 'omen': 32812, \"sean's\": 32813, 'infuses': 32814, \"kingdom's\": 32815, 'expected—particularly': 32816, \"enemy—who's\": 32817, 'imac': 32818, 'itunes': 32819, 'telecommunications': 32820, 'yakuza': 32821, 'mirio': 32822, \"jake's\": 32823, \"laden's\": 32824, 'repressed': 32825, \"'speedcubing'\": 32826, 'clocked': 32827, 'wildfire': 32828, \"speedcuber's\": 32829, '3x3x3': 32830, '252': 32831, '003': 32832, '274': 32833, '489': 32834, '856': 32835, '2x2x2': 32836, '4x4x4': 32837, '5x5x5': 32838, 'cubes': 32839, 'alleges': 32840, 'tauber': 32841, 'voight': 32842, 'maximilian': 32843, 'schell': 32844, 'unburdening': 32845, 'stoppard': 32846, 'sergei': 32847, 'magnitsky': 32848, 'whitewash': 32849, 'browder': 32850, 'unseasonably': 32851, \"sofia's\": 32852, 'stalls': 32853, 'trysts': 32854, \"greenwell's\": 32855, 'unstaged': 32856, 'textures': 32857, 'fuelling': 32858, 'pease': 32859, 'collating': 32860, '“leading': 32861, 'naysayer': 32862, 'palmisano': 32863, \"usain's\": 32864, 'buffet': 32865, '227': 32866, \"qi's\": 32867, \"simpler'\": 32868, 'anorexia': 32869, 'wore': 32870, 'underwear': 32871, '1871': 32872, 'kitchener': 32873, 'spaniels': 32874, 'spacesuit': 32875, \"'chuffy'\": 32876, 'chuffnell': 32877, 'chuffy': 32878, 'abetted': 32879, 'loony': 32880, 'ensue': 32881, 'lam': 32882, \"somebody's\": 32883, 'hacking1': 32884, 'hacking2': 32885, 'protocols3': 32886, 'linux4': 32887, 'printing5': 32888, 'hack6': 32889, 'system7': 32890, 'backdoors': 32891, 'worms9': 32892, 'engg': 32893, 'cryptography12': 32894, 'steganogrpahy13': 32895, 'hacking15': 32896, 'serviceability': 32897, 'testing17': 32898, 'exploitations18': 32899, \"saga's\": 32900, 'debartolo': 32901, 'aragonés': 32902, 'smirk': 32903, 'hum': 32904, \"d2's\": 32905, \"lucas's\": 32906, 'sued': 32907, 'sith': 32908, 'artifice': 32909, 'bluntly': 32910, 'riffling': 32911, \"erdnase's\": 32912, 'jacks': 32913, 'divining': 32914, \"'mechanics\": 32915, 'summerville': 32916, 'betrothal': 32917, 'reunify': 32918, 'vela': 32919, 'luis’s': 32920, 'abdicates': 32921, 'explosive…': 32922, 'scad': 32923, \"artists'\": 32924, \"heads'\": 32925, \"'nudists'\": 32926, \"'squelches'\": 32927, 'cheeck': 32928, 'retort': 32929, 'shroyer': 32930, 'safian': 32931, 'compile': 32932, 'disposing': 32933, 'bores': 32934, 'squelching': 32935, 'quarrelsome': 32936, 'recite': 32937, 'barkham': 32938, 'blackness': 32939, 'limbo': 32940, 'kerstin': 32941, \"gier's\": 32942, 'sapphire': 32943, 'chop': 32944, 'neet': 32945, 'bajpai’s': 32946, 'accentuated': 32947, 'deftness': 32948, 'dhoomketu': 32949, 'gyles': 32950, 'brandreth': 32951, 'meister': 32952, 'denizen': 32953, \"countdown's\": 32954, 'palindromes': 32955, 'pangrams': 32956, 'malaprops': 32957, 'anagrams': 32958, 'verbiage': 32959, 'verbarrhea': 32960, 'oddest': 32961, \"lennie's\": 32962, 'frindall': 32963, 'respecting': 32964, 'peeved': 32965, 'females': 32966, 'pleinsworth': 32967, 'tossed': 32968, 'speechless': 32969, \"quinn's\": 32970, 'tug': 32971, 'heartstrings': 32972, 'nymphs': 32973, 'riddhi': 32974, 'siddhi': 32975, 'buddhi': 32976, 'mandodari': 32977, 'damayanti’s': 32978, 'splendored': 32979, 'purana': 32980, 'tyrel': 32981, 'fe': 32982, 'world—plus': 32983, 'snorkeling': 32984, 'cousin’s': 32985, 'illegally': 32986, 'standardization': 32987, 'standardizing': 32988, 'readiness': 32989, 'deprecate': 32990, 'sprawl': 32991, 'pivotcharts': 32992, 'sunbursts': 32993, 'waterfalls': 32994, 'treemaps': 32995, 'powermap': 32996, 'suleiman': 32997, 'lookout': 32998, 'elsie': 32999, 'jawbone': 33000, 'farnese': 33001, 'spokesman': 33002, 'courtyard': 33003, 'tv’s': 33004, \"stewards'\": 33005, \"hughes'\": 33006, 'smithereens': 33007, \"ingenious'\": 33008, \"'hana\": 33009, 'haenyeo': 33010, 'chrysanthemum': 33011, 'geisha': 33012, '‘bots’': 33013, 'antifragile': 33014, 'thanthe': 33015, 'withstress': 33016, 'deanne': 33017, 'panday': 33018, 'shareswith': 33019, 'leadingpsychiatrists': 33020, 'cardiologists': 33021, 'endocrinologists': 33022, 'rajnikanth': 33023, 'buff': 33024, 'kadal': 33025, \"block's\": 33026, 'purposefully': 33027, 'sayyed': 33028, 'bozorg': 33029, \"'moody'\": 33030, 'squalor': 33031, 'chattels': 33032, 'sympathisers': 33033, \"khomeini's\": 33034, 'gogoi': 33035, 'shillong': 33036, 'rajveer': 33037, 'saini': 33038, \"barron's\": 33039, '978': 33040, '4380': 33041, '1179': 33042, '‘legacy': 33043, 'systems’': 33044, 'this―a': 33045, 'selfinterested': 33046, 'takes―moving': 33047, 'xv': 33048, 'avon': 33049, 'soubriquet': 33050, \"'satanas'\": 33051, 'plucks': 33052, 'urchin': 33053, 'comte': 33054, 'vire': 33055, 'splendours': 33056, 'leonie': 33057, 'conman': 33058, 'lott': 33059, 'conjuring': 33060, 'prestidigitators': 33061, 'houdin': 33062, 'professer': 33063, 'conjurers': 33064, 'cuffing': 33065, 'sleeving': 33066, 'concealments': 33067, \"hahne's\": 33068, \"capablanca's\": 33069, 'tartakower': 33070, \"tarrasch's\": 33071, 'thorold': 33072, \"chernev's\": 33073, 'capablanca': 33074, 'tarrasch': 33075, 'alekhine': 33076, 'lasker': 33077, 'maneuver': 33078, 'noting': 33079, 'knoll': 33080, 'fanelli': 33081, \"ani's\": 33082, 'lionsgate': 33083, \"'biting\": 33084, 'weisberger': 33085, 'conjurings': 33086, 'prestidigitation': 33087, 'untangled': 33088, 'rollaway': 33089, 'heavyset': 33090, 'sicily': 33091, 'salvatore': 33092, 'deceits': 33093, 'capo': 33094, 'capi': 33095, \"thoreau's\": 33096, 'humoristic': 33097, 'irrelevant': 33098, 'corresponds': 33099, 'mirroring': 33100, 'imperiously': 33101, 'masks': 33102, 'symphony': 33103, 'misha': 33104, 'awasthi': 33105, 'klapstein': 33106, 'kellycreates': 33107, 'exemplars': 33108, 'industry′s': 33109, 'glutes': 33110, 'kofer': 33111, 'unpaid': 33112, 'appalachia': 33113, 'lehman': 33114, 'fatter': 33115, 'meth': 33116, 'resampling': 33117, 'shrinkage': 33118, 'hastie': 33119, 'tibshirani': 33120, 'ethnocentric': 33121, 'spheres': 33122, 'excluding': 33123, 'truculent': 33124, \"harriet's\": 33125, 'vangers': 33126, 'persevere': 33127, 'treasuring': 33128, 'la’s': 33129, 'yashodhara': 33130, 'hitched': 33131, 'laidback': 33132, \"yashodhara's\": 33133, \"'tamper\": 33134, \"tentrums'\": 33135, \"vijay's\": 33136, 'zarreena': 33137, 'nutty': 33138, 'vivi': 33139, 'anoushka': 33140, 'peanut': 33141, 'divergent': 33142, 'sympathizing': 33143, 'wholeheartedly': 33144, \"filmmaker's\": 33145, 'ladykillers': 33146, 'mackendrick': 33147, 'impetus': 33148, 'handouts': 33149, 'everyones': 33150, 'kanav': 33151, 'snehanshu': 33152, 'distributors': 33153, 'crowdsourced': 33154, 'elie': 33155, 'wiesel': 33156, 'buchenwald': 33157, 'atrocity': 33158, 'weka': 33159, 'primers': 33160, 'airgunning': 33161, 'projectiles': 33162, 'plinking': 33163, 'zeroing': 33164, 'pernod': 33165, 'ballantyne': 33166, 'benbrook': 33167, 'stroll': 33168, \"penrod's\": 33169, 'addis': 33170, 'ababa': 33171, 'menelik': 33172, 'ay': 33173, '‘bigamy': 33174, 'monogamy': 33175, 'same’': 33176, 'daydreamer': 33177, 'isadora': 33178, 'ditching': 33179, 'criss': 33180, 'basque': 33181, 'acoca': 33182, 'megan': 33183, 'overpowering': 33184, 'rubio': 33185, 'arzano': 33186, 'graciela': 33187, 'ricardo': 33188, 'mellado': 33189, 'varushka': 33190, '‘settling': 33191, 'mitash': 33192, 'muir': 33193, 'octavia': 33194, 'chipko': 33195, 'greens': 33196, 'invigorated': 33197, 'infusions': 33198, 'bloc': 33199, 'prologue': 33200, '‘chase': 33201, \"murphy's\": 33202, \"sod's\": 33203, \"parkinson's\": 33204, 'murphology': 33205, 'exasparated': 33206, 'lech': 33207, 'condoms': 33208, 'rainstorm': 33209, 'midsummer': 33210, 'overgrown': 33211, 'entail': 33212, 'zenith': 33213, 'taxation': 33214, 'overleveraging': 33215, \"'stigmatized\": 33216, \"capitalism'\": 33217, 'wormwood': 33218, '‘lost’': 33219, 'lewis’s': 33220, 'ascending': 33221, '—b': 33222, 'inflammatory': 33223, 'bhimrao': 33224, 'strident': 33225, 'brahmanical': 33226, 'posits': 33227, 'ambedkar’s': 33228, '‘harijans’': 33229, 'streaking': 33230, 'sin’': 33231, 'halter': 33232, 'necks': 33233, 'shaved': 33234, 'mansarovar': 33235, 'achievement―a': 33236, 'bedi’s': 33237, 'duckling': 33238, '‘open’': 33239, 'son―a': 33240, 'pooja': 33241, 'sanctified': 33242, 'steamboats': 33243, 'magnolia': 33244, 'egotistical': 33245, 'upstart': 33246, \"casar's\": 33247, 'diree': 33248, 'absolutlifabulos': 33249, 'procure': 33250, 'evaluated': 33251, 'receipt': 33252, 'consignment': 33253, 'memo': 33254, 'debit': 33255, 'gr': 33256, 'gmat': 33257, \"easy'\": 33258, 'goyal': 33259, 'spelled': 33260, 'monotony': 33261, '686': 33262, 'wordland': 33263, 'metamorphosed': 33264, 'regularity': 33265, 'ullekh': 33266, \"ullekh's\": 33267, 'exemplar': 33268, 'fugue': 33269, \"hofstadter's\": 33270, 'substrate': 33271, 'neurons': 33272, 'gödel': 33273, 'escher': 33274, 'ever—witness': 33275, 'fingerprints—without': 33276, 'disguises': 33277, 'cocaine': 33278, 'irregulars': 33279, 'brawler': 33280, 'station—and': 33281, 'spattered': 33282, 'unattended': 33283, 'forever—and': 33284, 'government…and': 33285, 'cris': 33286, 'unendurable': 33287, 'elongated': 33288, 'accomplice': 33289, 'plotter': 33290, 'misato': 33291, 'extorting': 33292, 'tetsuya': 33293, 'evidently': 33294, 'disposed': 33295, 'kusanagi': 33296, 'speculating': 33297, 'abacus': 33298, 'harding': 33299, 'ecuadorian': 33300, \"'enjoyable\": 33301, 'feuding': 33302, 'statistician': 33303, 'snowed': 33304, \"lovers'\": 33305, 'watkyn': 33306, 'revd': 33307, 'confound': 33308, 'tetchily': 33309, 'chap': 33310, \"'it’s\": 33311, 'marseille': 33312, 'pandav': 33313, 'kaurav': 33314, 'perceptual': 33315, \"mumbai's\": 33316, 'worli': 33317, 'raaj': 33318, \"d'cunha\": 33319, 'unfaithful': 33320, 'nosedived': 33321, 'bhendi': 33322, \"com's\": 33323, \"'packed\": 33324, \"turner'\": 33325, 'kiewarra': 33326, 'slaughtering': 33327, \"'spellbinding'\": 33328, \"'stunningly\": 33329, 'johnsson': 33330, 'benedict': 33331, 'cumberbatch': 33332, 'pratt': 33333, 'saldana': 33334, \"'work'\": 33335, \"'thanks'\": 33336, \"'stop'\": 33337, \"'horses'\": 33338, 'yoram': 33339, 'explicable': 33340, 'varian': 33341, 'econ': 33342, 'humorously': 33343, 'chilly': 33344, '1792': 33345, \"'madame\": 33346, \"guillotine'\": 33347, \"'aristos'\": 33348, 'sparing': 33349, 'heathland': 33350, 'pimpernel': 33351, 'chauvelin': 33352, 'orczy': 33353, \"pimpernel'\": 33354, 'jalan’s': 33355, '1980–2000': 33356, '2000–15': 33357, 'states—rather': 33358, 'centre—responsible': 33359, 'parliament’s': 33360, 'orderly': 33361, 'ministries': 33362, 'rationalize': 33363, \"zoey's\": 33364, 'davydov': 33365, 'lash': 33366, 'adoptive': 33367, 'biopreparat': 33368, 'executes': 33369, 'fares': 33370, 'stapleton': 33371, 'pathologists': 33372, 'pneumonia': 33373, 'pressured': 33374, 'bioweapon': 33375, 'biotechnology': 33376, 'vidarbha': 33377, 'gangetic': 33378, 'historicizing': 33379, 'interrelationship': 33380, 'sociopolitical': 33381, 'saris': 33382, 'mnemonics–based': 33383, '92': 33384, 'associating': 33385, 'fill–in–the–blanks': 33386, 'cd–rom': 33387, 'closeness': 33388, \"lane's\": 33389, 'slices': 33390, 'searle': 33391, 'nozick': 33392, \"mind's\": 33393, 'speculations': 33394, 'triumphal': 33395, 'pell': 33396, 'mell': 33397, 'gekko': 33398, 'implausibly': 33399, 'brouhaha': 33400, \"'wickedly\": 33401, 'cascade': 33402, 'noughties': 33403, 'sublight': 33404, 'hyperspatial': 33405, 'takeshi': 33406, 'nicotine': 33407, 'world—and': 33408, 'feathers': 33409, 'wrens': 33410, 'flocking': 33411, 'starlings': 33412, 'bowerbirds': 33413, 'nutcrackers': 33414, 'albatrosses': 33415, 'mysteries—revealing': 33416, 'strycker': 33417, 'paharganj': 33418, 'hotel’s': 33419, 'tonya': 33420, 'kulpreet': 33421, 'yadav’s': 33422, '‘yadav': 33423, 'anjum': 33424, 'kitaab': 33425, 'cristina': 33426, 'layers—but': 33427, 'blasted': 33428, 'waterless': 33429, 'apaches': 33430, 'conceives': 33431, 'kunders': 33432, 'sars': 33433, 'cath': 33434, 'telemedicine': 33435, \"boyd's\": 33436, 'moncur': 33437, 'clergyman': 33438, 'soprano': 33439, \"brodie's\": 33440, 'alternatively': 33441, 'lavette': 33442, 'fisherman': 33443, 'commence': 33444, 'lucknawi': 33445, 'awadh': 33446, 'dressmaking': 33447, 'sewing': 33448, 'promiscuous': 33449, 'captivates': 33450, 'comparable': 33451, 'turow’s': 33452, '”—associated': 33453, 'son—shy': 33454, 'jacob’s': 33455, 'damning': 33456, 'neighbors’': 33457, '“ingenious': 33458, '“stunning': 33459, '“gripping': 33460, 'bedcovers': 33461, '“even': 33462, 'interlock': 33463, 'zipper': 33464, '“yes': 33465, 'thesauruses': 33466, 'advertiser': 33467, '1852': 33468, \"'centre\": 33469, 'sebag': 33470, \"jerusalem's\": 33471, 'empresses': 33472, 'whores': 33473, 'fles': 33474, 'ipc': 33475, 'rudiments': 33476, 'interrupt': 33477, 'smp': 33478, \"kernel's\": 33479, 'lieutenants': 33480, 'army–rather': 33481, \"'officer'\": 33482, \"'sahab'\": 33483, 'anju': 33484, \"'lady'\": 33485, 'mosher': 33486, 'floes': 33487, 'westfall': 33488, 'adarlan': 33489, 'torre': 33490, 'cesme': 33491, 'aelin': 33492, 'nesryn': 33493, 'erilea': 33494, 'papercrafters': 33495, 'pinwheel': 33496, 'unintimidating': 33497, 'cicada': 33498, 'incontinence': 33499, 'interact—the': 33500, 'together—interlocked': 33501, 'connectedness': 33502, 'comic’s': 33503, 'sonic’s': 33504, 'mobotropolis': 33505, \"dominion's\": 33506, 'powerbase': 33507, \"brazil's\": 33508, 'uttered': 33509, 'fortieth': 33510, 'orations': 33511, \"leader's\": 33512, 'headnotes': 33513, 'xampp': 33514, 'mysqli': 33515, 'css2': 33516, 'geolocation': 33517, 'bonsai': 33518, 'gopalakrishnan': 33519, '‘gopal': 33520, 'imbibing': 33521, 'intuition—increasingly': 33522, 'differentiator': 33523, 'circles—to': 33524, '’—ratan': 33525, 'casper': 33526, 'next…': 33527, 'liquids': 33528, 'vamped': 33529, 'informational': 33530, 'periodicals': 33531, 'footnoted': 33532, 'sinha’s': 33533, 'mobster': 33534, 'cosa': 33535, 'nostra': 33536, \"60's\": 33537, 'familial': 33538, 'characterisations': 33539, 'transpire': 33540, 'regains': 33541, \"'academy\": 33542, \"award'\": 33543, \"screenplay'\": 33544, 'instalments': 33545, 'mugging': 33546, 'claire’s': 33547, 'arnp': 33548, 'medically': 33549, 'altons': 33550, 'happen—that': 33551, 'locomotion': 33552, '“remarkable': 33553, 'orphan’s': 33554, 'harsh—and': 33555, 'short—in': 33556, 'camorr': 33557, 'dodges': 33558, 'fingered': 33559, 'underworld’s': 33560, 'game—or': 33561, '”—george': 33562, 'audacity': 33563, 'endear': 33564, 'silverberg': 33565, 'liss': 33566, '“high': 33567, '”—richard': 33568, 'bookchapter': 33569, 'systemsstorage': 33570, 'solutionscloud': 33571, 'storageentry': 33572, 'solutionmid': 33573, 'solutionenterprise': 33574, 'solutionhyper': 33575, 'convergedsummarycase': 33576, 'infrastructurelearning': 33577, 'objectivesintroductionhardware': 33578, 'stackphysical': 33579, 'arraystorage': 33580, 'componentssoftware': 33581, 'storagesecuring': 33582, 'infrastructuremanaging': 33583, 'infrastructuresummarycase': 33584, 'arraylearning': 33585, 'objectivesintroductioncontroller': 33586, 'modescaching': 33587, 'techniquesraid': 33588, 'conceptvirtual': 33589, 'raidstorage': 33590, 'virtualizationsummarycase': 33591, 'protocolslearning': 33592, 'objectivesintroductionscsi': 33593, 'protocolfibre': 33594, 'basicsiscsi': 33595, 'overviewfcoe': 33596, 'overviewfile': 33597, 'protocolssummarycase': 33598, 'studieschapter': 33599, 'networkinglearning': 33600, 'objectivesintroductionsan': 33601, 'topologyzoninglun': 33602, 'maskingmultipathsummarycase': 33603, 'performancelearning': 33604, 'objectivesintroductionperformance': 33605, 'terminologiesworkload': 33606, 'patterntiered': 33607, 'storagequality': 33608, 'xcopy': 33609, 'summarycase': 33610, 'replicationbackup': 33611, 'novel–the': 33612, 'obscenity': 33613, 'camus’s': 33614, 'stranger–is': 33615, 'indemnity–which': 33616, 'breath–is': 33617, 'tersely': 33618, 'shiftless': 33619, 'unreasoned': 33620, 'stories–“pastorale': 33621, 'icebox': 33622, '“dead': 33623, '“brush': 33624, 'storm”–that': 33625, 'leninism': 33626, \"'immensely\": 33627, 'lieven': 33628, \"individual'\": 33629, \"'lenin's\": 33630, 'shukman': 33631, 'rink': 33632, 'khadoos': 33633, 'cloning': 33634, \"park's\": 33635, 'grossing': 33636, 'hofmekler’s': 33637, 'readership–competitive': 33638, 'nutritionists': 33639, 'dieters': 33640, 'health–the': 33641, 'anabolic': 33642, 'reengineer': 33643, 'gainer': 33644, 'plateaus–in': 33645, 'metabolic': 33646, 'catchwords': 33647, 'lightup': 33648, 'descriptionif': 33649, 'bookmarking': 33650, 'redis': 33651, 'celery': 33652, 'pluggable': 33653, 'sujata': 33654, \"massey's\": 33655, \"hilda's\": 33656, 'ladylike': 33657, \"ayah's\": 33658, 'caregiver': 33659, 'lewes': 33660, 'shazia': 33661, 'shania': 33662, 'ajit': 33663, 'cotemporary': 33664, 'mystery’': 33665, '‘clalamity': 33666, 'stikes’': 33667, 'neighbor’s': 33668, 'imperfect’': 33669, 'pmo': 33670, '2006—part': 33671, 'balochi': 33672, 'enemies—mullah': 33673, 'isi—while': 33674, 'amiens': 33675, 'chopping': 33676, 'olduvai': 33677, \"'public\": 33678, \"decades'\": 33679, 'clifford': 33680, \"delights'\": 33681, 'thermodynamics': 33682, 'lubrication': 33683, 'acoustics': 33684, 'bioengineering': 33685, 'aeronautical': 33686, \"dba's\": 33687, '1090': 33688, '254': 33689, '316': 33690, 'guineas': 33691, 'casterbridge': 33692, 'dorsetshire': 33693, 'jamilla': 33694, 'knifed': 33695, \"sampson's\": 33696, 'daubed': 33697, 'emissary': 33698, 'bhrigukachchh': 33699, 'patan': 33700, 'chieftain': 33701, \"kaak's\": 33702, 'leeladevi': 33703, 'ranakdevi': 33704, \"munshi's\": 33705, 'greats—michael': 33706, 'opponent’s': 33707, 'clients—“don’t': 33708, 'think”—and': 33709, 'psyches': 33710, 'contempory': 33711, 'retested': 33712, 'unofficially': 33713, 'cautions': 33714, 'book—an': 33715, 'cking': 33716, 'eugene': 33717, 'foolishness': 33718, 'ahem': 33719, 'fashionista': 33720, 'perpetually': 33721, 'cked': 33722, \"word's\": 33723, 'enroll': 33724, 'youære': 33725, 'synchronizing': 33726, 'vioce': 33727, 'chivalrous': 33728, 'reverts': 33729, 'delancey': 33730, 'vitality—in': 33731, 'extramarital': 33732, 'pained': 33733, 'pitiful': 33734, 'blaring': 33735, 'originating': 33736, 'denaud': 33737, 'parasurama': 33738, 'propitious': 33739, 'jealously': 33740, '1793': 33741, 'gurukkal': 33742, 'balachandrian': 33743, 'trance': 33744, 'makeit': 33745, 'akhtar': 33746, 'clocking': 33747, 'tussles': 33748, \"shoaib's\": 33749, 'idealówhether': 33750, 'stoyan': 33751, 'stefanovósenior': 33752, 'yslow': 33753, 'toolójavascript': 33754, 'globals': 33755, 'var': 33756, 'italy—to': 33757, 'medici': 33758, 'boldari': 33759, 'mugging—and': 33760, 'namesake': 33761, 'herself—and': 33762, 'passion—the': 33763, 'te': 33764, '90th': 33765, \"milne's\": 33766, 'eeyore': 33767, 'frets': 33768, 'calculates': 33769, 'pontificates': 33770, \"weren't\": 33771, 'sociologist': 33772, 'klinenberg': 33773, \"ansari's\": 33774, '9781405331630': 33775, \"'ridiculous\": 33776, 'moxham': 33777, 'remnant': 33778, \"victoria's\": 33779, 'patrolled': 33780, 'yara': 33781, 'shahidi': 33782, 'melton': 33783, 'yoon': 33784, 'store―for': 33785, 'wonderfulness—just': 33786, 'forgotten—and': 33787, 'personnell': 33788, 'ruggedness': 33789, 'metacharacters': 33790, \"ware's\": 33791, \"riveting'\": 33792, 'aj': 33793, \"hal's\": 33794, \"stranger's\": 33795, 'scripter': 33796, 'saas': 33797, 'vernita': 33798, \"'it'\": 33799, 'goading': 33800, '1506': 33801, 'fernao': 33802, 'requested': 33803, 'azzam': 33804, '“of': 33805, 'mycroft': 33806, \"botswana's\": 33807, 'maids': 33808, \"ramotswe's\": 33809, \"mma's\": 33810, \"guardian's\": 33811, \"'fiction\": 33812, 'succulent': 33813, 'staple': 33814, 'chilies': 33815, 'assado': 33816, 'peppercorns': 33817, 'fermented': 33818, 'bol': 33819, 'semolina': 33820, 'offset': 33821, 'coastland': 33822, 'bebinca': 33823, 'mutton': 33824, 'xacuti': 33825, 'patties': 33826, 'balchao': 33827, 'sorpotel': 33828, 'vindaloo': 33829, 'wounding': 33830, 'marring': 33831, 'gisbourne': 33832, 'nottingham': 33833, 'polynomials': 33834, 'algebra’s': 33835, 'understand—while': 33836, 'relaying': 33837, 'format—the': 33838, 'harmonizes': 33839, 'ponds': 33840, 'dos': 33841, \"don'ts\": 33842, 'landscaping': 33843, 'renovate': 33844, 'chicago’s': 33845, 'feehan’s': 33846, 'impossible—let': 33847, 'needs—and': 33848, 'ferraro—or': 33849, 'billionaire’s': 33850, '“shadow': 33851, 'steamy': 33852, '”—fresh': 33853, 'fairley': 33854, 'shruti': 33855, 'mishap': 33856, 'shruthi': 33857, \"shruthi's\": 33858, 'subramaniam': 33859, 'thicken': 33860, 'bansenshukai': 33861, \"thieves'\": 33862, 'finery': 33863, \"milly's\": 33864, 'qing': 33865, 'vaporising': 33866, 'wrinkled': 33867, 'wiggly': 33868, 'uneven': 33869, 'euclidean': 33870, 'fractal': 33871, 'ferns': 33872, 'archibald': 33873, 'bohr': 33874, \"hailey's\": 33875, 'lung': 33876, 'invalid': 33877, 'ruckus': 33878, 'pianistic': 33879, 'orchestrations': 33880, 'idolized': 33881, 'reaped': 33882, \"'offending'\": 33883, 'coordinate': 33884, 'omni': 33885, 'infitonic': 33886, 'abrasive': 33887, 'grimoire': 33888, 'brother’': 33889, 'mukherjee’s': 33890, 'diverged': 33891, 'them—for': 33892, 'bauld': 33893, 'wrong—and': 33894, 'baum': 33895, 'platitudes': 33896, 'topics—the': 33897, 'from—fresh': 33898, 'puppy': 33899, 'pageant': 33900, '“through': 33901, '“hello': 33902, 'buttons’s': 33903, 'solidly': 33904, 'mast': 33905, 'protruded': 33906, 'bermuda': 33907, 'guiltiest': 33908, 'polymer': 33909, 'graphene': 33910, 'polymers': 33911, 'defects': 33912, 'narromine': 33913, \"'pigeon'\": 33914, 'baggy': 33915, 'perth': 33916, '563': 33917, \"mcgrath's\": 33918, 'funder': 33919, 'sufferers': 33920, 'mcgraths': 33921, 'rubbed': 33922, 'foiled': 33923, \"sei's\": 33924, 'systems•': 33925, 'studies•': 33926, 'competencies': 33927, 'prioritize': 33928, 'progress•': 33929, 'sei': 33930, 'industrytable': 33931, 'prefacechapter': 33932, 'softwarechapter': 33933, 'analysis—identifying': 33934, 'needschapter': 33935, 'modelschapter': 33936, 'competencieschapter': 33937, 'analysischapter': 33938, 'metricschapter': 33939, 'engineeringchapter': 33940, 'performancereferencesbibliographyappendix': 33941, 'wea': 33942, 'threadsappendix': 33943, 'mswa': 33944, 'addedappendix': 33945, 'projectappendix': 33946, 'designationsappendix': 33947, 'swa': 33948, 'mappingsappendix': 33949, 'bsimm': 33950, 'reportappendix': 33951, 'principlesindex': 33952, 'citywide': 33953, '156': 33954, '847': 33955, 'radicalization': 33956, '‘traditional': 33957, 'submissiveness’': 33958, 'demonstrably': 33959, 'politicized': 33960, 'hijab': 33961, 'stereotyping': 33962, 'misogyny': 33963, 'monolith': 33964, \"weiss's\": 33965, \"lives'\": 33966, 'pickering': 33967, 'rochelle': 33968, \"'eve\": 33969, \"hell'\": 33970, \"'j\": 33971, \"pleasures'\": 33972, \"sleuth'\": 33973, 'longform': 33974, \"'longform'\": 33975, 'dystopias': 33976, 'dreamscapes': 33977, 'moorthy': 33978, 'barroux': 33979, 'shyam': 33980, 'mistress’s': 33981, 'richardson’s': 33982, 'jewkes': 33983, 'doody’s': 33984, \"hindi'\": 33985, 'vowel': 33986, 'dimon': 33987, 'diaphragm': 33988, 'suspensory': 33989, 'palate': 33990, 'conditionally': 33991, 'excavating': 33992, 'hun': 33993, \"tombs'riches\": 33994, 'pottery': 33995, 'bagh': 33996, 'embodying': 33997, \"yates's\": 33998, \"anne's\": 33999, \"rory's\": 34000, \"who've\": 34001, 'shuxiu': 34002, 'shifang': 34003, 'ting': 34004, 'tian': 34005, 'qingeng': 34006, \"'bare\": 34007, \"branches'\": 34008, 'fong': 34009, 'concocts': 34010, 'dreamworld': 34011, 'kiersten': 34012, 'lotto': 34013, 'where\\x92s': 34014, 'insufferable': 34015, 'harrington': 34016, 'something\\x92s': 34017, '“austrian': 34018, 'oak”': 34019, 'aspired': 34020, 'brutal…': 34021, 'herculean': 34022, 'me—my': 34023, 'clicked': 34024, 'secrets—demonstrating': 34025, 'groups—each': 34026, 'peterrific': 34027, 'kann': 34028, 'goldilicious': 34029, 'pinkville': 34030, 'salwa': 34031, 'judum': 34032, 'excellently': 34033, 'proportioned': 34034, 'body—for': 34035, 'excercise': 34036, 'result—total': 34037, \"modernity's\": 34038, 'beckert': 34039, \"ironies'\": 34040, \"'shocking\": 34041, 'olusoga': 34042, 'sweeteners': 34043, \"walvin's\": 34044, \"mintz's\": 34045, 'duggery': 34046, 'reassert': 34047, 'disseminate': 34048, '“third': 34049, 'manafort': 34050, 'drawback': 34051, 'buccaneering': 34052, \"carr's\": 34053, \"cox's\": 34054, 'fms': 34055, 'instructively': 34056, 'unvanquished': 34057, 'swooping': 34058, \"vivre'\": 34059, 'transcription': 34060, 'bystander': 34061, 'dickensian': 34062, \"cast's\": 34063, 'poetess': 34064, 'schoenfeld': 34065, 'nsca': 34066, 'descriptionrecent': 34067, 'dqn': 34068, 'forsome': 34069, 'henri': 34070, 'charriere': 34071, 'guiana': 34072, '‘client’': 34073, '1948–89': 34074, '‘create': 34075, 'disorder’': 34076, 'inversely': 34077, '1989–90': 34078, 'quoist': 34079, 'kashmir’s': 34080, 'proclivities': 34081, 'downstream': 34082, 'detonated': 34083, 'marais': 34084, 'nom': 34085, 'guerre': 34086, 'papercutz': 34087, 'stiltons': 34088, 'ratford': 34089, 'pollute': 34090, \"wooden's\": 34091, 'winningest': 34092, 'bruins': 34093, 'ncaa': 34094, 'famer': 34095, 'americana': 34096, 'knute': 34097, 'rockne': 34098, 'orville': 34099, 'commend': 34100, 'lucinda': 34101, 'riley': 34102, 'chavan': 34103, 'heyday': 34104, \"princess's\": 34105, 'dartmoor': 34106, 'ari': 34107, \"anahita's\": 34108, 'pandava': 34109, \"indra's\": 34110, 'wiles': 34111, 'gandiva': 34112, 'evangelism': 34113, 'toyed': 34114, 'disparages': 34115, 'instigation': 34116, 'withholds': 34117, 'revaluates': 34118, \"christie's\": 34119, 'mustache': 34120, 'twirling': 34121, 'fascinatingbehaviors': 34122, 'poisonings': 34123, 'thefts—all': 34124, 'typicalskill': 34125, \"poirot's\": 34126, 'torquay': 34127, 'mousetrap': 34128, 'fosberg': 34129, \"hitman's\": 34130, 'piling': 34131, \"pilgrim's\": 34132, 'mourning': 34133, \"emily's\": 34134, 'karas': 34135, 'parsifal': 34136, \"'queer\": 34137, \"gender'\": 34138, 'scheele': 34139, 'lgbtq': 34140, 'exclusion': 34141, 'sexology': 34142, 'disputed': 34143, \"normal'\": 34144, \"kinsey's\": 34145, \"butler's\": 34146, 'gendered': 34147, 'desiring': 34148, 'songs—some': 34149, 'subcontinent’s': 34150, 'lyricists': 34151, 'distinctive—especially': 34152, 'songs—from': 34153, '‘mora': 34154, 'le’': 34155, 'dhoondta': 34156, 'hai’': 34157, 'mausam': 34158, '‘jiya': 34159, 'jale’': 34160, 'toh': 34161, 'bachcha': 34162, 'ji’': 34163, 'ishqiya': 34164, 'shailendra': 34165, 'lata': 34166, 'mangeshkar': 34167, 'rafi': 34168, 'bhosle': 34169, 'vani': 34170, 'jagjit': 34171, 'bhupinder': 34172, 'analysis—and': 34173, 'translation—this': 34174, 'rigors': 34175, 'kodak': 34176, 'rectal': 34177, 'peed': 34178, 'stinky': 34179, 'sugarcoat': 34180, 'affixed': 34181, 'upscale': 34182, 'reprints': 34183, \"sonic's\": 34184, 'cobal': 34185, 'knothole': 34186, \"assets'\": 34187, \"biffen's\": 34188, 'proprietor': 34189, \"conqueror'\": 34190, \"lightning'\": 34191, \"weather'\": 34192, 'henchman': 34193, 'postscript': 34194, 'pickings': 34195, 'popova': 34196, 'wordlessly': 34197, 'surfed': 34198, 'kickstarter': 34199, 'paralyzes': 34200, 'alcoholics': 34201, 'pimples': 34202, \"'misunderstood\": 34203, \"intellectual'\": 34204, \"adrian's\": 34205, 'howled': 34206, \"indeed'\": 34207, \"meaningful'\": 34208, \"tilman's\": 34209, \"herzog's\": 34210, '000m': 34211, 'menluntse': 34212, 'kunzang': 34213, 'sashindran': 34214, 'nilkanth': 34215, 'scuttle': 34216, 'colas': 34217, 'icts': 34218, \"'environmental\": 34219, \"manifesto'\": 34220, 'exigencies': 34221, 'bloodlands': 34222, 'polads': 34223, 'bloodsoaked': 34224, 'bok': 34225, 'talwar': 34226, 'aarushi’s': 34227, 'pending': 34228, 'arushi’s': 34229, 'avirook': 34230, 'carroll’s': 34231, 'alice’s': 34232, '“mad”': 34233, 'tales—filled': 34234, 'imagination—balance': 34235, 'continues—twice': 34236, 'pettijohn': 34237, 'whitridge': 34238, 'baynton': 34239, 'ducal': 34240, \"'homo\": 34241, 'before’': 34242, 'pauper': 34243, 'beggar': 34244, 'thunberg': 34245, '“mr': 34246, 'trumbo': 34247, 'mattered—not': 34248, '”—washington': 34249, 'agitating': 34250, 'wasteful': 34251, '”—boston': 34252, '”—chicago': 34253, 'skulls': 34254, 'thesa': 34255, 'jadir': 34256, 'angiers': 34257, 'miln': 34258, 'krasians': 34259, 'duchy': 34260, 'lakton': 34261, 'stronger—and': 34262, 'dickie': 34263, 'blofeld': 34264, 'trueman': 34265, 'flintoff': 34266, 'gough': 34267, 'denis': 34268, 'compton': 34269, 'merv': 34270, 'sledging': 34271, 'streaker': 34272, '“12': 34273, 'nagavara': 34274, 'ramarao': 34275, 'nrnm': 34276, 'ethically': 34277, 'fees': 34278, 'meritocracy': 34279, 'checkers': 34280, 'medicines': 34281, \"words'\": 34282, 'bma': 34283, 'surgeon’s': 34284, 'retractors': 34285, 'porcelain': 34286, 'oppressors': 34287, 'ollydbg': 34288, 'windbg': 34289, 'obfuscation': 34290, 'disassembly': 34291, 'unpacking': 34292, 'packers': 34293, 'excreta': 34294, 'piled': 34295, \"scavenger's\": 34296, 'baskets': 34297, 'adrishya': 34298, 'favorable': 34299, 'sukhdeo': 34300, 'tempests': 34301, 'bared': 34302, 'photography—whether': 34303, 'camera—as': 34304, 'bravado': 34305, \"tokyo's\": 34306, 'jingu': 34307, 'gaien': 34308, 'offense': 34309, 'theorem': 34310, 'more—all': 34311, 'poisson': 34312, 'throated': 34313, 'hummingbird': 34314, 'parakeet': 34315, 'songbirds': 34316, 'waterfowl': 34317, 'resue': 34318, \"chieftain's\": 34319, 'shielf': 34320, 'dockery': 34321, 'wallpaper': 34322, \"emmy's\": 34323, 'bookman': 34324, 'kidnappings': 34325, '2044': 34326, 'halliday': 34327, \"cline's\": 34328, '‘wildly': 34329, '‘part': 34330, 'heart’': 34331, '‘ernest': 34332, 'cline’s': 34333, 'classic’': 34334, 'scifinow': 34335, '‘gorgeously': 34336, \"'geronimo\": 34337, 'charms’': 34338, 'spunky': 34339, 'lusts': 34340, 'magnified': 34341, 'edizioni': 34342, 'piemme': 34343, 'elisabetta': 34344, 'dami': 34345, 'binges': 34346, \"'master\": 34347, 'osi': 34348, 'igrp': 34349, 'bgp': 34350, 'screened': 34351, 'subtopics': 34352, 'l2f': 34353, 'pptp': 34354, 'tunneling': 34355, 'candidate’s': 34356, 'netcat': 34357, 'testnet': 34358, 'bwmeter': 34359, \"ashoka's\": 34360, 'ashokan': 34361, 'orientalists': 34362, 'antiquarians': 34363, 'epigraphists': 34364, \"orphan's\": 34365, \"kommandant's\": 34366, 'bau': 34367, 'moldering': 34368, 'krysia': 34369, 'lipowski': 34370, 'gentile': 34371, 'richwalder': 34372, 'pam': 34373, 'jenoff': 34374, 'ww2’': 34375, '‘must': 34376, 'pudding…': 34377, 'chest…': 34378, 'quarrel': 34379, 'habits…': 34380, 'walli': 34381, 'jakes': 34382, 'verena': 34383, 'tania': 34384, 'dvorkin': 34385, 'dimka': 34386, 'expanses': 34387, 'englishwale': 34388, \"'practice\": 34389, \"'test\": 34390, 'causative': 34391, \"cd's\": 34392, 'ny': 34393, \"brandon's\": 34394, 'tenfold': 34395, 'astonish': 34396, 'nonphysical': 34397, 'theologists': 34398, 'straddles': 34399, 'standpoint': 34400, \"soul's\": 34401, 'simlas': 34402, 'cecil': 34403, 'clarkes': 34404, 'girdles': 34405, 'setter': 34406, 'bachi': 34407, 'karkaria': 34408, 'wilkes': 34409, 'splinted': 34410, 'mangled': 34411, \"paul's\": 34412, 'harvester': 34413, 'resale': 34414, 'enticements': 34415, 'uncultivated': 34416, 'unfocused': 34417, 'tenor': 34418, \"tv's\": 34419, 'fda': 34420, 'blocking': 34421, 'manav': 34422, 'byproduct': 34423, 'allay': 34424, 'conspires': 34425, 'ii’': 34426, 'rangnekar': 34427, 'jermaine': 34428, '‘answer’': 34429, 'dejection': 34430, 'normative': 34431, '‘straight’': 34432, \"'up\": 34433, 'reality—not': 34434, 'edibles': 34435, 'cottages': 34436, 'burglaries': 34437, 'life—why': 34438, '—as': 34439, 'enclave': 34440, 'adilshahi': 34441, 'sultanate': 34442, '1674': 34443, 'raigad': 34444, 'circumnavigate': 34445, 'tuvana': 34446, 'pastine': 34447, 'humberstone': 34448, 'jazmeen': 34449, 'minster': 34450, 'matron': 34451, 'rickie': 34452, 'khosla': 34453, 'mcraven': 34454, '“primer”': 34455, 'clr5': 34456, 'sms': 34457, 'smtp': 34458, '357': 34459, 'helikaon': 34460, 'dardania': 34461, 'andromache': 34462, 'argurios': 34463, 'disturbance': 34464, 'inflationary': 34465, 'recessionary': 34466, \"phillips's\": 34467, 'whittled': 34468, 'marinelli': 34469, 'ballerina': 34470, 'ilyushin': 34471, 'zverev—the': 34472, 'saunters': 34473, 'rekindled…': 34474, \"roman's\": 34475, \"wildness—there's\": 34476, \"santino's\": 34477, 'skater': 34478, 'nardozzi': 34479, 'uncharacteristic': 34480, 'mark…': 34481, \"javier's\": 34482, 'thawing': 34483, 'heats': 34484, 'sweeter': 34485, 'cindi': 34486, 'myers': 34487, 'price…': 34488, 'renfro': 34489, 'westfield…or': 34490, \"agent's\": 34491, 'faces—and': 34492, 'morgan—creates': 34493, \"morgan's\": 34494, \"criminal's\": 34495, 'innocent—and': 34496, \"'morrison's\": 34497, \"gratification'\": 34498, 'kenner': 34499, \"arizona's\": 34500, 'mojave': 34501, 'ararat': 34502, 'ragas': 34503, 'backgrounders': 34504, 'dhrupad': 34505, 'khayal': 34506, 'thumree': 34507, 'tappa': 34508, 'veena': 34509, 'sitar': 34510, 'surbahar': 34511, 'sarod': 34512, 'sarangi': 34513, 'shehnai': 34514, 'santoor': 34515, 'italics': 34516, 'musicological': 34517, 'valium': 34518, 'knickers': 34519, 'forked': 34520, 'cloisters': 34521, \"dublin's\": 34522, 'jacuzzis': 34523, 'gymnasiums': 34524, 'tepid': 34525, 'dearth': 34526, \"rock'n'roll\": 34527, 'stratifications': 34528, 'deliberates': 34529, 'demographer': 34530, 'versa': 34531, 'murty’s': 34532, \"'dollar\": 34533, 'sose’': 34534, 'yashashvi': 34535, 'tumula': 34536, 'paridihi': 34537, 'lokadalli': 34538, 'voltage': 34539, 'keypad': 34540, 'motorized': 34541, 'tashi': 34542, 'prerequisites': 34543, 'undefined': 34544, 'encompassed': 34545, 'panchayati': 34546, '73rd': 34547, 'amendment': 34548, 'mandated': 34549, \"'liberalization'\": 34550, 'decentralization': 34551, 'contextualizing': 34552, 'aranthur': 34553, 'slippery': 34554, \"ages'\": 34555, 'dantdm': 34556, 'trayaurus': 34557, '“friend”': 34558, 'zombified': 34559, 'evilest': 34560, 'all—evil': 34561, 'golem': 34562, 'dimwitted': 34563, 'regrettable': 34564, 'professional’s': 34565, 'bandage': 34566, 'genie': 34567, \"karachi's\": 34568, 'languishing': 34569, 'portsmouth': 34570, 'suntanned': 34571, 'pulleys': 34572, 'dumbbells': 34573, 'kettlebells': 34574, 'mitchell’s': 34575, '‘best': 34576, 'ages’': 34577, '‘5': 34578, 'up’': 34579, 'end’': 34580, '‘incredibly': 34581, 'unputdownable’': 34582, '‘very': 34583, 'suspenseful’': 34584, '‘had': 34585, 'finish’': 34586, 'grenades': 34587, 'cordons': 34588, 'seacoast': 34589, 'trawler': 34590, 'dinghy': 34591, 'disembark': 34592, 'machhimar': 34593, 'colaba': 34594, \"'maximum\": 34595, 'fidayeen': 34596, 'unnithan': 34597, 'stl': 34598, 'weightbiased': 34599, 'symmetric': 34600, 'min–max': 34601, 'splay': 34602, 'suffix': 34603, 'red–black': 34604, 'trie': 34605, '‘days': 34606, 'cricket’': 34607, 'diligent': 34608, '137': 34609, \"kapil's\": 34610, 'lifeblood': 34611, 'worrell': 34612, 'jessop': 34613, 'durani': 34614, 'chinaman': 34615, 'handers': 34616, 'hander’s': 34617, 'prasanna': 34618, 'laker': 34619, 'rubbing': 34620, 'venkataraghavan': 34621, 'umpire': 34622, 'acknowledgments': 34623, 'ballard': 34624, 'tenths': 34625, 'gramme': 34626, 'mescalin': 34627, 'creases': 34628, 'arose—men': 34629, 'combat—and': 34630, 'ka—the': 34631, 'dear—a': 34632, 'evil—from': 34633, '“thoroughly': 34634, 'addicting': 34635, '”—bookreporter': 34636, '“rewarding': 34637, '”—sffworld': 34638, 'thw': 34639, 'socialisation': 34640, 'thestate': 34641, 'marxian': 34642, 'laski': 34643, 'world—from': 34644, 'basu': 34645, 'nonplussed': 34646, 'leif': 34647, 'babin’s': 34648, 'ramadi': 34649, '“all': 34650, 'three’s': 34651, 'bruiser': 34652, 'leadership—at': 34653, 'level—is': 34654, 'departing': 34655, 'lemuel': 34656, 'lilliput': 34657, 'centimetres': 34658, 'brobdingnag': 34659, 'satirist': 34660, \"gulliver's\": 34661, 'trap—only': 34662, 'madres': 34663, 'boy’s': 34664, 'mobilizing': 34665, 'misjudging': 34666, 'courtyards': 34667, 'prey…': 34668, 'lina': 34669, 'bauer': 34670, 'terrorising': 34671, 'control…': 34672, 'billionaire…': 34673, 'kavakos': 34674, 'month’s': 34675, 'island…': 34676, 'keely': 34677, 'ariston’s': 34678, 'smouldering': 34679, 'weakens': 34680, 'bride…': 34681, 'polyester': 34682, 'chhabria': 34683, 'bhai': 34684, 'hese': 34685, 'walchand': 34686, 'hirachand': 34687, '9c': 34688, '‘–s’': 34689, 'reflexive': 34690, 'peculiarities': 34691, 'telephonic': 34692, 'exclamations': 34693, 'participles': 34694, 'riccardo': 34695, 'zacconi': 34696, \"app'\": 34697, 'acton': 34698, 'gran': 34699, 'torino': 34700, 'hosu': 34701, 'society—and': 34702, 'descriptiontensorflow': 34703, 'tensorforest': 34704, 'exoplanets': 34705, 'gaussian': 34706, 'trending': 34707, 'gpu': 34708, 'fortensorflow': 34709, 'issuses': 34710, 'standars': 34711, 'protectors': 34712, 'pavilions': 34713, 'martyn': 34714, 'juli': 34715, 'capped': 34716, 'kaye': 34717, 'kabbalah': 34718, 'buddhas': 34719, 'madonnas': 34720, 'beneficent': 34721, 'databank': 34722, '“lean': 34723, 'hitchhiking': 34724, 'scrounging': 34725, 'dumpsters': 34726, 'leftover': 34727, 'bagels': 34728, 'ids': 34729, 'school—a': 34730, '—los': 34731, '“amoruso': 34732, '—vanity': 34733, '—lena': 34734, 'sysadmins': 34735, 'manually': 34736, 'topologies': 34737, 'jinja': 34738, 'templating': 34739, 'stackstorm': 34740, '696': 34741, 'homeroom': 34742, 'putdowns': 34743, '¬and': 34744, 'makeover': 34745, 'niharika’s': 34746, 'tcp': 34747, 'enumerated': 34748, 'throughs': 34749, 'nic': 34750, 'arp': 34751, 'icmp': 34752, 'starman': 34753, 'giffen': 34754, 'tan': 34755, 'cheerleading': 34756, 'mccann': 34757, 'herewith': 34758, 'equips': 34759, 'prosthetics': 34760, 'dororo': 34761, 'testicular': 34762, 'uci': 34763, 'unblocking': 34764, 'uae': 34765, 'blockage': 34766, 'hampering': 34767, 'advisable': 34768, 'contravene': 34769, 'ankit': 34770, 'fadia': 34771, 'univibes': 34772, 'trinitv': 34773, 'santana': 34774, 'knopfler': 34775, 'hetfield': 34776, 'hammett': 34777, 'metallica': 34778, '‘exciting': 34779, 'along’': 34780, '‘tremendous': 34781, 'writer’': 34782, '‘horowitz': 34783, 'sequences’': 34784, 'joyously': 34785, 'book…you': 34786, 'slater': 34787, 'jeanette': 34788, 'winterson': 34789, 'lawson’s': 34790, 'fussy': 34791, 'convivial': 34792, 'savoured': 34793, 'bed’': 34794, 'yotam': 34795, 'ottolenghi': 34796, 'ibpa': 34797, 'verne’s': 34798, 'bandyopadhyay': 34799, 'fiction—looked': 34800, 'frivolous': 34801, 'ventures—he': 34802, 'chesterton’s': 34803, 'fifties': 34804, 'sleuth’s': 34805, 'chiriakhana': 34806, 'golap': 34807, 'porcupine': 34808, 'sreejata': 34809, 'guha’s': 34810, \"'pearson\": 34811, \"exam'\": 34812, '‘clever': 34813, 'child’': 34814, '‘tense': 34815, 'reads’': 34816, 'snappy': 34817, 'invariably': 34818, 'nutshells': 34819, 'dissimilar': 34820, 'chinatown': 34821, 'juno': 34822, \"syd's\": 34823, '”—laura': 34824, 'esquivel': 34825, 'callie': 34826, 'khouri': 34827, 'script—a': 34828, 'syn': 34829, \"screenwriter's\": 34830, '”—steven': 34831, 'bochco': 34832, 'kazu': 34833, \"kibuishi's\": 34834, 'amulets': 34835, 'bemoans': 34836, 'colons': 34837, 'impairing': 34838, 'disregarded': 34839, 'hyphens': 34840, 'ellipses': 34841, 'brackets': 34842, 'emoticons': 34843, 'syntactic': 34844, 'fictionalised': 34845, 'bpo': 34846, 'unveiled': 34847, \"'sacks\": 34848, \"sacks'\": 34849, 'occasioned': 34850, 'luria': 34851, \"ferguson's\": 34852, 'spiked': 34853, 'cartographers': 34854, 'chameleons': 34855, 'britishers': 34856, '23rd': 34857, 'motherland': 34858, 'kulbir': 34859, '“our': 34860, 'sanguinary': 34861, 'reskilling': 34862, 'intain': 34863, 'whee': 34864, 'teck': 34865, 'ong': 34866, 'inheritors': 34867, 'morlun': 34868, 'spiderverse': 34869, '2099': 34870, 'nice…': 34871, 'kettering': 34872, 'slumbers': 34873, 'disfiguring': 34874, 'rubies': 34875, 'ruth’s': 34876, 'enactment': 34877, 'board…': 34878, 'baillie': 34879, 'gifford': 34880, \"'elegantly\": 34881, 'mckie': 34882, 'microbes': 34883, 'zimmer’s': 34884, 'bioethical': 34885, 'quandaries': 34886, 'presumptions': 34887, \"ira's\": 34888, 'yusuf': 34889, 'tanuj': 34890, 'instructs': 34891, 'feinting': 34892, 'rawest': 34893, '—jack': 34894, 'jiffy': 34895, 'anbu': 34896, 'despairing': 34897, 'telegu': 34898, 'bangla': 34899, 'blackjack': 34900, 'success—and': 34901, 'unassailable': 34902, 'method—caused': 34903, 'quants': 34904, 'giuliani': 34905, 'detected': 34906, 'insoluble': 34907, 'classic—a': 34908, 'feynman’s': 34909, 'joking': 34910, 'aphorism': 34911, 'wins’': 34912, 'inclined': 34913, 'traders”—': 34914, 'anatomist': 34915, 'botanist': 34916, 'diversely': 34917, 'anchiano': 34918, 'verrocchio': 34919, 'fantastic—and': 34920, 'frightening—is': 34921, 'inside…': 34922, 'dorje': 34923, 'nanotech': 34924, 'incubator': 34925, \"tenzin's\": 34926, 'mirrored': 34927, 'vitalising': 34928, 'digits': 34929, 'anaconda': 34930, '“funny': 34931, 'hell”': 34932, 'rejoices': 34933, '“dogs': 34934, '“adventures': 34935, '“depression': 34936, 'humorist': 34937, 'snob': 34938, 'vaguely': 34939, 'authoritative—like': 34940, 'it—but': 34941, 'sneakiness': 34942, 'yhwach': 34943, 'reapers': 34944, 'quincies': 34945, 'pearsonpte': 34946, \"ronson's\": 34947, 'incision': 34948, 'maddest': 34949, \"moment's\": 34950, \"applicants'\": 34951, 'weinstein': 34952, 'inverted': 34953, 'derive': 34954, 'ballpark': 34955, 'estimates': 34956, \"girl'\": 34957, \"elle's\": 34958, 'caption': 34959, 'mackey': 34960, 'noticeboard': 34961, 'prickly': 34962, 'disentangle': 34963, 'dyson': 34964, 'valmik': 34965, '“fire': 34966, 'reboot': 34967, 'crypts': 34968, 'judas': 34969, 'capabilities—writing': 34970, 'sniffers': 34971, 'packets': 34972, 'infecting': 34973, 'sandboxing': 34974, 'keylogging': 34975, 'screenshotting': 34976, 'forensics': 34977, 'exfiltrate': 34978, 'sneakily': 34979, 'sensor': 34980, 'mesh': 34981, '11ah': 34982, 'lte': 34983, 'sigfox': 34984, 'lorawan': 34985, 'gateways': 34986, 'mqtt': 34987, 'coap': 34988, 'openfog': 34989, 'countering': 34990, 'perimeters': 34991, 'blockchains': 34992, 'vernacular': 34993, 'ecosphere': 34994, 'nurofen': 34995, 'oddity': 34996, 'circus”': 34997, 'likens': 34998, '“living': 34999, 'last”': 35000, 'rodgers': 35001, 'hughton': 35002, 'waddock': 35003, 'intermingle': 35004, '“mafū': 35005, 'forfeiting': 35006, 'malpractice': 35007, 'recessed': 35008, 'velcro': 35009, 'obha': 35010, 'obata': 35011, 'cannibals': 35012, 'rodents': 35013, \"sinbad's\": 35014, 'kishen': 35015, 'doberman': 35016, 'dissident': 35017, 'enmities': 35018, 'mobilized': 35019, 'unseat': 35020, 'octavian': 35021, 'championing': 35022, \"perveen's\": 35023, 'disinherit': 35024, \"farid's\": 35025, '“bible”': 35026, 'filmmaker’s': 35027, 'dslrs': 35028, 'theaters': 35029, \"scientist's\": 35030, 'bolte': 35031, 'deteriorate': 35032, 'alternated': 35033, 'kinesthetic': 35034, \"oprah's\": 35035, 'roget’sinternational': 35036, '7thedition': 35037, 'levelsan': 35038, 'featuresmore': 35039, 'entriesthat': 35040, 'added50': 35041, 'anyother': 35042, 'forstudents': 35043, 'one…': 35044, 'disposal': 35045, 'ghoul’s': 35046, 'kagune': 35047, 'qs': 35048, 'heroes…or': 35049, 'haise': 35050, 'ccg': 35051, 'vessels': 35052, 'rotating': 35053, 'extrusion': 35054, 'extremum': 35055, 'saib': 35056, 'tabrizi': 35057, 'gest': 35058, 'rasheed': 35059, 'acclam': 35060, \"bachelor's\": 35061, 'alette': 35062, '”—kirkus': 35063, 'overland': 35064, 'bering': 35065, 'yakut': 35066, 'alekhin': 35067, 'frontier—and': 35068, '‘is': 35069, 'plotting’': 35070, 'housesitter': 35071, 'house’s': 35072, 'mediators': 35073, 'patternsósuch': 35074, 'mvvmóare': 35075, 'creational': 35076, 'codeóincluding': 35077, 'asyncronous': 35078, 'commonjs': 35079, 'colouration': 35080, 'finishes': 35081, 'instill': 35082, 'fortnightly': 35083, 'governments—a': 35084, 'predicaments': 35085, 'dumbfounded': 35086, 'seance': 35087, 'pyschology': 35088, 'liars': 35089, 'wherefores': 35090, 'alternately': 35091, \"derren's\": 35092, \"professional's\": 35093, 'commandment': 35094, 'photocopies': 35095, \"'counterfactual\": 35096, 'dystopia': 35097, 'axis': 35098, \"'reality'\": 35099, 'yevgeny': 35100, \"zamyatin's\": 35101, \"'dick's\": 35102, \"published'\": 35103, '9781444101973': 35104, '9781444101980': 35105, \"forman's\": 35106, 'moretz': 35107, \"o'briens\": 35108, 'hypens': 35109, 'ablaze': 35110, 'dispiriting': 35111, 'inconsiderate': 35112, 'pigment': 35113, 'gurneyjourney': 35114, \"gurney's\": 35115, 'cabrera': 35116, 'frameworksóscikit': 35117, 'tensorflowóauthor': 35118, 'aurèlien': 35119, 'gèron': 35120, 'neurosciences': 35121, 'derivations': 35122, \"association's\": 35123, 'stimuli': 35124, 'starfleet': 35125, 'emperor’s': 35126, 'readied': 35127, 'felonious': 35128, 'harebrained': 35129, 'conniving': 35130, 'peteys': 35131, 'saiyans': 35132, 'universe’s': 35133, 'goten': 35134, 'dance—but': 35135, 'earth…': 35136, \"haggard's\": 35137, 'quartermain': 35138, 'twala': 35139, 'stories—you’ll': 35140, 'archie—and': 35141, 'book—fans': 35142, 'ramblings': 35143, 'pissed': 35144, 'cleaver': 35145, 'shazzer': 35146, 'chardonnay': 35147, \"fielding's\": 35148, 'zellweger': 35149, 'firth': 35150, 'bodyguards': 35151, 'filed': 35152, 'mps': 35153, \"suri's\": 35154, 'kudip': 35155, \"nayar's\": 35156, 'mountbatten': 35157, '‘revert': 35158, '‘at': 35159, 'correction': 35160, \"hussein's\": 35161, 'herrings': 35162, 'happify': 35163, \"marzi's\": 35164, 'treville': 35165, 'bonacieux': 35166, 'xiii': 35167, 'university…': 35168, '‘simply': 35169, 'exceptional…i': 35170, '1535': 35171, 'boleyn': 35172, 'koontz’s': 35173, 'personified': 35174, 'dead…': 35175, 'centuries—beginning': 35176, 'johannes': 35177, 'scientists—mostly': 35178, 'queer—whose': 35179, 'unclassifiable': 35180, 'hosmer': 35181, 'emanating': 35182, 'figures—ralph': 35183, 'douglass': 35184, 'whitman—and': 35185, 'transcendentalist': 35186, 'fomented': 35187, \"ganesha's\": 35188, 'uninterruptedly': 35189, 'draught': 35190, 'desertification': 35191, 'intensifying': 35192, 'vandana': 35193, 'damming': 35194, 'aquafarming': 35195, 'disenfranchisement': 35196, 'livelihoods': 35197, 'hammurabi': 35198, 'abolitionism': 35199, 'insolent': 35200, 'success’': 35201, 'summer’s': 35202, 'buds': 35203, 'long…': 35204, 'surrogate': 35205, 'panos’s': 35206, 'georgia’s': 35207, 'halfpenny': 35208, 'interferes': 35209, 'dissuade': 35210, 'rainfall': 35211, 'steepest': 35212, 'campfires': 35213, 'hearths': 35214, 'whitewater': 35215, 'flowed': 35216, \"robie's\": 35217, 'keggs': 35218, 'asthmatics': 35219, 'hilsa': 35220, 'cosmopolitanism': 35221, 'withdrawing': 35222, 'voluminous': 35223, 'belgrave': 35224, 'westerfield': 35225, 'propensity': 35226, 'obsess': 35227, 'too—but': 35228, 'all—love': 35229, 'snubbed': 35230, \"karna's\": 35231, 'unquestioningly': 35232, 'lionized': 35233, \"understated'\": 35234, 'madrick': 35235, \"'mazzucato\": 35236, \"breakthroughs'\": 35237, \"thinking'\": 35238, '1636': 35239, 'picardy': 35240, 'roi': 35241, 'chevalier': 35242, 'forfeit': 35243, \"invaders'\": 35244, 'butchery': 35245, \"chevalier's\": 35246, 'overconsumption': 35247, '“stuff”': 35248, 'galvanizing': 35249, 'colbert': 35250, 'tavis': 35251, 'stephanopolous': 35252, 'idea—that': 35253, 'disposal—annie': 35254, 'haiti': 35255, 'coltan': 35256, 'steered': 35257, \"sinha's\": 35258, 'hand—no': 35259, 'rename': 35260, 'watermark': 35261, 'pdfs': 35262, 'grunt': 35263, 'threes': 35264, 'kettle': 35265, \"delight'\": 35266, 'physics’': 35267, 'layman': 35268, 'nucleus': 35269, 'taller': 35270, 'delft': 35271, 'edx': 35272, 'courseware': 35273, \"'thriller\": 35274, \"'hypnotic\": 35275, \"'examines\": 35276, 'ludicrously': 35277, 'guinevere': 35278, \"kepnes'\": 35279, 'perversely': 35280, \"delicious'\": 35281, \"chilling'\": 35282, 'thailand': 35283, 'bagpipes': 35284, 'kheros': 35285, 'skllled': 35286, 'war…': 35287, 'inclusions': 35288, 'snubs': 35289, 'holes’': 35290, 'reside': 35291, 'tickles': 35292, 'copernicus': 35293, 'unfurled': 35294, \"flinn's\": 35295, 'musicianship': 35296, 'dizzyingly': 35297, 'believable’': 35298, 'eke': 35299, 'swindling': 35300, 'trades': 35301, 'gutting': 35302, 'bristow': 35303, 'orcas': 35304, '‘algorithms': 35305, 'by’': 35306, 'messiness': 35307, 'inbox': 35308, 'multilayer': 35309, 'perceptrons': 35310, 'brits': 35311, 'alanis': 35312, 'morrisette': 35313, 'tackls': 35314, 'chinua': 35315, 'achebe': 35316, 'pinter': 35317, 'paxman': 35318, 'vinyasas': 35319, 'inversions': 35320, 'alterations': 35321, 'kaminoff': 35322, 'patricide': 35323, 'dmitry': 35324, 'lurid': 35325, 'searhes': 35326, \"nsa's\": 35327, 'cryptographer': 35328, 'hallways': 35329, 'nerdiness': 35330, \"sloan's\": 35331, \"penumbra's\": 35332, 'needy': 35333, 'microorganisms': 35334, 'loaves': 35335, 'halloway': 35336, 'enrolling': 35337, \"gathering'\": 35338, \"joey's\": 35339, 'stephenie': 35340, 'turnips': 35341, 'yearwinner': 35342, 'marianne': 35343, \"rooney's\": 35344, 'sews': 35345, \"sensuality'\": 35346, \"accomplished'\": 35347, 'socrates—the': 35348, '6’4”': 35349, 'midfielder': 35350, 'smoked': 35351, 'coolness': 35352, 'heel—the': 35353, 'corinthians': 35354, 'democracy—a': 35355, 'kitman': 35356, 'club—socrates': 35357, 'fathered': 35358, 'socrates’s': 35359, 'downie': 35360, 'in—whatever': 35361, 'sexercise': 35362, 'quickie': 35363, 'lovemaking': 35364, \"'disappearances'\": 35365, 'vanishings': 35366, 'mixes': 35367, \"'lois\": 35368, \"witch'\": 35369, \"nurse's\": 35370, 'roams': 35371, 'northumberland': 35372, \"clare'\": 35373, 'doppelgänger': 35374, '496': 35375, 'preowned': 35376, 'corrigendum': 35377, 'editorials': 35378, 'valediction': 35379, 'paraphrasing': 35380, 'synergize': 35381, 'punctured': 35382, 'nontoxic': 35383, 'kaaren': 35384, \"pixton's\": 35385, 'carle': 35386, 'attains': 35387, 'issac': 35388, 'bahamas': 35389, 'equipping': 35390, 'indoor': 35391, 'plumbing': 35392, 'pussies': 35393, 'plated': 35394, 'telly': 35395, 'pinking': 35396, 'flea': 35397, 'collars': 35398, 'giblets': 35399, 'fridge': 35400, 'chauvinism': 35401, 'condemnation': 35402, 'up—here': 35403, 'compounded': 35404, 'home—it': 35405, \"'holiday'\": 35406, \"'woman'\": 35407, \"'along'\": 35408, \"'back'\": 35409, '7c': 35410, 'ornaments': 35411, 'caps': 35412, 'form—what': 35413, 'thinkingwithtype': 35414, 'coquette': 35415, 'watsons': 35416, 'sanditon': 35417, 'hypochondriacs': 35418, \"'sweets'\": 35419, \"'jump'\": 35420, \"'fish'\": 35421, \"you'llwant\": 35422, 'adsl': 35423, '3g': 35424, 'gigabit': 35425, 'ethernet': 35426, 'mlps': 35427, 'rifd': 35428, 'vrije': 35429, 'universiteit': 35430, 'netherlands': 35431, 'minix': 35432, 'expert—who': 35433, 'gilbreths': 35434, 'loy': 35435, 'methodist': 35436, \"pete's\": 35437, 'typedef': 35438, 'header': 35439, 'nama': 35440, 'chenghis': 35441, 'temur': 35442, 'uzbek': 35443, 'florentino': 35444, 'daza': 35445, 'urbino': 35446, 'urbino’s': 35447, 'josé': 35448, 'concordia': 35449, 'qap': 35450, 'newscast': 35451, 'neustadt': 35452, 'whore': 35453, '‘i’m': 35454, 'alone’': 35455, 'krüger': 35456, 'fingernail': 35457, 'crale': 35458, 'herbalist': 35459, 'elsa': 35460, 'divorcee': 35461, '‘wee': 35462, 'wee': 35463, 'wee’': 35464, 'mind…': 35465, 'elven': 35466, 'bonhart': 35467, 'bugging': 35468, 'socialization': 35469, 'kreisky': 35470, 'institut': 35471, 'livemint': 35472, 'branko': 35473, 'theorizing': 35474, 'orthodoxies': 35475, 'commendably': 35476, 'bourguignon': 35477, \"milanovic's\": 35478, 'hamlets': 35479, 'chirping': 35480, 'likings': 35481, 'kasauli': 35482, 'sahitya': 35483, 'vile': 35484, 'fiends': 35485, 'dullness': 35486, 'souza': 35487, 'gould': 35488, 'testosterone”': 35489, '“curious': 35490, 'george’s': 35491, 'pharmacy”': 35492, 'reexamines': 35493, 'primates': 35494, 'medicate': 35495, '“junk': 35496, 'monkeys”': 35497, 'baboons': 35498, '“circling': 35499, 'neurobiological': 35500, 'neurobiologist': 35501, 'pomposity': 35502, 'dissonance': 35503, 'larkin': 35504, 'dj': 35505, 'joffe': 35506, 'rhymed': 35507, 'quatrains': 35508, 'dryly': 35509, 'moraes': 35510, 'rodent': 35511, 'keki': 35512, 'daruwalla': 35513, 'coining': 35514, 'kaiser': 35515, 'haq': 35516, 'radicalised': 35517, 'whizzing': 35518, 'mambas': 35519, 'fogbound': 35520, 'shuichi': 35521, 'saito': 35522, 'withdrawn': 35523, 'kirie': 35524, 'goshima': 35525, 'uzumaki': 35526, 'manifests': 35527, 'seashells': 35528, \"shuichi's\": 35529, 'cochlea': 35530, 'whirlpool': 35531, 'finn’s': 35532, \"'neutral'\": 35533, 'finns': 35534, 'icelanders': 35535, 'parochialism': 35536, '’—sourav': 35537, 'walled': 35538, 'docile': 35539, 'supratim': 35540, '‘photographic': 35541, 'superimposition’': 35542, 'monoxide': 35543, \"'detective\": 35544, \"galileo'\": 35545, \"higashino's\": 35546, 'picanumba': 35547, \"winn's\": 35548, 'vcr': 35549, '18–32': 35550, 'farris': 35551, 'outings': 35552, 'jess’s': 35553, 'shields’s': 35554, 'jibber': 35555, 'jabber': 35556, \"interactions'\": 35557, \"'deceit\": 35558, \"loads'\": 35559, 'heroes—from': 35560, 'steel—and': 35561, 'extinguish': 35562, 'lantern’s': 35563, 'ivy’s': 35564, 'root—and': 35565, 'batman—and': 35566, 'comics’': 35567, 'talents—including': 35568, 'bisley': 35569, 'allred': 35570, 'totleben': 35571, 'aparo—the': 35572, 'afterwords': 35573, 'amanuensis': 35574, \"band'\": 35575, \"'silver\": 35576, \"blaze'\": 35577, \"problem'\": 35578, 'pears': 35579, 'glinert': 35580, 'caved': 35581, '9780525568070': 35582, 'mado': 35583, 'hinami': 35584, 'kaneki’s': 35585, 'mary’s': 35586, 'eustace': 35587, 'hailey': 35588, 'wynne': 35589, 'wynne’s': 35590, 'heuristics': 35591, \"toibin's\": 35592, 'lacey': 35593, 'lodging': 35594, 'homesick': 35595, 'elating': 35596, 'tóibín': 35597, \"masterwork'\": 35598, 'zoë': 35599, 'webster': 35600, 'words’': 35601, '9000': 35602, 'use…': 35603, 'contranyms': 35604, 'heteronyms': 35605, 'homographs': 35606, 'plaindromes': 35607, 'pangram': 35608, 'manias': 35609, 'botswana': 35610, 'centrally': 35611, 'tarahumara': 35612, 'toga': 35613, 'sandals': 35614, 'pausing': 35615, 'drifters': 35616, 'observational': 35617, '“greatest': 35618, 'hits”': 35619, 'anytime—drawn': 35620, 'readies': 35621, 'subdued': 35622, 'das—a': 35623, 'befriended—akbar': 35624, 'court—a': 35625, 'suspicion—danger': 35626, 'mountaineers—including': 35627, 'smythe—on': 35628, 'kamet': 35629, 'inclement': 35630, 'joshimath': 35631, 'uttarakhand': 35632, 'bhyundar': 35633, 'darjeeling—his': 35634, 'factotums—provisions': 35635, 'parbat': 35636, 'rataban': 35637, 'usefully': 35638, 'ruminating': 35639, 'idleness': 35640, 'introspection': 35641, 'dordogne': 35642, \"thesaurus'\": 35643, 'oxforddictionaries': 35644, 'ispeaker': 35645, 'delete': 35646, 'oo': 35647, '2023': 35648, 'minimally': 35649, 'jour': 35650, 'layoffs': 35651, 'baronet': 35652, 'dinshaw': 35653, 'forbade': 35654, 'notwithstanding': 35655, 'scandalized': 35656, 'petits': 35657, 'undemonstrative': 35658, 'bride—as': 35659, 'unbending': 35660, 'jinnah’s': 35661, 'dina': 35662, 'inconsolable': 35663, 'convulsed': 35664, '”–usa': 35665, 'introspective': 35666, 'age–or': 35667, 'age–holds': 35668, '”–los': 35669, 'ancestor': 35670, 'crankiness': 35671, 'humored': 35672, '”–chicago': 35673, '“fans': 35674, 'calligraphic': 35675, '“thank': 35676, 'assemblage': 35677, 'unfashionable': 35678, '”–studs': 35679, 'owens': 35680, \"bod's\": 35681, 'hamptom': 35682, 'galen': 35683, 'khosi': 35684, 'kree': 35685, \"thor's\": 35686, \"wolverine's\": 35687, 'bertram': 35688, \"gentlemen's\": 35689, \"wooster's\": 35690, 'ganymede': 35691, \"employer's\": 35692, 'snodsbury': 35693, '0478': 35694, '2210': 35695, '0473': 35696, 'looping': 35697, 'contents1': 35698, 'charts3': 35699, 'python4': 35700, 'operations5': 35701, 'control6': 35702, 'packages8': 35703, 'handling9': 35704, 'handling11': 35705, 'beefeaters': 35706, \"experiences'\": 35707, 'wrests': 35708, 'solberg’s': 35709, 'lie…': 35710, 'ajatashatru': 35711, 'conjurer': 35712, 'trickster': 35713, 'slumber': 35714, 'conned': 35715, 'rivière': 35716, 'lunching': 35717, 'abounds': 35718, 'callan': 35719, 'aquainted': 35720, 'switchwords': 35721, 'intentioned': 35722, 'bibighat': 35723, 'prise': 35724, 'gape': 35725, 'folktale': 35726, 'canonical': 35727, 'deify': 35728, 'humanize': 35729, 'ballads': 35730, 'operas': 35731, 'janaki': 35732, 'shiva’s': 35733, 'earth—offering': 35734, 'benardot': 35735, 'stocked': 35736, 'quantities': 35737, 'maximal': 35738, \"bases'\": 35739, 'mailings': 35740, 'fitnews': 35741, 'newsletter': 35742, 'topics—from': 35743, 'casteism': 35744, 'vivekananda—the': 35745, \"'storytelling'\": 35746, \"script'\": 35747, 'belling': 35748, 'client’s': 35749, 'methodical': 35750, 'concepts—from': 35751, 'packaging': 35752, 'zoom': 35753, 'boomerang': 35754, 's17': 35755, \"'design'\": 35756, 'the1908': 35757, \"alderman's\": 35758, 'letterers': 35759, 'calendars': 35760, 'waitress': 35761, 'debra': 35762, \"prosecution's\": 35763, 'jaihouse': 35764, 'snitches': 35765, 'infuriate': 35766, 'archmaester': 35767, 'gyldayn': 35768, 'wheatley': 35769, 'gibbon’s': 35770, 'conjuncts': 35771, 'interrogative': 35772, 'bereaved': 35773, 'garage': 35774, 'blumenthal’s': 35775, 'steve’s': 35776, '‘till': 35777, 'part…’': 35778, 'goofy': 35779, 'belcher’s': 35780, 'edible': 35781, 'bruschetta': 35782, 'bout': 35783, 'corral': 35784, 'okra': 35785, 'sweaty': 35786, 'cajun': 35787, 'series’s': 35788, '1853': 35789, 'preconditions': 35790, 'barring': 35791, 'wolmar': 35792, 'penalties': 35793, 'migalia': 35794, 'bevelin': 35795, 'tintinologist': 35796, 'brothe': 35797, 'thaw': 35798, 'fishes': 35799, 'octopuses': 35800, 'corals': 35801, 'jordanstone': 35802, \"'amazing\": 35803, \"steel'\": 35804, 'bandits': 35805, \"desert's\": 35806, 'kubernetes': 35807, 'runtimes': 35808, 'zun': 35809, 'kuryr': 35810, 'murano': 35811, 'kolla': 35812, 'coe': 35813, 'fatale': 35814, 'succubus': 35815, 'puzzler': 35816, 'moscovich': 35817, 'workman': 35818, 'interconnect': 35819, 'detentions': 35820, 'submits': 35821, 'mocking': 35822, 'betancourt': 35823, '‘most': 35824, 'mop': 35825, 'else’s': 35826, 'face’': 35827, 'counselling': 35828, 'nate’s': 35829, 'teachers’': 35830, 'frisbee': 35831, 'crossreferencing': 35832, 'untangles': 35833, 'roadmaps': 35834, 'interdisciplinarity': 35835, 'chilled': 35836, 'bryson’s': 35837, 'motorists': 35838, 'robbed': 35839, 'tripe': 35840, 'eyeballs': 35841, 'reeperbahn': 35842, 'liechtenstein': 35843, 'trill': 35844, 'wah': 35845, \"gbi's\": 35846, \"will's\": 35847, 'bleeds': 35848, 'lipped': 35849, 'alloted': 35850, 'presages': 35851, 'hubble': 35852, 'stephen’s': 35853, 'densely': 35854, 'interconnecting': 35855, 'serviced': 35856, 'desalinated': 35857, 'atira': 35858, 'darpana': 35859, 'time—everyone': 35860, 'iii—vikram': 35861, 'programme—and': 35862, 'marcel': 35863, 'rainforests': 35864, '007s': 35865, 'kbe': 35866, 'cocktails': 35867, 'overbooked': 35868, 'bullsh': 35869, 'surrey': 35870, 'ecb': 35871, 'umpiring': 35872, 'relaunched': 35873, 'ransacked': 35874, 'comatose': 35875, 'inquisition': 35876, 'malleus': 35877, 'maleficarum': 35878, 'bonnet': 35879, 'murderer—in': 35880, 'bastide': 35881, 'blanche': 35882, 'cigar': 35883, 'postal': 35884, 'brushwork': 35885, 'camilleri': 35886, 'whodunits': 35887, 'gustatory': 35888, 'costars': 35889, '“art': 35890, 'livelier': 35891, 'refine': 35892, 'extensible': 35893, \"web's\": 35894, \"javascript's\": 35895, 'dams': 35896, 'gallons': 35897, 'kangaroo': 35898, 'eggshells': 35899, 'shear': 35900, 'fracture': 35901, 'sailboat': 35902, 'joinery': 35903, 'ceilings': 35904, 'hulls': 35905, 'masts': 35906, 'buttresses': 35907, 'oversimplification': 35908, 'pentester': 35909, \"barbara's\": 35910, \"'enchantingly\": 35911, \"'honest\": 35912, \"'irresistible'\": 35913, 'tijinder': 35914, 'tuteja': 35915, 'pulverizers': 35916, 'khoobraj': 35917, \"pp's\": 35918, 'acest': 35919, 'bpl': 35920, 'pp': 35921, 'hh': 35922, 'hounds': 35923, 'humdinger': 35924, \"titu's\": 35925, 'perspire': 35926, 'copiously': 35927, 'gnawing': 35928, 'tilottama': 35929, 'dessert': 35930, 'romantics': 35931, 'mallery': 35932, 'attraction…': 35933, 'jameson': 35934, 'objection': 35935, 'tolerated': 35936, '…there': 35937, 'apprehensive': 35938, 'law—and': 35939, 'collar…': 35940, 'schaum': 35941, 'engravings': 35942, 'typesetting': 35943, '916': 35944, 'nologists': 35945, 'nologist': 35946, \"sheep's\": 35947, 'unrated': 35948, 'ssshh': 35949, 'ably': 35950, 'kinesthetics': 35951, 'perceiver': 35952, 'orchestrates': 35953, \"zuko's\": 35954, 'generalizations': 35955, 'misperceptions': 35956, 'abhijit': 35957, 'duflo': 35958, 'quirk': 35959, 'eri': 35960, 'chisaki’s': 35961, 'attempt—and': 35962, 'lion’s': 35963, 'mission…': 35964, \"wanted'\": 35965, 'carolyn': 35966, 'runningfor': 35967, \"usa's\": 35968, 'donovans': 35969, 'killed…': 35970, 'basking': 35971, 'discontented': 35972, 'eased': 35973, 'rheumatism': 35974, 'soldier’s': 35975, 'infuriatingly': 35976, 'major’s': 35977, 'story…': 35978, 'taslima': 35979, 'injunction': 35980, 'expunge': 35981, 'lawsuit': 35982, 'firecrackers': 35983, 'diyas': 35984, 'twinkling': 35985, 'compa\\xadnies': 35986, 'chal\\xadlenge': 35987, 'revolution’': 35988, 'patrikarakos': 35989, 'control’': 35990, 'cavendish': 35991, 'analytica': 35992, 'delicately': 35993, 'faculties': 35994, 'feudalism': 35995, 'monarchies': 35996, 'egil': 35997, 'derring': 35998, 'powering': 35999, 'yielding': 36000, 'rajeev': 36001, 'ronanki': 36002, \"fix's\": 36003, 'kleinberg': 36004, 'sendhil': 36005, 'mullainathan': 36006, 'niraj': 36007, 'dawar': 36008, 'heppelmann': 36009, 'd’aveni': 36010, 'daugherty': 36011, 'frick': 36012, 'flatpick': 36013, 'theoretic': 36014, 'duets': 36015, 'leveling': 36016, 'sonmez': 36017, 'hourly': 36018, 'coworkers': 36019, 'ontario': 36020, 'salaried': 36021, 'bootcampssome': 36022, 'flipflops': 36023, 'micromanager': 36024, 'recruiter': 36025, 'pigeonholed': 36026, \"manager's\": 36027, 'series—anna': 36028, 'todd’s': 36029, 'scott’s': 36030, 'phenom': 36031, 'lizzy': 36032, 'himself—his': 36033, 'tessa—which': 36034, 'deathbead': 36035, 'ruminate': 36036, 'mull': 36037, 'ekes': 36038, 'feroze': 36039, 'varun': 36040, 'doldrums': 36041, 'hearn': 36042, 'sa’ud': 36043, 'sasson': 36044, 'guardianship': 36045, 'imposition': 36046, 'manoeuvrings': 36047, 'legislative': 36048, \"pm's\": 36049, \"pmo'\": 36050, \"'of\": 36051, \"resonances'\": 36052, \"'bhushan\": 36053, \"vigilant'\": 36054, '9781409382928': 36055, 'scripps': 36056, 'petoskey': 36057, 'redistribution': 36058, 'motorcyclist': 36059, 'timber': 36060, 'information–and': 36061, 'vlsm': 36062, 'summarization': 36063, 'vlans': 36064, 'trunking': 36065, 'stp': 36066, 'etherchannel': 36067, 'ripng': 36068, 'eigrp': 36069, 'eigrpv6': 36070, 'ospfv2': 36071, 'ospfv3': 36072, 'ebgp': 36073, 'dhcp': 36074, 'fhrp': 36075, 'hsrp': 36076, 'cdp': 36077, 'lldp': 36078, '“create': 36079, 'journal”': 36080, 'cisco®': 36081, 'biweekly': 36082, 'amalgam': 36083, 'enthusiasm–or': 36084, 'ebert’s': 36085, 'nationalities': 36086, 'justly': 36087, 'scholar’s': 36088, 'corliss': 36089, 'persuasions': 36090, 'bonnie': 36091, 'clyde': 36092, 'dolce': 36093, 'vita': 36094, 'waterfront': 36095, 'thayer': 36096, 'leaks': 36097, 'restoration': 36098, 'lodgers': 36099, 'eileen': 36100, 'marya': 36101, 'hiss': 36102, 'politik': 36103, 'kingly': 36104, \"germany's\": 36105, 'nuremberg': 36106, 'kristallnacht': 36107, \"humane'\": 36108, \"impression'\": 36109, \"'likely\": 36110, \"evans's\": 36111, \"sympathy'\": 36112, \"'evans\": 36113, \"germans'\": 36114, \"catastrophe'\": 36115, 'ascherson': 36116, 'chick': 36117, 'plucky': 36118, 'sugarlump': 36119, 'pox': 36120, 'chowed': 36121, 'perrine': 36122, 'bornstein': 36123, 'hurlock': 36124, 'supersimple': 36125, 'superfood': 36126, 'segfaults': 36127, 'downloading': 36128, 'toolchain': 36129, \"'architect's\": 36130, 'place…': 36131, 'inoffensive': 36132, 'norton': 36133, 'allerton': 36134, 'craven': 36135, 'luttrell': 36136, 'carrington': 36137, \"bronte's\": 36138, 'markham': 36139, 'bronte': 36140, 'remorselessly': 36141, 'witnesses…': 36142, 'goodrick': 36143, 'overtone': 36144, 'improvising': 36145, 'oxbridge': 36146, 'vociferous': 36147, 'satin': 36148, 'music’': 36149, 'hirsute': 36150, 'gaudy': 36151, 'boogieing': 36152, 'dancefloor': 36153, 'annabel’s': 36154, 'lovemaker’': 36155, 'goldsmith': 36156, '‘devoted': 36157, 'husband’': 36158, 'gower': 36159, 'clapton': 36160, 'jagger': 36161, 'musharraf': 36162, '“top': 36163, 'protagonist’s': 36164, 'ankita’s': 36165, 'bubblegums': 36166, 'candies': 36167, \"'we're\": 36168, 'rear': 36169, 'prestige': 36170, \"'part\": 36171, 'birthing': 36172, \"'anand\": 36173, 'reinventions': 36174, 'luce': 36175, \"'rarely\": 36176, 'deciphered': 36177, 'locomotive': 36178, 'parag': 36179, 'jernau': 36180, 'morat': 36181, 'etymologicon': 36182, \"be'\": 36183, 'inimitably': 36184, 'lennon': 36185, 'wadsworth': 36186, 'fittings': 36187, 'kerri': 36188, 'maniscalco': 36189, 'koh': 36190, 'glosses': 36191, 'diagramatic': 36192, 'individualised': 36193, 'cassettes': 36194, 'honoria': 36195, 'peacekeeping': 36196, 'lankan': 36197, \"'eccentric’\": 36198, \"'bohemian’\": 36199, 'cleanliness': 36200, 'trashing': 36201, 'unmotivated': 36202, '“change”': 36203, 'shutting': 36204, 'll3r': 36205, 'estonian': 36206, 'lettish': 36207, 'lithuanian': 36208, 'puzzlist': 36209, 'waugh’s': 36210, 'acceptability': 36211, 'promoter—the': 36212, '—could': 36213, 'propulsion': 36214, 'riverside': 36215, '—robin': 36216, 'raiser': 36217, 'headfirst': 36218, 'craziest': 36219, \"kleon's\": 36220, \"victore's\": 36221, 'itinerary': 36222, 'poehler': 36223, 'teased': 36224, 'ape': 36225, 'ramona’s': 36226, 'revenge…in': 36227, 'mortified': 36228, 'rivero’s': 36229, 'father―the': 36230, 'revenge…': 36231, '“irresistible…seductive…with': 36232, 'page”': 36233, '—soon': 36234, 'spielberg’s': 36235, 'fassbender': 36236, 'cianfrance': 36237, 'sherbourne': 36238, 'janus': 36239, 'miscarriages': 36240, 'baby’s': 36241, 'onshore': 36242, 'withstood': 36243, '“gift': 36244, 'tom’s': 36245, '“elegantly': 36246, 'rendered…heart': 36247, 'wrenching…beautifully': 36248, 'drawn”': 36249, 'multitask': 36250, 'criterion': 36251, 'imageability': 36252, '8gb': 36253, 'i5': 36254, 'cpu': 36255, 'terminated': 36256, '668': 36257, '309': 36258, 'dp': 36259, 'trainings': 36260, '10000': 36261, '1732': 36262, 'almanacks': 36263, 'pulldown': 36264, 'compleat': 36265, \"carré's\": 36266, 'taciturn': 36267, 'practice…': 36268, 'blackened': 36269, 'outflung': 36270, 'anaesthetic': 36271, 'tantric': 36272, 'womenfolk': 36273, 'mithila': 36274, 'repainted': 36275, 'gurvich': 36276, 'sushma': 36277, 'bahl': 36278, '1572nd': 36279, 'bsc': 36280, 'sturctures': 36281, 'impressing': 36282, 'beers': 36283, 'flamethrower': 36284, 'blockhead': 36285, 'detector': 36286, \"bailey's\": 36287, \"force'\": 36288, 'smallangryplanet': 36289, 'mishmash': 36290, 'sissix': 36291, 'reptillian': 36292, 'kizzy': 36293, 'spaceships': 36294, 'wayfarers': 36295, \"'never\": 36296, \"involving'\": 36297, \"'explores\": 36298, 'quieter': 36299, 'wowing': 36300, \"imagination'\": 36301, \"opera'\": 36302, 'io9': 36303, 'nisevich': 36304, 'bede': 36305, 'shave': 36306, 'lacing': 36307, 'tusker': 36308, 'elderphant': 36309, 'rattlesnake': 36310, 'underperforming': 36311, 'disagreeing': 36312, 'defensiveness': 36313, 'gallic': 36314, 'caesar’s': 36315, 'rubicon': 36316, 'mcshane': 36317, 'rowson': 36318, 'chessis': 36319, \"math's\": 36320, 'wight': 36321, \"grandmaster's\": 36322, 'verlaine': 36323, 'roma': 36324, 'dover': 36325, 'quicksands': 36326, 'shivery': 36327, '‘heartwrenching': 36328, 'exertion': 36329, 'debilitating': 36330, 'hiked': 36331, '127': 36332, 'peaked': 36333, 'brontë’s': 36334, 'sisters’': 36335, 'picnic': 36336, 'chelseawhen': 36337, 'robson': 36338, 'porto': 36339, \"npr's\": 36340, 'deafness': 36341, 'keller’s': 36342, '1901': 36343, 'diehard': 36344, 'secularist': 36345, \"'see\": 36346, 'hancock': 36347, 'sanchez': 36348, 'upsetting': 36349, 'boorish': 36350, \"'safe\": 36351, 'haven’': 36352, 'notebook’': 36353, 'impudent': 36354, 'dexterous': 36355, 'was’': 36356, 'newgate': 36357, 'flanders': 36358, 'bigamy': 36359, 'pocketing': 36360, 'candour—and': 36361, '1722': 36362, 'plundering': 36363, 'valuables': 36364, \"juan's\": 36365, 'brul': 36366, 'corsair': 36367, 'approachability': 36368, 'siena': 36369, 'brural': 36370, 'qc': 36371, \"wind'\": 36372, \"custer's\": 36373, 'cheyenne': 36374, 'trib': 36375, 'sundae': 36376, 'parodic': 36377, 'sharjah': 36378, 'tatters': 36379, 'pleated': 36380, 'curved': 36381, 'churned': 36382, 'samrat': 36383, 'quell': 36384, \"arryn's\": 36385, 'katz': 36386, 'tics': 36387, 'chuckling': 36388, 'profoundest': 36389, 'gamely': 36390, 'dyke': 36391, 'collegium': 36392, 'frayed': 36393, 'suborn': 36394, \"'last\": 36395, \"standing'\": 36396, '“voice': 36397, 'coursework': 36398, '‘check': 36399, 'progress’': 36400, 'loke': 36401, 'trant': 36402, 'forest―its': 36403, 'nightmare…': 36404, '‘he’s': 36405, 'behrendt': 36406, 'satc': 36407, 'jenifer': 36408, 'barrymore': 36409, 'abia': 36410, \"terry's\": 36411, 'storeys': 36412, 'watermelon': 36413, 'launcher': 36414, 'karamasov': 36415, 'triangular': 36416, '“wicked': 36417, 'sentimental”': 36418, 'pavlovich': 36419, 'sons―the': 36420, 'cheeked': 36421, 'pevear': 36422, 'larissa': 36423, 'volokhonsky': 36424, 'sprightly': 36425, 'sprinkling': 36426, \"gilbert's\": 36427, \"smiley's\": 36428, \"rhimes's\": 36429, 'grasse': 36430, \"tyson's\": 36431, 'psychopathy': 36432, 'knife’s': 36433, 'steadfastness': 36434, 'socialized': 36435, 'avoidance': 36436, \"'home'\": 36437, 'dental': 36438, 'miffed': 36439, 'traders': 36440, \"pitt's\": 36441, 'heeding': 36442, 'bead': 36443, 'silicates': 36444, 'anion': 36445, 'mineralogist': 36446, 'intervening': 36447, 'vols': 36448, \"bendis'\": 36449, 'marvellously': 36450, 'cousins—the': 36451, 'kouravas—but': 36452, 'escorted': 36453, 'nondescript': 36454, \"zafar's\": 36455, 'expound': 36456, 'ascended': 36457, 'usurped': 36458, 'sepoy': 36459, 'bloodiest': 36460, 'duff': 36461, 'mahmood': 36462, 'farooqui': 36463, 'peoplewatching': 36464, 'unconsciously': 36465, 'disorganized': 36466, 'time—italy': 36467, \"frankl's\": 36468, 'extenuating': 36469, 'butts': 36470, 'dawdling': 36471, 'emile': 36472, 'theresienstadt': 36473, 'polyclinic': 36474, 'die…': 36475, 'babbington': 36476, 'choked': 36477, 'motive…': 36478, 'bjorn': 36479, 'borg': 36480, '257': 36481, 'tatum': 36482, \"o'neal\": 36483, 'reused': 36484, 'disjointed': 36485, 'filtered': 36486, '“year': 36487, '“moving': 36488, 'averages”': 36489, '“pattern': 36490, 'like”': 36491, 'onsite': 36492, 'costing': 36493, 'readers—especially': 36494, 'anthros': 36495, 'shoujo': 36496, 'shounen': 36497, 'wavered': 36498, 'oliveros': 36499, 'stapled': 36500, 'beaton': 36501, 'chester': 36502, 'deforge': 36503, 'miriam': 36504, 'katin': 36505, 'rutu': 36506, 'modan': 36507, 'sturm': 36508, 'jillian': 36509, 'tamaki': 36510, 'yoshihiro': 36511, 'tatsumi': 36512, 'drechsler': 36513, 'doucet': 36514, 'porcellino': 36515, 'tomine': 36516, 'devlin': 36517, 'olin': 36518, 'unferth': 36519, \"o'neill\": 36520, 'lemony': 36521, 'snicket': 36522, 'bertram’s': 36523, 'foresee': 36524, 'day…': 36525, 'waitresses': 36526, 'announce': 36527, 'holly’s': 36528, 'tranformation': 36529, 'reb': 36530, 'sevices': 36531, 'reappear': 36532, 'wushan': 36533, 'writinq': 36534, 'indebted': 36535, 'surinder': 36536, \"legionary's\": 36537, 'nefarius': 36538, 'purpus': 36539, 'dubuis': 36540, \"'accident'\": 36541, 'manus': 36542, 'clubfoot': 36543, 'maugham’s': 36544, '“excessively': 36545, 'mott': 36546, 'binchy': 36547, 'finable': 36548, 'dso': 36549, 'zionist': 36550, 'bwana': 36551, 'kilmer': 36552, 'brangwens': 36553, 'brangwen': 36554, \"gudrun's\": 36555, 'destructiveness': 36556, '2x4': 36557, '6x0': 36558, 'lbs': 36559, 'elijah': 36560, 'rebekah': 36561, 'mikaelson': 36562, '1766': 36563, 'bayou': 36564, 'werewolves': 36565, 'curse—one': 36566, 'ancestors—and': 36567, 'certain—the': 36568, 'madagascar': 36569, 'mouseletshelp': 36570, 'distinctly': 36571, 'gond': 36572, 'vyam': 36573, 'rangoli': 36574, 'chunky': 36575, 'walloping': 36576, 'pelting': 36577, 'simulating': 36578, 'wim': 36579, 'hof': 36580, 'autoimmune': 36581, '‘benjamin': 36582, 'isaacson’s': 36583, 'verifies': 36584, 'subatomic': 36585, 'anupam': 36586, \"kher's\": 36587, 'utilises': 36588, 'ceased': 36589, 'bhoomi': 36590, 'cities—madhavpur': 36591, 'ayudhpur': 36592, 'abode': 36593, 'does—the': 36594, 'manava': 36595, \"marcus'\": 36596, \"jimmy's\": 36597, 'humpty': 36598, 'dumpty': 36599, 'maitresse': 36600, 'neuilly': 36601, 'manacled': 36602, 'grotesquely': 36603, 'deformed': 36604, 'muhlheim': 36605, 'soars': 36606, \"forsyth's\": 36607, 'houghton': 36608, \"peter's\": 36609, \"sterling's\": 36610, 'irfan': 36611, 'kamran': 36612, 'creamy': 36613, 'mouthfuls': 36614, 'bookie': 36615, 'surat': 36616, 'polishing': 36617, 'tubelight': 36618, 'vocabularly': 36619, 'splotches': 36620, 'poignance': 36621, 'amateurish': 36622, 'cinetech': 36623, 'gaffer': 36624, 'landau': 36625, 'augments': 36626, 'lightingforcinematography': 36627, 'hollingdale': 36628, 'lame': 36629, 'preachings': 36630, 'interposition': 36631, 'apes': 36632, 'ubermensch': 36633, 'paralysed': 36634, 'vegetative': 36635, 'quadriplegic': 36636, 'wove': 36637, 'pent': 36638, 'airborne': 36639, 'chairborne': 36640, 'spurring': 36641, 'adversities': 36642, 'repose': 36643, 'substitute': 36644, 'kurchever': 36645, 'limpid': 36646, '2006—a': 36647, 'earths—is': 36648, 'omac': 36649, \"dcu's\": 36650, 'constrained': 36651, \"putin's\": 36652, 'annexation': 36653, 'ballad': 36654, \"gilligan's\": 36655, 'donkey': 36656, 'marcellus': 36657, '1498': 36658, 'vasco': 36659, 'gama': 36660, 'zamorins': 36661, 'heralding': 36662, 'martanda': 36663, 'consecrated': 36664, 'dynastic': 36665, \"pillai's\": 36666, 'sethu': 36667, 'bayi': 36668, 'wrathful': 36669, 'matriarchs': 36670, \"'violent\": 36671, 'profligate': 36672, \"sordid'\": 36673, 'swapping': 36674, 'quarrelling': 36675, 'consorts': 36676, 'lustful': 36677, 'splendorous': 36678, \"'go\": 36679, 'delphi': 36680, 'gautama’s': 36681, 'life―from': 36682, 'botticelli': 36683, 'dali': 36684, '9781405363303': 36685, 'variegated': 36686, 'chafed': 36687, 'disclosed': 36688, 'grownups': 36689, 'specialises': 36690, 'westminster': 36691, 'bestknown': 36692, 'awardwinning': 36693, 'sequels': 36694, \"cut'\": 36695, 'underverse': 36696, 'bizarros': 36697, 'chronovore': 36698, 'quitely': 36699, 'harkening': 36700, \"superman's\": 36701, 'bizarro': 36702, 'hurley': 36703, 'iniquity': 36704, 'ficciones': 36705, 'aleph': 36706, '1980’s': 36707, \"'soul\": 36708, 'nehab': 36709, 'watson’s': 36710, 'musgrave': 36711, 'bohemia': 36712, 'sherlock’s': 36713, 'anthologist': 36714, 'curated': 36715, 'english’s': 36716, 'sections—': 36717, '—metaphors': 36718, 'mardy’s': 36719, 'backstories': 36720, 'wordfinder': 36721, 'dhurv': 36722, 'showy': 36723, 'platformers': 36724, 'attest': 36725, 'solesource': 36726, \"eddard's\": 36727, 'khal': 36728, 'drogo': 36729, 'seventies—a': 36730, 'charge—enjoying': 36731, '125th': 36732, 'uncouth': 36733, '‘ten': 36734, 'boys’': 36735, 'dermody': 36736, 'gorman': 36737, 'neill': 36738, 'aidan': 36739, 'gallaverdin': 36740, 'sikhs': 36741, 'voyeuristic': 36742, 'swansong': 36743, 'shortt': 36744, 'ruttle': 36745, \"sandy's\": 36746, 'apprehend': 36747, 'croc': 36748, 'monstrosity': 36749, '491': 36750, '659': 36751, \"'93\": 36752, 'detachable': 36753, 'megalomaniac': 36754, 'meiden': 36755, 'coffeehouse': 36756, \"euwe's\": 36757, 'spat': 36758, 'moo': 36759, 'swerling': 36760, 'lazar': 36761, \"harold's\": 36762, 'vimrod': 36763, 'mug': 36764, 'grapes': 36765, 'stant': 36766, 'cabi': 36767, 'bungalows': 36768, 'playgrounds': 36769, \"architecture's\": 36770, 'ponniyin': 36771, 'selvan': 36772, 'piousness': 36773, 'nila': 36774, 'married…jennifer': 36775, 'her…': 36776, 'responsiveness': 36777, 'tomasz': 36778, 'nurkiewicz': 36779, 'callback': 36780, 'observables': 36781, 'oses': 36782, 'plastered': 36783, 'decorations': 36784, 'commercialised': 36785, 'customised': 36786, 'satyawadi': 36787, 'infirmary': 36788, 'farthings': 36789, 'kaufman': 36790, 'hakim': 36791, 'bishara': 36792, 'hertford': 36793, 'cliftons': 36794, 'barringtons': 36795, 'wring': 36796, 'ptc': 36797, 'thingworx': 36798, 'leveraged': 36799, \"'invisible\": 36800, 'cordelia': 36801, 'criado': 36802, 'perez': 36803, 'exclude': 36804, 'civilise': 36805, \"sawyer's\": 36806, 'shootouts': 36807, 'spacious': 36808, 'nonexperts': 36809, 'formulations': 36810, 'cormen': 36811, 'bork': 36812, \"cena's\": 36813, '1339': 36814, \"superstar's\": 36815, 'smackdown': 36816, 'nxt': 36817, '9781465431240': 36818, 'emboldened': 36819, 'villainesses': 36820, 'cheesy': 36821, 'bmat': 36822, 'tsa': 36823, 'lebanese': 36824, 'najwa': 36825, 'zebian': 36826, 'mistreated': 36827, 'misjudged': 36828, 'yearn': 36829, '149': 36830, 'dumbfound': 36831, 'billets': 36832, 'pellets': 36833, 'necromancy': 36834, 'editorship': 36835, 'psychometry': 36836, \"insiders'\": 36837, 'confederates': 36838, 'diversions': 36839, 'inclusiveness': 36840, 'navneesh': 36841, \"garg's\": 36842, 'integrations': 36843, 'firebug': 36844, 'firepath': 36845, 'testng': 36846, 'ie': 36847, 'firefox': 36848, 'chrome': 36849, 'ide': 36850, 'hp': 36851, 'uft': 36852, \"tester's\": 36853, 'creed™': 36854, 'hooded': 36855, 'biding': 36856, 'kenway—the': 36857, 'merchant—dreams': 36858, 'privateers': 36859, 'multiplatinum': 36860, 'ubisoft': 36861, 'countables': 36862, 'uncountables': 36863, 'preposterous': 36864, 'changabang': 36865, \"changabang's\": 36866, 'acclimatisation': 36867, 'salford': 36868, 'hammocks': 36869, 'punishingly': 36870, 'llewelyn': 36871, 'homemakers': 36872, \"'lean\": 36873, \"lead'\": 36874, 'peggy': 36875, 'mcintosh': 36876, '’lean': 36877, 'lead’': 36878, \"maas's\": 36879, 'rhysand': 36880, 'bernese': 36881, 'raw’s': 36882, 'jungfrau': 36883, 'code’': 36884, 'louvre': 36885, 'neveu': 36886, 'cryptologist': 36887, \"'enchanting\": 36888, 'randa': 36889, 'fattah': 36890, 'hijabs': 36891, 'kurtas': 36892, 'corsets': 36893, 'shamsi': 36894, 'interfering': 36895, 'naggers': 36896, 'flighty': 36897, 'judgmental': 36898, 'wormed': 36899, 'ingots': 36900, 'emeralds': 36901, 'oilman': 36902, 'talbot': 36903, \"'66\": 36904, 'enzo': 36905, 'bookpage': 36906, '“victory': 36907, 'override': 36908, 'smothering': 36909, '“home': 36910, 'kill—the': 36911, 'populate': 36912, 'saunders’s': 36913, 'december—through': 36914, 'redeemable': 36915, 'spirit—not': 36916, 'chekhov’s': 36917, '“prepare': 36918, '”—khaled': 36919, 'english—not': 36920, '‘arguably': 36921, '”—mary': 36922, 'karr': 36923, 'disenfranchised': 36924, '“saunders’s': 36925, 'dreamlike': 36926, 'extricating': 36927, 'surname': 36928, 'mannering': 36929, 'phipps': 36930, \"jeeves'\": 36931, 'africa’s': 36932, 'deluge': 36933, 'tolstoy’s': 36934, '1805': 36935, 'napoleon’s': 36936, 'marches': 36937, 'quixotic': 36938, 'andrey': 36939, 'interweave': 36940, 'entwines': 36941, 'themes—conflict': 36942, 'fate—with': 36943, 'imperfection': 36944, \"'target\": 36945, \"7'\": 36946, \"novel's\": 36947, \"dunne's\": 36948, 'resents': 36949, 'chomsky’s': 36950, 'backpocket': 36951, 'democracy—one': 36952, 'bludgeon': 36953, 'wilson’s': 36954, 'creel': 36955, 'mongering': 36956, 'lippmann’s': 36957, 'disinformation': 36958, 'practicality': 36959, '—even': 36960, 'writes—or': 36961, 'refocus': 36962, 'backlogs': 36963, 'srivastav': 36964, 'hailing': 36965, 'faridabad': 36966, \"monster'\": 36967, 'flaky': 36968, 'bitchy': 36969, 'excuse': 36970, 'zinger': 36971, \"2010'\": 36972, '155': 36973, 'grievous': 36974, 'mathematicians': 36975, 'sumpter': 36976, 'relinquish': 36977, 'pagan': 36978, 'abomination': 36979, 'cnut': 36980, 'longsword': 36981, 'uhtred’s': 36982, 'aethelflaed': 36983, 'bebbanburg’s': 36984, '109': 36985, \"'sometimes'\": 36986, \"'interesting'\": 36987, \"'jumping'\": 36988, \"'balloon'\": 36989, 'preschoolers': 36990, 'townend': 36991, 'schooldays': 36992, 'dulwich': 36993, \"vai's\": 36994, 'transcriber': 36995, 'whitehill': 36996, 'gw': 36997, 'forgetful': 36998, \"'jay\": 36999, \"arguing'\": 37000, 'rhetorician': 37001, 'sindh': 37002, 'latif': 37003, '1689': 37004, '1752': 37005, 'panjabi': 37006, 'bullhe': 37007, 'naskh': 37008, 'buffoonery': 37009, \"mulick's\": 37010, 'paramour': 37011, 'sublimating': 37012, 'beneficence': 37013, \"meher's\": 37014, 'shuklalal': 37015, 'comfacebook': 37016, 'shuklalalinstagram': 37017, 'theshuklalal': 37018, 'heels’': 37019, 'adjuster’s': 37020, '‘caste': 37021, 'off’': 37022, 'eye’': 37023, \"d'artagnan\": 37024, 'richelieu': 37025, 'milady': 37026, 'disable': 37027, 'undoubtable': 37028, 'proportionate': 37029, 'shonali': 37030, 'sabherwal': 37031, 'dhupia': 37032, 'eshadeol': 37033, 'fernandez': 37034, 'chitrangadha': 37035, 'bediamong': 37036, 'thighs': 37037, 'celeb': 37038, 'redefines': 37039, 'sl502': 37040, 'safest': 37041, 'faintest': 37042, 'humouring': 37043, 'frayn': 37044, 'dirac': 37045, 'theoretician': 37046, 'legendarily': 37047, 'empathize': 37048, 'postcards': 37049, \"dirac's\": 37050, 'waldegrave': 37051, 'webpack': 37052, 'httpclient': 37053, 'localized': 37054, 'relatability': 37055, 'nuisance': 37056, 'rowel': 37057, 'landline': 37058, 'grittiest': 37059, 'disfigurement': 37060, 'hemorrhoids': 37061, 'granny': 37062, 'panties': 37063, 'praskovya': 37064, 'fedorovna': 37065, 'golovina': 37066, 'golovin': 37067, 'devoid': 37068, 'gerasim': 37069, \"ivan's\": 37070, 'immanent': 37071, 'busying': 37072, 'smug': 37073, 'separateness': 37074, 'nabokov': 37075, \"cellent'\": 37076, 'cephas': 37077, 'buthelezi': 37078, 'wherewithal': 37079, 'ostrich': 37080, 'reassess': 37081, 'tlokweng': 37082, 'apprentices': 37083, 'hoopoe': 37084, 'makutsi': 37085, 'outreach': 37086, 'geostrategy': 37087, 'image’': 37088, 'essentially—statistically': 37089, 'speaking—there': 37090, 'removes': 37091, 'unreachable': 37092, 'bayles': 37093, 'orland': 37094, 'expeienced': 37095, 'artmakers': 37096, 'easel': 37097, 'alone—now': 37098, 'posting—has': 37099, 'artmaking': 37100, 'capra': 37101, 'rotstein': 37102, 'hovanes': 37103, 'cheryl': 37104, 'mabern': 37105, \"nypd's\": 37106, 'howzell': 37107, 'obstruct': 37108, \"expert'\": 37109, 'mendes': 37110, 'resourcefulness': 37111, 'headedness': 37112, \"brearley's\": 37113, \"movement'\": 37114, \"sophia's\": 37115, \"gal's\": 37116, 'daviesscience': 37117, 'mercenaries': 37118, \"countrymen's\": 37119, 'menlove': 37120, 'tilman': 37121, 'spender': 37122, 'journeyed': 37123, 'oxus': 37124, 'nepalese': 37125, 'venturesome': 37126, 'ragamuffins': 37127, \"shipton's\": 37128, \"treatment'\": 37129, 'squarely': 37130, 'crosshairs': 37131, 'indiebound': 37132, 'hungover': 37133, 'drinker': 37134, 'blackouts': 37135, 'aggravate': 37136, \"toye's\": 37137, 'diabetic': 37138, 'retinopathy': 37139, 'chatbot': 37140, 'captchas': 37141, 'gan': 37142, 'captcha': 37143, 'betrayal…': 37144, 'enchantment…': 37145, 'mortmain': 37146, 'jem': 37147, 'kien’s': 37148, 'battalion': 37149, 'kien': 37150, 'bao': 37151, 'ninh': 37152, 'era’': 37153, '“be': 37154, 'needed”': 37155, '“wooden': 37156, \"'competitive\": 37157, \"greatness'\": 37158, 'sinegal': 37159, 'costco': 37160, \"'then'\": 37161, \"'house'\": 37162, \"'make'\": 37163, '9781444174625': 37164, '9780071784672': 37165, 'talent’s': 37166, \"'elegant\": 37167, 'isma': 37168, 'resuming': 37169, 'deferred': 37170, 'aneeka': 37171, 'parvaiz': 37172, \"parvaiz's\": 37173, \"families'\": 37174, \"sophocles'\": 37175, 'antigone': 37176, 'kamila': 37177, 'shamsie': 37178, '“sab': 37179, 'ek”': 37180, 'devotee‘s': 37181, 'sadguru': 37182, 'baba‘s': 37183, 'evolutionarily': 37184, 'fallible': 37185, 'disorganised': 37186, 'burnett': 37187, 'sabotaging': 37188, 'underage': 37189, 'won′t': 37190, 'green–haired': 37191, 'superherologist': 37192, 'emmaline': 37193, 'florist': 37194, \"parker's\": 37195, 'beste': 37196, 'bier': 37197, 'autobahn': 37198, 'wilkommen': 37199, 'neuen': 37200, 'deutschen': 37201, 'freunde': 37202, 'deutsches': 37203, 'viele': 37204, 'phrasen': 37205, 'buch': 37206, 'conversions': 37207, 'ich': 37208, 'möchte': 37209, 'berliner': 37210, 'bitte': 37211, 'wie': 37212, 'kostet': 37213, 'käse': 37214, 'vater': 37215, 'kaufte': 37216, 'seinem': 37217, 'sohn': 37218, 'fahrrad': 37219, 'mehr': 37220, 'meisten': 37221, 'wird': 37222, 'dem': 37223, 'geschlagen': 37224, 'stoutest': 37225, 'lager': 37226, \"understanding'\": 37227, 'cornered': 37228, 'assailant': 37229, 'dominika': 37230, 'egorova': 37231, 'traitors': 37232, 'unorthadox': 37233, 'shimmers': 37234, \"authenticity'\": 37235, \"globe'\": 37236, \"'portrays\": 37237, 'pont': 37238, 'neuf': 37239, 'alleyways': 37240, 'smoothest': 37241, 'grandes': 37242, 'dames': 37243, 'chocolatier': 37244, 'prize—the': 37245, 'predestined': 37246, 'lookalike': 37247, \"noir's\": 37248, 'miraculouses': 37249, 'martyrs': 37250, 'hypochondria': 37251, 'seediness': 37252, 'jaunt': 37253, \"'t'\": 37254, 'tins': 37255, 'pineapple': 37256, 'montmorency': 37257, 'discursions': 37258, \"'clerking\": 37259, \"classes'\": 37260, 'daytripping': 37261, 'bicycling': 37262, '1859': 37263, 'walstall': 37264, 'staffordshire': 37265, 'marylebone': 37266, 'idler': 37267, \"gibbons's\": 37268, 'agundez': 37269, 'rashmi': 37270, 'saleable': 37271, 'haggis': 37272, 'articulate': 37273, 'intuitively': 37274, \"script's\": 37275, 'hottie': 37276, 'boyfriend—but': 37277, \"nickelodeon's\": 37278, 'neeti': 37279, 'palta': 37280, 'rj': 37281, 'missmalini': 37282, 'kanika': 37283, 'tekriwal': 37284, 'jetsetgo': 37285, 'freya': 37286, 'harun': 37287, \"loss'\": 37288, \"wow'\": 37289, \"'irresistible\": 37290, \"jerker'\": 37291, 'usp': 37292, 'subrata': 37293, 'akshaye': 37294, 'g7': 37295, 'd7': 37296, 'a7': 37297, 'unquoted': 37298, 'tal': 37299, \"'magician\": 37300, 'marketability': 37301, 'beatty': 37302, 'nineties': 37303, 'encapsulated': 37304, 'kissel': 37305, 'benetton': 37306, 'jenson': 37307, 'ecclestone': 37308, 'operated': 37309, 'yasmin': 37310, 'chapattis': 37311, 'dramatizing': 37312, 'bagchi’s': 37313, 'excelling': 37314, \"atlanta's\": 37315, 'lebron': 37316, 'celtics': 37317, 'espn': 37318, 'opens—and': 37319, 'all—every': 37320, 'reevaluating': 37321, 'inductees': 37322, 'simmons’s': 37323, 'haters': 37324, 'hardwood': 37325, 'courtside': 37326, 'stalkers': 37327, '‘shari': 37328, 'confounds': 37329, \"shari's\": 37330, \"finest'\": 37331, 'reading’': 37332, '‘smart': 37333, 'compelling’': 37334, 'cliché': 37335, \"'entirely\": 37336, \"'shari\": 37337, 'furnishes': 37338, '“wonderfully': 37339, 'hero”': 37340, \"directors'\": 37341, 'offending': 37342, 'bloggers': 37343, 'arnab': 37344, \"goswami's\": 37345, 'milkman': 37346, 'britney': 37347, 'spears': 37348, 'kejriwal': 37349, 'shawl': 37350, 'ministerial': 37351, \"balder's\": 37352, 'superhacker': 37353, 'thirsting': 37354, 'gardner’s': 37355, 'crossover': 37356, 'mutation': 37357, 'particle': 37358, 'pluralistic': 37359, 'frawley': 37360, 'known—eruptions': 37361, 'sequester': 37362, 'sediments': 37363, 'rajasaurus': 37364, 'discoveries—from': 37365, 'ahmedabad—are': 37366, 'indica': 37367, 'biochemist': 37368, 'beginning—from': 37369, 'coalesced': 37370, 'planet—and': 37371, 'kamlesh': 37372, 'basanti': 37373, \"squirrel's\": 37374, 'rapture': 37375, 'weimar': 37376, 'polarization': 37377, \"'monumental\": 37378, \"reich'\": 37379, \"nazis'\": 37380, 'byatt': 37381, 'hazel’s': 37382, 'dorn': 37383, 'pape': 37384, 'blond': 37385, 'familiarities': 37386, 'stiltonordhas': 37387, 'mouseletmega': 37388, 'micekingwarriors': 37389, 'micekingsbe': 37390, 'frosted': 37391, 'pastry': 37392, 'humankind—and': 37393, 'language—analogies': 37394, 'unnecessarily': 37395, '083': 37396, 'bhatnagar': 37397, '“18': 37398, 'days”': 37399, 'percussionist': 37400, 'allarakha': 37401, 'accompanists': 37402, 'percussionists': 37403, 'concerts': 37404, 'mclaughlin': 37405, 'vinayakram': 37406, \"ivory's\": 37407, \"paranjpye's\": 37408, 'saaz': 37409, 'bernardo': 37410, 'bertolucci': 37411, 'ismail': 37412, 'masseur': 37413, 'soundtracks': 37414, \"'unlucky'\": 37415, 'mahim': 37416, 'vilayat': 37417, 'negotiates': 37418, 'mon': 37419, 'voir': 37420, 'trendsetter': 37421, 'rainey': 37422, 'nasir': 37423, 'egyptology': 37424, 'ptolemaic': 37425, 'horus': 37426, 'schumer': 37427, 'wincing': 37428, 'nodding': 37429, 'amy’s': 37430, 'weirdo': 37431, \"'lisa\": 37432, 'wingate': 37433, 'mclain': 37434, 'shantyboat': 37435, 'wrenched': 37436, 'aiken': 37437, 'stafford': 37438, 'tann': 37439, 'idi': 37440, 'amin': 37441, \"'asians\": 37442, 'milked': 37443, 'niranjan': 37444, 'observance': 37445, 'niceties': 37446, \"desai's\": 37447, 'chile': 37448, \"general's\": 37449, 'accord': 37450, 'floriana': 37451, \"villa's\": 37452, 'jeopardizing': 37453, 'marina': 37454, 'pacify': 37455, 'mischel’s': 37456, \"'marshmallow\": 37457, 'prewired': 37458, 'infuser': 37459, 'rim': 37460, 'pairings': 37461, 'scrumptious': 37462, 'visualized': 37463, 'southport': 37464, 'talon': 37465, 'sizzles': 37466, 'desires—and': 37467, 'doubts—to': 37468, 'swarmed': 37469, 'junnar': 37470, 'bhaja': 37471, 'bedsa': 37472, 'kondane': 37473, 'kanheri': 37474, 'dwellings': 37475, 'chaityas': 37476, 'viharas': 37477, 'basalt': 37478, 'ghats': 37479, \"craftsperson's\": 37480, 'excavations': 37481, 'surendra': 37482, 'itineraries': 37483, 'deccan’s': 37484, '’80s': 37485, 'melchester': 37486, 'relegation': 37487, 'lineker': 37488, 'mendelsohn': 37489, 'childless': 37490, 'squeaking': 37491, 'woodshed': 37492, 'palgaum': 37493, 'members—husband': 37494, 'law—are': 37495, 'mind—kunal': 37496, 'unblemished': 37497, 'brides': 37498, \"cricket'\": 37499, 'bigwigs': 37500, 'waqar': 37501, 'younis': 37502, 'sourav': 37503, 'sportsmanship': 37504, 'waveforms': 37505, '1972—a': 37506, 'velis': 37507, 'purported': 37508, '1790—mireille': 37509, 'remy': 37510, 'fortresslike': 37511, 'constricted': 37512, 'charlemagne': 37513, 'tragedienne': 37514, \"mehta's\": 37515, \"kumari's\": 37516, 'pakeezah': 37517, 'meetawala': 37518, 'dadar': 37519, 'cemetery': 37520, 'cremated': 37521, 'amrohi': 37522, \"'unfairly\": 37523, \"luck'\": 37524, \"cinema's\": 37525, \"tragedienne'\": 37526, 'ernst': 37527, 'weeks’': 37528, 'canceled': 37529, 'asterx': 37530, 'dogamatirx': 37531, 'theory—gate': 37532, 'sinusoidal': 37533, 'laplace': 37534, 'olrik': 37535, \"mortimer's\": 37536, 'aerobatics': 37537, 'dived': 37538, 'waved': 37539, 'chuffed': 37540, 'walkout': 37541, \"thapar's\": 37542, 'homeschooling': 37543, 'polls—over': 37544, 'shi': 37545, \"huangdi's\": 37546, 'plantains': 37547, \"bauer's\": 37548, 'americas—find': 37549, 'tests—each': 37550, 'mangas': 37551, 'book1': 37552, 'naradatta': 37553, 'soa': 37554, 'traceability': 37555, 'alm': 37556, 'testability': 37557, '2fa': 37558, 'openid': 37559, 'bigdata': 37560, 'uk’s': 37561, 'wishlists': 37562, 'wonderkids': 37563, 'pogba': 37564, 'rashford': 37565, 'aubameyang': 37566, 'aguero': 37567, 'higuain': 37568, 'stars’': 37569, 'pics': 37570, 'emojis': 37571, 'blissful': 37572, 'childs': 37573, 'atlantic’s': 37574, '‘girls’': 37575, '‘tiny': 37576, 'furniture’': 37577, 'sexpert': 37578, 'disintegrates': 37579, 'mignola': 37580, 'frogs': 37581, 'sapien': 37582, 'sherman': 37583, 'drafting': 37584, 'homunculus': 37585, \"mignola's\": 37586, '368': 37587, 'ignacio': 37588, 'estrada': 37589, \"'multiple\": 37590, 'numeracy': 37591, \"'play'\": 37592, \"jahan's\": 37593, 'swordfights': 37594, 'flavia': 37595, \"hope's\": 37596, 'hentzau': 37597, \"dutt's\": 37598, 'dried': 37599, 'gratuity': 37600, \"lecturer's\": 37601, 'dramatics': 37602, 'oration': 37603, 'colvin': 37604, \"taluqdars'\": 37605, 'daly': 37606, 'haphazard': 37607, 'asterinx': 37608, 'bibliotheca': 37609, 'universalis': 37610, 'blurry': 37611, 'parcel': 37612, 'aramaic': 37613, 'backpacker': 37614, 'trekker': 37615, 'hiker': 37616, 'skurka—accomplished': 37617, 'writer—shares': 37618, 'skurka': 37619, 'hikes': 37620, 'stoves': 37621, 'campsite': 37622, 'vālmīki': 37623, 'rāma': 37624, 'ayodhyā': 37625, 'viṣṇu': 37626, 'rāvaṇa': 37627, 'ambitions—all': 37628, 'arshia': 37629, 'sattar’s': 37630, 'maintainability': 37631, 'kleppmann': 37632, \"abel's\": 37633, 'dwarfs': 37634, 'loosing': 37635, 'soumendu': 37636, 'buddhadeb': 37637, 'cameramen': 37638, 'charteris': 37639, 'ambitions…': 37640, 'saadat': 37641, 'hasan': 37642, 'shyam—the': 37643, 'hero—is': 37644, 'flirtatious': 37645, 'sitara': 37646, 'paro': 37647, 'neena': 37648, 'bunglings': 37649, 'kaashmiri': 37650, 'jehan': 37651, 'peccadilloes': 37652, 'rafiq': 37653, 'zeihan': 37654, 'enervate': 37655, 'overturning': 37656, 'shale': 37657, 'sidestep': 37658, 'sucking': 37659, 'maw': 37660, 'globalizing': 37661, '“items”': 37662, 'varargs': 37663, 'programmers—a': 37664, 'meters': 37665, 'quest—to': 37666, 'mountains—himalayan': 37667, 'ineffable': 37668, 'rarified': 37669, 'fondly': 37670, 'edophiles': 37671, 'edheads': 37672, \"runners'\": 37673, 'customisable': 37674, 'marathoners': 37675, 'puffington': 37676, 'nichols': 37677, 'biohacks': 37678, 'laserlike': 37679, 'biomarkers': 37680, 'bulletproof': 37681, 'lag': 37682, 'proteins': 37683, 'fats': 37684, 'toxin': 37685, 'boeing': 37686, '747': 37687, 'emptied': 37688, 'apocalyptically': 37689, 'plinth': 37690, 'vocabularies': 37691, 'cpf': 37692, 'vocalist': 37693, 'operatic': 37694, 'maiden’s': 37695, 'ott': 37696, 'songwriters': 37697, 'brewer': 37698, 'fencer': 37699, 'polymath': 37700, 'dispenser': 37701, 'jehangir': 37702, 'brighu': 37703, 'collectibles': 37704, 'shintu': 37705, 'greyer': 37706, 'sarnath': 37707, 'heighten': 37708, '‘prime': 37709, 'army’': 37710, 'purchases': 37711, 'cavitation': 37712, 'leased': 37713, 'sarah’s': 37714, 'moorcroft': 37715, 'encroaches': 37716, '–what': 37717, 'ultrasmart': 37718, 'soutaro': 37719, 'kanou': 37720, 'usui’s': 37721, 'kanhaiya': 37722, \"'india's\": 37723, 'loathed': 37724, \"student'\": 37725, 'permissions': 37726, 'alwayson': 37727, 'streamline': 37728, 'od': 37729, \"pro's\": 37730, 'blundering': 37731, \"'can't\": 37732, \"anything'\": 37733, 'nato': 37734, 'weiner': 37735, 'trajectories': 37736, \"'stroud\": 37737, 'riordan': 37738, \"raschka's\": 37739, 'modernized': 37740, 'raschka': 37741, 'vahid': 37742, \"mirjalili's\": 37743, 'crepes': 37744, 'atoms': 37745, 'broglie': 37746, 'heisenberg': 37747, 'heligoland': 37748, 'conscripted': 37749, 'archduke': 37750, 'conscripts': 37751, 'coped': 37752, 'preaker': 37753, 'reacquainting': 37754, 'precocious': 37755, \"renu's\": 37756, 'players—': 37757, '—charlie': 37758, 'polygon': 37759, '“d': 37760, 'acolytes': 37761, 'showrunners': 37762, '‘game': 37763, '”—neima': 37764, 'jahromi': 37765, 'adventures—to': 37766, 'orcish': 37767, 'traitorous': 37768, 'gnome': 37769, 'twitch': 37770, 'narrative—and': 37771, 'miniatures': 37772, 'wargaming': 37773, 'insead': 37774, 'starkly': 37775, 'harmoniously': 37776, 'gantt': 37777, \"clockmaker's\": 37778, 'simmer': 37779, 'clockmaker’s': 37780, \"'true\": 37781, \"false'\": 37782, 'busters': 37783, '9781780552491': 37784, '9781780553078': 37785, 'wordsearches': 37786, '9781780553085': 37787, '9781780555409': 37788, '9781780554723': 37789, '9781780555621': 37790, '9781780554730': 37791, 'mourned': 37792, 'return—accounts': 37793, 'film—was': 37794, 'sketchy': 37795, 'ajmal': 37796, 'gunman': 37797, 'lashkar': 37798, 'taiba': 37799, 'hafiz': 37800, 'saeed': 37801, 'zaki': 37802, 'lakhvi': 37803, 'bylanes': 37804, 'pok': 37805, 'jeeps': 37806, 'mumbai’s': 37807, 'rommel': 37808, 'rodrigues’': 37809, 'fleshes': 37810, 'it…': 37811, 'switzerland…to': 37812, 'dishonored': 37813, 'masterless': 37814, 'agat': 37815, 'questionologist': 37816, 'common—and': 37817, 'autopsies': 37818, 'abnormality': 37819, 'eve’s': 37820, 'games—where': 37821, 'spoilt': 37822, 'life—love': 37823, '‘modern’': 37824, 'befalls': 37825, 'kitty”': 37826, 'catthemed': 37827, 'inman': 37828, 'rollerblading': 37829, 'mrow': 37830, 'kneading': 37831, 'pullout': 37832, 'invisibles’': 37833, 'employable': 37834, 'traynor’s': 37835, 'caretaker': 37836, 'will’s': 37837, \"'buffalo\": 37838, \"bill'\": 37839, \"bill's\": 37840, 'germanic': 37841, \"goodwill's\": 37842, '1685': 37843, '1750': 37844, 'chorale': 37845, 'notating': 37846, '371': 37847, '69': 37848, 'schirmer': 37849, 'riemenschneider': 37850, 'zaphod': 37851, 'undeleted': 37852, 'catastrophes': 37853, 'eddy': 37854, 'vogons': 37855, 'slartibartfast': 37856, 'nifty': 37857, 'lamuella': 37858, 'sandwich': 37859, 'definitively': 37860, 'inaccurate': 37861, \"texas's\": 37862, 'reeves': 37863, \"exercise's\": 37864, 'concentric': 37865, 'predominant': 37866, \"lescroart's\": 37867, 'dismas': 37868, 'witt': 37869, 'presumes': 37870, 'judgements': 37871, 'tortuous': 37872, 'physicals': 37873, 'artificially': 37874, 'beholden': 37875, '\\x97usa': 37876, 'ireland\\x97so': 37877, 'schools\\x97colleges': 37878, 'saimhas': 37879, 'enforcer': 37880, 'bergamo': 37881, 'alpinism': 37882, 'epoca': 37883, \"'bonatti's\": 37884, 'cundill': 37885, 'ferrell': 37886, 'goenka': 37887, 'kissinger’s': 37888, 'bangladesh—which': 37889, 'brinkmanship': 37890, 'karneval': 37891, 'köln': 37892, 'dino': 37893, 'bavarian': 37894, 'collocative': 37895, 'disobeys': 37896, \"donaldson's\": 37897, \"scheffler's\": 37898, 'shrunk': 37899, 'nickel': 37900, 'blender': 37901, 'vibrate': 37902, '‘sing': 37903, 'europe’s': 37904, 'reaffirming': 37905, 'vibration': 37906, 'kyoto': 37907, 'cybernetics': 37908, 'hesse': 37909, 'kayser': 37910, 'gebser': 37911, 'hazrat': 37912, 'lnayat': 37913, 'coltrane': 37914, 'punchy': 37915, 'nilekani’s': 37916, 'scaramanga': 37917, \"daylights'\": 37918, 'bhatia': 37919, 'badlands': 37920, 'ascension': 37921, 'sujit': 37922, \"maya's\": 37923, 'shahzad': 37924, 'sabeena': 37925, 'matchmaker': 37926, 'demythsified': 37927, 'ankur': 37928, 'bagaria': 37929, 'ritansha': 37930, 'patni': 37931, 'mea': 37932, 'headout': 37933, 'drilled': 37934, 'smacked': 37935, 'separating': 37936, 'tamasha': 37937, 'policical': 37938, 'archana': 37939, 'samtani': 37940, 'soch': 37941, 'bogy': 37942, 'chakras': 37943, 'bouquets': 37944, 'irises': 37945, 'sunflowers': 37946, 'chrysanthemums': 37947, '“nothing': 37948, 'actors”': 37949, '—both': 37950, 'craft—bryan': 37951, 'steadier': 37952, '“must': 37953, 'memoir”': 37954, 'baines': 37955, 'broadway’s': 37956, 'bad’s': 37957, 'psyche”': 37958, '“by': 37959, 'sad”': 37960, \"climo's\": 37961, 'anteaters': 37962, 'grizzly': 37963, 'quirkily': 37964, 'otters': 37965, 'crustaceans': 37966, 'partial': 37967, 'devilry': 37968, 'rajguru': 37969, 'inquilab': 37970, 'zindabad': 37971, 'ght': 37972, 'glorifi': 37973, 'bertrand': 37974, 'commemorating': 37975, 'approver': 37976, 'gruesomeness': 37977, 'bravura': 37978, 'augusten': 37979, 'turkeys': 37980, 'outlandishly': 37981, 'ghanshyamdas': 37982, \"kudaisya's\": 37983, 'legendry': 37984, 'kudaisya': 37985, 'parted': 37986, 'heroine—tracy': 37987, 'comes—returns': 37988, 'conning': 37989, 'munroe’s': 37990, 'jetpack': 37991, 'richter': 37992, 'tornadoes': 37993, 'relativistic': 37994, 'hypothetical': 37995, 'xterra': 37996, 'athlete\\x92s': 37997, 'fitzgerald\\x92s': 37998, 'bracing': 37999, 'can\\x97only': 38000, 'loudest': 38001, 'wanjiru': 38002, 'lemond': 38003, 'siri': 38004, 'lindley': 38005, 'cadel': 38006, 'newby': 38007, 'vail': 38008, 'voeckler': 38009, 'overend': 38010, 'prefontaine': 38011, 'penguin\\x94': 38012, 'bingham': 38013, \"'trap\": 38014, \"unwary'\": 38015, 'foreboding': 38016, 'inseparability': 38017, 'ajay': 38018, 'birthdays': 38019, 'acknowledgement': 38020, 'ghosted': 38021, 'compendiums': 38022, 'picador': 38023, 'neville': 38024, 'cardus': 38025, 'naipaul': 38026, 'amphigorey': 38027, 'wittier': 38028, 'nonsensically': 38029, 'gorey’s': 38030, 'baby”': 38031, 'infant”': 38032, 'garden”': 38033, 'tragedy”': 38034, 'osbick': 38035, 'bird”': 38036, 'cousins”': 38037, 'sock”': 38038, '“story': 38039, 'sara”': 38040, 'limerick”': 38041, \"'crowdy'\": 38042, \"'aircraft'\": 38043, \"'cattle'\": 38044, \"are'\": 38045, \"is'\": 38046, \"spot'\": 38047, \"corner'\": 38048, \"'bestest'\": 38049, \"at'\": 38050, \"'topper'\": 38051, \"'ranker'\": 38052, \"'revert\": 38053, 'backside': 38054, 'upendran’s': 38055, 'susanna': 38056, 'kaysen': 38057, 'mclean': 38058, 'clientele—sylvia': 38059, 'charles—as': 38060, \"kaysen's\": 38061, 'kaleidoscopically': 38062, 'fouryear': 38063, \"beauty's\": 38064, 'westcliff—a': 38065, 'despises': 38066, \"earl's\": 38067, 'endangers': 38068, \"lillian's\": 38069, 'safety—and': 38070, 'life—will': 38071, 'condoleezza': 38072, \"rice's\": 38073, 'appraisals': 38074, 'epix': 38075, 'schnetzer': 38076, 'wayans': 38077, 'madsen': 38078, '“unimpeachably': 38079, 'publisher’s': 38080, 'marcus’s': 38081, 'sensationally': 38082, 'implicated': 38083, 'kellergan—whom': 38084, 'mentor’s': 38085, 'backwoods': 38086, 'somerset’s': 38087, 'boosts': 38088, 'desani': 38089, 'incongruities': 38090, 'indres': 38091, 'suta': 38092, \"mephisto's\": 38093, 'kinshino': 38094, 'cobras': 38095, 'thok': 38096, 'smithers': 38097, 'arupa': 38098, 'lampoons': 38099, 'conventionality': 38100, \"desani's\": 38101, 'pirated': 38102, 'cathartic': 38103, 'mythologies': 38104, \"moore's\": 38105, 'villainy': 38106, 'franticness': 38107, 'archnemesis': 38108, \"joker's\": 38109, 'miscreant': 38110, \"'batman\": 38111, 'bolland': 38112, 'miracleman': 38113, 'promethea': 38114, 'endeavouring': 38115, 'rectory': 38116, 'norfolk': 38117, 'eiffel': 38118, 'crinolines': 38119, 'decimal': 38120, 'surds': 38121, 'alligation': 38122, \"banker's\": 38123, 'tabulation': 38124, '•ibps': 38125, 'nabard': 38126, 'idbi': 38127, 'chsl': 38128, 'fci': 38129, 'cpo': 38130, 'asi': 38131, 'gic': 38132, 'uiico': 38133, 'aaos': 38134, 'csat': 38135, 'scra': 38136, \"'d'\": 38137, 'cmat': 38138, 'cet': 38139, 'bbm': 38140, 'ntse': 38141, 'clat': 38142, 'biometric': 38143, 'impinges': 38144, 'biometrics': 38145, 'centralised': 38146, 'campaigners': 38147, 'teltumbde': 38148, 'suraj': 38149, 'yengde': 38150, 'unpack': 38151, 'sukhadeo': 38152, 'bhagavan': 38153, 'semitism': 38154, 'diamonstein': 38155, 'spielvogel': 38156, 'solidifies': 38157, 'sebald': 38158, \"cole's\": 38159, 'ambedkarites': 38160, 'bamcef': 38161, 'bahujan': 38162, 'badri': 38163, 'rouses': 38164, \"dalits'\": 38165, 'protégée': 38166, 'topursue': 38167, \"kanshiram's\": 38168, 'equaled': 38169, 'children—and': 38170, 'adults—for': 38171, '67': 38172, \"highlights'\": 38173, 'surefire': 38174, 'francoise': 38175, 'pasis': 38176, 'paloma': 38177, 'matisse': 38178, 'braque': 38179, 'barrels': 38180, 'upmarket': 38181, 'scobie': 38182, 'distrusted': 38183, 'shove': 38184, \"buchan's\": 38185, \"fleming's\": 38186, 'section—or': 38187, 'psychometricians': 38188, 'prep—kaplan': 38189, 'kaptest': 38190, 'reviving': 38191, \"iacocca's\": 38192, 'rheumatic': 38193, 'lehigh': 38194, 'semesters': 38195, '54th': 38196, 'mammon': 38197, 'hickman': 38198, 'tomm': 38199, 'coker': 38200, 'crypto': 38201, 'occultism': 38202, 'popes': 38203, 'hitmen': 38204, 'dictators': 38205, 'zedong': 38206, 'deified': 38207, 'krushchev': 38208, 'zhisui': 38209, 'dotage': 38210, 'demi': 38211, 'rehabilitate': 38212, '“pain': 38213, 'pb': 38214, 'prognosis': 38215, \"leaders'\": 38216, 'elberse': 38217, 'moritz': 38218, 'longstanding': 38219, 'refusing': 38220, 'encroached': 38221, 'depressants': 38222, 'hari´s': 38223, \"wrong'\": 38224, 'bunking': 38225, 'pakao': 38226, 'fresher': 38227, 'gyan': 38228, \"topper's\": 38229, \"nerd's\": 38230, 'trilingual': 38231, 'etymologies': 38232, 'inflections': 38233, \"this'\": 38234, 'rizzoli': 38235, 'maura': 38236, 'chidren': 38237, \"'crime\": 38238, 'clay―whose': 38239, 'google―shares': 38240, 'minibooks': 38241, 'competitively': 38242, 'no–nonsense': 38243, 'industry–specific': 38244, 'bing': 38245, 'you′re': 38246, 'synoptic': 38247, 'bradman': 38248, 'weeks…': 38249, 'marcello': 38250, 'relishes': 38251, 'challenge—but': 38252, 'fevered': 38253, 'consequences…': 38254, 'resist—especially': 38255, 'whis': 38256, 'moniker': 38257, '“super': 38258, \"ghosts'\": 38259, 'undertakers': 38260, 'shovels': 38261, 'paperwork': 38262, \"'backstory'\": 38263, 'site—albeit': 38264, 'scientists—which': 38265, \"fragments'\": 38266, '2004–2006': 38267, 'colombo': 38268, 'wallflower’': 38269, 'seniors': 38270, 'indulging': 38271, 'prettiest': 38272, 'charlie’s': 38273, 'bronx': 38274, 'impresario': 38275, \"'jerry\": 38276, 'altman': 38277, 'potentates': 38278, 'confidants': 38279, 'zeppelin': 38280, 'fc’s': 38281, 'gérard': 38282, 'houllier': 38283, 'benítez': 38284, 'shambolic': 38285, 'hicks': 38286, 'gillett': 38287, 'interviewees': 38288, 'carragher': 38289, 'xabi': 38290, 'alonso': 38291, 'melwood’s': 38292, 'cardiff': 38293, 'undermined': 38294, 'tress': 38295, 'ug': 38296, 'delacy': 38297, 'shahara': 38298, 'gnomes': 38299, \"geralt's\": 38300, 'danusia': 38301, 'stok': 38302, 'nielit': 38303, 'browse': 38304, 'cordon': 38305, 'prastut': 38306, '‘english': 38307, 'kors’': 38308, 'baajaar': 38309, 'upalabdh': 38310, 'vaalee': 38311, 'pustakon': 38312, 'alag': 38313, 'hatakar': 38314, 'vaigyaanik': 38315, 'paddhati': 38316, 'likhee': 38317, 'gayee': 38318, 'aadhunik': 38319, 'shailee': 38320, 'prayog': 38321, 'kiya': 38322, 'jisase': 38323, 'paathakon': 38324, 'samajhane': 38325, 'dharaapravaah': 38326, 'bolane': 38327, 'aasaanee': 38328, 'hogee': 38329, 'nimnalikhit': 38330, 'khandon': 38331, 'baanta': 38332, 'visheshataen': 38333, 'shabdon': 38334, 'sahee': 38335, 'uchchaaran': 38336, 'shabd': 38337, 'angerajee': 38338, 'vyaakaran': 38339, 'saral': 38340, 'prastuti': 38341, 'pratyek': 38342, 'paristhiti': 38343, 'vyakti': 38344, 'anukool': 38345, 'shreshth': 38346, 'vaartaalaap': 38347, 'patra': 38348, 'lekhan': 38349, 'anuvaad': 38350, 'vistrt': 38351, 'adhyaay': 38352, 'vargeekrt': 38353, 'shabdaavalee': 38354, 'hindee': 38355, 'sammilit': 38356, 'aatm': 38357, 'pareekshan': 38358, 'hetu': 38359, 'abhyaas': 38360, 'sahit': 38361, \"'english\": 38362, 'pronence': 38363, 'wordsword': 38364, 'syntaxsimple': 38365, 'sentence50': 38366, 'englishletter': 38367, 'whatsappclassified': 38368, 'englishwith': 38369, 'karni': 38370, 'sena': 38371, 'padmaavat': 38372, \"muslim'\": 38373, \"impact'\": 38374, \"reeling'\": 38375, 'ladiesfinger': 38376, 'shalini': 38377, \"'hard\": 38378, \"hitting'\": 38379, 'vasant': 38380, 'comets': 38381, \"sky's\": 38382, 'nebulae': 38383, 'stargazing': 38384, 'minnows': 38385, 'bookmakers': 38386, 'hopers': 38387, 'disagreements': 38388, 'selectorial': 38389, 'chandrababu': 38390, 'hardwork': 38391, \"naidu's\": 38392, 'mcallister': 38393, 'straightened': 38394, 'daimler': 38395, 'fixated': 38396, 'normalcy': 38397, 'annelies': 38398, \"frank's\": 38399, 'goats': 38400, 'brauchmann': 38401, \"'hits\": 38402, 'past…': 38403, 'modernise': 38404, 'dredging': 38405, 'stairs…': 38406, 'exorcise': 38407, '‘perfect’': 38408, 'motorboat': 38409, 'sakade': 38410, 'yehezkel': 38411, 'resheff': 38412, 'itay': 38413, 'lieder': 38414, 'audienceófrom': 38415, 'painlessly': 38416, 'bolsheviks': 38417, \"moscow's\": 38418, 'barricade': 38419, 'assertions': 38420, 'varnishes': 38421, \"painter's\": 38422, 'velasquez': 38423, 'tonal': 38424, 'giotto': 38425, 'vermeer': 38426, 'ingres': 38427, 'titian': 38428, 'giorgione': 38429, 'poussin': 38430, 'corot': 38431, 'veronese': 38432, 'leavens': 38433, \"bennett's\": 38434, \"crilley's\": 38435, 'overeager': 38436, 'flustered': 38437, 'phases—nehru': 38438, 'zoogeography': 38439, 'arthropods': 38440, 'invertebrates': 38441, 'amphibians': 38442, 'activational': 38443, 'aqueous': 38444, 'immunization': 38445, 'psocoptera': 38446, 'biomechanics': 38447, 'neurophysiology': 38448, 'immunology': 38449, 'homepages': 38450, 'geologic': 38451, 'tikrit': 38452, 'telltale': 38453, 'tattoos': 38454, 'era’s': 38455, 'strongmen': 38456, 'hussein’s': 38457, 'policymakers―and': 38458, \"blair's\": 38459, '―astray': 38460, 'misreading': 38461, 'pasricha': 38462, 'hunterwali': 38463, '‘fearless’': 38464, 'roofs': 38465, '‘zidane': 38466, 'zinedine': 38467, 'zidane’s': 38468, '£46million': 38469, 'butting': 38470, 'materazzi': 38471, '‘speaks': 38472, 'ball’': 38473, 'durbeyfield': 38474, 'kinship': 38475, \"'cousin'\": 38476, 'censored': 38477, 'nefertiti': 38478, 'beyonce': 38479, 'tackled': 38480, 'agenda—be': 38481, 'terrorise': 38482, 'aggregated': 38483, 'theoretically': 38484, 'litmus': 38485, 'uday': 38486, 'mahurkar': 38487, 'paralysis': 38488, 'salvaging': 38489, 'uprooting': 38490, 'subjecting': 38491, '‘deception': 38492, 'point’': 38493, 'ming': 38494, 'mangor': 38495, 'tolland': 38496, 'marlinson': 38497, 'privatised': 38498, 'ekstrom': 38499, 'tench': 38500, '—holly': 38501, '7s': 38502, 'falkoff’s': 38503, 'outgrow': 38504, 'sam—listen': 38505, 'earbuds': 38506, '—bulletin': 38507, 'lightens': 38508, '—ala': 38509, '“realistic': 38510, '—school': 38511, 'delicacy': 38512, '—publishers': 38513, '“truly': 38514, 'manphodgunj': 38515, 'babuji': 38516, 'jesh': 38517, 'bajpai': 38518, '‘madam': 38519, '38d': 38520, '‘happy': 38521, 'lucky’': 38522, 'gajendra': 38523, 'leaned': 38524, 'workbench': 38525, 'dupatta': 38526, 'cleavage': 38527, 'pouted': 38528, 'palatial': 38529, 'bungalow': 38530, 'minions': 38531, 'yelled': 38532, '‘ramu': 38533, 'jagdamba’': 38534, 'pores': 38535, 'brajesh’s': 38536, 'ramlal': 38537, 'encroachment': 38538, 'feigned': 38539, 'exasperation': 38540, '‘marriz…': 38541, 'marriz…': 38542, 'marriz': 38543, 'chumki': 38544, 'elope': 38545, 'azhar': 38546, 'creams': 38547, 'binny’s': 38548, 'compelled': 38549, '‘gorment’': 38550, 'tarun': 38551, 'sanskari': 38552, 'coming’': 38553, '’kumud’s': 38554, 'chacha': 38555, 'widow’': 38556, 'saturn’': 38557, 'bajaa': 38558, 'thaneeya': 38559, 'mcardle': 38560, 'detach': 38561, \"perfect'\": 38562, 'commuter': 38563, '‘jess': 38564, 'jason’': 38565, 'everything’s': 38566, 'train…': 38567, 'patanjali': 38568, 'ramanuja': 38569, 'paramahamsa': 38570, 'hinduism’s': 38571, 'purusharthas': 38572, 'bhakti': 38573, 'ecumenism': 38574, '‘hinduism': 38575, 'habit’': 38576, 'adherents': 38577, 'deen': 38578, '‘bhakts’': 38579, 'unequivocal': 38580, 'imperilled': 38581, '‘fundamentalists’': 38582, 'tyjustwrite': 38583, 'ibbotson': 38584, \"'get\": 38585, \"'frightful\": 38586, 'shriekers': 38587, 'spectres': 38588, 'brittles': 38589, '133': 38590, 'arcana': 38591, '9781444177398': 38592, '9781444185256': 38593, 'marge': 38594, 'bassist': 38595, 'wooten': 38596, 'notes…and': 38597, '”—multiple': 38598, 'award–winning': 38599, 'saxophonist': 38600, 'brecker': 38601, \"explores'\": 38602, 'iwan': 38603, 'rheon': 38604, 'tremaine': 38605, 'dirt—the': 38606, \"band's\": 38607, 'addictions': 38608, 'tommyland': 38609, 'sixx': 38610, 'photographer’s': 38611, 'anthropomorphised': 38612, '‘tiger': 38613, 'icons’': 38614, '‘brand': 38615, 'ambassadors’': 38616, 'shivang': 38617, 'montane': 38618, 'life—loving': 38619, 'seem—normal': 38620, 'gnaws': 38621, 'punishes': 38622, 'brahmi—a': 38623, 'tiniest': 38624, 'teeniest': 38625, \"bloomwood's\": 38626, \"film'\": 38627, 'flop': 38628, 'sippys': 38629, \"dengzongpa's\": 38630, 'fim': 38631, 'handing': 38632, 'gabbar': 38633, 'amjad': 38634, 'sporty': 38635, 'dream—to': 38636, 'graham’s': 38637, 'coop’s': 38638, 'piper’s': 38639, 'bevy': 38640, 'wants—even': 38641, 'biscayne': 38642, 'ef': 38643, 'wpf': 38644, 'wcf': 38645, 'cil': 38646, 'opcodes': 38647, 'emitting': 38648, 'xaml': 38649, 'seeta': 38650, '3392': 38651, 'manas': 38652, 'radhika’s': 38653, 'sameer’s': 38654, 'cupids': 38655, 'avaricious': 38656, 'ifyou': 38657, 'pummeled': 38658, 'slapped': 38659, 'bantering': 38660, 'proclaims': 38661, 'benevolent': 38662, 'distringuished': 38663, 'pangloss': 38664, \"voltaire's\": 38665, 'technologist': 38666, 'halves': 38667, \"shalvis's\": 38668, 'macomber': 38669, 'babysit': 38670, 'wildstone': 38671, 'cedar': 38672, 'mcarthur': 38673, \"origami's\": 38674, 'joisel': 38675, 'demaine': 38676, 'kwan': 38677, 'lafosse': 38678, 'jeannine': 38679, 'moseley': 38680, 'yoshizawa': 38681, \"mcarthur's\": 38682, 'works—from': 38683, \"cooper's\": 38684, \"mihara's\": 38685, \"joisel's\": 38686, 'pangolin': 38687, 'model—folding': 38688, 'bakshis': 38689, 'imtiaz': 38690, 'alis': 38691, 'nihalanis': 38692, 'ashutosh': 38693, 'gowarikers': 38694, 'faring': 38695, 'elocution': 38696, 'ghais': 38697, 'bhardwaj': 38698, 'harmonium': 38699, 'pragati': 38700, 'maidan': 38701, 'luring': 38702, \"'coben\": 38703, \"coben's\": 38704, 'moreish': 38705, \"'simply\": 38706, \"'harlan\": 38707, 'quadruple': 38708, 'hickcock': 38709, 'picturised': 38710, 'unkind': 38711, 'trampled': 38712, 'harbinger': 38713, 'lohar': 38714, 'mediaeval': 38715, 'mahmud': 38716, 'gonefishing': 38717, 'angler': 38718, 'muhhamad': 38719, 'legalese': 38720, 'learned\\x97until': 38721, 'protects': 38722, 'overconfidence': 38723, 'confidence\\x97confidence': 38724, 'ambassadorship': 38725, 'relocates': 38726, 'romania': 38727, 'jaclyn': 38728, 'pradosh': 38729, 'mitter': 38730, 'jaisalmer': 38731, 'simla': 38732, 'varanasi': 38733, 'buttered': 38734, 'trifling': 38735, \"fiancée's\": 38736, 'disapproves': 38737, \"milady's\": 38738, 'boudoir': 38739, 'trotter': 38740, 'anatole': 38741, \"trotter's\": 38742, 'rhc': 38743, '‘smoke': 38744, 'crafted’': 38745, \"'surrender\": 38746, 'fiction’': 38747, '‘cracking…': 38748, '‘kautilya': 38749, 'machiavelli’': 38750, 'machiavelli’s': 38751, 'discorsi': 38752, 'principe': 38753, 'kautilya’s': 38754, 'text—a': 38755, 'thinking—to': 38756, 'mitra': 38757, 'liebig': 38758, 'romanticizing': 38759, 'bullshit': 38760, 'smarmy': 38761, 'nikhil': 38762, 'distracting': 38763, 'crotchety': 38764, 'seddon—two': 38765, 'world—the': 38766, 'typographers': 38767, \"garfields's\": 38768, 'campaigned': 38769, 'postfeminism': 38770, 'inverting': 38771, \"'proved\": 38772, \"abstract'\": 38773, 'gegenpressing': 38774, 'ralf': 38775, 'rangnick': 38776, 'jürgen': 38777, 'midfielders': 38778, \"bronnie's\": 38779, 'positively': 38780, 'england—and': 38781, 'women…': 38782, 'sheriff’s': 38783, 'them—which': 38784, 'frighten': 38785, 'macallister': 38786, 'booke—a': 38787, 'secrets—and': 38788, 'still…': 38789, 'vociferously': 38790, 'singleminded': 38791, 'aitken': 38792, 'bumptious': 38793, '5a': 38794, '1984–86': 38795, 'marvin': 38796, 'minsky': 38797, 'hopfield': 38798, '“feynmanesque”': 38799, 'reversible': 38800, 'implementers': 38801, \"pikachu's\": 38802, 'cerise': 38803, 'baddest': 38804, 'redoubtable': 38805, 'horace': 38806, 'lacy': 38807, 'ombersleys': 38808, 'pedantic': 38809, 'bluestocking': 38810, 'tiresome': 38811, 'hubert': 38812, 'lender': 38813, \"ombersley's\": 38814, 'meddlesome': 38815, 'gumshoe': 38816, \"dwight's\": 38817, 'numb': 38818, 'gemstones': 38819, 'rockhound': 38820, 'gemmologist': 38821, 'flurry': 38822, 'kable': 38823, 'mercer': 38824, \"florida's\": 38825, 'camino': 38826, \"grandmother's\": 38827, 'baylor': 38828, \"rudy's\": 38829, 'industrialized': 38830, 'inefficient': 38831, 'protectionism': 38832, 'symbolically': 38833, '1836': 38834, 'stylesof': 38835, 'secretarial': 38836, 'stenographic': 38837, 'soundhand': 38838, 'knighted': 38839, '–a': 38840, 'petrescu': 38841, 'bucharest': 38842, 'bandaged': 38843, 'tolkien”': 38844, 'series—as': 38845, 'balance—beset': 38846, 'stone—a': 38847, 'skinchangers': 38848, 'turnings': 38849, '“long': 38850, 'dervish': 38851, 'tellers': 38852, 'muddy': 38853, 'bushwalker': 38854, 'disintegrating': 38855, 'grandpa': 38856, 'birkin': 38857, \"alexa's\": 38858, 'inspo': 38859, 'pump': 38860, 'dynasts': 38861, 'roost': 38862, 'meritocratic': 38863, 'disapproved': 38864, \"understood'\": 38865, \"camus'\": 38866, 'buss': 38867, 'clamence': 38868, 'regales': 38869, 'hollowness': 38870, \"camus's\": 38871, 'nausea': 38872, \"'camus\": 38873, 'afflictions': 38874, 'things—a': 38875, 'adelman': 38876, 'whuch': 38877, 'intorduces': 38878, 'dotty': 38879, 'threepwood': 38880, 'imposter': 38881, \"itself'\": 38882, 'egytian': 38883, 'doctorow': 38884, 'leahy': 38885, 'denny': 38886, 'tollin': 38887, 'batwoman': 38888, 'mite': 38889, 'archvillains': 38890, 'sayre': 38891, \"wayne's\": 38892, 'denys': 38893, 'cowan': 38894, 'parishioners': 38895, \"cathedral's\": 38896, 'reliquary': 38897, 'carjackings': 38898, 'walkmans': 38899, 'slogans': 38900, 'phased': 38901, 'wink': 38902, 'signings': 38903, 'cauldrons': 38904, 'wands': 38905, \"weasleys'\": 38906, 'wheezes': 38907, \"hermione's\": 38908, 'ironbelly': 38909, 'hallows': 38910, 'dating—just': 38911, 'ever—even': 38912, 'brunette': 38913, \"muslims'\": 38914, 'madrasas': 38915, \"'muslim\": 38916, \"issues'\": 38917, 'talaq': 38918, \"'other'\": 38919, 'everydayness': 38920, 'hilal': 38921, '554': 38922, 'structurally': 38923, 'carving': 38924, 'spokes': 38925, 'saurashtra': 38926, 'vindhya': 38927, 'cochin': 38928, 'miscellany': 38929, 'baroda': 38930, 'incorporation': 38931, 'unification': 38932, 'retrospect': 38933, 'inconvenienced': 38934, 'downhill': 38935, 'ramps': 38936, 'wharf': 38937, 'henman': 38938, 'gielgud': 38939, 'voluptuous': 38940, 'impales': 38941, 'modish': 38942, 'fiercer': 38943, \"iraq'\": 38944, \"'richly\": 38945, \"wisdom'\": 38946, \"'completely\": 38947, 'baobab': 38948, 'handstand': 38949, 'coats': 38950, 'jackets': 38951, 'bottoms': 38952, 'tricycles': 38953, 'chika': 38954, 'miyata': 38955, 'follmi': 38956, \"follmi's\": 38957, \"'national\": 38958, \"olympiad'\": 38959, \"achievers'\": 38960, 'stanbury': 38961, 'raycraft': 38962, \"fiancé's\": 38963, 'casket': 38964, 'yellowing': 38965, 'finder': 38966, 'ruiz': 38967, 'zafon': 38968, 'eevee': 38969, 'golding': 38970, 'yeoh': 38971, \"chu's\": 38972, 'mourns': 38973, 'matthes': 38974, \"elkeles'\": 38975, 'elkeles': 38976, 'enchantedbeasts': 38977, 'inthe': 38978, 'isoverworked': 38979, 'thingsmagic': 38980, 'likejust': 38981, 'yorkcity': 38982, 'mysticalallies': 38983, 'wildlybeyond': 38984, 'hissuper': 38985, 'pagechapter': 38986, 'interactiveillustrations': 38987, 'magdalena': 38988, 'mook': 38989, 'icf': 38990, 'whitmore': 38991, 'barclays': 38992, 'thecityuk': 38993, 'jew—muslim': 38994, 'jihadis': 38995, 'tarek': 38996, 'fueled': 38997, 'hadith': 38998, 'supremacist': 38999, 'reinterprets': 39000, 'kkk': 39001, 'novice’s': 39002, 'uncovered…': 39003, 'shakuntala': 39004, 'masquerade': 39005, 'doniger': 39006, 'accomplishes': 39007, 'seashore': 39008, 'meet’': 39009, 'rothenstein': 39010, 'yeats': 39011, '“stirred': 39012, '“these': 39013, 'poet’s': 39014, '“elegant': 39015, 'poetry”': 39016, 'denotes': 39017, '‘précis’': 39018, 'substitutions’': 39019, 'shortening': 39020, 'preccis': 39021, 'ifs': 39022, 'capf': 39023, 'cisf': 39024, 'melmottes': 39025, 'excerable': 39026, 'allowances': 39027, 'whirl': 39028, \"melmotte's\": 39029, '9626': 39030, 'ornamented': 39031, 'classi': 39032, 'kamdani': 39033, 'badla': 39034, 'tilla': 39035, 'danka': 39036, 'gota': 39037, 'zardozi': 39038, 'vasli': 39039, 'mukke': 39040, 'kaam': 39041, 'varak': 39042, 'wanes': 39043, 'yokohama': 39044, 'gai': 39045, 'shogunate': 39046, 'yoshi': 39047, 'toranaga': 39048, \"ancestor's\": 39049, 'diet’': 39050, 'myth’': 39051, 'fitter': 39052, 'grains': 39053, 'right’': 39054, 'physiologist': 39055, 'handles': 39056, 'eradication': 39057, 'repayment': 39058, 'lending': 39059, 'destitution': 39060, 'vanderbilt': 39061, 'rapt': 39062, \"maryam's\": 39063, 'francine': 39064, 'lasala': 39065, \"caraval's\": 39066, 'incitement': 39067, \"bjp's\": 39068, 'vignelli': 39069, \"bierut's\": 39070, 'plump': 39071, 'skinnedfourteen': 39072, 'glamoroussex': 39073, 'thereclusive': 39074, 'relationshipwith': 39075, 'loveaffairs': 39076, 'curiousrelationship': 39077, 'androgynous': 39078, 'farzana': 39079, 'malachy': 39080, 'corr': 39081, 'quizzed': 39082, 'ade': 39083, 'dazzles': 39084, 'ever”': 39085, 'merrick': 39086, 'claymore': 39087, 'taunts': 39088, 'will—until': 39089, 'web…a': 39090, 'irregularities': 39091, 'snares': 39092, \"reed's\": 39093, 'scotvec': 39094, 'ryeland': 39095, 'pund': 39096, 'pye': 39097, 'magpie': 39098, 'izuku’s': 39099, 'might’s': 39100, 'adharanand': 39101, 'iten': 39102, 'higgs': 39103, 'boson': 39104, 'mercantile': 39105, 'larded': 39106, 'precision’s': 39107, '‘iron': 39108, 'mad’': 39109, 'maudslay': 39110, 'bramah': 39111, 'whitworth': 39112, 'hattori’s': 39113, 'seiko': 39114, 'lobsang': 39115, 'rampa': 39116, 'preordained': 39117, 'chakpori': 39118, 'lamasery': 39119, 'astral': 39120, 'projection': 39121, 'gazing': 39122, 'reassuringly': 39123, 'playwrights': 39124, 'kickstart': 39125, 'jiraiya': 39126, 'bonewitz': 39127, 'antiquities': 39128, \"smithsonian's\": 39129, 'malachite': 39130, 'bowdlerization': 39131, 'imperishable': 39132, 'repositories': 39133, 'petrification': 39134, 'grim—until': 39135, 'schooler': 39136, 'petrified': 39137, 'colloboration': 39138, 'storywriter': 39139, 'riichiro': 39140, 'inagaki': 39141, 'eyeshield': 39142, 'boichi': 39143, 'renoir': 39144, 'nocturnes': 39145, 'piazzas': 39146, \"'hush\": 39147, \"floor'\": 39148, 'recedes': 39149, 'turquand': 39150, 'pretender': 39151, 'darryl': 39152, \"rebus'\": 39153, 'ger': 39154, 'cafferty': 39155, \"become'\": 39156, \"tunes'\": 39157, \"'effortless\": 39158, \"throughout'\": 39159, \"riddell's\": 39160, '366': 39161, \"d's\": 39162, 'devendra': 39163, 'countenance': 39164, 'outdid': 39165, 'rupa': 39166, \"novelist'\": 39167, \"gre's\": 39168, 'tsukuyomi': 39169, 'activated': 39170, 'kaguya': 39171, 'ballagarey': 39172, 'hander': 39173, 'leant': 39174, \"tucked'\": 39175, 'stares': 39176, 'slogs': 39177, 'podium': 39178, 'flipside': 39179, \"guy's\": 39180, \"tournament's\": 39181, 'goalscorer': 39182, 'ligne': 39183, 'admiration': 39184, 'serialised': 39185, 'chamberlain’s': 39186, 'judders': 39187, 'fuhrer’s': 39188, 'steams': 39189, '‘grips': 39190, 'superb’': 39191, \"'and'\": 39192, \"'i'\": 39193, \"'likes'\": 39194, \"'has'\": 39195, 'bhubaneshwar': 39196, 'hawkridge': 39197, \"nonesuch'\": 39198, 'broom': 39199, 'nonesuch': 39200, 'eveidence': 39201, \"century'\": 39202, 'criminality': 39203, 'transgressive': 39204, 'inland': 39205, 'marwari': 39206, 'timberg': 39207, 'conserving': 39208, 'researchdriven': 39209, \"lessons'\": 39210, 'businessline': 39211, 'trina': 39212, 'resurrecting': 39213, \"'intrigue\": 39214, 'abrahamson': 39215, 'trailed': 39216, 'phelps’': 39217, 'beijing’s': 39218, '“performance': 39219, 'reality”': 39220, \"substitutions'\": 39221, 'appropriately': 39222, 'katas': 39223, '\\x93three': 39224, 'spirit\\x97and': 39225, 'pervez': 39226, 'rehabilitative': 39227, 'musculoskeletal': 39228, 'kata\\x92s': 39229, 'mistry\\x92s': 39230, 'potboiler': 39231, 'resemblances': 39232, 'mellow': 39233, 'judgemental': 39234, 'rusty’s': 39235, 'chabouté': 39236, 'ardor': 39237, 'weathers': 39238, 'initials': 39239, 'chabouté’s': 39240, 'yours—the': 39241, 'mishal’s': 39242, 'israeli’s': 39243, 'moshe': 39244, 'dayan': 39245, 'netanyahu': 39246, 'avigdor': 39247, 'kahalani': 39248, 'latrun': 39249, 'essed': 39250, \"radhika's\": 39251, \"mermaid's\": 39252, 'bompa': 39253, \"athlete's\": 39254, 'buzzichelli': 39255, 'adapatation': 39256, \"kipling's\": 39257, 'seeonee': 39258, 'cub': 39259, 'cera': 39260, 'dennings': 39261, \"rachel's\": 39262, \"eli's\": 39263, 'libba': 39264, 'maryland': 39265, 'shrimp': 39266, 'trumble': 39267, 'ensnares': 39268, 'replenish': 39269, 'rebound': 39270, 'distractibility': 39271, 'delegate': 39272, \"employees'\": 39273, 'overloaded': 39274, 'elucidating': 39275, 'uncontested': 39276, 'scorecard': 39277, 'allocate': 39278, 'elderings': 39279, 'highbrow': 39280, 'iconoclasts': 39281, \"hungryalists'\": 39282, 'ginsberg': 39283, 'lear': 39284, 'english®': 39285, 'demarcation': 39286, 'kausalya': 39287, 'manoah': 39288, 'bagging': 39289, 'razmirez': 39290, 'viorel': 39291, 'slanting': 39292, 'cheekbones': 39293, 'auditioned': 39294, 'heathcliff': 39295, \"razmirez's\": 39296, 'cheque': 39297, 'fly…': 39298, 'clenching': 39299, 'canadians': 39300, \"clerk's\": 39301, \"marilyn's\": 39302, 'conspicuous': 39303, \"ng's\": 39304, 'eveywhere': 39305, \"'ng\": 39306, 'implore': 39307, \"provoking'\": 39308, \"suzuki's\": 39309, 'bowings': 39310, 'fingerings': 39311, 'tonalization': 39312, 'vibrato': 39313, 'humoresque': 39314, 'dvorák': 39315, 'becker': 39316, 'bourrée': 39317, 'printings': 39318, 'ampv': 39319, 'smartmusic': 39320, \"repository's\": 39321, 'sited': 39322, 'retrieving': 39323, 'reusing': 39324, 'justices': 39325, 'trudeau': 39326, 'polluters': 39327, 'composite': 39328, 'juxtaposition': 39329, 'inconclusively': 39330, 'inculcates': 39331, 'coherently': 39332, 'hiro': 39333, 'hamada': 39334, 'fransokyo': 39335, 'creation—including': 39336, 'maquette': 39337, 'sculpts': 39338, 'colorscripts': 39339, 'more—illuminated': 39340, 'deadlocks': 39341, 'solaris': 39342, 'classwork': 39343, 'photocopying': 39344, 'councillors': 39345, 'heretics': 39346, 'beleaguered': 39347, 'sympathizers': 39348, 'clutched': 39349, 'printshops': 39350, \"catherine's\": 39351, 'litigants': 39352, 'dissolution': 39353, 'heartstone': 39354, \"'scarecrow'\": 39355, 'aba': 39356, 'thanh': 39357, 'nguyen': 39358, 'thi': 39359, 'gloucestershire': 39360, 'falconer': 39361, 'barrow': 39362, 'malvern': 39363, 'fortnum': 39364, 'mason’s': 39365, 'jeopardise': 39366, \"'aesthetics'\": 39367, 'korobi': 39368, \"gift'\": 39369, 'deteriorates': 39370, 'undergrowth': 39371, 'backpackers': 39372, 'rafting': 39373, 'yossi': 39374, 'improvise': 39375, 'forage': 39376, 'cupped': 39377, 'stroking': 39378, '“ruin': 39379, 'auctioned': 39380, 'collectabillia': 39381, 'zaheer': 39382, 'most”': 39383, '‘yuvi’': 39384, 'briar': 39385, 'alpaca': 39386, '“lost': 39387, 'incas': 39388, 'it—and': 39389, 'rashly': 39390, 'dine': 39391, 'wreath': 39392, 'goldendelicius': 39393, 'swap': 39394, 'parsley': 39395, 'grin': 39396, 'uphill': 39397, 'ledgers': 39398, 'dapps': 39399, 'descriptionblockchain': 39400, 'permissioned': 39401, 'forthe': 39402, 'captivity': 39403, 'heirlooms': 39404, 'itself—to': 39405, 'who—like': 39406, 'him—are': 39407, 'embodiments': 39408, 'instilled': 39409, 'hurwitz': 39410, 'bombastic': 39411, 'subjectsfrom': 39412, 'vvip': 39413, 'quickest': 39414, 'typist': 39415, 'sofa…': 39416, 'stenographer': 39417, 'wilbraham': 39418, 'sprawled': 39419, 'o’clock': 39420, 'house…': 39421, 'ofthe': 39422, 'defeatsexperienced': 39423, 'thewomen': 39424, 'tagorehousehold': 39425, 'iaccount': 39426, 'jorasanko': 39427, \"society'\": 39428, 'bryant—now': 39429, 'quickness': 39430, 'can’t”': 39431, '“just': 39432, 'program—but': 39433, 'galle': 39434, 'deoghar': 39435, 'nimdeora': 39436, 'sangram': 39437, 'talukdar’s': 39438, 'c99': 39439, 'wallflowers': 39440, 'peyton': 39441, 'suitors—if': 39442, \"simon's\": 39443, 'outmaneuver': 39444, 'plans—and': 39445, 'diverting': 39446, \"available'\": 39447, \"practicality'\": 39448, \"dictionaries'\": 39449, 'pullman': 39450, 'ramble': 39451, 'wordgames': 39452, \"concrete'\": 39453, 'melvyn': 39454, 'bragg': 39455, \"'enriching'\": 39456, 'tiebreaks': 39457, 'hodgkinson': 39458, 'basel': 39459, \"baba's\": 39460, 'tailspin': 39461, 'deter': 39462, 'curb': 39463, 'unorganized': 39464, 'dented': 39465, 'counterfeiting': 39466, 'widened': 39467, 'workspace': 39468, \"fotedar's\": 39469, 'straightening': 39470, 'chinar': 39471, 'unmade': 39472, 'giani': 39473, 'zail': 39474, 'tiff': 39475, 'signatory': 39476, 'exercised': 39477, 'weinersmith': 39478, 'swarms': 39479, 'toasters': 39480, \"zach's\": 39481, 'weinersmiths': 39482, 'quercus': 39483, '8c': 39484, 'anterograde': 39485, 'getaway': 39486, '—dostoevsky': 39487, 'besetting': 39488, 'life—abject': 39489, 'child—dostoevsky': 39490, 'saintly': 39491, 'myshkin’s': 39492, 'quintessentially': 39493, '“they': 39494, \"mcdaniels'\": 39495, 'maui': 39496, \"'normal\": 39497, \"reporting'\": 39498, \"2012's\": 39499, 'churning': 39500, 'ashwatthama': 39501, 'generously': 39502, 'feshbach': 39503, 'nanomechanics': 39504, 'sagnac': 39505, 'dampen': 39506, 'swerves': 39507, 'behaving': 39508, 'sequestered': 39509, \"jurors'\": 39510, \"garcia's\": 39511, 'smothered': 39512, 'restriction': 39513, 'edicts': 39514, 'precautions': 39515, 'forewarned': 39516, 'forearmed': 39517, 'vinita': 39518, 'gynaecologist': 39519, 'gs': 39520, 'stillbirths': 39521, 'adolescents': 39522, 'fibroids': 39523, 'menstrual': 39524, 'dysfunction': 39525, 'bourgeoisie': 39526, 'veb': 39527, 'pseudocode': 39528, 'emde': 39529, 'boas': 39530, 'curators': 39531, 'herbology': 39532, 'backshall': 39533, 'eccleshare': 39534, 'highfield': 39535, 'kloves': 39536, 'pavord': 39537, 'peake': 39538, 'pore': 39539, 'scrolls': 39540, 'vials': 39541, 'dragon’s': 39542, 'centaurs': 39543, 'witch’s': 39544, 'broomstick': 39545, 'assemblers': 39546, 'linkers': 39547, 'programmer’s': 39548, 'isa': 39549, 'hll': 39550, 'p4': 39551, 'pipelining': 39552, 'fpgas': 39553, 'xilinx': 39554, 'fpga': 39555, 'verilog': 39556, 'vhdl': 39557, 'homeworks': 39558, 'elsevier': 39559, 'benchmarking': 39560, 'intrinsity': 39561, 'fastmath': 39562, 'worcester': 39563, 'thence': 39564, 'sussex': 39565, 'reconciled': 39566, 'chalked': 39567, 'gourmet': 39568, 'redbeard': 39569, 'trellis': 39570, 'manova': 39571, 'multilevel': 39572, 'bansal': 39573, 'afsana': 39574, \"afsana's\": 39575, 'prissy': 39576, 'flirt': 39577, 'testosterone': 39578, 'opd': 39579, 'facetime': 39580, 'updike': 39581, 'threadbare': 39582, \"'pinky'\": 39583, 'pusher': 39584, 'slings': 39585, 'diarists': 39586, 'hilda': 39587, \"grocer's\": 39588, 'grantham': 39589, \"knives'\": 39590, \"followers'\": 39591, \"'townsend\": 39592, 'helms': 39593, 'pranayamas': 39594, 'kriyas': 39595, 'wholesomeness': 39596, 'repositions': 39597, 'jayadeva': 39598, 'treatises': 39599, 'inculcate': 39600, \"'starts\": 39601, 'dd': 39602, '‘lisa': 39603, 'more’': 39604, \"'terror\": 39605, '1848': 39606, 'theses': 39607, 'dialectics': 39608, 'vmmorpg': 39609, \"'sword\": 39610, \"online'\": 39611, 'silica': 39612, 'tamer': 39613, \"tanizaki's\": 39614, 'lacquerware': 39615, 'curbed': 39616, 'affront': 39617, 'triumphalists': 39618, \"'august\": 39619, \"spies'\": 39620, 'spoiling': 39621, 'reasonably': 39622, 'multiplied': 39623, 'lalitaji’s': 39624, 'mil—whose': 39625, 'bratty': 39626, 'catty': 39627, '‘half': 39628, \"'tomorrow\": 39629, \"far'\": 39630, \"'imitation'\": 39631, 'nigerian': 39632, 'camouflage': 39633, 'typo': 39634, 'jonnysun': 39635, 'tweeted': 39636, 'outsider’s': 39637, '‘”i': 39638, 'hav': 39639, 'reflexes’': 39640, '‘prove': 39641, '‘looks': 39642, 'cat’”': 39643, 'poignat': 39644, '“when': 39645, '“im': 39646, 'u’ll': 39647, 'im': 39648, \"mouseford's\": 39649, 'ucles': 39650, 'easton': 39651, 'fourth–highest': 39652, 'city–state': 39653, \"state's\": 39654, 'consisted': 39655, 'racially': 39656, 'colonial–era': 39657, 'shrubs': 39658, 'greening': 39659, 'unabashedly': 39660, 'fostered': 39661, 'navigated': 39662, 'poetry–spouting': 39663, 'jiang': 39664, 'zemin': 39665, 'ideologues': 39666, 'deng': 39667, 'xiaoping': 39668, 'kwa': 39669, 'geok': 39670, 'choo': 39671, '––': 39672, 'hsien': 39673, 'loong': 39674, \"visionary's\": 39675, \"norman's\": 39676, 'mccoy': 39677, \"endangered's\": 39678, 'phrasing': 39679, \"noad's\": 39680, 'noad': 39681, 'anglophile': 39682, \"chaudhuri's\": 39683, 'potshots': 39684, \"indian's\": 39685, 'arisen': 39686, 'unrecognizable': 39687, 'terming': 39688, '455': 39689, 'naisha': 39690, 'ahsian': 39691, 'hardness': 39692, 'garza': 39693, 'onions': 39694, 'weeps': 39695, 'fixings': 39696, 'submitted': 39697, 'establishments': 39698, 'venables': 39699, '1am': 39700, 'abseil': 39701, 'somersaulting': 39702, 'icefield': 39703, 'suffern': 39704, 'lou’s': 39705, 'measure…': 39706, 'abut': 39707, 'hayworth': 39708, 'entices': 39709, \"mall's\": 39710, 'grocer': 39711, \"sonu's\": 39712, 'fumble': 39713, 'dmart': 39714, \"ching's\": 39715, 'sercret': 39716, \"knorr's\": 39717, \"maggi's\": 39718, \"yippie's\": 39719, 'graders': 39720, \"army's\": 39721, 'colombia': 39722, 'commitment—margaret': 39723, 'alva’s': 39724, 'autobiography—details': 39725, 'ministers—indira': 39726, 'legacy—the': 39727, 'alvas': 39728, 'century—margaret': 39729, 'career—her': 39730, 'spats': 39731, 'hawala': 39732, 'denunciation': 39733, 'aicc': 39734, 'mangalore': 39735, 'machinations—courage': 39736, 'politeness': 39737, 'learners’': 39738, 'tata’s': 39739, 'coil': 39740, 'eccentricity': 39741, 'shipwrecked': 39742, 'robinsons': 39743, 'pastor': 39744, 'gns': 39745, '7million': 39746, 'videoscan': 39747, 'theatrically': 39748, 'yajirobe': 39749, 'zip': 39750, 'fasteners': 39751, '“berger’s': 39752, 'reimagine': 39753, 'pieces—essays': 39754, 'translations—which': 39755, 'infuenced': 39756, 'luxemburg': 39757, 'bertolt': 39758, 'brecht': 39759, 'artists—from': 39760, 'present—while': 39761, 'neglecting': 39762, '“landscape”': 39763, 'animating': 39764, 'defnition': 39765, 'landscapes—alongside': 39766, 'portraits—completes': 39767, 'moncrieff': 39768, 'kilmartin': 39769, 'sodom': 39770, 'gomorrah': 39771, \"proust's\": 39772, 'trodden': 39773, \"sita's\": 39774, 'samhita': 39775, 'arni': 39776, 'patua': 39777, 'moyna': 39778, 'chitrakar': 39779, \"subtle'\": 39780, 'craiglockhart': 39781, 'sassoon': 39782, 'wilfred': 39783, \"rivers's\": 39784, 'mending': 39785, \"youth'\": 39786, '“mythical”': 39787, 'awesomely': 39788, 'tomfoolery': 39789, '2075': 39790, 'eulogy': 39791, 'mahendra': 39792, \"chennai's\": 39793, 'thala': 39794, 'wicketkeeping': 39795, 'je': 39796, 'sais': 39797, 'quoi': 39798, \"'dhoni\": 39799, \"helps'\": 39800, \"country'\": 39801, 'overhauled': 39802, 'scaachikoul': 39803, 'outrages': 39804, 'andmortifying': 39805, 'madeher': 39806, 'despair—whether': 39807, 'conversationswith': 39808, 'waxer': 39809, 'vacationinghalfway': 39810, 'thefears': 39811, 'arepointed': 39812, 'aspectof': 39813, 'derision': 39814, 'wherestrict': 39815, 'littleroom': 39816, 'acareer': 39817, 'culturalobserver': 39818, 'koul': 39819, 'atmodern': 39820, 'doordarshan’s': 39821, 'umpteen': 39822, 'disproportionate': 39823, 'unﬂinching': 39824, 'ﬁeld': 39825, 'marginalisation': 39826, 'reporter–editor': 39827, 'cockpit': 39828, 'lakenheath': 39829, 'haywire': 39830, 'orthopaedix': 39831, 'funnily': 39832, 'chieftaincy': 39833, 'alia': 39834, 'piles': 39835, \"esther's\": 39836, 'partially': 39837, 'content1': 39838, 'development4': 39839, 'strings6': 39840, 'lists7': 39841, 'stacks8': 39842, 'quacks9': 39843, 'trees10': 39844, 'heaps11': 39845, 'graphs12': 39846, 'hashing13': 39847, 'merging14': 39848, 'socialise': 39849, 'fostering': 39850, 'luddites': 39851, 'righter': 39852, 'enact': 39853, 'plaza': 39854, 'lessened': 39855, 'reconstructing': 39856, 'reallocating': 39857, '“source': 39858, 'code”': 39859, 'crosswalk': 39860, 'auckland': 39861, 'plazas': 39862, 'livable': 39863, 'tweaks': 39864, 'henderson’s': 39865, 'repurposing': 39866, 'arranging': 39867, '‘where': 39868, 'comedienne': 39869, 'unscientific': 39870, 'chubster': 39871, '‘hilarious’': 39872, 'hingorani': 39873, \"akash's\": 39874, 'adamson': 39875, \"tendler's\": 39876, \"jared's\": 39877, 'supernova': 39878, 'lovebirds': 39879, 'harsh’s': 39880, 'greens…': 39881, 'refuse…': 39882, 'gulfstream': 39883, 'discretion': 39884, 'bahamani': 39885, 'malwa': 39886, 'har': 39887, 'specificities': 39888, 'worksheet': 39889, 'overtime': 39890, 'mistaking': 39891, 'unarguable': 39892, 'budgets': 39893, 'refund': 39894, 'feverish': 39895, 'merlyn': 39896, 'artie': 39897, 'ferment': 39898, 'esports': 39899, '“nathie”': 39900, '“stallion83”': 39901, 'xbox': 39902, 'gamerscore': 39903, 'minecraft’s': 39904, 'involved…': 39905, 'speedily': 39906, 'roblox': 39907, 'terraria': 39908, '‘bewitching…': 39909, \"weep'\": 39910, \"'totally\": 39911, \"'full\": 39912, \"well'\": 39913, 'strang': 39914, 'subspaces': 39915, 'decompositions': 39916, 'marices': 39917, 'compressed': 39918, 'stochastic': 39919, 'baudelaire': 39920, 'ascribe': 39921, \"coleridge's\": 39922, \"scholar's\": 39923, \"'daily\": 39924, \"decathlon'\": 39925, 'adhd': 39926, \"phelps'\": 39927, 'schaller': 39928, 'posterior': 39929, 'alchemist’': 39930, 'happiness…': 39931, 'reality…the': 39932, 'ghengis': 39933, 'friendship…a': 39934, 'destiny…the': 39935, '‘like': 39936, 'river’': 39937, \"coelho's\": 39938, 'ei': 39939, 'eigh': 39940, 'ture': 39941, 'wayne–': 39942, 'uspga': 39943, 'hoover’s': 39944, 'loveably': 39945, 'gallbladder': 39946, 'farmington': 39947, 'dreariness': 39948, 'brothers’': 39949, 'hiromu': 39950, 'arakawa': 39951, '“auto': 39952, 'mail”': 39953, 'bodies…the': 39954, 'making…': 39955, 'gemstone': 39956, 'shivshankar': 39957, 'overt': 39958, \"lanka's\": 39959, 'heft': 39960, 'colossally': 39961, \"'milk\": 39962, \"brother'\": 39963, 'homelife': 39964, 'mullahs': 39965, 'eachother': 39966, 'manhood': 39967, 'stonewalling': 39968, 'anfield': 39969, 'shelf’': 39970, '1727': 39971, 'botox': 39972, 'rant': 39973, 'verdict…': 39974, '‘old': 39975, 'shadows’': 39976, 'administratvie': 39977, 'thinger': 39978, 'finnish': 39979, \"ferrari's\": 39980, 'grands': 39981, '1300': 39982, 'rescuer': 39983, \"storm's\": 39984, 'specter': 39985, 'someone…': 39986, 'stains': 39987, 'insane…': 39988, 'infotainment': 39989, 'schooling': 39990, 'spartacus…': 39991, \"'purer'\": 39992, \"'older'\": 39993, 'castra': 39994, 'sanguinarius': 39995, 'krackao': 39996, 'elephants’': 39997, \"strips'\": 39998, '9780525568063': 39999, 'craftspeople': 40000, 'stencil': 40001, 'paisley': 40002, 'geometrics': 40003, '9781444174595': 40004, '9781444151008': 40005, 'recoverability': 40006, 'virtualize': 40007, 'esxi': 40008, \"stiefvater's\": 40009, 'shiver': 40010, 'culpeper': 40011, 'fakes': 40012, 'markedly': 40013, \"garden'\": 40014, 'hayley': 40015, \"garden's'\": 40016, 'landscaper': 40017, 'kitridge': 40018, 'increasion': 40019, 'starlin': 40020, 'recital': 40021, 'whipping': 40022, 'evoked': 40023, 'savarna': 40024, 'unnamati': 40025, 'sundar': 40026, 'enver': 40027, 'thoughtless': 40028, 'incisional': 40029, 'hitrecord’s': 40030, 'andwriters': 40031, 'tinybook': 40032, 'sourcedcreative': 40033, 'hitrecord': 40034, 'grassrootscreative': 40035, 'levitt': 40036, 'forhis': 40037, 'aforum': 40038, 'theirpeers’': 40039, 'alongsidedean': 40040, 'haspiel’s': 40041, 'vate': 40042, 'comicscollective': 40043, 'jr’s': 40044, 'hitrecordis': 40045, 'collectivehas': 40046, 'asits': 40047, 'supervillain': 40048, 'moloch': 40049, 'ozymandias': 40050, 'weekly’s': 40051, 'style—making': 40052, 'bible’s': 40053, 'flow—and': 40054, 'cariello': 40055, 'fitzchivalry': 40056, 'farseer’s': 40057, 'withywoods': 40058, 'fitzvigilant': 40059, 'chade': 40060, 'chade’s': 40061, 'stableboy': 40062, \"bee's\": 40063, 'elderling': 40064, 'kelsingra': 40065, 'reunions': 40066, 'vivacia': 40067, 'crews': 40068, 'olympus': 40069, 'fuses': 40070, 'geographers': 40071, 'surfers': 40072, 'plainly': 40073, 'extort': 40074, 'thom': 40075, 'furnishings': 40076, 'india—where': 40077, 'dissension': 40078, 'high—while': 40079, 'transitional': 40080, 'deepdive': 40081, 'jaguar': 40082, 'rover': 40083, 'corus': 40084, 'quiver': 40085, 'sevenoaks': 40086, 'way…': 40087, 'implodes': 40088, 'kitty’s': 40089, 'strangers’': 40090, 'automobiles': 40091, 'hatchbacks': 40092, 'saloons': 40093, 'hybrids': 40094, 'forever…': 40095, 'canons': 40096, 'zoos': 40097, 'nudes': 40098, 'you\\x92re': 40099, 'chockfull': 40100, 'helpful\\x97and': 40101, '\\x97drawing': 40102, 'oddparents': 40103, 'remarkablefigures': 40104, 'verrier': 40105, 'elwin': 40106, '1902–64': 40107, 'hedonist': 40108, 'sidedman': 40109, 'asarthur': 40110, 'koestler': 40111, 'lovedthe': 40112, 'monographs': 40113, 'homogeneity': 40114, 'runners—with': 40115, 'results—and': 40116, 'program—in': 40117, 'intensity—is': 40118, 'draining': 40119, 'runners—as': 40120, 'cyclists': 40121, 'triathletes': 40122, 'seekers—can': 40123, 'murakami’s': 40124, 'appropriates': 40125, 'pastoral': 40126, 'hull': 40127, 'duress': 40128, 'superstructure': 40129, 'anvi': 40130, 'gunshot': 40131, 'vratika': 40132, 'crushes': 40133, 'footlights': 40134, 'shaposhnikovs': 40135, 'stalingrad': 40136, \"grossman's\": 40137, 'destiniesin': 40138, 'marnie': 40139, 'kenington': 40140, 'spurn': 40141, 'kyriazis': 40142, 'him—nor': 40143, 'leaps…': 40144, 'apart…piece': 40145, 'piece…': 40146, 'connelly’s': 40147, 'superstore': 40148, 'shoppers': 40149, 'giverny': 40150, 'monet': 40151, 'morval': 40152, \"monet's\": 40153, \"morval's\": 40154, 'leonova’s': 40155, 'luca’s': 40156, 'natasha’s': 40157, 'labels': 40158, 'protogonist': 40159, 'studious': 40160, 'fellowmen': 40161, \"'black\": 40162, \"poet'\": 40163, 'katth': 40164, 'buro': 40165, 'gondho': 40166, 'bichar': 40167, 'bhooturey': 40168, 'khela': 40169, 'ahlaadi': 40170, 'bibliographical': 40171, 'sukumar': 40172, 'albertalli': 40173, \"rishi's\": 40174, 'sucky': 40175, 'awol': 40176, 'smugly': 40177, 'patels': 40178, 'subclause': 40179, 'auntie': 40180, \"sweetie's\": 40181, 'melissa': 40182, 'cruz': 40183, 'folklorists': 40184, \"ananthamurthy's\": 40185, 'samskara': 40186, 'regenstein': 40187, 'guillermo': 40188, 'rodríguez': 40189, \"ramanujan's\": 40190, 'sanzo': 40191, '1883': 40192, 'kimono': 40193, '348': 40194, 'cobain': 40195, 'aberdeen': 40196, 'magazinecoauthor': 40197, \"'vision\": 40198, \"mission'\": 40199, 'kalanidhi': 40200, 'landry': 40201, 'sandling': 40202, 'howls': 40203, \"'nicci\": 40204, \"minute'\": 40205, \"plentiful'\": 40206, \"novels'\": 40207, \"'french\": 40208, 'unguessable': 40209, \"tension'\": 40210, 'jangling': 40211, 'spatters': 40212, 'mused': 40213, 'nypsd': 40214, 'ungrateful': 40215, 'reinhold': 40216, \"jerald's\": 40217, 'kumble': 40218, 'hirwani': 40219, 'ravichandran': 40220, 'padmakar': 40221, 'shivalkar': 40222, 'yuzvendra': 40223, 'chahal': 40224, 'curtly': 40225, 'sarfraz': 40226, 'bowled': 40227, 'gilmour': 40228, 'clarrie': 40229, 'grimmett': 40230, 'vvs': 40231, 'profited': 40232, 'shrikant': 40233, 'countable': 40234, 'uncountable': 40235, 'mispronounced': 40236, 'misspelt': 40237, 'duplication': 40238, 'opting': 40239, 'vanilla': 40240, 'sprinkles': 40241, 'maahis': 40242, 'revitalize': 40243, \"shrine's\": 40244, 'tomoe': 40245, \"tomoe's\": 40246, 'naysaying': 40247, 'neutralise': 40248, \"pages'\": 40249, \"action'\": 40250, 'chintadripet': 40251, '1835': 40252, \"chapman's\": 40253, 'peninsular': 40254, 'rowland': 40255, \"stephenson's\": 40256, 'merged': 40257, 'dwarkanauth': 40258, 'badshahi': 40259, 'chaudhuri': 40260, 'apu': 40261, 'parineeta': 40262, 'barfi': 40263, 'wasseypur': 40264, 'junctions': 40265, 'battlefronts': 40266, 'arup': 40267, 'bogey': 40268, 'chai': 40269, 'arteries': 40270, 'tabish': 40271, 'tuneless': 40272, 'rajah': 40273, \"watzit's\": 40274, 'hoodunnit': 40275, \"watziznehm's\": 40276, \"bard's\": 40277, 'corker': 40278, \"rhyme's\": 40279, 'wrinkle': 40280, 'mouthed': 40281, 'crumbles': 40282, 'jittery': 40283, 'potholes': 40284, 'labours': 40285, 'namesake…': 40286, 'resembled': 40287, 'ridding': 40288, '‘labours’': 40289, 'savitribai': 40290, 'indorekar': 40291, 'accompanist': 40292, 'ghulaam': 40293, 'saab': 40294, 'leela': 40295, 'leela’s': 40296, 'bhavanipur': 40297, 'savitribai’s': 40298, 'satiate': 40299, 'contingent': 40300, 'wolverine': 40301, 'sabertooth': 40302, 'marvel’s': 40303, '7000': 40304, \"gibbons'\": 40305, '—hollywood': 40306, 'recesses': 40307, 'wally—the': 40308, 'flash—can': 40309, 'pervading': 40310, 'rebirth…': 40311, 'industry’s': 40312, 'reis': 40313, 'sciver': 40314, 'expanse': 40315, 'rudimentary': 40316, 'motherboard': 40317, 'bios': 40318, 'smps': 40319, 'tft': 40320, 'scanners': 40321, 'dvi': 40322, 'commoner': 40323, 'cognizance': 40324, 'brainless': 40325, 'satirized': 40326, 'über': 40327, 'tweet': 40328, 'refunds': 40329, 'impeached': 40330, \"cnn's\": 40331, 'tapper': 40332, 'anna’s': 40333, 'russells': 40334, \"sia's\": 40335, '‘indira': 40336, 'pupul': 40337, 'jayakar': 40338, 'gandhi—her': 40339, 'portrait—at': 40340, 'unprejudiced—of': 40341, 'standards—including': 40342, 'works—and': 40343, 'families—american': 40344, 'welsh—through': 40345, 'dewar': 40346, 'crucible': 40347, 'peshkov': 40348, 'volodya': 40349, \"ken's\": 40350, 'debuting': 40351, \"hallowe'en\": 40352, 'tartt': 40353, 'goldfinch': 40354, 'rambling': 40355, 'theodora': 40356, 'hangsaman': 40357, 'sundial': 40358, \"marvellous'\": 40359, 'exerts': 40360, \"spell'\": 40361, \"right'\": 40362, '1new': 40363, 'conjunctures': 40364, 'sexualities': 40365, \"'pénélope\": 40366, '“strong': 40367, 'braver': 40368, 'volcanologists': 40369, 'whisperers': 40370, 'pénélope': 40371, 'blazers': 40372, 'handmaid’s': 40373, 'rori': 40374, \"ana's\": 40375, \"drake's\": 40376, 'easy–to–use': 40377, 'you′ve': 40378, 'math–intensive': 40379, 'probabilities': 40380, \"'intelligent\": 40381, 'denounce': 40382, 'crimefighting': 40383, 'settlers': 40384, 'westerns': 40385, 'novels—the': 40386, 'virginian': 40387, 'wister': 40388, 'cather': 40389, 'zane': 40390, 'gunman’s': 40391, 'brand—tell': 40392, 'caius': 40393, 'preposterus': 40394, 'menhir': 40395, 'crackpot': 40396, 'château': 40397, \"d'if\": 40398, 'marquis': 40399, 'merriville': 40400, 'charis': 40401, 'hen': 40402, 'jessamy': 40403, 'scamp': 40404, \"hannay's\": 40405, 'qadir': 40406, 'dextrous': 40407, 'executor': 40408, 'parallelized': 40409, 'executors': 40410, 'mutts': 40411, 'unscheduled': 40412, 'bombshells': 40413, 'revert': 40414, 'lark': 40415, 'smarts': 40416, 'remake': 40417, 'thumbing': 40418, 'chandipur': 40419, 'bela': 40420, 'couldnt': 40421, 'papads': 40422, 'routledge': 40423, 'colloquials': 40424, 'naples': 40425, 'calaprice': 40426, 'lipscombe': 40427, 'debunked': 40428, 'photoelectric': 40429, 'dragonzord': 40430, \"tommy's\": 40431, 'morphin': 40432, 'masuji': 40433, 'ono': 40434, 'militarism': 40435, 'spitting': 40436, 'ramsays': 40437, 'madnesses': 40438, \"'disreputable'\": 40439, 'hanna': 40440, 'donnelly': 40441, 'captain’s': 40442, 'pampered': 40443, 'malikov': 40444, 'galaxy’s': 40445, 'kady': 40446, 'hypatia': 40447, 'heimdall': 40448, 'kerenza': 40449, 'illuminae': 40450, 'gemina': 40451, 'fortuneteller': 40452, 'kiyoshi': 40453, 'mitarai': 40454, 'umezawa': 40455, 'chopped': 40456, 'azoth': 40457, 'redfield': 40458, 'depressive': 40459, 'depressions': 40460, \"beauty'\": 40461, \"'affecting\": 40462, \"touching'\": 40463, 'alembick': 40464, 'comprosed': 40465, 'aac': 40466, 'aiare': 40467, 'vetted': 40468, \"aac's\": 40469, 'avy': 40470, 'mcmullen': 40471, '“and': 40472, 'completeness': 40473, 'swadeshi': 40474, 'lectures—written': 40475, 'metaphoric': 40476, 'prose—are': 40477, 'farsighted': 40478, 'forebodings': 40479, 'articulated': 40480, 'executable': 40481, '1869': 40482, 'instituting': 40483, 'throes': 40484, 'protesters': 40485, 'sportswriter': 40486, 'brijnath': 40487, 'duce’s': 40488, 'braggadocio': 40489, 'mussolini’s': 40490, 'mafiosi': 40491, 'numero': 40492, 'reverberates': 40493, 'platinum': 40494, 'cinematheque': 40495, 'filmgoers': 40496, 'novelistic': 40497, 'deadlift': 40498, 'plateaus': 40499, 'shredded': 40500, 'percentages': 40501, 'intermittent': 40502, 'argumentative': 40503, 'abolish': 40504, 'recognitions': 40505, 'felicitations': 40506, 'ndtv': 40507, \"la's\": 40508, 'duarte': 40509, \"vibiana's\": 40510, 'donatella': 40511, 'dragna': 40512, 'nurtured': 40513, 'colonized': 40514, 'hindered': 40515, 'cutters': 40516, 'roused': 40517, 'antagonised': 40518, \"'their\": 40519, \"particle'\": 40520, 'kundera': 40521, 'polyphonic': 40522, 'admins': 40523, 'grandchildren': 40524, 'kindled': 40525, 'barsamian': 40526, 'advantagesóitís': 40527, 'nodes': 40528, 'lorin': 40529, 'hochstein': 40530, 'renè': 40531, 'moser': 40532, 'ansibleís': 40533, 'toolís': 40534, 'declarative': 40535, 'needóand': 40536, '“spectacular': 40537, 'blacker': 40538, 'grimmer': 40539, 'plane–the': 40540, 'duffle': 40541, 'enterprising': 40542, 'counting”': 40543, 'recyclable': 40544, 'shadier': 40545, 'annawadi’s': 40546, 'kalu': 40547, 'inching': 40548, 'recession': 40549, 'forevers': 40550, 'worlds—and': 40551, 'library’s': 40552, 'minneapolis': 40553, '”—junot': 40554, 'díaz': 40555, '“reported': 40556, '”—judges’': 40557, '”—amartya': 40558, '“there': 40559, '”—adrian': 40560, 'leblanc': 40561, '”—o': 40562, '“inspiring': 40563, 'twofold': 40564, 'plummeting': 40565, \"worlds'\": 40566, 'staff—of': 40567, 'cypriot': 40568, 'headley’s': 40569, 'incompetence': 40570, '‘levy': 40571, 'ferreting': 40572, 'haseverything': 40573, 'ifthere': 40574, 'freespiritedcara': 40575, 'allhis': 40576, 'thensomething': 40577, 'anotherwoman': 40578, \"sharecropper's\": 40579, 'schoolroom': 40580, 'troopers': 40581, 'aydin': 40582, 'swallow': 40583, 'ala': 40584, \"digest's\": 40585, \"schools'\": 40586, 'marquette': 40587, 'glyph': 40588, 'comicsalliance': 40589, 'muammar': 40590, 'gaddafi': 40591, 'benghazi': 40592, 'hardeep': 40593, 'itch': 40594, 'intervene': 40595, 'junction': 40596, 'topeka': 40597, 'superflu': 40598, 'hambry': 40599, 'delgado': 40600, 'tet': 40601, 'cuthbert': 40602, 'harrier': 40603, '67th': 40604, 'nomina': 40605, 'tion': 40606, 'ministration': 40607, 'traordinary': 40608, 'frac': 40609, 'tured': 40610, 'grappled': 40611, 'perity': 40612, 'dispensable': 40613, '“baddest': 40614, 'ufc': 40615, 'lesner': 40616, 'popular—and': 40617, 'polarizing—figures': 40618, 'summited': 40619, 'rappelled': 40620, 'limestone': 40621, 'leadman': 40622, 'hanc': 40623, 'decoy': 40624, 'entrap': 40625, 'entrapment': 40626, \"claire's\": 40627, \"hawkins'\": 40628, 'plantar': 40629, 'fasciitis': 40630, 'fractures': 40631, 'traven': 40632, 'lives…but': 40633, 'ashford': 40634, 'captives': 40635, 'travens’': 40636, 'dal’s': 40637, 'army—ready': 40638, 'gated': 40639, 'shrug': 40640, 'numbing': 40641, 'neighbours’': 40642, 'trespassing': 40643, '—fundamentals': 40644, '—space': 40645, '—surface': 40646, '—environments': 40647, '—elements': 40648, '—resources': 40649, 'berklee': 40650, 'mash': 40651, 'copyleft': 40652, 'spotify': 40653, 'deficiencies': 40654, 'seagrave': 40655, 'mothering': 40656, \"ways'\": 40657, '—now': 40658, 'vindicated': 40659, 'narrowing': 40660, 'bloom’s': 40661, 'furor': 40662, 'resists': 40663, 'rilke': 40664, 'cadet': 40665, 'gaga': 40666, 'hyde’s': 40667, \"dictionary'\": 40668, \"undergraduates'\": 40669, 'vocalizations': 40670, 'malfunction': 40671, 'vibrates': 40672, 'acupuncture': 40673, 'meridians': 40674, 'blockages': 40675, 'bear’s': 40676, 'mirai’s': 40677, 'republics': 40678, 'tribunals': 40679, 'chand': 40680, 'unwind': 40681, 'slowest': 40682, 'lagom': 40683, 'exhausting': 40684, 'limbed': 40685, 'shaggy': 40686, 'restorative': 40687, '6am': 40688, '‘exercise’': 40689, 'iqs': 40690, 'dispelling': 40691, 'sabhas': 40692, '09': 40693, 'real—but': 40694, 'birds’': 40695, 'biodiversity': 40696, 'biologists': 40697, 'laman': 40698, 'plumes': 40699, \"compton's\": 40700, 'cottrell': 40701, 'boyce': 40702, 'attired': 40703, 'shutdown': 40704, 'liveness': 40705, 'synchronizers': 40706, \"way's\": 40707, \"enough'\": 40708, 'townshend': 40709, 'trevelyan': 40710, 'maxim’s': 40711, 'demachi': 40712, 'malevolence': 40713, '“mister': 40714, 'palinesque': 40715, \"national'\": 40716, 'foregrounded': 40717, 'choric': 40718, 'proliferating': 40719, \"'charges'\": 40720, 'disquiet': 40721, 'unanticipated': 40722, \"'freedom\": 40723, \"square'\": 40724, \"jnu's\": 40725, 'refocusing': 40726, 'historicized': 40727, \"1930's\": 40728, 'busboy': 40729, 'cents': 40730, 'librettos': 40731, 'balanchine': 40732, 'choreographer': 40733, 'cary': 40734, 'selznick': 40735, 'dory': 40736, 'schar': 40737, 'mortis': 40738, 'her—the': 40739, 'wedlock': 40740, \"everybody's\": 40741, \"goodwin's\": 40742, \"burr's\": 40743, 'despots': 40744, \"mandrake's\": 40745, 'crucibles': 40746, 'announcements': 40747, 'nightfalll': 40748, 'bhattoji': 40749, 'dikshita': 40750, 'sidhantakaumudi': 40751, 'sages': 40752, \"borges's\": 40753, 'maurois': 40754, 'spellbinder': 40755, \"'library\": 40756, \"babel'\": 40757, \"'funes\": 40758, \"memorious'\": 40759, \"'pierre\": 40760, 'menard': 40761, \"quixote'\": 40762, 'quixote': 40763, '1899': 40764, \"kafka's\": 40765, 'metamorphosis': 40766, \"eternity'\": 40767, \"spanish'\": 40768, 'woodall': 40769, \"'probably\": 40770, \"prize'\": 40771, 'suranne': 40772, 'unhindered': 40773, 'blocked': 40774, 'obscured': 40775, 'goodies': 40776, 'villanus': 40777, 'unscrupulus': 40778, \"eurasia's\": 40779, \"warrior's\": 40780, 'vexed': 40781, 'cv': 40782, 'sandberg’s': 40783, 'catchphrase': 40784, 'op': 40785, 'organisational': 40786, 'advisors': 40787, 'mellody': 40788, \"friends'\": 40789, 'bethemagic': 40790, \"'wise\": 40791, 'barnett': 40792, 'rakish': 40793, 'edwina': 40794, 'biddable': 40795, \"edwina's\": 40796, \"'quinn's\": 40797, 'regency': 40798, 'hooray': 40799, 'rambhadracharya': 40800, 'nityanand': 40801, \"'hanuman\": 40802, \"chalisa'\": 40803, 'upanishads': 40804, 'endnotes': 40805, 'chanters': 40806, 'instrumentalists': 40807, 'reciters': 40808, \"tulsidas's\": 40809, 'typeface': 40810, 'rejoins': 40811, 'faceoff': 40812, \"aang's\": 40813, \"chopra's\": 40814, \"'lambi\": 40815, \"ghoda'\": 40816, 'diverge': 40817, \"'mere\": 40818, 'parodied': 40819, 'vinay': 40820, 'dialectic': 40821, 'preliminaries': 40822, 'awt': 40823, 'jelavich': 40824, \"'sean\": 40825, 'mcmeekin': 40826, \"greenmantle'\": 40827, 'unresearched': 40828, \"mcmeekin's\": 40829, 'lamentable': 40830, 'gallipoli': 40831, \"'scotland's\": 40832, \"thrones'\": 40833, 'sassenach': 40834, 'jacobites': 40835, 'redcoats': 40836, 'hostel…': 40837, 'kleptomania': 40838, 'vandalized': 40839, 'flannel': 40840, 'rucksack': 40841, 'congratulated': 40842, '‘unique': 40843, 'problem’': 40844, 'gus': 40845, \"mccrae's\": 40846, 'slicker': 40847, 'witless': 40848, 'lorena': 40849, \"gus's\": 40850, 'hellhole': 40851, 'lonesome': 40852, 'laredo': 40853, 'johar': 40854, 'flagship': 40855, 'srk': 40856, 'kajol': 40857, 'aib': 40858, 'gangland': 40859, 'latvia': 40860, \"wallander's\": 40861, 'enzyme': 40862, 'prolong': 40863, \"numa's\": 40864, 'klinger': 40865, 'declarer': 40866, '“timeless': 40867, 'love”': 40868, '90’s': 40869, 'extraordinaries': 40870, 'eos': 40871, 'nemeses': 40872, 'esthetics': 40873, 'discontinuity': 40874, '199': 40875, 'nicknames': 40876, 'bucknor': 40877, 'rambo': 40878, 'tangier': 40879, 'getty': 40880, 'wholeness': 40881, 'stabilize': 40882, 'gel': 40883, 'watercolors': 40884, 'optimise': 40885, 'seduced…': 40886, 'chambermaid': 40887, 'sicily’s': 40888, 'conti': 40889, 'shamed…': 40890, 'bastiano’s': 40891, 'flickers': 40892, 'child…by': 40893, 'motd': 40894, 'paz': 40895, 'ketch': 40896, 'heaviness': 40897, 'refuel': 40898, '‘success’': 40899, '‘meaning’': 40900, 'to’': 40901, 'turbo': 40902, 'roshen': 40903, 'dalal': 40904, 'malavikagnimitram': 40905, 'agnimitra': 40906, 'malavika': 40907, 'interludes': 40908, 'srinivas': 40909, \"reddy's\": 40910, \"dramatist's\": 40911, 'mancini': 40912, \"'saves\": 40913, 'cheques': 40914, 'cruises': 40915, 'hinkley': 40916, 'horseracing': 40917, 'sauna': 40918, 'boiling': 40919, \"see'\": 40920, \"intrigue'\": 40921, \"felix's\": 40922, '“gary': 40923, '—dick': 40924, 'costolo': 40925, 'vaynerchuk—one': 40926, 'bloomberg': 40927, 'businessweek’s': 40928, '“20': 40929, 'follow”—looks': 40930, 'wilfrid': 40931, 'decipherment': 40932, 'voynichese': 40933, 'ritwika': 40934, \"aditi's\": 40935, \"jayant's\": 40936, 'indefinite': 40937, 'lokesh': 40938, 'groundswell': 40939, 'whittling': 40940, 'foraging': 40941, 'retro': 40942, 'chime': 40943, 'diversified': 40944, 'namaste': 40945, \"'shanti'\": 40946, 'acrobat': 40947, 'subdues': 40948, 'batcave': 40949, 'elfixaliens': 40950, 'spacemice': 40951, 'elfixhave': 40952, 'stiltonixand': 40953, 'donate': 40954, 'calibre': 40955, 'afremow': 40956, 'heisman': 40957, 'puck': 40958, 'grafity': 40959, 'tenements': 40960, 'rapper': 40961, 'chasma': 40962, 'saira': 40963, 'impeded': 40964, 'daughter’s': 40965, 'lammenais': 40966, 'california’s': 40967, 'napa': 40968, 'vineyards': 40969, 'winery': 40970, 'christophe’s': 40971, 'bordeaux': 40972, 'joy’s': 40973, 'camille’s': 40974, 'armentrout': 40975, 'lux': 40976, 'katy': 40977, 'luxen': 40978, 'sift': 40979, 'outlier': 40980, 'imploring': 40981, \"1940's\": 40982, 'parlors': 40983, 'cuffs': 40984, 'conversant': 40985, 'javascriptís': 40986, 'theyíre': 40987, 'delegationóa': 40988, 'cloned': 40989, 'ìyou': 40990, 'jsî': 40991, 'trickier': 40992, 'youíd': 40993, 'mixin': 40994, 'jsís': 40995, 'oloo': 40996, 'comprehendible': 40997, 'bayross': 40998, 'powerweb': 40999, 'glengarry': 41000, 'plow': 41001, 'directing—from': 41002, 'room—to': 41003, \"university's\": 41004, 'intelligence—just': 41005, '—sidney': 41006, 'jojo’s': 41007, 'multigenerational': 41008, 'joestar': 41009, 'joestar’s': 41010, 'hateful': 41011, '“li': 41012, 'unconditionally': 41013, 'hughart': 41014, '”—anne': 41015, 'mccaffrey': 41016, 'wiseman': 41017, 'adventures—and': 41018, 'perturbed': 41019, 'espouses': 41020, 'culture’s': 41021, '‘blockchain’': 41022, 'plumber': 41023, 'lot’': 41024, 'cybercurrencies': 41025, 'gatekeepers': 41026, 'lawless': 41027, '‘fascinating’': 41028, '‘not': 41029, 'system’': 41030, 'mauldin': 41031, 'holds’': 41032, 'eswar': 41033, 'ethem': 41034, 'mainframes': 41035, 'segmentation': 41036, \"courtney's\": 41037, \"'robber's\": 41038, \"road'\": 41039, \"'wilbur\": 41040, 'benchmarks': 41041, \"compared'\": 41042, 'masterstory': 41043, \"teller'\": 41044, 'leavenworth': 41045, \"bobby's\": 41046, \"'whichever\": 41047, 'commandeer': 41048, '‘jeeves': 41049, \"‘don't\": 41050, 'assuring': 41051, '‘sublime': 41052, 'elton': 41053, \"'solomon\": 41054, 'beckon': 41055, \"mi6's\": 41056, 'gawky': 41057, 'synced': 41058, 'hiatus': 41059, 'dooni': 41060, 'chaar': 41061, 'agneepath': 41062, 'brio': 41063, 'khullam': 41064, 'khulla': 41065, 'neetu': 41066, 'bookend': 41067, 'can\\x92t': 41068, 'dixon\\x92s': 41069, 'fayter': 41070, 'cheddar': 41071, 'jake\\x92s': 41072, 'hadn\\x92t': 41073, 'remembered\\x85and': 41074, 'bends': 41075, \"'mora\": 41076, \"le'\": 41077, 'lyric': 41078, \"ho'\": 41079, \"nature's\": 41080, 'chronicles—introducing': 41081, 'o’sullivan': 41082, 'wolfhound': 41083, 'old—when': 41084, 'wields': 41085, 'fragarach': 41086, 'answerer': 41087, 'power—plus': 41088, 'attorneys': 41089, 'irish—to': 41090, 'hearne’s': 41091, 'hexed': 41092, \"'general\": 41093, \"training'\": 41094, 'parameswaran': 41095, 'sartorial': 41096, 'affluence': 41097, \"'only\": 41098, \"vimal'\": 41099, 'jawan': 41100, \"kisan'\": 41101, \"'jo\": 41102, 'kare': 41103, \"pyaar'\": 41104, 'tuffs': 41105, 'creak': 41106, 'floorboard': 41107, 'appall': 41108, \"lalli's\": 41109, 'yen': 41110, 'houseguest': 41111, 'hissterical': 41112, 'financée': 41113, 'know…his': 41114, 'blushing': 41115, 'tow—yon': 41116, 'mu': 41117, 'purrsuaded': 41118, 'cuteness': 41119, \"horowitz's\": 41120, 'late’': 41121, 'bludgeoned': 41122, 'lafite': 41123, 'vallabhardboundhai': 41124, 'jhaverbhai': 41125, \"mind'\": 41126, \"'effortlessly\": 41127, 'colonise': 41128, 'royalties': 41129, 'wrox': 41130, '9781119038634': 41131, '9781118907443': 41132, 'ciel': 41133, \"ciel's\": 41134, 'fullmetal': 41135, 'armor': 41136, \"philsopher's\": 41137, 'metamorphoses': 41138, 'forthrightly': 41139, 'orsatti': 41140, 'toughen': 41141, 'enristine': 41142, 'jeannie': 41143, 'concubine': 41144, 'valmiki': 41145, \"stepmother's\": 41146, 'crutch': 41147, 'countryfile': 41148, '850': 41149, \"gooley's\": 41150, 'walkers': 41151, 'calamities': 41152, \"suspenseful'\": 41153, 'taylor’s': 41154, \"'until\": 41155, \"maurier's\": 41156, 'underwriting': 41157, 'wolfson': 41158, 'fluctuating': 41159, 'ottomans': 41160, 'interdependence': 41161, 'traumatised': 41162, 'hypnotise': 41163, 'tying': 41164, 'intimidation': 41165, 'boroughs': 41166, 'offences': 41167, \"asaram's\": 41168, 'godhood': 41169, 'captioning': 41170, 'factorization': 41171, 'radial': 41172, 'rbf': 41173, 'boltzmann': 41174, 'kohonen': 41175, 'wirebound': 41176, 'godard': 41177, 'uttam': 41178, 'koontz': 41179, 'storyteller’s': 41180, 'grill': 41181, 'llewellyn': 41182, 'hyena': 41183, 'storm…': 41184, \"austin's\": 41185, 'throttled': 41186, 'cricket—india’s': 41187, 'sport—packed': 41188, 'sport—going': 41189, 'monkeygate': 41190, 'patronize': 41191, 'chartered': 41192, 'kwik': 41193, 'battersea': 41194, 'yves': 41195, 'laurent': 41196, 'threat—goku': 41197, 'psychics': 41198, '2380': 41199, 'cadets': 41200, 'boneheaded': 41201, 'dregs': 41202, 'sociopath': 41203, 'bunkmates': 41204, 'techwiz': 41205, \"galaxy's\": 41206, \"ty's\": 41207, \"that'd\": 41208, 'jie': 41209, \"o'malley\": 41210, 'cryo': 41211, 'auri': 41212, \"strunk's\": 41213, 'affordably': 41214, 'enclose': 41215, 'parenthetic': 41216, 'participial': 41217, 'conformity': 41218, 'impracticable': 41219, 'r60': 41220, 'pub': 41221, 'fended': 41222, 'biker': 41223, 'satnavs': 41224, '‘cracking': 41225, 'sambalpore': 41226, 'maharaja’s': 41227, 'adhir': 41228, '‘surrender': 41229, 'not’': 41230, 'sapphic': 41231, \"bechdel's\": 41232, \"satrapi's\": 41233, \"alison's\": 41234, 'closeted': 41235, 'babysitter': 41236, 'euphoric': 41237, 'descriptionmanaging': 41238, 'prepped': 41239, 'cdn': 41240, 'cognito': 41241, 'dynamodb': 41242, 'sqs': 41243, 'sns': 41244, 'kinesis': 41245, 'forfor': 41246, 'glazer': 41247, \"elephant'\": 41248, \"bush'\": 41249, \"giant'\": 41250, 'drolly': 41251, 'quotidian': 41252, 'misappropriations': 41253, 'indianisms': 41254, 'condensation': 41255, 'flashcard': 41256, 'gratify': 41257, '‘stranger’': 41258, \"'forget\": 41259, \"stranger'\": 41260, 'hallucination': 41261, 'hiya': 41262, '‘forget': 41263, 'spun': 41264, 'starbucks': 41265, 'continues…': 41266, 'orpheus': 41267, 'rages—and': 41268, 'vengeance—only': 41269, 'dominoes': 41270, 'malana': 41271, 'chores': 41272, \"saoirse's\": 41273, '“steps': 41274, 'architecture”': 41275, 'coa': 41276, 'orthographic': 41277, 'almighty': 41278, 'enshrined': 41279, 'visibly': 41280, 'inclusivity': 41281, 'cringe': 41282, 'bundled': 41283, 'strippers': 41284, 'environmentalists': 41285, 'navidson': 41286, 'growl': 41287, 'sensationalist': 41288, 'afflicts': 41289, 'unrealized': 41290, \"'demographic\": 41291, \"dividend'\": 41292, 'innards': 41293, 'sown': 41294, 'trumphere': 41295, 'skyline': 41296, 'zeros': 41297, \"maker's\": 41298, 'unguarded': 41299, 'streetwise': 41300, 'vegeta’s': 41301, 'fare': 41302, 'freeza’s': 41303, 'ginyu': 41304, 'force—a': 41305, 'namek': 41306, 'hope…as': 41307, 'resolutely': 41308, 'fallibility': 41309, 'sharpened': 41310, 'prodger': 41311, \"'infinitely\": 41312, 'sundaytimes': 41313, \"millennium's\": 41314, \"'isaacson\": 41315, \"'isaacson's\": 41316, \"vinci's\": 41317, 'vigorous': 41318, 'mccullough': 41319, 'salvator': 41320, 'mundi': 41321, 'gbp341': 41322, 'flirted': 41323, 'vitruvian': 41324, 'peeled': 41325, 'cornea': 41326, 'instilling': 41327, \"hearthbreaking'\": 41328, 'strap': 41329, 'doesn´t': 41330, \"carey's\": 41331, 'china–pakistan': 41332, '‘disputed': 41333, 'territory’': 41334, 'policemen': 41335, 'knotted': 41336, 'matchstick': 41337, 'preet': 41338, 'timurid': 41339, 'unmarried': 41340, 'minted': 41341, 'patronised': 41342, 'khanzada': 41343, 'parley': 41344, 'gulbadan': 41345, 'akbar’s': 41346, 'jiji': 41347, 'maham': 41348, 'jahangir’s': 41349, 'jahan’s': 41350, 'daityas': 41351, 'mallikarjuna': 41352, 'unjustly': 41353, 'naraka': 41354, 'daitya': 41355, 'foldout': 41356, 'underlays': 41357, 'overlays': 41358, 'acrylics': 41359, 'casein': 41360, \"lasch's\": 41361, 'narcissism': 41362, \"postman's\": 41363, 'hedges': 41364, 'wwf': 41365, 'kohut': 41366, 'conservatory': 41367, 'trawls': 41368, 'porn': 41369, 'cooker': 41370, 'klemmer': 41371, 'voyeurism': 41372, 'masochism': 41373, 'elfreide': 41374, \"jelinek's\": 41375, 'jelinek': 41376, \"'musical\": 41377, 'subjugating': 41378, 'haneke': 41379, 'huppert': 41380, '9781444106831': 41381, 'appointees': 41382, 'uninformed': 41383, 'linchpins': 41384, 'proactivity': 41385, 'there…it’s': 41386, 'mattresses': 41387, 'nailed': 41388, 'malerman’s': 41389, '‘nonalignment’': 41390, 'zorawar': 41391, 'daulet': 41392, 'worldviews': 41393, 'geopolitics': 41394, 'precepts': 41395, 'exertions': 41396, 'apposite': 41397, 'ameerpet': 41398, 'santoshtechnologies': 41399, 'jsf': 41400, 'innominds': 41401, 'esoft': 41402, 'blackbook': 41403, 'hql': 41404, 'orm': 41405, '‘boutique': 41406, \"firm'\": 41407, 'specialise': 41408, 'divorces': 41409, 'incessantly': 41410, 'southwest': 41411, 'krayoxx': 41412, 'varrick': 41413, 'stoughton': 41414, 'outspokenly': 41415, 'lingo': 41416, '£300': 41417, 'flatten': 41418, 'rebounds': 41419, '\\x93finally': 41420, '\\x93dan': 41421, 'university\\x93this': 41422, 'i\\x92ll': 41423, 'unc': 41424, 'greensboro': 41425, '\\x93soccer': 41426, 'there\\x92s': 41427, 'fluff': 41428, 'holeman': 41429, 'obeyed': 41430, 'postmodernism': 41431, 'groves': 41432, 'stockman': 41433, 'planting': 41434, \"'record'\": 41435, \"subject's\": 41436, \"'zoom'\": 41437, 'mic': 41438, 'graduations': 41439, '‘i’ll': 41440, 'mentor—both': 41441, 'tis': 41442, 'cornel': 41443, 'mfa': 41444, 'unflattering': 41445, 'infian': 41446, 'intelligentsia': 41447, 'vajpeyee': 41448, 'swamy': 41449, 'swab': 41450, 'scribble': 41451, 'dreamoji': 41452, 'fy': 41453, 'nadiya': 41454, 'wyvernage': 41455, 'amirs': 41456, 'headscarf': 41457, 'kowtow': 41458, 'unsavoury': 41459, 'detainment': 41460, 'mulls': 41461, 'sprout': 41462, 'premiered': 41463, 'anoosh': 41464, 'mattias': 41465, 'ripa': 41466, 'europa': 41467, \"carrick's\": 41468, 'whiteout': 41469, 'oxenford': 41470, 'sniffs': 41471, 'racking': 41472, 'boning': 41473, 'graff': 41474, \"seles'\": 41475, \"un's\": 41476, 'despondent': 41477, 'is—each': 41478, 'bedridden': 41479, 'spaceboy': 41480, 'kraken': 41481, 'littlest': 41482, 'massacres…all': 41483, 'jfk': 41484, 'bookexpo': 41485, '—grant': 41486, 'penciller': 41487, 'inker': 41488, 'invitied': 41489, \"'genius'\": 41490, \"'stunning'\": 41491, \"'gripping'\": 41492, \"engrossing'\": 41493, 'searched': 41494, 'combs': 41495, \"siobhan's\": 41496, \"'rebus\": 41497, \"writing's\": 41498, \"'rankin\": 41499, 'elly': 41500, \"'definitely\": 41501, \"'thrillingly\": 41502, \"'masterful\": 41503, 'grazia': 41504, \"'intriguing\": 41505, \"'complex\": 41506, 'ludlum': 41507, 'caravaggio': 41508, 'monsignor': 41509, 'luigi': 41510, \"michelangelo's\": 41511, 'service—and': 41512, 'razor’s': 41513, 'sosaku': 41514, 'kobayashi': 41515, 'totto': 41516, 'tetsuko': 41517, 'kuroyanagi': 41518, 'rods': 41519, 'clunkers': 41520, 'royces': 41521, 'bentleys': 41522, 'davidsons': 41523, 'ames’s': 41524, 'bynum': 41525, 'lamar': 41526, 'mumford’s': 41527, 'roomed': 41528, 'erving': 41529, 'numbed': 41530, 'zinn’s': 41531, 'cushion': 41532, 'team—someone': 41533, 'yoda': 41534, '—george': 41535, 'history\\x97most': 41536, 'dispensation': 41537, 'morarji': 41538, 'exaggerations': 41539, 'ethnically': 41540, 'linguistically': 41541, 'eluding': 41542, 'generalization': 41543, 'demography': 41544, 'bawdiest': 41545, 'watered': 41546, 'shaman': 41547, 'unidentified…wait': 41548, 'o’brien': 41549, 'loudly': 41550, 'cronus': 41551, 'narcissus': 41552, 'masturbate': 41553, 'discounts': 41554, 'bestiality': 41555, 'ganesh': 41556, 'fetus': 41557, 'thigh': 41558, 'sekhmet': 41559, 'parties…on': 41560, 'freyja': 41561, 'consented': 41562, 'gangbang': 41563, 'transactional': 41564, 'marketer−consumer−influencer': 41565, 'b2b': 41566, 'microblogging': 41567, 'wordpress': 41568, 'tumblr': 41569, 'skype': 41570, 'quora': 41571, 'classifications': 41572, '1g': 41573, 'wap': 41574, 'marketer−consumer': 41575, 'localize': 41576, 'narrowcast': 41577, 'dollars’': 41578, 'pats': 41579, 'adventuresome': 41580, 'buscema’s': 41581, 'indecisive': 41582, 'annulment': 41583, 'kinsella’s': 41584, 'shopper': 41585, \"mum's\": 41586, 'glade': 41587, 'venetian': 41588, 'fin': 41589, 'siecle': 41590, \"'no'\": 41591, 'spring’s': 41592, 'angra': 41593, 'disappeared—thanks': 41594, 'cordellan': 41595, 'primoria’s': 41596, 'chasm’s': 41597, 'safe—even': 41598, 'jannuari—leaving': 41599, 'cordell’s': 41600, 'freedom—and': 41601, 'fumio': 41602, \"vitalstatistix'\": 41603, \"chief's\": 41604, 'abba': 41605, 'inversion': 41606, 'removing': 41607, 'sachdev': 41608, 'adieu': 41609, 'imitate': 41610, 'severs': 41611, \"comedians'\": 41612, 'catharsis': 41613, 'rattle': 41614, 'her—not': 41615, '“sexiest': 41616, 'alive”': 41617, 'heartthrob’s': 41618, '‘sleepless': 41619, 'rachel’s': 41620, '‘morgan': 41621, 'along…': 41622, 'matt–but': 41623, 'race…': 41624, 'umbra': 41625, \"watch's\": 41626, \"umbra's\": 41627, 'fiedler': 41628, 'passports': 41629, 'safehouses': 41630, 'overflow': 41631, 'bounded': 41632, 'restrict': 41633, \"'week'\": 41634, \"'slow'\": 41635, \"'clothes'\": 41636, \"'nothing'\": 41637, \"conrad's\": 41638, 'chrismas': 41639, 'o’neil': 41640, 'snowflakes': 41641, 'sleigh': 41642, '“sir': 41643, 'mingle': 41644, 'donne': 41645, 'shan’t': 41646, '“homosexuality': 41647, 'gen': 41648, 'squared': 41649, 'srm': 41650, 'anuradha': 41651, 'chandresh': 41652, 'lofty': 41653, 'romanised': 41654, 'chapterisation': 41655, 'aule': 41656, 'murderess': 41657, 'solando': 41658, \"ashecliffe's\": 41659, 'sedai': 41660, \"reveal'\": 41661, \"phenomenon'\": 41662, 'francke': 41663, \"region's\": 41664, 'refuges': 41665, 'knowledgably': 41666, 'stimulated': 41667, 'revived': 41668, 'designating': 41669, \"'tibetan\": 41670, 'peissel': 41671, 'zanskar': 41672, 'rupshu': 41673, 'nubra': 41674, 'dahhanu': 41675, 'aryans': 41676, 'sexiest': 41677, 'girder': 41678, \"abra's\": 41679, \"cody's\": 41680, 'imprints': 41681, 'siachen': 41682, 'borderless': 41683, \"'tomorrow'\": 41684, \"'today'…\": 41685, 'direction’s': 41686, 'awards—winning': 41687, 'encoremusicbooks': 41688, 'sketches—complete': 41689, 'habits—now': 41690, 'up—dark': 41691, 'mothers’': 41692, 'juvenilia': 41693, 'england”': 41694, 'fashioning': 41695, 'jagdish': 41696, 'comparatively': 41697, \"comic's\": 41698, \"mcdermott's\": 41699, 'badder': 41700, \"'beautiful'\": 41701, \"kari's\": 41702, \"stalker's\": 41703, \"'triangle'\": 41704, 'therein': 41705, \"mo's\": 41706, \"jonathan'\": 41707, 'personals': 41708, \"present'\": 41709, 'bishops': 41710, 'modelesque': 41711, 'amber’s': 41712, 'insinuates': 41713, 'daphne’s': 41714, 'unattempted': 41715, 'verifying': 41716, 'crystallize': 41717, 'conceptualizations': 41718, '‘licence': 41719, 'impinge': 41720, 'theorizes': 41721, 'meats': 41722, 'potage': 41723, 'malayali': 41724, 'asgardian': 41725, 'uru': 41726, 'bith': 41727, 'asgard': 41728, \"'flotsam\": 41729, \"jetsam'\": 41730, 'plantations': 41731, \"scar'\": 41732, \"wrath'\": 41733, 'reprobate': 41734, 'maidservant': 41735, 'stirrings': 41736, 'again—to': 41737, '‘sujata': 41738, 'heroine’s': 41739, 'story’—amulya': 41740, 'malladi': 41741, 'montezuma': 41742, 'cached': 41743, 'trappers': 41744, 'rafen': 41745, 'falvey': 41746, \"mcgregor's\": 41747, 'breeder': 41748, 'patenting': 41749, 'collectables': 41750, 'undeveloped': 41751, 'nutkin': 41752, 'gloucester': 41753, 'flopsy': 41754, 'bunnies': 41755, 'tittlemouse': 41756, 'timmy': 41757, 'tiptoes': 41758, 'tod': 41759, 'pigling': 41760, 'moppet': 41761, 'appley': 41762, \"dapply's\": 41763, 'cecily': 41764, \"parsley's\": 41765, 'freezy': 41766, 'zap': 41767, \"tinkletrousers'\": 41768, '9780439014571': 41769, '9780439995443': 41770, '9780439997102': 41771, '9780439998192': 41772, 'xerxes': 41773, 'sumant': 41774, 'moolgaokar': 41775, 'jquery…': 41776, 'conform': 41777, 'sequentially': 41778, 'programmatic': 41779, 'bookexplains': 41780, 'performanceimprovements': 41781, 'javaprogramming': 41782, 'spliterators': 41783, 'microbenchmarking': 41784, 'catrina': 41785, 'cystic': 41786, 'fibrosis': 41787, 'salty': 41788, 'initiates': 41789, 'bacchus': 41790, 'naves': 41791, 'irresistable': 41792, 'revulsion': 41793, '‘marriage’': 41794, 'sunita’s': 41795, 'horoscopes': 41796, 'plunging': 41797, 'morphed—in': 41798, 'months—from': 41799, 'rich’s': 41800, 'epic5—five': 41801, 'reservoirs': 41802, 'untapped': 41803, 'cavemice': 41804, 'megalithic': 41805, 'stiltonoot': 41806, 'saber': 41807, 'toothed': 41808, 'tragical': 41809, 'mckean’s': 41810, 'ala’s': 41811, 'avoid—the': 41812, 'are…': 41813, 'don’t”': 41814, 'sidney’s': 41815, 'without…': 41816, 'firefly': 41817, 'flashlight': 41818, 'tsukino': 41819, 'senshi': 41820, 'alfheim': 41821, 'asuna': 41822, 'alo': 41823, 'gameplay': 41824, 'spriggan': 41825, \"asuna's\": 41826, 'salamanders': 41827, 'sylph': 41828, 'pixie': 41829, 'thibault': 41830, 'bentley': 41831, \"'whether\": 41832, 'suspects—the': 41833, 'sasadhar': 41834, 'helmut': 41835, 'aplomb': 41836, \"egypt's\": 41837, 'occultists': 41838, 'fakirs': 41839, 'hajj': 41840, \"mohammad's\": 41841, 'osiris': 41842, 'illumines': 41843, 'nourishment': 41844, 'needleman': 41845, '‘fiction': 41846, 'incisive’': 41847, 'calicut': 41848, 'undistinguished': 41849, 'everytown': 41850, 'illiterate': 41851, 'sexologist': 41852, 'cartographer’s': 41853, 'novelist’s': 41854, 'aravind': 41855, 'composes': 41856, 'webcrawler': 41857, 'tuberculosis': 41858, 'patil': 41859, 'khandesh': 41860, 'freeships': 41861, 'scholarships': 41862, 'wishers': 41863, 'dint': 41864, \"rajesh's\": 41865, 'odia': 41866, 'undefeated': 41867, \"squash's\": 41868, 'expectant': 41869, 'age—thomas': 41870, 'westinghouse—battled': 41871, 'vied': 41872, 'jonnes': 41873, 'folksy': 41874, 'pittsburgh': 41875, 'gaslight': 41876, 'plentiful': 41877, 'hurly': 41878, 'burly': 41879, 'eeriest': 41880, 'chamber—jonnes': 41881, 'hallway': 41882, 'kemmler': 41883, '“mysterious': 41884, 'demonlevel': 41885, 'norbu': 41886, 'vibhor': 41887, 'counselor': 41888, \"aisha's\": 41889, 'man–woman': 41890, 'rabelais': 41891, \"miles's\": 41892, 'culver': 41893, 'breading': 41894, 'printz': 41895, \"'looking\": 41896, 'hank': 41897, 'vlogbrothers': 41898, 'vidcon': 41899, 'abject': 41900, 'vanaras': 41901, 'sandwiched': 41902, 'naras': 41903, 'kishkindha': 41904, 'emancipated': 41905, 'intervened': 41906, 'neelakantan': 41907, 'ajaya': 41908, 'sivagami': 41909, 'baahubali': 41910, 'ainslie': 41911, 'chaplain': 41912, 'outsiders’': 41913, 'trunk…': 41914, 'unsolvable': 41915, 'gunning': 41916, \"laurie's\": 41917, 'groening': 41918, \"mccloud's\": 41919, \"seneca's\": 41920, 'stoicism—the': 41921, 'overmastering': 41922, 'setbacks—while': 41923, 'criticizing': 41924, 'seneca’s': 41925, 'accuses': 41926, \"experts'\": 41927, 'dissemble': 41928, 'breitbart': 41929, 'peddle': 41930, 'verifiable': 41931, 'up—she': 41932, 'scrubs': 41933, 'ryle’s': 41934, 'aversion': 41935, 'dating”': 41936, 'corrigan—her': 41937, 'hostilities': 41938, 'uncontrolled': 41939, 'looting': 41940, 'scoured': 41941, \"anthony's\": 41942, \"saddam's\": 41943, 'deplorable': 41944, 'ostriches': 41945, \"dictator's\": 41946, 'thoroughbred': 41947, 'magni': 41948, '210': 41949, 'smyslov': 41950, 'keres': 41951, 'reshevsky': 41952, 'algebraic': 41953, '352': 41954, \"'theodore\": 41955, 'houts': 41956, '‘instagram’s': 41957, 'illustrator’': 41958, 'satirizing': 41959, 'jooleeloren': 41960, '‘slightly': 41961, 'antisocial': 41962, 'heroines’': 41963, 'refinery29': 41964, '‘girlboss’': 41965, 'houts’s': 41966, 'squiggly': 41967, 'bundobast': 41968, 'isaque': 41969, 'upright': 41970, \"'90s\": 41971, 'descriptionsplunk': 41972, 'eventgen': 41973, 'splunktrust': 41974, 'civardi': 41975, \"civardi's\": 41976, 'lineaments': 41977, \"pri's\": 41978, \"mothe's\": 41979, 'pashmina': 41980, 'chanani': 41981, '251': 41982, 'answers’': 41983, 'efl': 41984, 'student’s': 41985, \"invincible's\": 41986, '–hunts': 41987, \"murderer's\": 41988, 'star…': 41989, \"player's\": 41990, 'skyhorse': 41991, 'boating': 41992, 'interceptors': 41993, 'instabilities': 41994, 'bofors': 41995, 'tarnish': 41996, 'steroid': 41997, 'numerically': 41998, 'cnc': 41999, 'castings': 42000, 'forgings': 42001, 'drilling': 42002, 'slitless': 42003, 'hydraulic': 42004, 'collets': 42005, 'mandrels': 42006, '2d': 42007, 'spectacular”': 42008, '“player”': 42009, '“soup': 42010, 'divulges': 42011, 'outdated': 42012, 'monetize': 42013, '“sweet': 42014, 'spot”': 42015, 'tilting': 42016, '“tilt”': 42017, 'disseminating': 42018, 'podcast': 42019, 'diversification': 42020, 'monetization': 42021, 'podcaster': 42022, 'pulizzi’s': 42023, 'squeaks': 42024, 'dacoits': 42025, \"shambu's\": 42026, \"'feminism'\": 42027, '1890s': 42028, 'reconfigured': 42029, 'larson': 42030, 'donoghue': 42031, \"'room\": 42032, 'niffenegger': 42033, 'coinciding': 42034, 'unarguably': 42035, 'wracking': 42036, 'peter’s': 42037, 'preparatory': 42038, 'sixths': 42039, 'peripatetic': 42040, 'marty': 42041, 'feldman': 42042, 'cleese’s': 42043, 'waterskiing': 42044, 'rajpaland': 42045, 'chronically': 42046, 'unenthusiastic': 42047, \"seattle's\": 42048, 'energise': 42049, 'judgments’': 42050, 'irresolute': 42051, 'maneka': 42052, 'begam': 42053, 'olga': 42054, 'tellis': 42055, 'jurisprudence': 42056, 'court’s': 42057, 'verdicts': 42058, 'impacting': 42059, 'mody': 42060, 'mergers': 42061, 'azb': 42062, \"bustle'\": 42063, \"warmth'\": 42064, 'giggles': 42065, 'romcom': 42066, 'sandhya': 42067, \"sahil's\": 42068, 'adorkable': 42069, 'unmanned': 42070, 'vehicles—drones—to': 42071, 'informants—and': 42072, 'coordinated': 42073, 'neutralized': 42074, 'moorjani': 42075, 'ailment': 42076, 'instantaneously': 42077, 'obligated': 42078, \"'being\": 42079, \"'look'\": 42080, \"'to'\": 42081, \"'go'\": 42082, \"'yes'\": 42083, 'वह': 42084, 'परिवेश': 42085, 'जिसे': 42086, 'हम': 42087, 'देहात': 42088, 'कहते': 42089, 'एक': 42090, 'नई': 42091, 'सामाजिकता': 42092, 'निर्मित': 42093, 'इसी': 42094, 'सामने': 42095, 'रखती': 42096, 'बढ़ा': 42097, 'दूर': 42098, 'कर': 42099, 'अपने': 42100, 'संदर्भों': 42101, 'इसकी': 42102, 'करती': 42103, 'नवें': 42104, 'दशक': 42105, 'आर्थिक': 42106, 'सुधारों': 42107, 'बाद': 42108, 'किसानी': 42109, 'जीवन': 42110, 'अपनी': 42111, 'धमक': 42112, 'खो': 42113, 'दी।': 42114, 'अर्थव्यवस्था': 42115, 'शहर': 42116, 'शहरीकरण': 42117, 'बोलबाला': 42118, 'हो': 42119, 'गया।': 42120, 'नये': 42121, 'बनते': 42122, 'बिगड़ते': 42123, 'पास': 42124, 'कोई': 42125, 'मुक़म्मल': 42126, 'तस्वीर': 42127, 'रही।': 42128, 'पिछले': 42129, 'तीन': 42130, 'दशकों': 42131, 'विकसित': 42132, 'शहरनुमा': 42133, 'समाजशास्त्रीय': 42134, 'जाँच': 42135, 'गई': 42136, 'khanpur': 42137, 'harmonize': 42138, 'incoherence': 42139, 'dislocation': 42140, 'satendra': 42141, 'sociality': 42142, \"anthropology's\": 42143, 'forfeiture': 42144, 'plot’s': 42145, 'fingerprint': 42146, 'samana': 42147, 'reciting': 42148, 'beatlemania': 42149, 'gardening': 42150, \"musician's\": 42151, '141': 42152, '59': 42153, 'shepard': 42154, 'fairey': 42155, 'sissy': 42156, 'opal': 42157, 'sic': 42158, 'torpedoes': 42159, 'organization—anything': 42160, 'taopaipai': 42161, 'faze': 42162, \"uranai's—he'll\": 42163, 'cuddled': 42164, 'knitted': 42165, 'meik': 42166, \"meik's\": 42167, \"dallas's\": 42168, \"eve's\": 42169, \"'urgent\": 42170, '‘10': 42171, 'rule’': 42172, 'dabble': 42173, 'generalists': 42174, 'retiree': 42175, 'quitters': 42176, 'silo': 42177, 'dhaba': 42178, 'shen': 42179, 'petersen': 42180, 'copeland': 42181, 'snack': 42182, 'hangry': 42183, \"arm's\": 42184, 'tissues': 42185, 'hoang': 42186, \"'harnessing\": 42187, \"jimenez's\": 42188, \"jerking'\": 42189, 'weeklyfont': 42190, \"'let's\": 42191, \"changer'\": 42192, 'coms': 42193, 'tfz': 42194, \"rough'\": 42195, 'teared': 42196, \"comedy'\": 42197, \"real'\": 42198, \"healing'\": 42199, 'hoaxes': 42200, 'disseminated': 42201, 'lynching': 42202, 'alt': 42203, 'pratik': 42204, 'sumaiya': 42205, 'shaikh': 42206, 'sidharth': 42207, 'purveyors': 42208, 'badrinath': 42209, 'scopes': 42210, 'sims': 42211, 'pinot': 42212, 'grigio': 42213, 'moppets': 42214, 'minder': 42215, 'slots': 42216, 'pta': 42217, 'fayre': 42218, 'surged': 42219, 'more—the': 42220, 'shar’dama': 42221, 'weapons—a': 42222, 'crown—that': 42223, 'inevera’s': 42224, 'all—those': 42225, 'fantasy’s': 42226, '“brett’s': 42227, '”—fixed': 42228, '”—roqoo': 42229, 'depot': 42230, '“after': 42231, 'contender': 42232, '”—jet': 42233, '“brett': 42234, '”—best': 42235, 'is—in': 42236, 'weingarten’s': 42237, 'stories—the': 42238, 'zucchini': 42239, '“armpit': 42240, 'america”': 42241, 'nonvoter—all': 42242, 'budge': 42243, 'studiously': 42244, 'loss…': 42245, 'steeple': 42246, 'bumpleigh': 42247, 'craye': 42248, 'worplesdon': 42249, 'trample': 42250, 'fatheads': 42251, 'orthodoxy': 42252, 'heretic': 42253, \"disney·pixar's\": 42254, 'concealable': 42255, 'hypnotically': 42256, 'sprouting': 42257, 'moulting': 42258, 'arunachal': 42259, '152': 42260, 'birding': 42261, 'akin': 42262, 'fibres': 42263, 'egyptians': 42264, 'hue': 42265, 'botanical': 42266, \"indigo's\": 42267, 'pynchon': 42268, \"gravity's\": 42269, 'motocross': 42270, 'feasible': 42271, 'obliterating': 42272, 'wormhole': 42273, 'nurturer': 42274, 'merciful': 42275, 'absolve': 42276, 'ganga’s': 42277, 'bhishma': 42278, 'kartikeya': 42279, 'chandramouli': 42280, 'bestow': 42281, 'moksha': 42282, 'recrimination': 42283, 'possessiveness': 42284, 'barkley': 42285, 'interrogating': 42286, 'starkness': 42287, 'transfix': 42288, \"'marks\": 42289, 'shatteringly': 42290, 'sperm': 42291, 'harpooners': 42292, 'whalers': 42293, \"melville's\": 42294, 'sybex’s': 42295, '8088': 42296, '80186': 42297, '80286': 42298, '80386': 42299, '80486': 42300, 'powerpc': 42301, 'pic': 42302, 'cisc': 42303, 'sparc': 42304, 'alp': 42305, 'ratified': 42306, \"smith'\": 42307, 'snicko': 42308, 'hotspot': 42309, 'nandita': 42310, 'denigrated': 42311, 'latent': 42312, 'babasaheb': 42313, 'valerio': 42314, \"manfredi's\": 42315, 'autolykos': 42316, 'mycenae': 42317, 'absolution': 42318, 'alluring…': 42319, 'cleo’s': 42320, 'treetop': 42321, 'canopy': 42322, 'medications': 42323, 'actuaries': 42324, 'closures': 42325, 'vectorization': 42326, 'pasiv': 42327, 'extractors': 42328, 'patternsónow': 42329, '8óshows': 42330, 'bowater': 42331, 'atomhawk': 42332, 'stenning': 42333, 'posing': 42334, 'kabaadiwala': 42335, 'ecologies': 42336, 'anthropological': 42337, 'assa': 42338, 'doron': 42339, 'stigmatized': 42340, 'stunting': 42341, 'underclasses': 42342, 'landless': 42343, 'wolverton': 42344, 'hellion': 42345, 'navneet': 42346, 'tabloid': 42347, 'implode': 42348, 'ackroyd’': 42349, 'blackmailing': 42350, 'scrap': 42351, '1545': 42352, 'vanquished': 42353, 'aztec': 42354, 'yucatan': 42355, \"pelati's\": 42356, \"'emotionally\": 42357, 'ducts': 42358, \"limit'\": 42359, \"anna's\": 42360, 'donor': 42361, 'fatally': 42362, \"jodi's\": 42363, \"'sex\": 42364, \"cuisine'\": 42365, 'bourdain': 42366, 'gironde': 42367, 'honky': 42368, 'tonk': 42369, 'provincetown': 42370, \"bourdain's\": 42371, '6460': 42372, 'errorless': 42373, 'ushers': 42374, 'paralytic': 42375, 'garb': 42376, 'slum': 42377, '‘unattainable’': 42378, 'cowardly': 42379, 'effrontery': 42380, 'environs': 42381, 'cacophony': 42382, 'overpopulated': 42383, 'duos': 42384, 'novels—batman': 42385, 'family—are': 42386, 'cardozo': 42387, 'precedent': 42388, 'better—and': 42389, 'lap—with': 42390, 'laughlin': 42391, 'movement—not': 42392, 'ability—that': 42393, 'choreographed': 42394, 'drills—practiced': 42395, 'yoga—that': 42396, 'appleyard': 42397, \"'human\": 42398, \"locomotion'\": 42399, 'collisions': 42400, 'polities': 42401, 'proliferates': 42402, \"tennyson's\": 42403, \"historian's\": 42404, 'sui': 42405, 'generis': 42406, 'circumnavigation': 42407, 'baku': 42408, 'emirates': 42409, 'tasmania': 42410, 'tahiti': 42411, 'excavates': 42412, 'tramps': 42413, 'summarises': 42414, 'austronesian': 42415, 'malaysian': 42416, 'airliner': 42417, \"'including\": 42418, 'grumpy': 42419, 'painless': 42420, \"raikar's\": 42421, 'swilling': 42422, 'twister': 42423, 'coalmine': 42424, 'duo’s': 42425, 'decker’s': 42426, 'extradimensional': 42427, 'evildoers': 42428, 'exploit…': 42429, 'same…': 42430, 'there—be': 42431, \"stockman's\": 42432, 'unrealistic': 42433, \"'governor\": 42434, 'permit': 42435, 'draconian': 42436, 'forex': 42437, 'jaswant': 42438, 'chidambaram': 42439, 'liquidity': 42440, 'chaired': 42441, 'tibbet’s': 42442, 'deliverer’s': 42443, 'plague…': 42444, \"'sleep'\": 42445, \"'sedative'\": 42446, \"'hypnotic'\": 42447, \"'soporific'\": 42448, 'puzzlers': 42449, \"'sherlock\": 42450, 'logician': 42451, 'flushed': 42452, 'darkened': 42453, 'strand': 42454, \"carbuncle'\": 42455, \"league'\": 42456, '‘bird': 42457, 'bird’': 42458, 'lamott’s': 42459, 'blighted': 42460, 'tommo': 42461, 'ww1': 42462, 'kanha': 42463, 'maikal': 42464, 'hazard': 42465, 'descriptionwhile': 42466, 'adversely': 42467, 'aop': 42468, 'multithreading': 42469, 'khairgarh': 42470, 'jagmohan': 42471, 'schiff': 42472, 'glaring': 42473, 'ingrained': 42474, 'doesn': 42475, 'irwin': 42476, 'multimillionaire': 42477, 'sadhana': 42478, \"rajput's\": 42479, 'oaks': 42480, 'tunings': 42481, 'jit': 42482, '1873…': 42483, 'humboldt': 42484, 'fort’s': 42485, 'escorting': 42486, 'paiute': 42487, 'starnberg': 42488, 'insignia': 42489, \"up'\": 42490, 'ambushed': 42491, 'drugged': 42492, 'juxtaposes': 42493, 'transsexuality': 42494, 'bereavement': 42495, \"japan'\": 42496, 'sidewalk': 42497, 'mobsters': 42498, 'moguls': 42499, 'trot': 42500, 'udated': 42501, \"vogler's\": 42502, \"mythology's\": 42503, 'editons': 42504, \"'classic'\": 42505, 'machete': 42506, 'morbidly': 42507, 'bakugo’s': 42508, 'heroes—all': 42509, 'particular—and': 42510, 'appears—all': 42511, 'mastercard': 42512, 'matinee': 42513, 'combing': 42514, \"murakami's\": 42515, 'adidas': 42516, 'rudi': 42517, 'dassler': 42518, 'dasslers': 42519, 'namath': 42520, \"kurzweil's\": 42521, 'exceeding': 42522, 'abjection': 42523, 'zeugma': 42524, 'versification': 42525, \"measure'\": 42526, 'meghan': 42527, \"shaughnessy's\": 42528, 'whipcrack': 42529, \"eyes'\": 42530, \"compulsively'\": 42531, \"'robotham\": 42532, \"audie's\": 42533, 'guests—often': 42534, 'world—to': 42535, 'transgressions': 42536, 'ization': 42537, 'fear¬lessly': 42538, 'onassis': 42539, 'manufac¬tured': 42540, 'reportorial': 42541, 'confidentiality': 42542, 'winfrey’s': 42543, 'filmed—one': 42544, 'microscope': 42545, 'prevalence': 42546, 'birther': 42547, 'confederacy': 42548, 'intellectualism': 42549, 'unreality': 42550, 'dismantling': 42551, 'stepwise': 42552, 'wolfgang': 42553, 'boogie': 42554, 'resurrections': 42555, 'deconstruction': 42556, '682': 42557, '683': 42558, 'rebooted': 42559, 'staples': 42560, 'domingos': 42561, \"algorithm'\": 42562, 'deriving': 42563, 'gruff': 42564, 'hypochondriac': 42565, 'memorists': 42566, 'peabody': 42567, 'serrated': 42568, 'gasps': 42569, 'loomed': 42570, 'zealous': 42571, 'bumpy': 42572, 'hindrances': 42573, 'kin': 42574, 'precedence': 42575, 'basharat': 42576, 'youths': 42577, 'illusioned': 42578, 'godfathers': 42579, 'refurbished': 42580, 'rigged': 42581, 'landmines': 42582, 'bunkers': 42583, 'crucifiction': 42584, 'neurosurgery': 42585, \"surgeon's\": 42586, 'english—about': 42587, 'angami': 42588, 'disrupting': 42589, 'english—hindu': 42590, 'easterine': 42591, 'kire': 42592, 'clashed': 42593, 'outumbered': 42594, 'change—within': 42595, 'here\\x92s': 42596, '\\x93save': 42597, 'cat\\x94': 42598, 'ironclad': 42599, 'loglinethe': 42600, 'physicsthe': 42601, '\\x97': 42602, 'they\\x92re': 42603, 'scriptwhy': 42604, 'ideamastering': 42605, 'beatscreating': 42606, '\\x93perfect': 42607, 'beast\\x94': 42608, 'changehow': 42609, 'repairthis': 42610, 'insider\\x92s': 42611, 'who\\x92s': 42612, '95': 42613, 'wrested': 42614, 'polarizing': 42615, 'marino': 42616, '282': 42617, '336': 42618, 'dwarfish': 42619, 'faustus': 42620, \"grass's\": 42621, \"realism'\": 42622, 'gunter': 42623, 'danzig': 42624, 'alongside—and': 42625, 'with—niall': 42626, 'alive…until': 42627, 'flirty': 42628, 'loosen': 42629, \"'brought'\": 42630, \"'breakfast'\": 42631, \"'lightning'\": 42632, \"'safe'\": 42633, 'clemons': 42634, 'salaries': 42635, 'endorsements': 42636, 'oarsmen': 42637, 'appeased': 42638, 'sculls': 42639, 'sportswriting': 42640, 'lehmann': 42641, 'haupt': 42642, 'levin': 42643, 'overpowers': 42644, 'reinstates': 42645, 'eternally': 42646, 'diya': 42647, 'subterranean': 42648, 'heaving': 42649, \"simpson's\": 42650, 'peruvian': 42651, 'siula': 42652, 'staggered': 42653, \"licken's\": 42654, 'licken': 42655, 'davidar': 42656, 'cheetal': 42657, 'tahr': 42658, 'nilgiris': 42659, 'cameroons': 42660, 'family—cholmondeley': 42661, 'chimpanzee': 42662, 'others—they': 42663, 'romping': 42664, 'noah’s': 42665, '1q84': 42666, 'moons': 42667, 'tengo': 42668, 'aomame': 42669, 'diagramming': 42670, 'piloting': 42671, 'conceivably': 42672, \"bear's\": 42673, 'porridge': 42674, 'voilà': 42675, 'iota': 42676, 'chairs—these': 42677, 'grubby': 42678, 'invoke': 42679, 'emblem': 42680, 'descriptionamazon': 42681, 'realtime': 42682, 'elb': 42683, 'denbe': 42684, 'bcom': 42685, 'hons': 42686, 'distributional': 42687, 'millman': 42688, 'winegarner': 42689, 'millman\\x92s': 42690, 'thepeaceful': 42691, 'academy…': 42692, 'inwards': 42693, 'elza': 42694, 'soares': 42695, 'samba': 42696, '‘funny': 42697, 'zealously': 42698, 'told’': 42699, \"wilkinson's\": 42700, \"press's\": 42701, 'panelled': 42702, 'signet': 42703, 'legalised': 42704, \"thorpe's\": 42705, 'deceives': 42706, 'embezzles': 42707, 'winks': 42708, 'richard’s': 42709, 'geometricshapes': 42710, 'harnesses': 42711, 'muslins': 42712, 'garments': 42713, 'radiology': 42714, 'ultrasound': 42715, 'trainees': 42716, 'radiation': 42717, 'cultlike': 42718, 'lpga': 42719, '“short': 42720, 'pros”': 42721, 'sev\\xaderal': 42722, 'sieckmann’s': 42723, '81': 42724, 'memory—including': 42725, 'seve': 42726, 'ballesteros': 42727, 'corey': 42728, 'pavin': 42729, 'floyd—to': 42730, 'guide—the': 42731, 'clients—to': 42732, 'reticence': 42733, 'aides': 42734, 'untrained': 42735, \"'turn\": 42736, 'professionalize': 42737, 'unrecognized': 42738, 'insanely': 42739, \"amazon's\": 42740, 'misprints': 42741, 'ly': 42742, '2b9xxbf': 42743, 'aarm': 42744, 'interviewers': 42745, 'brainstorm': 42746, 'recommending': 42747, 'monetized': 42748, 'domination…': 42749, 'doorway': 42750, 'uninvited': 42751, 'gaunt': 42752, 'swayed': 42753, 'four’': 42754, 'suber': 42755, 'supplies―from': 42756, 'pens―you': 42757, 'digitize': 42758, 'forsgren': 42759, 'humourists': 42760, 'idling': 42761, 'do’': 42762, '‘that': 42763, 'complete’': 42764, '‘being': 42765, 'trifle': 42766, 'sting’': 42767, '‘love': 42768, 'once’': 42769, 'xuja': 42770, 'caddie': 42771, 'dispensing': 42772, 'sandomierz': 42773, 'semitic': 42774, 'neilakshi': 42775, \"neil's\": 42776, 'prowl': 42777, 'trackers': 42778, 'vaillant': 42779, 'haware': 42780, 'rera': 42781, 'demonetisation': 42782, 'reit': 42783, 'redevelopment': 42784, 'flashlights': 42785, 'seesaws': 42786, 'braille': 42787, 'petzold': 42788, 'story—and': 42789, 'technophile': 42790, 'workhouse': 42791, 'marigold': 42792, 'maddy’s': 42793, 'wendon': 42794, '“demonstrations”': 42795, 'grunts': 42796, 'cowboys': 42797, 'provocateur': 42798, 'liar’s': 42799, '‘chaos': 42800, 'monkey’': 42801, 'caffeine': 42802, 'bailing': 42803, 'poached': 42804, 'facebook’s': 42805, 'users’': 42806, '‘zuck’': 42807, 'hyperactivity': 42808, \"zuckerberg's\": 42809, 'overpaid': 42810, 'monetisation': 42811, '‘privacy’': 42812, 'fascinatingly': 42813, 'insular': 42814, 'sealegacy': 42815, 'nicklen': 42816, 'antarctic': 42817, \"nicklen's\": 42818, 'palpable': 42819, 'dicaprio': 42820, 'frog': 42821, 'tamed': 42822, 'gatiss': 42823, 'reichenbach': 42824, 'milverton': 42825, 'bygone': 42826, 'cochair': 42827, 'specify': 42828, 'feedforward': 42829, 'bioinformatics': 42830, 'renauld': 42831, 'renauld’s': 42832, 'disinherited': 42833, 'lhasa': 42834, 'enthroned': 42835, 'potala': 42836, 'jawaharal': 42837, 'verification': 42838, 'leds': 42839, 'potentiometer': 42840, 'accelerometer': 42841, 'wamp': 42842, 'intranet': 42843, 'temp': 42844, 'multicolor': 42845, 'moisture': 42846, 'timur': 42847, 'plunderer': 42848, 'dynasties': 42849, \"babur's\": 42850, 'snatch': 42851, 'stapleton’s': 42852, 'contagion': 42853, 'influenza': 42854, 'overdrive': 42855, 'inconclusive': 42856, 'mayor’s': 42857, 'afield': 42858, 'vat': 42859, 'cst': 42860, 'payroll': 42861, 'maters': 42862, 'erp9': 42863, 'challan': 42864, 'statutory': 42865, \"rommy's\": 42866, 'wynn': 42867, 'starczek': 42868, \"turow's\": 42869, 'intersecting': 42870, 'scammers': 42871, '‘rama’': 42872, '‘sita’': 42873, 'meek': 42874, 'janaka': 42875, '‘ashoka': 42876, 'vatika’': 42877, 'magnanimous': 42878, 'restating': 42879, 'kids’': 42880, 'herds': 42881, 'pekingese': 42882, 'tricki': 42883, \"herriot's\": 42884, \"then'\": 42885, 'iz0': 42886, 'arraylists': 42887, 'braverythat': 42888, \"'mogul'\": 42889, \"bedi's\": 42890, 'screeching': 42891, \"'did\": 42892, 'howdunnit': 42893, \"cameras'\": 42894, 'makesthe': 42895, \"appealing'\": 42896, 'signalled': 42897, \"remarkable'\": 42898, 'melds': 42899, \"debut'\": 42900, \"odds'\": 42901, \"'page\": 42902, 'rosamund': 42903, 'lupton': 42904, \"second'\": 42905, 'beukes': 42906, 'oliva': 42907, \"sees'\": 42908, 'countries’': 42909, '—sunil': 42910, '—ravi': 42911, '—moeen': 42912, 'smallish': 42913, 'longish': 42914, 'ickle': 42915, 'seadragonus': 42916, 'giganticus': 42917, 'berk': 42918, \"fairy'\": 42919, 'mentalities': 42920, 'aptly': 42921, 'doused': 42922, 'laxman’s': 42923, 'sharpe’s': 42924, 'mysore’s': 42925, 'obadiah': 42926, '95th': 42927, 'adeptly': 42928, \"syldavia's\": 42929, 'maidans': 42930, 'plainspeak': 42931, 'laconic': 42932, \"sentence'\": 42933, 'hogarth': 42934, 'simulate': 42935, \"problem's\": 42936, 'knox': 42937, 'aches': 42938, 'belgrade': 42939, 'gluten': 42940, 'trimmer': 42941, 'inglethorp': 42942, 'stepson': 42943, 'grease': 42944, 'deems': 42945, 'shapers': 42946, 'biomechanical': 42947, 'slot': 42948, 'rep': 42949, \"dicharry's\": 42950, 'duroy': 42951, 'sleazy': 42952, 'mistresses': 42953, 'financiers': 42954, \"kohli's\": 42955, 'drawbacks': 42956, 'gully': 42957, 'squads': 42958, 'ginormous': 42959, 'granov': 42960, 'everett': 42961, 'acquaintances—a': 42962, 'prisms': 42963, 'resurfacing': 42964, 'peacemakers': 42965, 'mongers': 42966, 'footprint': 42967, \"composition'\": 42968, 'kuchiki': 42969, 'absorbs': 42970, \"ichigo's\": 42971, 'ornament': 42972, 'stupa': 42973, 'sanchi': 42974, 'juxtaposed': 42975, 'transpositions': 42976, 'plaster': 42977, \"cross's\": 42978, 'scorpion': 42979, 'corp': 42980, 'remotest': 42981, \"chemaly's\": 42982, 'diminished': 42983, 'bitches': 42984, 'shrill': 42985, 'objectification': 42986, 'repress': 42987, 'marshalling': 42988, 'hazy': 42989, 'name—shreyasi': 42990, 'lapse': 42991, 'jhansee': 42992, 'maharani': 42993, 'salute': 42994, \"patrick's\": 42995, \"calendar's\": 42996, 'jeph': 42997, 'goyer': 42998, \"'emotional\": 42999, 'gregoria': 43000, \"isabella's\": 43001, \"amadeo's\": 43002, 'florentine': 43003, 'choosen': 43004, 'byobserverguardiantelegraphirish': 43005, 'timesnew': 43006, 'statesmantimes': 43007, 'supplementheraldwhen': 43008, 'inhabiting': 43009, 'prachett': 43010, \"cadbury's\": 43011, 'bullies': 43012, \"roald's\": 43013, \"foster's\": 43014, 'tabbed': 43015, 'dhepa': 43016, 'ordeals': 43017, 'acolyte': 43018, 'assaji': 43019, 'devadatta’s': 43020, 'buddha’s': 43021, 'morgue': 43022, \"rss'\": 43023, \"negotiator'\": 43024, \"'more'\": 43025, \"kenya's\": 43026, 'stewarding': 43027, 'artistaes': 43028, 'do—the': 43029, 'founded—lee': 43030, 'synthesizing': 43031, 'signposted': 43032, 'summarise': 43033, \"'workshop'\": 43034, 'journalling': 43035, 'triton': 43036, 'brahman': 43037, 'govinda': 43038, 'realizations': 43039, \"hesse's\": 43040, \"1910's\": 43041, 'oscar’s': 43042, 'dudley': 43043, 'terence': 43044, 'unwary': 43045, \"benchley's\": 43046, 'annoy': 43047, 'delia': 43048, \"waterstone's\": 43049, 'suggestions–compelling': 43050, 'wryly': 43051, 'perceptive–for': 43052, 'biologically': 43053, 'biblically': 43054, 'mushy': 43055, 'winded': 43056, 'sickly': 43057, \"blob'\": 43058, 'advisers': 43059, 'shoeless': 43060, 'sampled': 43061, \"'coffee\": 43062, \"chocolate'\": 43063, \"'rastafarian\": 43064, \"diet'\": 43065, 'foodism': 43066, 'humanly': 43067, 'slob': 43068, 'beyonce’s': 43069, '“flawless': 43070, 'socialises': 43071, 'reshma’s': 43072, '“teach': 43073, 'perfection”': 43074, \"robot'\": 43075, 'paralegals': 43076, 'reassessment': 43077, \"intelligence's\": 43078, \"robots'\": 43079, \"mary's\": 43080, \"marple's\": 43081, 'listerdale': 43082, 'shanti': 43083, \"monkey's\": 43084, 'clears': 43085, 'headway': 43086, 'executioner': 43087, 'accentuation': 43088, 'tertiary': 43089, 'martin’': 43090, 'pretenders': 43091, 'comet': 43092, 'fratricide': 43093, 'moms': 43094, 'maternal': 43095, 'passerby': 43096, 'ducklings': 43097, 'oprahmag': 43098, 'claremont': 43099, 'diaz': 43100, 'genius―his': 43101, 'instragramable': 43102, 'unstuffy': 43103, 'derail': 43104, \"mcquiston's\": 43105, 'sexy―this': 43106, 'unhoneymooners': 43107, 'dandelions': 43108, 'alibi…': 43109, 'humbleby': 43110, \"crispin's\": 43111, 'xlix': 43112, \"patriots'\": 43113, 'intercepted': 43114, 'attributable': 43115, 'jerk': 43116, 'wookiees': 43117, 'wampas': 43118, 'dad”': 43119, 'kid”': 43120, 'vlogger': 43121, 'preteen': 43122, 'vlogging': 43123, 'brood': 43124, 'shaytards': 43125, 'cheeseburgers': 43126, 'handfuls': 43127, 'shayloss': 43128, '“maintaining”': 43129, 'gavin’s': 43130, 'lifestyle—even': 43131, 'tortoise': 43132, 'trustworthy': 43133, 'tact': 43134, 'elm': 43135, 'years”': 43136, 'maddox—his': 43137, 'friend—find': 43138, 'chillingly': 43139, 'naturalis': 43140, 'principia': 43141, 'mathematica': 43142, '1687': 43143, 'gottfried': 43144, 'regenerated': 43145, 'custard': 43146, 'presiding': 43147, '“sandy”': 43148, 'action—and': 43149, 'sense—are': 43150, 'crumbs': 43151, 'structures—classes': 43152, 'markets—and': 43153, 'group’s': 43154, 'maximizes': 43155, 'work—and': 43156, 'delays': 43157, 'impediments': 43158, 'litigation': 43159, 'interjections': 43160, 'uncluttered': 43161, 'ashes’': 43162, 'poked': 43163, 'guide—written': 43164, 'margarita': 43165, 'madrigal—that': 43166, 'method—each': 43167, 'vocabulary—right': 43168, 'supernormal': 43169, 'informally': 43170, 'fulves': 43171, 'zola': 43172, 'bucks': 43173, 'rowena': 43174, 'romanticized': 43175, 'dispossessed': 43176, 'overlords': 43177, 'fawcett': 43178, \"mendl's\": 43179, 'mendl': 43180, 'touchett': 43181, \"'young\": 43182, 'affronting': 43183, \"destiny'\": 43184, 'poignancy': 43185, 'daring—and': 43186, 'successful—military': 43187, 'concerned—especially': 43188, 'plutonium': 43189, '“hot': 43190, 'menachem': 43191, 'reactor—a': 43192, 'contemplated': 43193, 'ret': 43194, 'ilan': 43195, 'iraq’s': 43196, 'rodger': 43197, 'kilometer': 43198, 'intelligence’s': 43199, '“black': 43200, 'ops”': 43201, 'pilot’s': 43202, 'roared': 43203, 'sinai': 43204, 'tito’s': 43205, 'yugoslavs': 43206, 'relay': 43207, 'corner…': 43208, 'schildt': 43209, 'outdo': 43210, 'aplenty': 43211, 'gentlemouse': 43212, 'enormouse': 43213, 'hankering': 43214, 'pedal': 43215, 'paddies': 43216, 'mekong': 43217, 'burningham': 43218, 'benchwick': 43219, 'bindloss': 43220, 'waterson': 43221, 'swaine': 43222, 'bain': 43223, 'rew': 43224, 'haddad': 43225, 'bodry': 43226, 'carillet': 43227, 'sieg': 43228, 'bloomfeild': 43229, \"travellers'\": 43230, 'caraval―stephanie': 43231, 'garber’s': 43232, 'whisks': 43233, 'caraval’s': 43234, 'organizer': 43235, 'season’s': 43236, 'domino': 43237, 'indienext': 43238, '―entertainment': 43239, '“impressive': 43240, '―usa': 43241, '“spellbinding': 43242, '―us': 43243, '“magnificent': 43244, '―publishers': 43245, '―sabaa': 43246, '“beautifully': 43247, '―renée': 43248, 'ahdieh': 43249, '“shimmers': 43250, '―marie': 43251, 'rutkoski': 43252, 'winner’s': 43253, '“darkly': 43254, '―kiersten': 43255, 'darken': 43256, '“decadent': 43257, '―roshani': 43258, '―stacey': 43259, '“destined': 43260, '―kirkus': 43261, '“ideal': 43262, 'stardust': 43263, '―school': 43264, 'soota': 43265, 'distil': 43266, 'validation': 43267, 'vcs': 43268, 'ipo—soota': 43269, 'replicates': 43270, 'goleman': 43271, 'herminia': 43272, 'ibarra': 43273, 'morten': 43274, 'hansen': 43275, 'slaying': 43276, 'gazettes': 43277, 'ofrudraprayag': 43278, 'guadagnino': 43279, 'armie': 43280, 'timothee': 43281, 'chalamet': 43282, 'vulture': 43283, \"aciman's\": 43284, 'cliffside': 43285, 'lakeside': 43286, 'micekingchief': 43287, 'shouter': 43288, 'smarty': 43289, 'stiltonordinto': 43290, 'mouseking': 43291, \"dragon's\": 43292, 'squids': 43293, 'micekinghelmet': 43294, 'ooty': 43295, 'middlings': 43296, 'meese': 43297, 'muniswamy': 43298, 'beerappan': 43299, 'madumalai': 43300, 'colt': 43301, 'recalcitrant': 43302, 'unaccustomed': 43303, 'horowitz’s': 43304, 'bbc’s': 43305, \"level'\": 43306, 'aled': 43307, 'elan': 43308, 'blurs': 43309, \"'splendidly\": 43310, \"'sharp\": 43311, \"tree'\": 43312, \"'deduction\": 43313, 'lovesey': 43314, 'channelers': 43315, 'clamor': 43316, 'housekeeper’s': 43317, 'one—with': 43318, 'monterey': 43319, 'templeton’s': 43320, 'most—her': 43321, 'laura—or': 43322, 'way—no': 43323, 'leonid': 43324, 'sanction': 43325, 'blames': 43326, \"arkadin's\": 43327, 'treadstone': 43328, 'spray': 43329, \"torturer's\": 43330, 'thrax': 43331, \"executioner's\": 43332, 'terminus': 43333, 'urth': 43334, 'kids—build': 43335, 'edu': 43336, 'blaster': 43337, 'kids—and': 43338, 'parents—practice': 43339, 'coders': 43340, 'collaboratively': 43341, '\\x93chronic': 43342, 'cardio\\x94': 43343, 'triathlon\\x92s': 43344, 'doping': 43345, 'medalist': 43346, 'whitfield': 43347, 'zabriskie': 43348, 'sisson\\x92s': 43349, 'duathlon': 43350, '\\x93go': 43351, 'primal\\x94': 43352, 'reprogramming': 43353, 'glycogen': 43354, '\\x93active': 43355, 'sluggish': 43356, 'regimented': 43357, '\\x93type': 43358, 'a\\x94': 43359, 'today\\x92s': 43360, \"mystery'\": 43361, 'greenberg': 43362, 'benjam': 43363, 'chaparro': 43364, 'reacquainted': 43365, 'hornos': 43366, \"argentina's\": 43367, 'journeyfilm': 43368, 'dean’s': 43369, '262': 43370, 'nonrunners': 43371, 'fussell': 43372, 'sanctums': 43373, 'butch': 43374, 'cassidy': 43375, 'sundance': 43376, 'redford': 43377, '’the': 43378, 'westland': 43379, 'gunas': 43380, 'suryavanshi': 43381, 'daksha': 43382, 'meluha': 43383, 'eisenhower': 43384, \"'scion\": 43385, 'ikshvaku’': 43386, \"dictionary's\": 43387, 'mina’s': 43388, 'ring…': 43389, 'dharia': 43390, 'thee': 43391, 'begging': 43392, 'marcie': 43393, \"'injustice\": 43394, 'missive': 43395, 'cataclysms': 43396, 'disruptions': 43397, 'moiseyev': 43398, 'hesitated': 43399, 'applicationsuthe': 43400, 'vbscript': 43401, 'dhtml': 43402, 'asps': 43403, 'barto': 43404, 'shaded': 43405, 'ucb': 43406, 'sarsa': 43407, 'fourier': 43408, \"learning's\": 43409, \"watson's\": 43410, 'wagering': 43411, 'willenbrink': 43412, 'deduce': 43413, \"pande's\": 43414, 'russel': 43415, \"mead's\": 43416, 'seller’s': 43417, 'eiji': 43418, 'yoshikawa': 43419, '40’s': 43420, 'yoshikawa’s': 43421, 'nhk’s': 43422, 'yohikawa’s': 43423, 'musashi’s': 43424, 'kodansha': 43425, 'biggers': 43426, \"buyer's\": 43427, 'inoue’s': 43428, 'takehiko': 43429, 'shinmen': 43430, 'kiler': 43431, 'descriptionnatural': 43432, 'thushan': 43433, 'ganegedara': 43434, 'embeddings': 43435, 'paving': 43436, 'breakers': 43437, 'strongman': 43438, 'jumbo': 43439, 'infographic': 43440, 'outstretch': 43441, 'guinnessworldrecords': 43442, 'autocad': 43443, 'unitary': 43444, 'amie': 43445, 'easy–to–understand': 43446, 'student–friendly': 43447, 'by–side': 43448, 'fawned': 43449, \"miranda's\": 43450, \"boyfriend's\": 43451, 'andrea’s': 43452, 'hairstyle': 43453, 'pomp': 43454, 'eff': 43455, 'ect': 43456, \"homes'\": 43457, 'worthington': 43458, 'medic': 43459, '50bc': 43460, 'gears': 43461, 'collapsing': 43462, '“gendercide': 43463, 'planet—the': 43464, 'agonizingly': 43465, 'fiancée—and': 43466, 'juneja': 43467, 'historiographical': 43468, \"juneja's\": 43469, 'fabricate': 43470, 'heterogeneous': 43471, 'contentiously': 43472, 'misnamed': 43473, \"'muslim'\": 43474, 'colonialists': 43475, 'postmodernists': 43476, 'resensitise': 43477, 'historiography': 43478, 'belied': 43479, '209': 43480, 'excitedly': 43481, 'apatow': 43482, 'egghead': 43483, 'breakups': 43484, 'barbers': 43485, 'engel': 43486, 'chris’s': 43487, 'sensitivities': 43488, 'etiquettes': 43489, 'guy’s': 43490, '—buzzfeed': 43491, 'tutankhamen': 43492, 'marcia': 43493, \"williams'\": 43494, 'retellings': 43495, 'coronet': 43496, 'slimming': 43497, 'haughty': 43498, 'twistleton': 43499, 'debonair': 43500, 'descriptionbuilt': 43501, 'puncturing': 43502, 'unpalatable': 43503, 'cocoon': 43504, 'wrapping': 43505, 'dooling': 43506, \"dooling's\": 43507, 'norcross': 43508, 'begun…': 43509, 'arose―men': 43510, 'combat―and': 43511, 'ka―the': 43512, 'dear―a': 43513, 'evil―from': 43514, 'fundraisers': 43515, 'packard’s': 43516, '“motivation': 43517, 'obsolescence': 43518, 'logo’': 43519, 'anniversay': 43520, 'slackers': 43521, '‘alternative’': 43522, 'tentatively': 43523, 'abrasively': 43524, 'noakhali': 43525, 'educationist': 43526, 'omission': 43527, 'headquarters—the': 43528, 'googleplex—to': 43529, 'digitizing': 43530, 'pampers': 43531, 'engineers—free': 43532, 'masseuses—and': 43533, 'nimbler': 43534, 'plex': 43535, 'irritate': 43536, 'konohagakure': 43537, 'hero’s': 43538, 'welcome…': 43539, 'racecar': 43540, 'blackstone': 43541, 'girlhood': 43542, 'stay…': 43543, 'kay’s': 43544, 'dobby': 43545, 'gilderoy': 43546, 'augustinian': 43547, 'moravia': 43548, \"'unit\": 43549, \"heredity'\": 43550, \"darwin's\": 43551, 'eugenics': 43552, 'reorganizes': 43553, 'mendel': 43554, 'rosalind': 43555, 'vitally': 43556, \"'write'\": 43557, 'rapist': 43558, 'mcnamara': 43559, \"mcnamara's\": 43560, 'biggar': 43561, 'absconded': 43562, 'winnings': 43563, 'belfry': 43564, 'rowcester': 43565, \"severity'\": 43566, 'brainwork': 43567, '”—marieke': 43568, 'nijkamp': 43569, 'times–bestselling': 43570, '”—school': 43571, 'ladyconstellation': 43572, 'warland': 43573, 'built—her': 43574, 'sanity—begins': 43575, 'tayshas': 43576, 'shivprasad': 43577, 'koirala': 43578, 'majorly': 43579, 'notification': 43580, 'ado': 43581, 'dts': 43582, 'utterance': 43583, 'rasputin': 43584, '‘steel': 43585, 'frame’': 43586, 'envisaged': 43587, 'budo': 43588, 'finalists': 43589, 'yamcha': 43590, '“fist': 43591, 'fang”': 43592, 'fuan': 43593, 'giran': 43594, 'rubbery': 43595, 'namu': 43596, 'chun': 43597, 'florals': 43598, \"'colourist\": 43599, \"queen'\": 43600, 'poppies': 43601, 'accentuates': 43602, 'compliments': 43603, 'untextured': 43604, 'nib': 43605, 'evenly': 43606, 'feathering': 43607, 'sakyong': 43608, 'mipham': 43609, 'horsemanship': 43610, 'stoking': 43611, 'interstate': 43612, 'exacerbating': 43613, 'transboundary': 43614, 'chellaney': 43615, \"o'hagan\": 43616, 'meteorites': 43617, 'methane': 43618, 'volcanoes': 43619, 'dictating': 43620, 'symbolises': 43621, 'consubstantial': 43622, 'author’': 43623, 'montaigne': 43624, 'moderation': 43625, 'bicyclists': 43626, 'aigoual': 43627, 'crotchet': 43628, 'krupin': 43629, 'imprison': 43630, 'krupin’s': 43631, 'averting': 43632, 'thronged': 43633, 'televising': 43634, 'chander': 43635, 'jangarh': 43636, 'deorala': 43637, 'mindedness': 43638, 'mcguff': 43639, 'drdo': 43640, 'barc': 43641, 'ongc': 43642, 'ntpc': 43643, 'bhel': 43644, 'mtnl': 43645, 'mushrooming': 43646, 'forethought': 43647, 'exactitude': 43648, 'snuff': 43649, 'abreast': 43650, 'proffers': 43651, 'felicitous': 43652, 'furtherance': 43653, 'pointwise': 43654, 'paperweight': 43655, 'undramatically': 43656, 'gregson': 43657, \"omelette'\": 43658, \"bird'\": 43659, 'solomon’s': 43660, 'free…': 43661, '‘lofty’': 43662, 'wiseman’s': 43663, 'recognising': 43664, 'rules…': 43665, 'ashbury’s': 43666, 'glowering': 43667, 'ne’er': 43668, 'heir—which': 43669, 'gladstone': 43670, 'vicar’s': 43671, 'seamstress': 43672, 'pushover': 43673, \"anya's\": 43674, 'mcmeel': 43675, 'leav’s': 43676, \"charlie's\": 43677, 'qantas': 43678, 'safeguards': 43679, \"'satisfy'\": 43680, \"'collective\": 43681, 'guru’s': 43682, 'lycurgus': 43683, 'pericles': 43684, 'solon': 43685, 'nicias': 43686, 'themistocles': 43687, 'alcibiades': 43688, 'cimon': 43689, 'agesilaus': 43690, 'plutarch': 43691, 'implicitly': 43692, \"teller's\": 43693, \"plutarch's\": 43694, 'holed': 43695, 'drowns': 43696, 'attractively': 43697, '112': 43698, \"'somebody\": 43699, \"'nice\": 43700, \"guys'\": 43701, 'norwood': 43702, 'birch': 43703, 'lopping': 43704, 'hatchet': 43705, 'horseflies': 43706, 'bicker': 43707, 'unimaginably': 43708, 'clerks': 43709, 'spiritually': 43710, 'years―in': 43711, 'physician―“as': 43712, '“self': 43713, 'bhagwan”': 43714, '“zorba': 43715, 'councils': 43716, 'oratory': 43717, 'memrise': 43718, 'firmware': 43719, '8217': 43720, 'iphones': 43721, 'ipads': 43722, 'transmits': 43723, \"'that’s\": 43724, 'set’': 43725, 'turbulences': 43726, 'aditya’s': 43727, '26th': 43728, 'achiever’s': 43729, 'toss': 43730, 'illegality': 43731, 'lineages': 43732, 'ashni': 43733, 'biyani': 43734, 'ideator': 43735, 'bearer': 43736, 'manasi': 43737, 'inheriting': 43738, \"pudumjee's\": 43739, 'thermax': 43740, 'tigris': 43741, 'iraq…': 43742, 'leatheran': 43743, 'hassanieh': 43744, '‘lovely': 43745, 'louise’': 43746, 'leidner': 43747, 'days’': 43748, 'late…': 43749, 'murmurs': 43750, 'fixers': 43751, 'bans': 43752, 'chandramohan': 43753, 'puppala': 43754, 'naylor': 43755, 'oakley’s': 43756, 'butler’s': 43757, 'bizaardvark': 43758, 'hbr': 43759, 'menage': 43760, 'trois': 43761, 'poirot…': 43762, '‘eternal': 43763, 'triangle’': 43764, 'mews': 43765, 'rhodes': 43766, 'nanomaterials': 43767, 'transplant': 43768, 'liver': 43769, 'abelhinha': 43770, \"bee'\": 43771, 'hastings…': 43772, 'diamond…': 43773, '‘suicide’': 43774, 'absurdly': 43775, 'chaep': 43776, 'flat…': 43777, 'gunroom…': 43778, 'robbery…': 43779, 'pharoah’s': 43780, 'tomb…': 43781, 'sea…': 43782, 'minister…': 43783, 'banker…': 43784, 'willl': 43785, 'gaw': 43786, 'midwife': 43787, \"fleetwood's\": 43788, 'hardev': 43789, 'bahri': 43790, 'familiarised': 43791, 'syntaxes': 43792, 'history…': 43793, 'ku': 43794, 'klux': 43795, 'klan': 43796, 'ben’s': 43797, 'surete': 43798, 'townships': 43799, 'picket': 43800, 'fences': 43801, \"'chief\": 43802, 'cleeves': 43803, '432': 43804, 'misdeeds': 43805, 'agnes’': 43806, 'harpertorch': 43807, 'mir': 43808, 'fantastiki': 43809, 'mmorpg': 43810, 'warped': 43811, 'voodoo': 43812, 'solitaire': 43813, 'smersh': 43814, 'big’s': 43815, 'harlem’s': 43816, 'throbbing': 43817, 'desais': 43818, 'barons': 43819, 'sahnis': 43820, 'karna': 43821, 'nehwal': 43822, \"saina's\": 43823, 'sleepovers': 43824, 'champ': 43825, 'environment–development': 43826, 'equitable': 43827, 'elucidated': 43828, 'naturopathic': 43829, 'talib': 43830, 'psoriasis': 43831, 'sagging': 43832, 'wenzel’s': 43833, 'tolkien’s': 43834, 'baggins': 43835, 'gandalf': 43836, 'smaug': 43837, 'scanned': 43838, 'command…': 43839, '‘boss’': 43840, 'casilieris': 43841, 'imagined…especially': 43842, 'valentina’s': 43843, 'limits…and': 43844, \"montgomery's\": 43845, 'unthinkable…convince': 43846, 'kasie': 43847, 'love…and': 43848, 'judicious': 43849, 'quotable': 43850, 'lustre': 43851, 'cowl': 43852, 'knightfall': 43853, '512': 43854, '514': 43855, '676': 43856, '677': 43857, '679': 43858, '681': 43859, 'macintyre': 43860, 'professionalism': 43861, 'rodric': 43862, 'braithwaite': 43863, 'chollet': 43864, 'rabindra': 43865, 'chitravali': 43866, 'overarching': 43867, 'siva': 43868, 'reproduce': 43869, 'correlated': 43870, 'accolade': 43871, 'exemplifies': 43872, \"'john\": 43873, \"grisham'\": 43874, 'informant': 43875, 'gawker': 43876, 'gothamist': 43877, '270': 43878, 'corvette': 43879, 'convoys': 43880, 'darting': 43881, 'freckles': 43882, 'apricot': 43883, 'cod': 43884, 'guacamole': 43885, 'melba': 43886, 'nye': 43887, 'interconnections': 43888, 'krech': 43889, 'todo': 43890, 'middlebury': 43891, 'ramkrishna': 43892, 'incarnate': 43893, 'paramhamsa': 43894, 'misdemeanours': 43895, 'walia': 43896, 'marwah': 43897, 'sardonic': 43898, 'slurs': 43899, 'heightens': 43900, 'gurdaspur': 43901, 'suraiya': 43902, 'geeta': 43903, 'nutan': 43904, 'vyjayanthimala': 43905, 'zeenat': 43906, 'aman': 43907, 'munim': 43908, 'incomprehension': 43909, 'irrelevance': 43910, 'stabilizing': 43911, 'seedbed': 43912, 'scold': 43913, 'enabler': 43914, 'mujahedeen': 43915, 'covertly': 43916, \"wilson's\": 43917, \"haq's\": 43918, 'haqqani': 43919, 'misbegotten': 43920, 'demis': 43921, 'hassabis': 43922, 'hinton': 43923, 'yann': 43924, 'lecun': 43925, 'yoshua': 43926, 'bengio': 43927, 'montreal': 43928, 'koller': 43929, 'bostrom': 43930, 'grosz': 43931, 'ferrucci': 43932, 'manyika': 43933, 'tenenbaum': 43934, 'kaliouby': 43935, 'affectiva': 43936, 'daniela': 43937, 'rus': 43938, 'cynthia': 43939, 'breazeal': 43940, 'oren': 43941, 'etzioni': 43942, 'nyu': 43943, 'jobless': 43944, 'arbor': 43945, 'sunglasses': 43946, \"pacy'\": 43947, \"'nonstop\": 43948, \"galore'\": 43949, 'reenters': 43950, 'vrmmos': 43951, 'sinon': 43952, 'ryn': 43953, 'organizationís': 43954, 'inflection': 43955, 'punched': 43956, 'binders': 43957, 'teleconferencing': 43958, 'conferencing': 43959, 'fulfills': 43960, '•created': 43961, '•more': 43962, '•concise': 43963, '•a': 43964, '•no': 43965, 'furman': 43966, 'murr': 43967, 'optimizes': 43968, 'performance―results': 43969, '‘wondering': 43970, \"nel's\": 43971, '‘paula': 43972, '‘fans': 43973, 'hawkins’': 43974, 'whodunnit': 43975, 'gratified': 43976, '‘dark': 43977, 'goodrich': 43978, 'tamassia': 43979, 'reinstated': 43980, 'ensued': 43981, 'mounted': 43982, 'bloodier': 43983, 'bickered': 43984, 'brunt': 43985, 'worryingly': 43986, 'croatia': 43987, 'cavern': 43988, 'necropolis': 43989, 'falter': 43990, 'life—whether': 43991, 'overwork': 43992, 'overuse': 43993, 'technology—are': 43994, 'pinched': 43995, 'walker’s': 43996, 'rehabilitation': 43997, 'numbering': 43998, \"'wings\": 43999, 'recollects': 44000, 'langley': 44001, 'wallops': 44002, \"'missile\": 44003, 'solution–to': 44004, 'cycling—working': 44005, 'sparingly': 44006, 'undereating': 44007, 'personalizing': 44008, 'forewords': 44009, 'udo': 44010, 'fad': 44011, 'evocatively': 44012, 'conductor': 44013, 'granada': 44014, 'daria': 44015, \"song's\": 44016, 'cranes': 44017, 'farthing': 44018, 'bicycles': 44019, 'trolleys': 44020, 'braided': 44021, 'follett’s': 44022, '‘sociological': 44023, 'imagination’': 44024, 'reveres': 44025, 'satirise': 44026, 'brilliant’': 44027, 'budokai': 44028, 'heavens”': 44029, 'pipsqueak': 44030, 'hemit': 44031, '“martini': 44032, '—tennessean': 44033, 'there…': 44034, 'brauer’s': 44035, 'died—under': 44036, 'talisman—a': 44037, \"schwarzenegger's\": 44038, 'thal': 44039, \"crime's\": 44040, 'unrestrained': 44041, 'impromptu': 44042, 'disquisitions': 44043, 'aja': 44044, 'dvr': 44045, 'mageddon': 44046, 'arrowed': 44047, 'khalil': 44048, 'amandla': 44049, 'stenberg': 44050, 'anxiously': 44051, 'consul': 44052, 'shangri': 44053, 'parlayed': 44054, 'wal': 44055, 'retailer': 44056, 'legible': 44057, 'helpers': 44058, 'kashyapa': 44059, 'timi': 44060, 'vinata': 44061, 'kadru': 44062, 'crawl': 44063, 'surabhi': 44064, 'hooves': 44065, 'sarama': 44066, 'surasa': 44067, 'pedometers': 44068, 'ligaments': 44069, 'laps': 44070, 'discouraging': 44071, 'rarer': 44072, \"find'\": 44073, 'daldry': 44074, 'dumpsite': 44075, 'eking': 44076, 'sifting': 44077, 'gardo': 44078, 'evacuated': 44079, 'counterterrorism': 44080, \"'sizzles\": 44081, \"secrets'\": 44082, \"prisoners'\": 44083, \"'vince\": 44084, \"label'\": 44085, 'krasinski': 44086, 'clancy—a': 44087, 'ryan—nominated': 44088, 'on—and': 44089, 'powers—a': 44090, 'macnamara': 44091, \"savannah's\": 44092, 'precinct': 44093, \"duncan's\": 44094, 'amory’s': 44095, 'osborne’s': 44096, 'canon’': 44097, 'milwaukee': 44098, 'fatherless': 44099, 'tels': 44100, 'restroom': 44101, 'unstintingly': 44102, 'happyness': 44103, 'horatio': 44104, 'alger': 44105, 'antwone': 44106, \"'america's\": 44107, 'richmond': 44108, '1765': 44109, \"qur'an\": 44110, 'belie': 44111, 'spellberg': 44112, 'toleration': 44113, 'catholics': 44114, 'rancorous': 44115, \"jefferson's\": 44116, 'vilify': 44117, 'incapability': 44118, 'kasam': 44119, 'junkie': 44120, 'serendipity': 44121, \"alaska's\": 44122, 'valdez': 44123, 'bungee': 44124, 'jumpers': 44125, 'paragliders': 44126, 'daredevils': 44127, 'eiger': 44128, 'beachcomber': 44129, 'clemency': 44130, \"ronan's\": 44131, 'martha': 44132, 'cintra': 44133, 'niflgaard': 44134, 'factorisation': 44135, 'spectral': 44136, \"wide''\": 44137, 'crackdown': 44138, 'azeem': 44139, 'ibrahim’s': 44140, 'rakhine': 44141, 'myanmar—a': 44142, '‘national': 44143, 'races’': 44144, 'fomenting': 44145, 'moves—a': 44146, 'hatsume': 44147, 'provisional': 44148, 'arita': 44149, \"victims'\": 44150, 'recreating': 44151, \"nationalities'\": 44152, 'offing': 44153, 'duration': 44154, 'complacence': 44155, 'nab': 44156, 'intekhaab': 44157, 'abbas': 44158, 'nupur': 44159, 'anand’s': 44160, 'opulent': 44161, 'downturn': 44162, 'worsened': 44163, \"kingfisher's\": 44164, 'loaned': 44165, \"mallya's\": 44166, 'pilfered': 44167, 'misappropriation': 44168, 'defaulter': 44169, 'kingfizzer': 44170, 'gadabout': 44171, \"'air\": 44172, \"ambulance'\": 44173, 'sunburnt': 44174, 'sneezy': 44175, 'jack’s': 44176, '“perhaps': 44177, 'dupe': 44178, 'chopin': 44179, 'louisiana': 44180, 'wrench': 44181, 'edna': 44182, 'pontellier': 44183, 'greenbaum': 44184, \"press'\": 44185, 'berenson': 44186, 'outlet': 44187, \"alicia's\": 44188, 'belligerent': 44189, '“riveting': 44190, '–buzzfeed': 44191, '—popsugar': 44192, 'six—an': 44193, 'all”': 44194, 'jumpstart': 44195, '‘80s': 44196, 'unspools': 44197, 'monique’s': 44198, '“heartbreaking': 44199, 'beautiful”': 44200, 'blynn': 44201, '“tinseltown': 44202, 'finest”': 44203, 'means—and': 44204, 'costs—to': 44205, '“fascinating”': 44206, 'ibm’s': 44207, 'trounced': 44208, 'technologies—with': 44209, 'core—will': 44210, 'retailing': 44211, 'mcafee—two': 44212, 'field—reveal': 44213, 'kinds—from': 44214, 'drivers—will': 44215, 'revamping': 44216, 'socialists': 44217, 'abortive': 44218, 'putsch': 44219, 'hess': 44220, 'newlywed': 44221, \"r's\": 44222, 'uglier': 44223, 'unsolicited': 44224, 'life—quite': 44225, 'annoyingly—is': 44226, 'begrudgingly': 44227, 'curvy': 44228, \"'after\": 44229, 'shaurya': 44230, 'cluttering': 44231, 'bloods': 44232, 'bred': 44233, '“fundamentals': 44234, 'computers”': 44235, 'interconnection': 44236, 'wifi': 44237, 'wimax': 44238, '“fundamental': 44239, 'knowledge”': 44240, 'multiprogramming': 44241, 'dma': 44242, 'dsp': 44243, 'rfid': 44244, 'wigig': 44245, 'gsm': 44246, 'cdma': 44247, 'mpeg': 44248, 'gauhati': 44249, 'vocational': 44250, 'guwahati': 44251, 'burdwan': 44252, 'polytechnics': 44253, 'utkal': 44254, 'biju': 44255, 'patnayak': 44256, 'kiit': 44257, 'bhagalpur': 44258, 'magadh': 44259, 'bodh': 44260, 'aryabhat': 44261, 'ctr': 44262, 'dibrugarh': 44263, 'microcomputers': 44264, 'sans': 44265, 'tiered': 44266, 'minya': 44267, 'direst': 44268, \"citadel's\": 44269, 'mesarthim': 44270, 'mogadorians': 44271, \"'number\": 44272, 'transformers': 44273, 'ingo': 44274, \"arndt's\": 44275, 'wetlands': 44276, 'bined': 44277, 'mating': 44278, 'justification': 44279, 'featuresbrings': 44280, 'reversal': 44281, 'cyberbullying': 44282, \"justice'\": 44283, \"refugees'\": 44284, \"'defamation\": 44285, \"religion'\": 44286, \"'give\": 44287, 'buns': 44288, 'biscuits': 44289, 'eclairs': 44290, 'gallops': 44291, \"horse's\": 44292, 'comeuppance': 44293, 'cunningduck': 44294, 'noyes': 44295, \"highwayman'\": 44296, 'ceators': 44297, 'candace': 44298, 'pondered': 44299, 'biomolecular': 44300, 'bayes': 44301, 'pregel': 44302, 'bwla': 44303, \"lerner's\": 44304, \"longman's\": 44305, \"english's\": 44306, 'curricular': 44307, 'bs': 44308, 'romola': 44309, 'garai': 44310, 'athene': 44311, 'emporium': 44312, 'spectre': 44313, \"warlock's\": 44314, 'magus': 44315, 'stockpile': 44316, 'enchants': 44317, 'antagonizes': 44318, 'hamburgers': 44319, 'franchising': 44320, 'valve': 44321, 'comics—collected': 44322, 'time—that': 44323, 'enviably': 44324, 'crept': 44325, 'pled': 44326, 'mcas': 44327, 'pg': 44328, 'weaken': 44329, 'bureaucratic': 44330, 'asim': 44331, 'cossipore': 44332, 'baranagar': 44333, 'narmada': 44334, 'bachao': 44335, '1zo': 44336, '899': 44337, '‘trick’': 44338, 'job’': 44339, 'plymouth': 44340, \"nike's\": 44341, 'gutsy': 44342, 'sewer': 44343, 'livened': 44344, 'hanon': 44345, 'ethnography': 44346, 'repositioned': 44347, 'provid': 44348, 'disciplining': 44349, 'clube': 44350, 'lisbon': 44351, 'bioconductor': 44352, 'lattice': 44353, '‘eeny': 44354, 'meeny': 44355, 'miny': 44356, 'moe': 44357, 'pine’s': 44358, 'mule': 44359, 'carvings': 44360, \"hocking's\": 44361, 'everly': 44362, 'förening': 44363, 'lemillion': 44364, 'midoriya’s': 44365, 'yehi': 44366, '‘tahir': 44367, 'fantasy’': 44368, 'torch': 44369, 'kauf': 44370, 'outfox': 44371, 'treacherousness': 44372, 'herself—a': 44373, 'sharan': 44374, \"'am\": 44375, 'publicize': 44376, 'mahar': 44377, 'perosn': 44378, 'numbling': 44379, 'unthinking': 44380, 'devy': 44381, 'lajos': 44382, 'tranform': 44383, \"doll's\": 44384, \"egri's\": 44385, 'cafes': 44386, 'volatile—no': 44387, 'soccer’s': 44388, 'germain': 44389, 'bosnian': 44390, 'kids—opponents': 44391, 'underdogs': 44392, '“terrific': 44393, 'ibra’s': 44394, '”—sports': 44395, 'footballer’s': 44396, 'zlatan—from': 44397, 'ever—is': 44398, '”—aleksandar': 44399, 'hemon': 44400, '”—marcus': 44401, 'samuelsson': 44402, '“probably': 44403, 'obligatory': 44404, 'snigger': 44405, '‘footballer’s': 44406, 'portnoy’s': 44407, '”—financial': 44408, '“he': 44409, '”—policymic': 44410, 'ramanath': 44411, 'sahai': 44412, '1700': 44413, \"'citizen\": 44414, \"journalism'\": 44415, \"'e\": 44416, \"learning'\": 44417, \"'hate\": 44418, \"'learning\": 44419, \"disability'\": 44420, \"'newsworthy'\": 44421, \"'road\": 44422, \"rage'\": 44423, \"'dwarf\": 44424, \"'remuneration'\": 44425, \"'reforestation'\": 44426, \"'tax\": 44427, \"evasion'\": 44428, \"'namaskar'\": 44429, \"'nirvana'\": 44430, \"'brain\": 44431, \"drain'\": 44432, \"'electrocardiogram'\": 44433, \"'metal\": 44434, \"detector'\": 44435, \"'microsurgery'\": 44436, \"'acid\": 44437, \"test'\": 44438, \"'aqua'\": 44439, \"'call\": 44440, \"centre'\": 44441, \"'correspondence\": 44442, \"'malware'\": 44443, \"'computer'\": 44444, \"'software'\": 44445, \"'smartphone'\": 44446, \"'multiplex'\": 44447, 'crestwell': 44448, \"hotel's\": 44449, 'mcintyre': 44450, 'wilder': 44451, 'josie': 44452, 'brady—the': 44453, 'old—a': 44454, '“athlete’s': 44455, 'bible”': 44456, 'jeter': 44457, '“brady': 44458, 'mentally”': 44459, 'pliability': 44460, 'living—and': 44461, 'supplementation': 44462, 'level—achieve': 44463, 'haldi': 44464, 'kumkum': 44465, 'bhasm': 44466, 'chandan': 44467, 'signify': 44468, 'puja': 44469, 'thali': 44470, 'channel’s': 44471, 'dhyan': 44472, 'darshan': 44473, 'aastik': 44474, 'nastik': 44475, 'varaha': 44476, '249': 44477, 'sobriquet': 44478, 'jamaican': 44479, 'viv': 44480, 'shirk': 44481, 'halcyon': 44482, 'laments': 44483, '20m': 44484, \"icc's\": 44485, 'twenty20': 44486, 'hersh': 44487, 'awakenings': 44488, \"'economics'\": 44489, 'shlokas': 44490, 'tunisia': 44491, \"'commodity\": 44492, 'fraudulent': 44493, 'framebusting': 44494, 'summarized': 44495, 'checklist': 44496, 'cehv8': 44497, '9781119072171': 44498, 'nebraska': 44499, 'streeter': 44500, 'dyestuffs': 44501, 'mordanting': 44502, 'modifiers': 44503, 'fastness': 44504, 'woad': 44505, 'swatches': 44506, 'knitters': 44507, 'weavers': 44508, 'gardeners': 44509, 'diyers': 44510, 'nixon’s': 44511, '“deep': 44512, 'aleksandra': 44513, 'festered': 44514, 'inauguration': 44515, \"zoran's\": 44516, 'jeopardizes': 44517, 'grossest': 44518, 'eek': 44519, '920': 44520, 'tiles': 44521, 'yuck': 44522, 'married—but': 44523, 'nicest': 44524, \"“i'm\": 44525, 'muddled': 44526, 'smokescreen': 44527, 'hesitant': 44528, \"'quick\": 44529, \"fix'\": 44530, \"'jugaad'\": 44531, 'laissez': 44532, \"'chalta\": 44533, \"'m\": 44534, \"document'\": 44535, 'demographics': 44536, 'thangkas': 44537, 'intra': 44538, 'dead—to': 44539, 'mankell': 44540, 'håkan': 44541, \"nesser's\": 44542, 'smashes': 44543, 'maardam': 44544, \"veeteren's\": 44545, 'weeping': 44546, 'putana': 44547, 'trinavarta': 44548, 'vatasura': 44549, 'bakasura': 44550, 'extenstive': 44551, 'readerwho': 44552, '31st': 44553, '535': 44554, '01': 44555, 'dialga': 44556, 'pokémon—and': 44557, 'dtd': 44558, 'xpath': 44559, \"'like\": 44560, \"yesterday'\": 44561, 'hounding': 44562, 'vaccinations': 44563, 'hurtful': 44564, 'indulgences': 44565, \"dickens'\": 44566, 'unexceptional': 44567, 'estella': 44568, 'copperfield': 44569, 'aylmer': 44570, 'maude': 44571, 'karenina': 44572, 'abounding': 44573, '1466': 44574, '1536': 44575, 'pieties': 44576, \"individual's\": 44577, 'personification': 44578, 'drunkenness': 44579, 'lambast': 44580, 'pretensions': 44581, \"'folly'\": 44582, 'maarten': 44583, 'dorp': 44584, '142': 44585, '790': 44586, 'altitudinal': 44587, 'sis': 44588, 'parchin': 44589, 'mountainous': 44590, 'armenia': 44591, 'fiasco': 44592, 'irgc': 44593, 'hardliners': 44594, 'accountant': 44595, 'diplomas': 44596, 'turnkey': 44597, 'computerization': 44598, 'assisting': 44599, \"music's\": 44600, \"music'\": 44601, 'unappreciated': 44602, 'bharatanatyam': 44603, 'cine': 44604, 'motherboards': 44605, 'antivirus': 44606, 'spammers': 44607, 'smaller…': 44608, 'ipods': 44609, 'eink': 44610, 'refrigerates': 44611, 'plasma': 44612, 'dlp': 44613, 'oled': 44614, 'wi': 44615, 'broadband': 44616, 'dsl': 44617, 'more…': 44618, '11a': 44619, '129': 44620, '11b': 44621, '483': 44622, 'ridesharing': 44623, 'vanguard': 44624, \"deloitte's\": 44625, 'megafoundations': 44626, 'recyclebank': 44627, 'relayrides': 44628, 'livinggoods': 44629, 'flexibly': 44630, \"northrup's\": 44631, 'qr': 44632, 'con\\xadditioning': 44633, '\\xadnot': 44634, 'cockiness': 44635, 'brashness': 44636, 'improperly': 44637, 'jekyll': 44638, 'hawley': 44639, 'cawthorne': 44640, 'glove': 44641, 'mujahideen': 44642, 'fidget': 44643, '“maker”': 44644, 'kart': 44645, '“making': 44646, 'apollo': 44647, 'mission’s': 44648, 'home”': 44649, 'tove': 44650, 'jansson': 44651, 'moomins': 44652, 'ennobling': 44653, 'heightening': 44654, 'cyril': 44655, 'anthea': 44656, 'psammead': 44657, 'odysseys': 44658, 'zamperini’s': 44659, 'california–berkeley': 44660, 'grievances': 44661, 'expediency': 44662, 'sprints': 44663, 'thicket': 44664, 'tribalism': 44665, 'hedonism': 44666, 'subjectivism': 44667, \"eternal'\": 44668, 'avanti': 44669, 'ujjayini': 44670, 'barbaric': 44671, 'hunas': 44672, 'sakas': 44673, 'sindhuvarta': 44674, 'vikramaditya’s': 44675, 'meditated': 44676, 'banyan': 44677, 'grove': 44678, 'ravana’s': 44679, 'bilva': 44680, 'meghna': 44681, 'shobhaa': 44682, 'sanghi': 44683, 'kandasamy': 44684, 'fulltime': 44685, 'reared': 44686, 'maniacs': 44687, 'propitiation': 44688, '“loving”': 44689, 'bloodletting': 44690, 'computerís': 44691, 'vectorized': 44692, 'advantageous': 44693, 'limits—getting': 44694, 'weighted': 44695, \"'indian'\": 44696, 'embroils': 44697, 'pollsters': 44698, 'democrat': 44699, 'scum': 44700, 'microchip': 44701, 'resurfaces': 44702, 'assertiveness': 44703, '‘iron’': 44704, 'sloman': 44705, 'keidis’s': 44706, '‘scar': 44707, 'tissue’': 44708, 'unputdown': 44709, 'infamy': 44710, 'scaly': 44711, \"ames'\": 44712, 'bengalis': 44713, 'soumitra': 44714, 'tizzy': 44715, \"feluda's\": 44716, 'cocooned': 44717, 'onscreen': 44718, 'remade': 44719, 'language―but': 44720, 'anchored': 44721, 'machines―and': 44722, 'nantucket': 44723, 'kapenash': 44724, \"groom's\": 44725, \"'sink\": 44726, 'scented': 44727, \"'hilderbrand\": 44728, 'curveballs': 44729, \"'hilderbrand's\": 44730, \"better'\": 44731, 'bookreporter': 44732, 'signifies': 44733, 'candis': 44734, \"vishy's\": 44735, \"upsc's\": 44736, 'reattempts': 44737, \"'mount\": 44738, \"ias'\": 44739, 'guessed': 44740, \"gripped'\": 44741, 'essex': 44742, 'makingly': 44743, \"writing'\": 44744, 'caserta': 44745, 'wiley': 44746, 'injures': 44747, 'preoccupied': 44748, '29th': 44749, 'kunugigaoka': 44750, 'earth—unless': 44751, 'sugino': 44752, 'okuda': 44753, 'ets': 44754, 'maker’s': 44755, 'powerprep®': 44756, 'shikhandi': 44757, 'mahadeva': 44758, 'devotee’s': 44759, 'chudala': 44760, 'samavan': 44761, 'mesopotamian': 44762, 'spellinbinding': 44763, 'told…': 44764, 'macgregor': 44765, 'picking–up': 44766, '80’s': 44767, 'heart–stopping': 44768, 'provactive': 44769, 'leighton': 44770, 'stereotypical': 44771, 'booklover': 44772, 'combinatorics': 44773, 'lattices': 44774, 'manager’s': 44775, 'ssdt': 44776, 'to…': 44777, 'drillthroughs': 44778, 'cireson': 44779, \"meyler's\": 44780, 'circular': 44781, 'traversal': 44782, 'stocking': 44783, 'filler': 44784, 'torments': 44785, \"anastasia's\": 44786, 'cropped': 44787, \"vetticad's\": 44788, 'balans': 44789, 'chopras': 44790, 'kapoors': 44791, 'bjork': 44792, 'whoopi': 44793, 'macarthur': 44794, 'gerwig': 44795, 'bocelli': 44796, 'hua': 44797, 'mulan': 44798, 'microscopes': 44799, 'bunsen': 44800, 'burners': 44801, 'nephews': 44802, 'nieces': 44803, \"huffington's\": 44804, 'cheekbone': 44805, 'gash': 44806, 'gracing': 44807, 'mri': 44808, 'echocardiogram': 44809, 'commencement': 44810, 'likened': 44811, 'stool': 44812, 'temporarily': 44813, 'eulogies': 44814, 'promotions': 44815, 'kindnesses': 44816, 'harried': 44817, 'unplugging': 44818, 'lubanovic': 44819, 'pythonís': 44820, \"gawande's\": 44821, \"mortal'\": 44822, 'anesthesiologist': 44823, 'temporality': 44824, 'unsatisfied': 44825, 'goose': 44826, 'carbuncle': 44827, 'boruto’s': 44828, 'daimyo’s': 44829, 'tento': 44830, 'babysitting': 44831, \"network's\": 44832, 'toonami': 44833, 'hulu': 44834, \"'fresh\": 44835, \"characters'\": 44836, 'durrant': 44837, 'undertakings': 44838, 'adolph': 44839, 'eichmann': 44840, 'mossad’s': 44841, 'immigrated': 44842, 'hebrew': 44843, 'knesset': 44844, 'sokolov': 44845, 'gurion': 44846, 'shimon': 44847, 'peres': 44848, 'bulgarian': 44849, 'sixtieth': 44850, 'anniversaries': 44851, \"israel's\": 44852, 'shlomo': 44853, 'retroactively': 44854, 'besson': 44855, \"latter's\": 44856, 'futurology': 44857, '—financial': 44858, 'renting': 44859, 'thuggee': 44860, 'formulaic': 44861, 'nivedita': 44862, 'harassment': 44863, 'surrogacy': 44864, \"servant's\": 44865, 'chaddi': 44866, 'hsk': 44867, 'notcfl': 44868, 'herculepoiratand': 44869, 'rancid': 44870, 'ricotta': 44871, 'repetoire': 44872, 'achievable': 44873, 'certify': 44874, 'outlasted': 44875, 'easing': 44876, 'stockholders': 44877, 'unimpressed': 44878, 'distrustful': 44879, 'inexperience': 44880, 'banded': 44881, 'ignites': 44882, 'holmesthe': 44883, 'storiesvolume': 44884, 'iisince': 44885, 'beeton’s': 44886, 'grimpen': 44887, 'moor': 44888, 'cabs': 44889, 'fogs': 44890, \"cabuliwallah'\": 44891, 'langkawi': 44892, 'tioman': 44893, 'kinabalu': 44894, 'keeney': 44895, \"travelers'\": 44896, 'miserly': 44897, 'triumphs—after': 44898, 'seelye': 44899, 'harman': 44900, 'wars™': 44901, 'dorling': 44902, 'kindersley': 44903, 'lucasfilm': 44904, 'condolence': 44905, 'matrimonials': 44906, 'inquiries': 44907, 'remittance': 44908, 'circulars': 44909, 'deviations': 44910, 'ficers': 44911, 'titters': 44912, 'aesop': 44913, 'academician': 44914, 'avocation': 44915, 'leit': 44916, 'motif': 44917, 'quizzing': 44918, \"3r's\": 44919, \"trainer's\": 44920, 'consolations': 44921, 'atheists': 44922, \"expressed'\": 44923, \"'alain\": 44924, \"innocence'\": 44925, \"daisy'\": 44926, 'satisfactorily': 44927, \"taleb's\": 44928, 'procrustes': 44929, 'demolish': 44930, 'philistinism': 44931, 'phoniness': 44932, 'rand’s': 44933, 'gonda': 44934, 'ideal—a': 44935, 'pleas': 44936, 'peikoff': 44937, 'chamb': 44938, 'countered': 44939, 'detonating': 44940, 'olivegreen': 44941, 'rachna': 44942, 'bisht': 44943, 'rawat': 44944, 'genitives': 44945, 'plurals': 44946, 'haben': 44947, 'sein': 44948, \"ender's\": 44949, 'pulverised': 44950, 'gethin’s': 44951, 'publication—bodybuilding': 44952, 'com—creating': 44953, 'kris’s': 44954, 'started—': 44955, 'kaged': 44956, 'educating': 44957, 'certifying': 44958, 'ranveer': 44959, 'spokesperson': 44960, 'world—www': 44961, 'com—and': 44962, 'grooming': 44963, 'greive': 44964, \"btg's\": 44965, 'btg': 44966, 'barbell': 44967, 'goals—in': 44968, 'fashionistas': 44969, 'yuko': 44970, \"moriguchi's\": 44971, 'bash': 44972, 'scraper': 44973, 'things—including': 44974, 'pokémon—through': 44975, 'people…but': 44976, 'within—or': 44977, 'erupt': 44978, \"zone'\": 44979, 'conjoined': 44980, \"'inspired'\": 44981, \"'spiritual'\": 44982, 'inhibit': 44983, 'ragdoll': 44984, \"'ragdoll'\": 44985, \"'bait'\": 44986, \"'puppet'\": 44987, 'dci': 44988, 'rouche': 44989, \"s'agrandit\": 44990, 'renouveles': 44991, 'approche': 44992, 'ouverte': 44993, 'monde': 44994, \"d'outils\": 44995, \"l'apprentissage\": 44996, 'langue': 44997, 'niveau': 44998, 'formes': 44999, 'annexe': 45000, 'lexique': 45001, 'thematique': 45002, 'abecedaire': 45003, 'culturel': 45004, 'actualise': 45005, 'fiches': 45006, 'developper': 45007, 'redaction': 45008, 'universitaires': 45009, 'professionnelles': 45010, 'reorganisation': 45011, 'dossiers': 45012, 'eclairages': 45013, 'entrer': 45014, 'varies': 45015, 'ludiques': 45016, 'approfondir': 45017, 'decouvrir': 45018, 'textes': 45019, 'litteraires': 45020, 'apprendre': 45021, 'argumenter': 45022, \"l'oral\": 45023, \"l'ecrit\": 45024, \"s'exercer\": 45025, 'renforcer': 45026, 'acquis': 45027, 'lexicaux': 45028, 'pragmatiques': 45029, 'travail': 45030, 'approfondi': 45031, 'chaque': 45032, 'delf': 45033, 'composants': 45034, 'livre': 45035, \"l'eleve\": 45036, 'enregistrements': 45037, 'complementaires': 45038, 'inclus': 45039, 'cahier': 45040, 'classe': 45041, 'x3': 45042, 'numerique': 45043, 'interactif': 45044, \"l'enseignant\": 45045, 'cle': 45046, 'apprises': 45047, 'testimonials': 45048, 'concepts—classes': 45049, 'virtusa': 45050, 'hcl': 45051, 'aon': 45052, 'hewitt': 45053, 'convergys': 45054, 'csc': 45055, 'wipro': 45056, 'reviewer’s': 45057, 'ela': 45058, 'requisite': 45059, '—komal': 45060, 'panzade': 45061, 'horoscope': 45062, \"jaffrey's\": 45063, 'cumin': 45064, 'palanquin': 45065, 'sultanas': 45066, 'cauliflowers': 45067, 'coriander': 45068, 'pooris': 45069, 'leitmotiv': 45070, 'dinners': 45071, 'breakfasts': 45072, 'northumbria': 45073, 'indissolubly': 45074, 'paganism': 45075, \"cornwell's\": 45076, 'ivar': 45077, 'boneless': 45078, 'ubba': 45079, 'rarest': 45080, \"robertson's\": 45081, 'narrows': 45082, 'synthesizes': 45083, 'expenditure': 45084, 'stantly': 45085, 'enslave': 45086, 'nally': 45087, 'doremon': 45088, 'kinnikuman': 45089, 'lukewarm': 45090, 'serialisation': 45091, 'seishi': 45092, 'blazer': 45093, 'shimoto': 45094, 'monstrously': 45095, 'prospects—and': 45096, 'goal…or': 45097, \"stuff'\": 45098, 'lezard': 45099, 'padua': 45100, \"babbage's\": 45101, 'mortar': 45102, 'chanting': 45103, \"moeen's\": 45104, 'sparkhill': 45105, 'tenacious': 45106, 'operationalize': 45107, 'pivotal’s': 45108, 'cloudfoundry': 45109, 'digvijay': 45110, 'deo': 45111, 'sportspersons': 45112, 'medallists': 45113, 'balbir': 45114, 'leander': 45115, 'paes': 45116, 'karnam': 45117, 'malleswari': 45118, 'bindra': 45119, 'sushil': 45120, 'milkha': 45121, 'usha': 45122, 'bhagwat': 45123, 'duddle': 45124, 'tomislav': 45125, 'tomic': 45126, 'approved': 45127, 'eagle': 45128, 'hippogriff': 45129, 'basilisk': 45130, 'horntail': 45131, 'magizoologist': 45132, 'copy’': 45133, 'lowery': 45134, 'arkansas': 45135, 'bert': 45136, 'gleason': 45137, 'consults': 45138, '1888–1955': 45139, 'maryville': 45140, 'harboured': 45141, '1930’s': 45142, 'canvases': 45143, 'affirmed': 45144, 'taschen': 45145, 'airlines': 45146, 'orientated': 45147, 'overlay': 45148, 'remington': 45149, 'yusaku': 45150, 'kamekura': 45151, 'stankowski': 45152, 'daiei': 45153, 'yvon': 45154, 'chouinard—legendary': 45155, 'patagonia': 45156, '—shares': 45157, 'corporation—it': 45158, 'tat': 45159, '—choice': 45160, 'strauss': 45161, 'audi': 45162, 'unilever': 45163, 'hegarty’s': 45164, 'saatchi': 45165, 'bartle': 45166, 'bogle': 45167, 'bbh': 45168, 'renown': 45169, 'executive’s': 45170, 'cha': 45171, 'yu': 45172, 'impermanent': 45173, 'mind—an': 45174, 'surroundings—as': 45175, 'lighted': 45176, 'advice—so': 45177, \"'rogue'\": 45178, 'zululand': 45179, \"herd's\": 45180, 'astro': 45181, 'kimba': 45182, 'toshio': 45183, 'lancer': 45184, 'merchandising': 45185, 'frederik': 45186, 'schodt': 45187, 'l991': 45188, 'intifada': 45189, 'l996': 45190, '‘greedily': 45191, 'engorged': 45192, 'milton’s': 45193, 'disobeying': 45194, 'pracitcalpractical': 45195, 'latency': 45196, 'throughput': 45197, '156th': 45198, 'wisdenalmanack': 45199, 'candles': 45200, 'gashed': 45201, 'alas': 45202, 'judicature': 45203, 'vivaciously': 45204, 'municipalities': 45205, 'carrion': 45206, 'demogoblin': 45207, 'shriek': 45208, 'outmanned': 45209, 'overpowered': 45210, 'cloak': 45211, 'firestar': 45212, 'deathlok': 45213, \"carnage's\": 45214, 'cronies': 45215, 'violate': 45216, 'slinger': 45217, '378': 45218, '201': 45219, '203': 45220, 'croissants': 45221, 'kamikaze': 45222, 'pried': 45223, 'sprinted': 45224, 'washer': 45225, 'squeegee': 45226, 'staking': 45227, 'infernos': 45228, 'herosim': 45229, '749': 45230, 'dollops': 45231, 'above\\x85': 45232, 'spook’s': 45233, \"'unbeatable\": 45234, 'lovereading4kids': 45235, 'satisfactory': 45236, 'tacit': 45237, 'maneuvering': 45238, 'deterring': 45239, 'squeak': 45240, 'transparent': 45241, 'hows': 45242, 'chiusano': 45243, 'rúnar': 45244, 'bjarnason': 45245, 'scalaz': 45246, 'strictness': 45247, 'laziness': 45248, 'combinator': 45249, 'parser': 45250, 'combinators': 45251, 'monoids': 45252, 'monads': 45253, 'applicative': 45254, 'traversable': 45255, 'functors': 45256, 'mutable': 45257, 'mission—after': 45258, 'harinath': 45259, 'karidhal': 45260, 'spacecraft’s': 45261, 'moumita': 45262, 'minal': 45263, 'sampat': 45264, '‘wonder': 45265, 'gaganyaan': 45266, 'stars—and': 45267, 'psg': 45268, \"sportsman's\": 45269, \"nuts'\": 45270, 'snarling': 45271, 'spit': 45272, 'fireplace': 45273, 'roddy': 45274, 'grieve': 45275, 'msp': 45276, 'tidyverse': 45277, 'wickham': 45278, 'wrangleótransform': 45279, 'programólearn': 45280, 'exploreóexamine': 45281, 'modelóprovide': 45282, 'communicateólearn': 45283, 'markdown': 45284, 'bahmani': 45285, 'traversed': 45286, 'owed': 45287, 'bahmanis': 45288, 'supplanted': 45289, 'shahis': 45290, 'annihilated': 45291, '1680s': 45292, 'fortresses': 45293, 'apartments': 45294, 'firuzabad': 45295, 'decaying': 45296, 'kumatgi': 45297, 'himself—jeet': 45298, 'do—is': 45299, 'turnover': 45300, 'sip': 45301, 'piramal': 45302, 'conglomerates': 45303, \"'satan'\": 45304, 'reopen': 45305, 'gliding': 45306, 'grayskull—now': 45307, 'hordak’s': 45308, 'abnett': 45309, 'mhan': 45310, 'dark…': 45311, \"tibbet's\": 45312, 'harmed': 45313, 'materialize': 45314, 'vapours': 45315, \"leesha's\": 45316, 'jongleur': 45317, '‘heap': 45318, 'fish’': 45319, 'spiritualist': 45320, '‘blue': 45321, 'geranium’': 45322, '‘tuesday': 45323, 'club’': 45324, '1839': 45325, 'groomed': 45326, 'jamshedpur': 45327, 'hydro': 45328, 'off—as': 45329, 'curzon': 45330, 'imphal': 45331, 'kangla': 45332, 'imas': 45333, 'thangjam': 45334, 'manorama': 45335, 'aloft': 45336, '‘take': 45337, 'flesh’': 45338, 'capone': 45339, 'lonsdale': 45340, 'hergé': 45341, 'clappy': 45342, 'projectile': 45343, 'vomiting': 45344, \"'shit\": 45345, \"happens'\": 45346, 'ashwini': 45347, 'fútbol': 45348, '“beautiful': 45349, 'players—not': 45350, 'barcelona’s': 45351, 'friends—even': 45352, 'doordarshan': 45353, '‘chal': 45354, 'meri': 45355, 'luna’': 45356, '‘airtel': 45357, 'ads’': 45358, 'jargons': 45359, 'alyque': 45360, 'padamsee': 45361, 'piyush': 45362, 'kakkar': 45363, 'balki': 45364, 'agnello': 45365, 'chakravarty': 45366, 'nitesh': 45367, 'madhvani': 45368, 'khajanji': 45369, 'parshuraman': 45370, 'krishnamurthy': 45371, 'sheorey': 45372, \"judy's\": 45373, 'leprosy': 45374, 'leper': 45375, 'spinalonga': 45376, 'crete': 45377, \"'passionately\": 45378, \"'hislop\": 45379, 'cretans': 45380, \"outcasts'\": 45381, 'trainable': 45382, 'ergogenic': 45383, 'robbing': 45384, 'escalated': 45385, 'shaves': 45386, 'feds': 45387, 'wailing': 45388, 'seeped': 45389, '‘nine': 45390, 'unknown’': 45391, 'safekeeping': 45392, 'unspool': 45393, 'bettering': 45394, 'alienating': 45395, \"ronnie's\": 45396, \"veronica's\": 45397, 'stukely': 45398, 'cheltenham': 45399, 'artisan': 45400, 'blower': 45401, 'molten': 45402, 'furnace': 45403, 'reckons': 45404, 'thorny': 45405, \"logan's\": 45406, 'beauregard': 45407, 'outswim': 45408, 'bercaws': 45409, \"golf'\": 45410, 'marvelled': 45411, 'putter': 45412, 'phonological': 45413, 'kerouac': 45414, 'requireements': 45415, 'type′s': 45416, 'dimensioned': 45417, 'built–in': 45418, \"congress'\": 45419, 'contextual': 45420, 'situating': 45421, 'coalitional': 45422, '‘magnificent…': 45423, 'football’': 45424, 'soccernomics': 45425, 'blemishes': 45426, 'blackswan’s': 45427, 'literature—both': 45428, 'level—this': 45429, 'prosodic': 45430, '‘rhetoric’': 45431, '‘prosody’': 45432, 'scansion': 45433, '8021': 45434, 'sathaye': 45435, 'humourist': 45436, 'virender': 45437, 'sehwag': 45438, 'wizadry': 45439, 'gwynevere': 45440, 'lancelot': 45441, 'damsels': 45442, \"baine's\": 45443, 'morte': 45444, \"d'arthur\": 45445, \"malory's\": 45446, 'masterpiece—that': 45447, 'symbolizes': 45448, 'brendon': 45449, 'mccullum': 45450, \"baz'\": 45451, 'aroon': 45452, 'qutub': 45453, 'minar': 45454, 'chandrasekhar': 45455, 'meenakshi': 45456, 'pirzada': 45457, 'neutralizing': 45458, 'wmd': 45459, 'mau': 45460, 'yim': 45461, 'classically': 45462, \"'de\": 45463, 'to”': 45464, 'tutori': 45465, 'als': 45466, 'talentless': 45467, 'gilda': 45468, 'radner': 45469, 'illus': 45470, 'trated': 45471, 'sequins': 45472, '“magic': 45473, 'lying”': 45474, 'isms': 45475, 'it…well': 45476, 'say…“haters': 45477, 'knew’': 45478, 'hangover': 45479, 'worcestershire': 45480, 'tomato': 45481, 'bertie’s': 45482, 'spence': 45483, 'hostilely': 45484, 'pubic': 45485, 'discourage': 45486, 'shakers': 45487, 'trumps': 45488, 'findable': 45489, 'practicalities': 45490, \"ironwoods'\": 45491, \"'captivating'\": 45492, \"forget'\": 45493, 'ridenhour': 45494, 'bungled': 45495, 'scrawny': 45496, 'insurgent': 45497, 'neutrality': 45498, 'ballabh': 45499, 'bhadur': 45500, 'todays': 45501, 'uri': 45502, 'massa': 45503, 'wrml': 45504, 'uris': 45505, 'headers': 45506, 'configured': 45507, 'characterizing': 45508, 'improvisational': 45509, 'portland': 45510, 'booknews': 45511, \"'we'd\": 45512, \"competition'\": 45513, \"sinek's\": 45514, 'axioms': 45515, 'stomachs': 45516, 'jetlag': 45517, 'shortcomings': 45518, 'remain—indispensable': 45519, 'interrogates': 45520, 'precautionary': 45521, 'rajhans': 45522, \"college's\": 45523, 'vikrant': 45524, 'anamika': 45525, 'yuvi': 45526, 'types’': 45527, 'confidence’': 45528, 'accountants': 45529, 'telepresence': 45530, \"'practical\": 45531, \"expertise'\": 45532, \"'grand\": 45533, \"bargain'\": 45534, 'antiquated': 45535, \"'open\": 45536, 'collared': 45537, \"'gatekeepers'\": 45538, \"draw'\": 45539, 'laughingly': 45540, 'millionths': 45541, 'readers—of': 45542, 'hamlets—ruskin': 45543, 'steadied': 45544, 'autobiography—his': 45545, 'books—one': 45546, 'jamnagar': 45547, 'sea—where': 45548, 'poem—and': 45549, '1940s—where': 45550, 'undercut': 45551, 'book—the': 45552, 'india’—the': 45553, 'eccentrics—and': 45554, 'place—and': 45555, \"greeks'\": 45556, \"fibonacci's\": 45557, 'gestalt': 45558, 'hilaire': 45559, 'belloc': 45560, 'bertiehimself': 45561, 'madeleine': 45562, \"emsworth's\": 45563, \"'looney\": 45564, \"doctor'\": 45565, 'mulliner': 45566, 'buckley': 45567, 'brakes': 45568, 'norling': 45569, 'entailed': 45570, 'gurpreet': 45571, 'mahajan': 45572, 'hermeneutic': 45573, 'conceptualization': 45574, 'landlords': 45575, 'moneylenders': 45576, 'girls—just': 45577, 'kitschy': 45578, '‘polite’': 45579, 'cinema—the': 45580, 'avijit': 45581, 'ghosh’s': 45582, 'maiya': 45583, 'tohe': 45584, 'piyari': 45585, 'chadhaibo': 45586, 'dinesh': 45587, '‘nirahua’': 45588, 'behind—rural': 45589, 'fun—or': 45590, 'trash': 45591, 'tripping': 45592, 'create—and': 45593, 'destroy—south': 45594, 'pre–columbian': 45595, '“mixing': 45596, 'irresponsible': 45597, '”—david': 45598, 'gutowski': 45599, 'mylifeaseva': 45600, 'munchkin': 45601, 'sock': 45602, '“holy': 45603, 'schnitzel': 45604, 'fans—over': 45605, 'besties': 45606, 'through—and': 45607, 'xo': 45608, \"zebra's\": 45609, \"nfl's\": 45610, '49ers': 45611, \"'wisdom\": 45612, \"walsh'\": 45613, 'womanising': 45614, 'receptions': 45615, 'hispanic': 45616, \"o'donnell\": 45617, 'cherub': 45618, 'milord': 45619, 'garvin': 45620, \"modesty's\": 45621, 'pads': 45622, 'loc': 45623, 'sailed': 45624, 'provocation': 45625, \"alexis's\": 45626, 'unlit': 45627, 'unplugged': 45628, 'conditioner': 45629, \"kasey's\": 45630, 'danced': 45631, 'tully’s': 45632, 'jackson’s': 45633, 'motown’s': 45634, 'friends…friends': 45635, 'astaire': 45636, 'michael’s': 45637, \"'those\": 45638, \"muted'\": 45639, 'barbarity': 45640, \"evil'\": 45641, 'birrell': 45642, \"'courageous\": 45643, \"insisted'\": 45644, 'croix': 45645, 'vaclav': 45646, 'havel': 45647, 'murad': 45648, 'hungrily': 45649, 'eyebrows': 45650, 'definite': 45651, 'matunga': 45652, 'd’souza': 45653, 'zende': 45654, 'love—a': 45655, 'siqueira': 45656, 'suburbs’': 45657, 'novel—set': 45658, 'other—is': 45659, 'strattenberg': 45660, 'fretted': 45661, 'uke': 45662, \"lil'\": 45663, 'ukuleles': 45664, 'tablature': 45665, 'mensa': 45666, '534': 45667, \"solver's\": 45668, 'castilla': 45669, 'fascists': 45670, 'brawls': 45671, 'alfredo': 45672, 'stéfano': 45673, 'syllabic': 45674, 'timespan': 45675, 'customized': 45676, \"competition's\": 45677, 'vertically': 45678, 'supplant': 45679, '2049': 45680, 'wilfully': 45681, 'aiding': 45682, 'wakeup': 45683, 'misread': 45684, 'whittington': 45685, 'roped': 45686, 'candia': 45687, 'mcwilliam': 45688, \"'block'\": 45689, 'predatory': 45690, 'deplorably': 45691, 'baffin': 45692, \"baffin's\": 45693, 'moths': 45694, 'wharton’s': 45695, '1870s': 45696, 'newland': 45697, 'welland': 45698, 'olenska': 45699, '“society”': 45700, 'literature—an': 45701, 'capricious': 45702, 'professes': 45703, 'inviolable': 45704, \"ba'ath\": 45705, 'meagre': 45706, 'brochures': 45707, 'summarily': 45708, 'baladiyat': 45709, \"'shadow\": 45710, \"women'\": 45711, 'sherezades': 45712, \"mayada's\": 45713, \"iraq's\": 45714, 'cultured': 45715, 'freshers': 45716, 'jni': 45717, '“death”': 45718, 'bhaswar': 45719, 'saurabh': 45720, 'dhrishti': 45721, 'supriya': 45722, 'unni': 45723, 'satyarth': 45724, 'mathew': 45725, 'vibha': 45726, 'lohani': 45727, 'pooppotte': 45728, 'swaha': 45729, 'neelamani': 45730, 'sutar': 45731, 'subhobrata': 45732, 'pushkar': 45733, 'pande': 45734, 'nalini': 45735, 'chandran': 45736, 'gopinath': 45737, 'garg': 45738, 'ila': 45739, 'tulika': 45740, 'dubey': 45741, 'bhowmick': 45742, 'mukherjeeand': 45743, 'shiggaon': 45744, 'mtech': 45745, 'tillotson': 45746, 'indraprastha': 45747, 'symmetries': 45748, 'toaster': 45749, 'ruptured': 45750, 'bicep': 45751, 'tendon': 45752, 'briton': 45753, 'morocco': 45754, \"adam's\": 45755, 'dragomir': 45756, 'anika’s': 45757, 'choose—ignore': 45758, 'pariah': 45759, 'conversations—and': 45760, 'ocm': 45761, 'nio': 45762, 'travelers—magicians': 45763, 'londons': 45764, 'body\\x97the': 45765, 'hinder': 45766, 'perform\\x97and': 45767, 'squatuniversity': 45768, 'body\\x97and': 45769, 'nationalisms': 45770, 'havana': 45771, 'algiers': 45772, \"kennedy's\": 45773, 'kruschev': 45774, 'tse': 45775, 'tung': 45776, 'aleida': 45777, 'pegs': 45778, 'ecologically': 45779, 'overtaking': 45780, 'exempt': 45781, 'chamba': 45782, 'kangra': 45783, 'shahryar': 45784, 'pique': 45785, \"evening's\": 45786, 'aladdin': 45787, 'renata': 45788, 'fucikova': 45789, 'jindra': 45790, 'capek': 45791, 'leatherbound': 45792, 'gilt': 45793, 'edging': 45794, 'purified': 45795, 'bodhisatta—the': 45796, 'life—not': 45797, 'pali': 45798, 'theravada': 45799, 'perfections—giving': 45800, 'truthfulness': 45801, 'equanimity': 45802, 'perfections': 45803, 'specials': 45804, \"hedgehog's\": 45805, 'spaziante': 45806, 'mecha': 45807, 'memoization': 45808, 'parallelization': 45809, 'eliminates': 45810, \"madrigal's\": 45811, \"destiny's\": 45812, 'godlike': 45813, 'luster': 45814, 'samyukta': 45815, 'harmony—with': 45816, 'redstart': 45817, 'consolation': 45818, 'upholds': 45819, 'jumper': 45820, 'justice—and': 45821, 'dalton': 45822, 'rich—and': 45823, 'cambodia’': 45824, 'troupe': 45825, 'cambodian': 45826, 'sisowath': 45827, 'khmer': 45828, 'rouge': 45829, 'andaman': 45830, 'nicobar': 45831, 'islands': 45832, '‘september': 45833, '11’': 45834, 'retrieved': 45835, 'almanacs': 45836, 'clipping': 45837, 'durham': 45838, 'potpourri': 45839, 'incited': 45840, 'nova': 45841, 'scotia': 45842, 'secures': 45843, 'businesswomen': 45844, 'alder': 45845, 'triumphantly': 45846, 'mythos': 45847, 'atalanta': 45848, 'bellerophon': 45849, 'fitzsimons': 45850, 'crackles': 45851, 'bonfire': 45852, \"'gone\": 45853, \"triumph'\": 45854, \"captivating'\": 45855, 'jolting': 45856, \"'clear\": 45857, 'denouncement': 45858, 'sting': 45859, \"macabre'\": 45860, 'ratchets': 45861, \"motherhood'\": 45862, \"twists'\": 45863, \"'lydia\": 45864, \"betrayals'\": 45865, \"'deliciously\": 45866, \"'liz\": 45867, 'redman': 45868, 'flinch': 45869, \"o'shea\": 45870, 'rté': 45871, 'book—more': 45872, 'sold—for': 45873, 'is—is': 45874, 'mottola': 45875, 'superbad': 45876, 'fun—building': 45877, 'carts': 45878, 'electromagnets': 45879, 'stickball': 45880, 'slingshots': 45881, 'timers': 45882, \"baseball's\": 45883, 'lexington': 45884, 'alamo': 45885, 'gettysburg': 45886, \"talkers'\": 45887, 'formations': 45888, 'periscope': 45889, 've': 45890, 'sank': 45891, 'branched': 45892, 'europe—to': 45893, 'america—and': 45894, 'states…': 45895, 'stinging': 45896, 'reversals': 45897, 'sleuthing': 45898, 'snobbish': 45899, 'satterthwaite…': 45900, 'satterthwaite': 45901, 'quin': 45902, \"nero's\": 45903, 'devours': 45904, 'nero': 45905, \"lion's\": 45906, 'bloodstained': 45907, 'tigress': 45908, '436': 45909, 'samant': 45910, \"bihar's\": 45911, 'ganglords': 45912, \"amit's\": 45913, 'shekhpura': 45914, 'mofussil': 45915, 'ganglord': 45916, 'ips': 45917, 'exos': 45918, '“tactical': 45919, 'athletes”—u': 45920, 'verstegen’s': 45921, 'regimens': 45922, 'real–world': 45923, 'groovy': 45924, 'mybatis': 45925, 'jpa': 45926, 'boros': 45927, 'alien’s': 45928, 'supervisor': 45929, 'loathes': 45930, 'enamored': 45931, 'meekly': 45932, \"shiva's\": 45933, 'amputee': 45934, 'ricks': 45935, 'lumber': 45936, 'bouncer': 45937, \"'generative\": 45938, \"enterprise'\": 45939, \"'minimalist\": 45940, \"program'\": 45941, 'truth’': 45942, 'immovable': 45943, \"'maha\": 45944, \"atma'\": 45945, 'unordinary': 45946, 'travesties': 45947, 'amiable': 45948, 'unshakable': 45949, 'people’': 45950, 'surpassing': 45951, 'amphigory': 45952, 'unstrung': 45953, 'lozenge': 45954, 'willowdale': 45955, 'handcar': 45956, 'gashlycrumb': 45957, 'tinies': 45958, 'insect': 45959, 'wuggly': 45960, 'ump': 45961, 'amercian': 45962, 'petra…': 45963, 'petra': 45964, 'detestable': 45965, 'met…': 45966, 'adventurously': 45967, 'divya': 45968, 'vivek': 45969, 'emu': 45970, \"'leo\": 45971, \"investigates'\": 45972, 'anarchists': 45973, 'unmasked': 45974, \"chester's\": 45975, \"'told\": 45976, \"potter'\": 45977, 'nit': 45978, 'jaffa': 45979, \"'same\": 45980, \"died'\": 45981, 'uncaring': 45982, 'unattainable': 45983, \"labour's\": 45984, 'sponging': 45985, \"hero'\": 45986, \"face'\": 45987, 'compiling': 45988, 'httpservlet': 45989, 'restlet': 45990, 'perl': 45991, 'jas': 45992, '‘meticulously': 45993, \"late'\": 45994, 'regina': 45995, 'clausen': 45996, 'gabrielle': 45997, 'disembarked': 45998, \"regina's\": 45999, \"susan's\": 46000, 'shimmer': 46001, 'entraced': 46002, \"yesterday's\": 46003, 'centipede': 46004, 'serendipitous': 46005, \"atari's\": 46006, '·and': 46007, 'joystick': 46008, 'collins—star': 46009, 'apply—is': 46010, '‘google': 46011, '‘microblogging': 46012, '‘youth': 46013, '‘walkmantm': 46014, '‘webcam’': 46015, '‘cafeteria’': 46016, '‘calorie’': 46017, '‘microchip’': 46018, '‘website’': 46019, '‘alarm': 46020, '‘call': 46021, '‘shopping': 46022, '‘computer’': 46023, '‘multimedia’': 46024, '‘smartphone’': 46025, '‘software’': 46026, 'sexes—part': 46027, 'whitefield': 46028, 'roebuck': 46029, 'characters’': 46030, '“don': 46031, 'laurence': 46032, 'shaw’s': 46033, 'handbook”': 46034, 'dharmacharya': 46035, 'shantum': 46036, 'intertwining': 46037, \"vikram's\": 46038, \"shantum's\": 46039, \"aradhana's\": 46040, 'kildip': 46041, \"disturbing'\": 46042, \"urgent'\": 46043, 'federica': 46044, 'cocco': 46045, 'quant': 46046, 'incontestable': 46047, 'cvs': 46048, 'modellers': 46049, 'coolly': 46050, 'shopworn': 46051, 'perfumed': 46052, 'grafter': 46053, 'gutman': 46054, 'brigid': 46055, 'o’shaughnessy': 46056, 'maltese': 46057, 'édouard': 46058, 'jeanneret': 46059, '1887–1965': 46060, 'vers': 46061, \"l'esprit\": 46062, 'nouveau': 46063, 'chapel': 46064, 'ronchamp': 46065, 'dormitory': 46066, 'cité': 46067, 'universitaire': 46068, 'unité': 46069, \"d'habitation\": 46070, 'marseilles': 46071, 'epigrammatic': 46072, 'expiring': 46073, 'historians―but': 46074, 'sadomasochistic': 46075, 'submission': 46076, 'beaconsfield': 46077, 'writ': 46078, 'roomful': 46079, 'players…': 46080, 'shaitana': 46081, 'shaitana’s': 46082, 'altogether…': 46083, \"'1'\": 46084, \"snap's\": 46085, 'litany': 46086, 'lotteries': 46087, 'emancipation': 46088, 'kaurava': 46089, 'interlaced': 46090, '\\x93great': 46091, 'crashwagons': 46092, 'back\\x97and': 46093, 'wunderkinder': 46094, 'huey': 46095, 'dewey': 46096, 'louie': 46097, 'rascally': 46098, 'richnik': 46099, 'flintheart': 46100, 'glomgold': 46101, 'we\\x92re': 46102, 'rosa\\x97following': 46103, 'barks': 46104, 'barksian': 46105, '\\x93life': 46106, 'mcduck\\x94': 46107, 'twodecades\\x92': 46108, 'fair\\x92s': 46109, 'flintheart\\x92s': 46110, '\\x93crocodile': 46111, 'donald\\x92s': 46112, 'reptile': 46113, '\\x93last': 46114, 'barks\\x92': 46115, '\\x93back': 46116, 'klondike\\x94\\x97featuring': 46117, 'o\\x92gilt': 46118, 'uhoh': 46119, '\\x93cash': 46120, 'flow\\x94': 46121, 'beagle': 46122, 'factoids': 46123, 'duckburg': 46124, 'time\\x97at': 46125, 'unthinkable…': 46126, 'isaca': 46127, '73': 46128, 'nomenclature': 46129, 'lu': 46130, 'hsun': 46131, 'common–the': 46132, 'untrusting': 46133, \"laila's\": 46134, 'akrasia': 46135, 'zetetic': 46136, 'gambler’s': 46137, 'occam’s': 46138, 'epistemology': 46139, 'derrida': 46140, 'habermas': 46141, \"'heartbreaking\": 46142, 'comedyland': 46143, 'readerly': 46144, 'namedroppy': 46145, \"melancholy'\": 46146, \"'arguably\": 46147, \"englishman'\": 46148, \"enjoyable'\": 46149, \"'fry's\": 46150, 'wildean': 46151, 'blackadder': 46152, \"stars'\": 46153, 'hippopotamus': 46154, 'sundry': 46155, 'screen’': 46156, 'valentine’s': 46157, 'radiance': 46158, 'histrionic': 46159, 'resorting': 46160, 'melodramatic': 46161, 'contortions': 46162, 'superhits': 46163, 'tarana': 46164, '’55': 46165, 'chalti': 46166, 'interacted': 46167, '‘reel': 46168, 'basant': 46169, 'acrimony': 46170, 'rewriting': 46171, 'shelved': 46172, '•understand': 46173, '•use': 46174, 'tarantino': 46175, '•recover': 46176, '11g': 46177, 'postgresql': 46178, 'unpivot': 46179, 'listagg': 46180, 'nth': 46181, 'postgresqls': 46182, 'timestamp': 46183, 'rockwell': 46184, 'tanzania': 46185, 'rwanda': 46186, 'somalia': 46187, 'malawi': 46188, 'zambia': 46189, 'ghana': 46190, 'viesturs—the': 46191, 'peaks—trains': 46192, 'everest’s': 46193, 'ascents—both': 46194, 'mountaineering’s': 46195, 'heraclitus’s': 46196, 'maxim—“character': 46197, 'destiny”—is': 46198, 'congregation': 46199, 'carmel': 46200, 'catholicism': 46201, 'cheduthies': 46202, 'clouded': 46203, 'amen': 46204, 'affirms': 46205, 'jesme’s': 46206, 'muskets': 46207, '1863': 46208, 'anachronistic': 46209, '1865': 46210, 'raider': 46211, 'farrell': 46212, 'takeover': 46213, 'volt': 46214, 'mouseumwas': 46215, 'dues': 46216, \"'diary\": 46217, \"kid'\": 46218, '224': 46219, 'morales': 46220, \"death's\": 46221, 'gervais': 46222, \"groundhog's\": 46223, 'giorgia': 46224, 'alistair': 46225, \"suzanne's\": 46226, 'rougher': 46227, 'barnard': 46228, \"zoella's\": 46229, 'schoolers': 46230, 'haiku': 46231, 'madan': 46232, \"foer's\": 46233, 'mislaying': 46234, \"'memory\": 46235, \"palace'\": 46236, 'simonides': 46237, \"'mental\": 46238, 'shuffled': 46239, \"'passionate\": 46240, 'michiko': 46241, \"'delightful\": 46242, \"memorable'\": 46243, 'pisani': 46244, \"revelatory'\": 46245, 'burkeman': 46246, \"designer's\": 46247, 'savile': 46248, 'tailoring': 46249, 'bolton': 46250, 'frankel': 46251, 'blanks': 46252, 'sundsbo': 46253, 'lenticular': 46254, 'requisites': 46255, 'goncourt': 46256, 'prewar': 46257, 'indochina': 46258, 'marguerite': 46259, 'duras’s': 46260, 'duras': 46261, 'waning': 46262, \"duras's\": 46263, 'perspective—that': 46264, 'equipments': 46265, 'eclipses': 46266, \"audience's\": 46267, 'puducherry': 46268, 'chirag': 46269, \"priya's\": 46270, \"chirag's\": 46271, 'transitions': 46272, 'mal': 46273, 'workaholic': 46274, 'divert': 46275, \"'author\": 46276, 'halwai': 46277, \"'white\": 46278, \"tiger'\": 46279, 'teashop': 46280, 'chauffeur': 46281, 'cockroaches': 46282, '004': 46283, 'malls': 46284, 'oaktree': 46285, 'brim': 46286, \"marks's\": 46287, 'abstract”': 46288, 'dreamland’s': 46289, 'buster': 46290, 'yourself”': 46291, 'prescribe': 46292, 'lowered': 46293, 'leafs': 46294, 'black’s': 46295, 'alone—his': 46296, 'roshi': 46297, 'zamas’s': 46298, 'quasimodo': 46299, 'frollo': 46300, 'esmeralda': 46301, 'sufferings': 46302, 'hapgood': 46303, 'andalusia': 46304, 'frenchwoman': 46305, 'treason’s': 46306, 'inserted': 46307, 'west–': 46308, 'vaunted': 46309, '“laugh': 46310, 'enlightenment”': 46311, 'vip”': 46312, 'brahm’s': 46313, 'postcapitalism': 46314, 'continual': 46315, \"faulks's\": 46316, 'demotic': 46317, \"oneself'\": 46318, \"wilde's\": 46319, 'witticisms': 46320, 'sincerity': 46321, 'duplicitous': 46322, \"classics'\": 46323}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df['Synopsis_sequences'] = s"
      ],
      "metadata": {
        "id": "a6V31X5eHzEo"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yzJBf21-H3Xd",
        "outputId": "6e39c5c8-3afb-4ae9-fea0-c7353a5be8f5"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life   \n",
              "\n",
              "                                            Synopsis   Price  New_Ratings  \\\n",
              "0  THE HUNTERS return in their third brilliant no...  220.00            8   \n",
              "1  A layered portrait of a troubled genius for wh...  202.93           14   \n",
              "2  \"During the time men live without a common Pow...  299.00            6   \n",
              "3  A handful of grain is found in the pocket of a...  180.00           13   \n",
              "4  For seven decades, \"Life\" has been thrilling t...  965.62            1   \n",
              "\n",
              "   New_Reviews Year_of_publish  Format_(French),Paperback2010  \\\n",
              "0          4.0            2016                              0   \n",
              "1          3.9            2012                              0   \n",
              "2          4.8            1982                              0   \n",
              "3          4.1            2017                              0   \n",
              "4          5.0            2006                              0   \n",
              "\n",
              "   Format_(German),Paperback2014  Format_(Kannada),Paperback2014  ...  \\\n",
              "0                              0                               0  ...   \n",
              "1                              0                               0  ...   \n",
              "2                              0                               0  ...   \n",
              "3                              0                               0  ...   \n",
              "4                              0                               0  ...   \n",
              "\n",
              "   BookCategory_Biographies, Diaries & True Accounts  \\\n",
              "0                                                  0   \n",
              "1                                                  1   \n",
              "2                                                  0   \n",
              "3                                                  0   \n",
              "4                                                  0   \n",
              "\n",
              "   BookCategory_Comics & Mangas  \\\n",
              "0                             0   \n",
              "1                             0   \n",
              "2                             0   \n",
              "3                             0   \n",
              "4                             0   \n",
              "\n",
              "   BookCategory_Computing, Internet & Digital Media  \\\n",
              "0                                                 0   \n",
              "1                                                 0   \n",
              "2                                                 0   \n",
              "3                                                 0   \n",
              "4                                                 0   \n",
              "\n",
              "   BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "0                                       0                    0   \n",
              "1                                       0                    0   \n",
              "2                                       0                    1   \n",
              "3                                       1                    0   \n",
              "4                                       0                    0   \n",
              "\n",
              "   BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "0                                             0                      0   \n",
              "1                                             0                      0   \n",
              "2                                             0                      0   \n",
              "3                                             0                      0   \n",
              "4                                             0                      0   \n",
              "\n",
              "   BookCategory_Romance  BookCategory_Sports  \\\n",
              "0                     0                    0   \n",
              "1                     0                    0   \n",
              "2                     0                    0   \n",
              "3                     0                    0   \n",
              "4                     0                    0   \n",
              "\n",
              "                                  Synopsis_sequences  \n",
              "0  [1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...  \n",
              "1  [4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...  \n",
              "2  [400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...  \n",
              "3  [4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...  \n",
              "4  [8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d6d444a-2bff-4332-91a4-a5d11535302a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Synopsis</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Biographies, Diaries &amp; True Accounts</th>\n",
              "      <th>BookCategory_Comics &amp; Mangas</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "      <th>Synopsis_sequences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>THE HUNTERS return in their third brilliant no...</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>A layered portrait of a troubled genius for wh...</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>\"During the time men live without a common Pow...</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>A handful of grain is found in the pocket of a...</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>For seven decades, \"Life\" has been thrilling t...</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d6d444a-2bff-4332-91a4-a5d11535302a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d6d444a-2bff-4332-91a4-a5d11535302a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d6d444a-2bff-4332-91a4-a5d11535302a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4dbccb5d-fc8c-43c0-b3a4-715549110793\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4dbccb5d-fc8c-43c0-b3a4-715549110793')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4dbccb5d-fc8c-43c0-b3a4-715549110793 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_df = cat_df.drop(['Synopsis'],axis=1)"
      ],
      "metadata": {
        "id": "vTaa9UPkLrVr"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Author**"
      ],
      "metadata": {
        "id": "QxMPmlBOJZ0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "au_df = cat_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "bHMKQRLxJiob"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s,w = embedding(au_df,'Author')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrYaHHIjH6rg",
        "outputId": "4c2ec26a-d316-4ecd-d346-225fd17ba1f4"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4610 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgxLlBrYLLvg",
        "outputId": "5718c442-cf1b-4777-e5cb-cb56df918ee8"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'john': 1, 'james': 2, 'david': 3, 'r': 4, 'michael': 5, 'k': 6, 'j': 7, 's': 8, 'p': 9, 'singh': 10, 'robert': 11, 'agatha': 12, 'christie': 13, 'm': 14, 'a': 15, 'g': 16, 'george': 17, 'dk': 18, 'ladybird': 19, 'd': 20, 'bill': 21, 'stephen': 22, 'albert': 23, 'peter': 24, 'v': 25, 'lee': 26, 'william': 27, 'uderzo': 28, 'martin': 29, 'paul': 30, 'tom': 31, 'richard': 32, 'dan': 33, 'patterson': 34, 'sidney': 35, 'scott': 36, 'l': 37, 'smith': 38, 'c': 39, 'herge': 40, 'mark': 41, 'dr': 42, 'roberts': 43, 'sheldon': 44, 'cussler': 45, 'e': 46, 'watterson': 47, 'grisham': 48, 'press': 49, 'kumar': 50, 'b': 51, 'daniel': 52, 'clive': 53, 'wodehouse': 54, 'brown': 55, 'ian': 56, 'jim': 57, 'h': 58, 'chris': 59, 'thomas': 60, 'nora': 61, 'n': 62, 'andrew': 63, 'sharma': 64, 'king': 65, 'sophie': 66, 'trinity': 67, 'college': 68, 'taylor': 69, 'anthony': 70, 'simon': 71, 'sarah': 72, 'kinsella': 73, 'matthew': 74, 'charles': 75, 'christopher': 76, 'louis': 77, 'various': 78, 'stilton': 79, 'joe': 80, 'ken': 81, 'gupta': 82, 'ray': 83, 'brian': 84, 'frederick': 85, 'baldacci': 86, 'frank': 87, 'danielle': 88, 'steel': 89, 'editorial': 90, 'akira': 91, 'oliver': 92, 'bowden': 93, 'wilbur': 94, 'steve': 95, 'jonathan': 96, 'walter': 97, 'forsyth': 98, 'jeffrey': 99, 'archer': 100, 'edward': 101, 'alex': 102, 'silva': 103, 'adam': 104, 'lewis': 105, 'publications': 106, 'graham': 107, 'julia': 108, 'chakraborty': 109, 'kate': 110, 't': 111, 'child': 112, 'geronimo': 113, 'roy': 114, 'neil': 115, 'crichton': 116, 'dreamland': 117, 'oxford': 118, 'rollins': 119, 'carol': 120, 'davis': 121, 'collins': 122, 'arthur': 123, 'london': 124, 'lisa': 125, 'steven': 126, 'toriyama': 127, 'ali': 128, 'guha': 129, 'harris': 130, 'alistair': 131, 'tim': 132, 'bryson': 133, 'dictionaries': 134, 'philip': 135, 'joseph': 136, 'publishing': 137, \"l'amour\": 138, 'the': 139, 'nicholas': 140, 'jean': 141, 'board': 142, 'krishna': 143, 'reilly': 144, 'gaiman': 145, 'jenny': 146, 'maclean': 147, 'ruskin': 148, 'bond': 149, 'abdul': 150, 'mukherjee': 151, 'follett': 152, 'bruce': 153, 'national': 154, 'w': 155, 'doyle': 156, 'clare': 157, 'ernest': 158, 'menon': 159, 'robin': 160, 'geoff': 161, 'jason': 162, 'mike': 163, 'miller': 164, 'ludlum': 165, 'eric': 166, 'pai': 167, 'books': 168, 'rowling': 169, 'anand': 170, 'mary': 171, 'hal': 172, 'douglas': 173, 'luca': 174, 'caioli': 175, 'norman': 176, 'kalam': 177, 'eiichiro': 178, 'oda': 179, 'terry': 180, 'one': 181, 'ed': 182, 'conn': 183, 'iggulden': 184, 'sam': 185, 'novoneel': 186, 'geographic': 187, 'datta': 188, 'judith': 189, 'ramachandra': 190, 'maple': 191, 'mathur': 192, 'conan': 193, 'alan': 194, 'jr': 195, 'jane': 196, 'anant': 197, 'henry': 198, 'jon': 199, 'andy': 200, 'cecelia': 201, 'ahern': 202, 'fleming': 203, 'goscinny': 204, 'india': 205, 'horikoshi': 206, 'helen': 207, 'hbr': 208, 'tsugumi': 209, 'ohba': 210, 'haruki': 211, 'murakami': 212, 'de': 213, 'susan': 214, 'lond': 215, 'satish': 216, 'nikita': 217, 'tiwari': 218, 'dav': 219, 'pilkey': 220, 'masashi': 221, 'kishimoto': 222, 'francis': 223, 'archie': 224, 'superstars': 225, 'durjoy': 226, 'chandra': 227, 'anderson': 228, 'dean': 229, 'thompson': 230, 'f': 231, 'morton': 232, 'victoria': 233, 'hemingway': 234, 'khan': 235, 'maria': 236, 'satyajit': 237, 'sir': 238, 'laxman': 239, 'julie': 240, 'annie': 241, 'rené': 242, 'kohei': 243, 'leonard': 244, 'harper': 245, 'cambridge': 246, 'matt': 247, 'arun': 248, 'russ': 249, 'gandhi': 250, 'jackson': 251, 'sudha': 252, 'kenneth': 253, 'raymond': 254, 'luis': 255, 'connelly': 256, 'anne': 257, 'ivan': 258, 'allan': 259, 'donaldson': 260, 'le': 261, 'banerjee': 262, 'rao': 263, 'horowitz': 264, 'jack': 265, 'johns': 266, 'monica': 267, 'lal': 268, 'barbara': 269, 'brett': 270, 'gardner': 271, 'gopal': 272, 'bhakti': 273, 'review': 274, 'moriarty': 275, 'jill': 276, 'jain': 277, 'running': 278, 'moore': 279, 'vorderman': 280, 'jennifer': 281, 'don': 282, 'manning': 283, 'osamu': 284, 'tezuka': 285, 'jenkins': 286, 'snyder': 287, 'pandey': 288, 'mehta': 289, 'edgar': 290, 'grant': 291, 'shashi': 292, 'kevin': 293, 'gregg': 294, 'brandon': 295, 'yashavant': 296, 'mcnaught': 297, 'isaacson': 298, 'ben': 299, 'mehra': 300, 'ryan': 301, 'scholastic': 302, 'mackintosh': 303, 'evans': 304, 'rankin': 305, 'roger': 306, 'khanna': 307, 'clancy': 308, 'field': 309, 'galbraith': 310, 'karin': 311, 'slaughter': 312, 'corbett': 313, 'erich': 314, 'sarkar': 315, 'stan': 316, 'harlan': 317, 'coben': 318, 'nick': 319, 'lincoln': 320, 'peirce': 321, 'liane': 322, 'hidenori': 323, 'kusaka': 324, 'graphic': 325, 'williams': 326, 'emma': 327, 'lawrence': 328, 'amit': 329, 'sanjay': 330, 'corporation': 331, 'fitzgerald': 332, 'mario': 333, 'hart': 334, 'reema': 335, 'reid': 336, 'priya': 337, 'puri': 338, 'stone': 339, 'rahul': 340, 'dave': 341, 'greene': 342, 'bhattacharya': 343, 'nelson': 344, 'sylvia': 345, 'raman': 346, 'rachel': 347, 'fernandes': 348, 'praveen': 349, 'bryan': 350, 'prasad': 351, 'sabaa': 352, 'tahir': 353, 'phillips': 354, 'kazuo': 355, 'ishiguro': 356, 'vibrant': 357, 'publishers': 358, 'pittacus': 359, 'lore': 360, 'hurwitz': 361, 'karen': 362, 'swan': 363, 'murphy': 364, 'kanetkar': 365, 'yadav': 366, 'mccarthy': 367, 'lang': 368, 'carré': 369, 'sui': 370, 'ishida': 371, 'anna': 372, 'rick': 373, 'gautam': 374, 'andersen': 375, 'bose': 376, 'orhan': 377, 'pamuk': 378, 'quinn': 379, 'al': 380, 'adams': 381, 'laini': 382, 'pattanaik': 383, 'alexander': 384, 'dasgupta': 385, 'hawking': 386, 'sachin': 387, 'osho': 388, 'hall': 389, 'sudeep': 390, 'nagarkar': 391, 'lauren': 392, 'syd': 393, 'anil': 394, 'noam': 395, 'chomsky': 396, 'tony': 397, 'jojo': 398, 'moyes': 399, 'ron': 400, 'dale': 401, 'carnegie': 402, 'howard': 403, 'jha': 404, 'kristin': 405, 'pierce': 406, 'rose': 407, 'bernard': 408, 'rph': 409, 'griffiths': 410, 'erin': 411, 'ahmed': 412, 'gavin': 413, 'flynn': 414, 'guy': 415, 'morgan': 416, 'karl': 417, 'das': 418, 'lawson': 419, 'elizabeth': 420, 'noah': 421, 'robb': 422, 'gordon': 423, 'clark': 424, 'university': 425, 'garg': 426, 'gene': 427, 'of': 428, 'maurice': 429, 'rajiv': 430, 'clavell': 431, 'hiro': 432, 'puzo': 433, 'orwell': 434, 'bear': 435, 'grylls': 436, 'kapoor': 437, 'bob': 438, 'erik': 439, 'entertainment': 440, 'juniper': 441, 'phil': 442, 'jacobs': 443, 'mahajan': 444, 'fyodor': 445, 'sengupta': 446, 'chauhan': 447, 'reki': 448, 'kawahara': 449, 'fry': 450, 'mishra': 451, 'willard': 452, 'palmer': 453, 'cameron': 454, 'viesturs': 455, 'georgette': 456, 'heyer': 457, 'murty': 458, 'laura': 459, 'agarwal': 460, 'matthews': 461, 'sean': 462, 'abhinav': 463, 'van': 464, 'curtis': 465, 'colleen': 466, 'hoover': 467, 'seth': 468, 'thea': 469, 'kulpreet': 470, 'timothy': 471, 'ruth': 472, 'tilly': 473, 'bagshawe': 474, 'gabriel': 475, 'leav': 476, 'todd': 477, 'han': 478, 'allen': 479, 'patrick': 480, 'manus': 481, 'gregory': 482, 'michel': 483, 'marie': 484, 'devdutt': 485, 'porter': 486, 'mohan': 487, 'victor': 488, 'patil': 489, 'barry': 490, 'townsend': 491, 'vs': 492, 'jordan': 493, 'lynch': 494, 'max': 495, 'sacco': 496, 'casey': 497, 'palin': 498, '0': 499, 'claire': 500, 'amy': 501, 'keri': 502, 'vikal': 503, 'alice': 504, 'jerry': 505, 'shiv': 506, 'pradeep': 507, 'hannah': 508, 'herman': 509, 'dalrymple': 510, 'bhatia': 511, 'gerald': 512, 'durrell': 513, 'princeton': 514, 'payal': 515, 'aditya': 516, 'jodi': 517, 'cathy': 518, 'aung': 519, 'than': 520, 'sinha': 521, 'cornwell': 522, 'shakespeare': 523, 'johnson': 524, 'judy': 525, 'o': 526, 'morrison': 527, 'arundhati': 528, \"o'donnell\": 529, 'srivastava': 530, 'yousafzai': 531, 'atul': 532, 'lonely': 533, 'planet': 534, 'randall': 535, 'koirala': 536, 'narayanan': 537, 'dick': 538, 'ph': 539, 'swati': 540, 'greg': 541, 'language': 542, 'sood': 543, 'prasada': 544, 'sue': 545, 'rajesh': 546, 'suresh': 547, 'andrzej': 548, 'sapkowski': 549, 'naomi': 550, 'tharoor': 551, 'shah': 552, 'sparks': 553, 'team': 554, 'jay': 555, 'kuzneski': 556, 'editors': 557, 'ondaatje': 558, 'thareja': 559, 'doug': 560, 'ella': 561, 'sanders': 562, 'vikram': 563, 'fredrik': 564, 'backman': 565, 'günter': 566, 'grass': 567, 'linda': 568, 'bach': 569, 'vikrant': 570, 'baron': 571, 'ajay': 572, 'iain': 573, 'y': 574, 'iyer': 575, 'tina': 576, 'anuja': 577, 'weiss': 578, 'jules': 579, 'verne': 580, 'singhal': 581, 'rand': 582, 'anita': 583, 'eckersley': 584, 'kuper': 585, 'kay': 586, 'sanderson': 587, 'herriot': 588, 'bipan': 589, 'kurtz': 590, 'saran': 591, 'scarrow': 592, 'rajani': 593, 'thindiath': 594, 'shukla': 595, 'weeks': 596, 'koontz': 597, 'marvel': 598, 'comics': 599, 'leo': 600, 'tolstoy': 601, 'kelly': 602, 'nath': 603, 'riordan': 604, 'andre': 605, 'hunter': 606, 'clutterbuck': 607, 'larry': 608, 'katherine': 609, 'marjane': 610, 'satrapi': 611, 'abir': 612, 'jeff': 613, 'vimal': 614, 'peterson': 615, 'alison': 616, 'barker': 617, 'craig': 618, 'chatterjee': 619, 'brent': 620, 'keith': 621, 'marx': 622, 'sonic': 623, 'scribes': 624, 'ann': 625, 'hardy': 626, 'abercrombie': 627, 'thakur': 628, 'ravi': 629, 'wonder': 630, 'house': 631, 'stanton': 632, \"o'brien\": 633, 'marian': 634, 'cox': 635, 'kleypas': 636, 'ronald': 637, 'rainbow': 638, 'rowell': 639, 'chuck': 640, 'palahniuk': 641, 'strunk': 642, 'chimamanda': 643, 'ngozi': 644, 'adichie': 645, 'chan': 646, 'wright': 647, 'pamela': 648, 'deepak': 649, 'chopra': 650, 'paulo': 651, 'coelho': 652, 'aroor': 653, 'lars': 654, 'kepler': 655, 'verma': 656, 'ranjan': 657, 'patricia': 658, 'joyce': 659, 'keyes': 660, 'bandyopadhyay': 661, 'higgins': 662, 'gidwani': 663, 'bashir': 664, 'gairns': 665, 'jones': 666, 'picoult': 667, 'freeman': 668, 'wood': 669, 'gillian': 670, 'wells': 671, 'squires': 672, 'rob': 673, 'sara': 674, 'graeme': 675, 'carter': 676, 'liz': 677, 'gary': 678, 'frawley': 679, 'majumdar': 680, 'berry': 681, 'chase': 682, 'malala': 683, 'ellen': 684, 'and': 685, 'mccloud': 686, 'aggarwal': 687, 'rabindranath': 688, 'tagore': 689, 'sen': 690, 'santa': 691, 'pease': 692, 'prakash': 693, 'ravinder': 694, 'subramanian': 695, 'klein': 696, 'knight': 697, 'assessment': 698, 'boyd': 699, 'dutta': 700, 'massey': 701, 'benjamin': 702, 'nancy': 703, 'faber': 704, 'law': 705, 'schwarzenegger': 706, 'hoang': 707, 'sagar': 708, 'tite': 709, 'kubo': 710, 'ms': 711, 'french': 712, 'nayar': 713, 'wilson': 714, 'blake': 715, 'michelle': 716, 'airey': 717, 'luen': 718, 'yang': 719, 'simone': 720, 'arnold': 721, 'madan': 722, 'om': 723, 'trevor': 724, 'jerome': 725, 'donald': 726, 'stephanie': 727, 'emily': 728, 'adrian': 729, 'owen': 730, 'schwab': 731, 'fred': 732, 'chaturvedi': 733, 'druon': 734, 'malhotra': 735, 'will': 736, 'ross': 737, 'frances': 738, 'hussain': 739, 'crilley': 740, 'suzuki': 741, 'dickens': 742, 'keigo': 743, 'higashino': 744, 'deborah': 745, 'jacob': 746, 'sinden': 747, 'education': 748, 'kapadia': 749, 'lidwell': 750, 'vinod': 751, 'truby': 752, 'vijay': 753, 'liu': 754, 'anirban': 755, 'justin': 756, 'venkat': 757, 'burgess': 758, 'miranda': 759, 'hindol': 760, 'mr': 761, 'day': 762, 'stevenson': 763, 'makoto': 764, 'bourne': 765, 'diana': 766, 'gabaldon': 767, 'jawaharlal': 768, 'nehru': 769, 'rajan': 770, 'montgomery': 771, 'lynsay': 772, 'sands': 773, 'russell': 774, 'ayn': 775, 'upendran': 776, 'manohar': 777, 'gill': 778, 'maas': 779, 'deshpande': 780, 'bader': 781, 'arjun': 782, 'ghosh': 783, 'plato': 784, 'pillai': 785, 'roald': 786, 'dahl': 787, 'greer': 788, 'loomis': 789, 'syed': 790, 'jamie': 791, 'banks': 792, 'arora': 793, 'pete': 794, 'mann': 795, 'goldman': 796, 'vinay': 797, 'lynn': 798, 'nisioisin': 799, 'margaret': 800, 'kane': 801, 'goodwin': 802, 'hoffman': 803, 'leigh': 804, 'bardugo': 805, 'somerset': 806, 'maugham': 807, 'sundaresan': 808, 'penelope': 809, 'lively': 810, 'shatrujeet': 811, 'carmine': 812, 'kurt': 813, 'vonnegut': 814, 'gonick': 815, 'marc': 816, 'bhattacharjee': 817, 'satoshi': 818, 'frederic': 819, 'delavier': 820, 'carr': 821, 'narayan': 822, 'cassandra': 823, 'mantak': 824, 'chia': 825, 'branson': 826, 'kaur': 827, 'hopkins': 828, 'larousse': 829, 'ritu': 830, 'holt': 831, 'chandler': 832, 'shin': 833, 'wickham': 834, 'pierre': 835, 'mccall': 836, 'broughton': 837, 'coburn': 838, 'nikhil': 839, 'lowe': 840, 'surender': 841, 'pathak': 842, 'hugo': 843, 'vaughan': 844, 'stuart': 845, 'friedrich': 846, 'nietzsche': 847, 'shane': 848, 'val': 849, 'betty': 850, 'basu': 851, 'delaney': 852, 'luigi': 853, 'pirandello': 854, 'vance': 855, 'charlie': 856, 'dhar': 857, 'leon': 858, 'saurabh': 859, 'kapur': 860, 'cooper': 861, 'brooks': 862, 'alexandre': 863, 'dumas': 864, 'caroline': 865, 'love': 866, 'kaplan': 867, 'dobbs': 868, 'ware': 869, 'kris': 870, 'gethin': 871, 'jessica': 872, 'diane': 873, 'kipfer': 874, 'kerr': 875, 'bakshi': 876, 'wheeler': 877, 'burns': 878, 'cullen': 879, 'poehler': 880, 'bradley': 881, 'phd': 882, 'jackie': 883, 'melville': 884, 'remarque': 885, 'murdoch': 886, 'reynolds': 887, 'milton': 888, 'ronson': 889, 'yes': 890, 'dennis': 891, 'carlin': 892, 'ukyo': 893, 'kodachi': 894, 'bhushan': 895, 'mira': 896, 'ang': 897, 'segal': 898, 'morgenstern': 899, 'redman': 900, 'jewell': 901, 'muhammad': 902, 'trevanian': 903, 'hope': 904, 'kyle': 905, 'varun': 906, 'eliot': 907, 'sidemen': 908, 'alexandra': 909, 'markus': 910, 'zusak': 911, 'fujiwara': 912, 'bell': 913, 'prem': 914, 'lenin': 915, 'jenks': 916, 'ford': 917, 'perkins': 918, 'alain': 919, 'botton': 920, 'honnor': 921, 'ashwin': 922, 'stieg': 923, 'larrson': 924, 'esol': 925, 'ranjit': 926, 'gareth': 927, 'gayle': 928, 'jeremy': 929, 'tendulkar': 930, 'kalra': 931, 'jonas': 932, 'jonasson': 933, 'moss': 934, 'boria': 935, 'golding': 936, 'farrell': 937, 'deepali': 938, 'bapna': 939, 'jalan': 940, 'mckee': 941, 'umberto': 942, 'eco': 943, 'thier': 944, 'amartya': 945, 'galloway': 946, 'atkinson': 947, 'sonia': 948, 'magazine': 949, 'white': 950, 'robinson': 951, 'geetha': 952, 'sony': 953, 'interactive': 954, 'studios': 955, 'garth': 956, 'kim': 957, 'paris': 958, 'lin': 959, 'sunil': 960, 'simsion': 961, 'yashodhara': 962, 'luke': 963, 'amruta': 964, 'vish': 965, 'dhamija': 966, 'english': 967, 'cook': 968, 'taplin': 969, 'raghu': 970, 'nair': 971, 'sujata': 972, 'sebastian': 973, 'kothari': 974, 'husain': 975, 'adolf': 976, 'hitler': 977, 'ng': 978, 'wernham': 979, 'bhalla': 980, 'watson': 981, 'brad': 982, 'nagulakonda': 983, 'namita': 984, 'nag': 985, 'srinivasan': 986, 'walker': 987, 'shari': 988, 'lapena': 989, 'sandeep': 990, 'helfand': 991, 'nigella': 992, 'fiona': 993, 'lesley': 994, 'sims': 995, 'ferguson': 996, 'rea': 997, 'anurag': 998, 'kuldip': 999, 'sourav': 1000, 'christian': 1001, 'dostoevsky': 1002, 'obama': 1003, 'bansal': 1004, 'clarke': 1005, 'garcia': 1006, 'junji': 1007, 'ito': 1008, 'booth': 1009, 'inc': 1010, 'cole': 1011, 'shaw': 1012, 'taslima': 1013, 'nasrin': 1014, 'harvard': 1015, 'business': 1016, 'aggarwala': 1017, 'prashant': 1018, 'nash': 1019, 'khushwant': 1020, 'paula': 1021, 'hawkins': 1022, 'braverman': 1023, 'snider': 1024, 'farrarons': 1025, 'art': 1026, 'spiegelman': 1027, 'brearley': 1028, 'travis': 1029, 'diego': 1030, 'malcolm': 1031, 'simmons': 1032, 'tracey': 1033, 'karan': 1034, 'devapriya': 1035, 'simpson': 1036, 'berger': 1037, 'dhillon': 1038, 'digvijay': 1039, 'deo': 1040, 'lake': 1041, 'sahil': 1042, 'sainath': 1043, 'caitlin': 1044, 'shetty': 1045, 'garber': 1046, 'barrows': 1047, 'yandell': 1048, 'guinness': 1049, 'world': 1050, 'records': 1051, 'newey': 1052, 'west': 1053, 'garrett': 1054, 'ram': 1055, 'wiseman': 1056, 'chandramouli': 1057, 'inskipp': 1058, 'murray': 1059, 'lodha': 1060, 'hobbes': 1061, 'danny': 1062, 'dreyer': 1063, 'sivananda': 1064, 'yoga': 1065, 'vedanta': 1066, 'centre': 1067, 'tapscott': 1068, 'charlotte': 1069, 'takashi': 1070, 'yano': 1071, 'arikawa': 1072, 'shantanu': 1073, 'jung': 1074, 'winston': 1075, 'nolan': 1076, 'naughty': 1077, 'dog': 1078, 'zaidi': 1079, 'coomi': 1080, 'abrsm': 1081, 'alika': 1082, 'elias': 1083, 'ullekh': 1084, 'aldous': 1085, 'huxley': 1086, 'rishabh': 1087, 'henrik': 1088, 'ibsen': 1089, 'dawkin': 1090, 'skuggevik': 1091, 'tore': 1092, 'rem': 1093, 'zag': 1094, 'sedaris': 1095, 'rafael': 1096, 'nadal': 1097, 'pilling': 1098, 'pande': 1099, 'calligraphy': 1100, 'book': 1101, 'agrawal': 1102, 'yates': 1103, 'meyers': 1104, 'rujuta': 1105, 'diwekar': 1106, 'richmal': 1107, 'crompton': 1108, 'hitchens': 1109, 'hayao': 1110, 'miyazaki': 1111, 'rishi': 1112, 'may': 1113, 'goswamy': 1114, 'carreyrou': 1115, 'odom': 1116, 'seitz': 1117, 'irakli': 1118, 'nadareishvili': 1119, 'anupama': 1120, 'dostoyevsky': 1121, 'sings': 1122, 'naoki': 1123, 'fey': 1124, 'shinkai': 1125, 'foley': 1126, 'tripathi': 1127, 'michell': 1128, 'apj': 1129, 'kenworthy': 1130, 'carver': 1131, 'dirk': 1132, 'mariana': 1133, 'mazzucato': 1134, 'mamta': 1135, 'satya': 1136, 'harinder': 1137, 'tayeb': 1138, 'salih': 1139, 'patel': 1140, 'buchan': 1141, 'rohit': 1142, 'giovanni': 1143, 'bacon': 1144, 'pietersen': 1145, 'hillenbrand': 1146, 'gbs': 1147, 'sidhu': 1148, 'geer': 1149, 'chandrachud': 1150, 'ripley': 1151, 'josh': 1152, 'salinger': 1153, 'tuttle': 1154, 'urvashi': 1155, 'butalia': 1156, 'thich': 1157, 'nhat': 1158, 'chaplin': 1159, 'frye': 1160, 'poe': 1161, 'catherine': 1162, 'sitapati': 1163, 'siegel': 1164, 'chapple': 1165, 'zahn': 1166, 'sathya': 1167, 'bhaskar': 1168, 'chattopadhyay': 1169, 'edmund': 1170, 'raghavan': 1171, 'mascie': 1172, 'macfarlane': 1173, 'prateek': 1174, 'navneet': 1175, 'doris': 1176, 'kearns': 1177, 'morland': 1178, 'viet': 1179, 'thanh': 1180, 'nguyen': 1181, 'dani': 1182, 'priyanka': 1183, 'mall': 1184, 'indu': 1185, 'abbasi': 1186, 'bobby': 1187, 'dini': 1188, 'jin': 1189, 'yong': 1190, 'stinson': 1191, 'gallo': 1192, 'agassi': 1193, 'hari': 1194, 'sanjoy': 1195, 'hazarika': 1196, 'anirudha': 1197, 'matthes': 1198, 'woodward': 1199, 'catana': 1200, 'chetwynd': 1201, 'kon': 1202, 'gustave': 1203, 'marwaha': 1204, 'yasmin': 1205, 'mayekar': 1206, 'marrs': 1207, 'abhay': 1208, 'hayes': 1209, 'brunton': 1210, 'colin': 1211, 'forbes': 1212, 'samah': 1213, 'ruchir': 1214, 'rickards': 1215, 'hugard': 1216, 'sumit': 1217, 'ganguly': 1218, 'gurmehar': 1219, 'andré': 1220, 'niall': 1221, 'kentaro': 1222, 'ascher': 1223, 'cormac': 1224, 'critchlow': 1225, 'song': 1226, 'tara': 1227, 'stroud': 1228, 'maz': 1229, 'mayhew': 1230, 'sweigart': 1231, 'allaby': 1232, 'fishburne': 1233, 'mayank': 1234, 'madeleine': 1235, 'hajime': 1236, 'isayama': 1237, 'stephens': 1238, 'pavitte': 1239, 'beverly': 1240, 'cleary': 1241, 'bussi': 1242, 'himanshu': 1243, 'rai': 1244, 'long': 1245, 'riddle': 1246, 'aspen': 1247, 'matis': 1248, 'hashmi': 1249, 'bilal': 1250, 'siddiqi': 1251, 'tommy': 1252, 'lopez': 1253, 'jakeman': 1254, 'sagarika': 1255, 'ghose': 1256, 'ziauddin': 1257, 'sardar': 1258, 'karna': 1259, 'varsha': 1260, 'marcus': 1261, 'neha': 1262, 'experts': 1263, 'nagar': 1264, 'twinkle': 1265, 'warne': 1266, 'leah': 1267, 'christina': 1268, 'howell': 1269, 'trilok': 1270, 'pamel': 1271, 'lidiard': 1272, 'fitch': 1273, 'edwards': 1274, 'meenakshi': 1275, 'simar': 1276, 'nadia': 1277, 'aaron': 1278, 'mason': 1279, 'srinivas': 1280, 'abraham': 1281, 'ashlee': 1282, 'philippe': 1283, 'pankhurst': 1284, 'anuj': 1285, 'voss': 1286, 'einstein': 1287, 'herbert': 1288, 'kamala': 1289, 'gleick': 1290, 'sharpe': 1291, 'kaufman': 1292, 'tiger': 1293, 'mariani': 1294, 'howe': 1295, 'rogers': 1296, 'ted': 1297, 'sanjeev': 1298, 'sanyal': 1299, 'comfort': 1300, 'weir': 1301, 'camilla': 1302, 'et': 1303, 'armstrong': 1304, 'mukhopadhyay': 1305, 'cep': 1306, 'grossman': 1307, 'peggy': 1308, 'parish': 1309, 'shahid': 1310, 'afridi': 1311, 'hissey': 1312, 'winslow': 1313, 'constantin': 1314, 'stanislavski': 1315, 'marshall': 1316, 'eleanor': 1317, 'north': 1318, 'kiran': 1319, 'temple': 1320, 'chandan': 1321, 'deshmukh': 1322, 'marquez': 1323, 'gerrard': 1324, 'garwood': 1325, 'pinto': 1326, 'subroto': 1327, 'bagchi': 1328, 'ilika': 1329, 'highsmith': 1330, 'ramanand': 1331, 'irving': 1332, 'maulshree': 1333, 'somani': 1334, 'clegg': 1335, 'coyle': 1336, 'rituparna': 1337, 'crabtree': 1338, 'hideo': 1339, 'yokoyama': 1340, 'harrison': 1341, 'nandini': 1342, 'regnier': 1343, 'tremain': 1344, 'pankaj': 1345, 'pandya': 1346, 'vijayakar': 1347, 'archana': 1348, 'varma': 1349, 'oyinkan': 1350, 'braithwaite': 1351, 'mitchell': 1352, 'saba': 1353, 'mahmood': 1354, 'mcraney': 1355, 'saadawi': 1356, 'randy': 1357, 'pausch': 1358, 'devang': 1359, 'hanon': 1360, 'ba': 1361, 'fabio': 1362, 'moon': 1363, 'wade': 1364, 'gatcum': 1365, 'mills': 1366, 'christine': 1367, 'mayya': 1368, 'chhikara': 1369, 'seluk': 1370, 'brosh': 1371, 'giri': 1372, 'martand': 1373, 'rallison': 1374, 'delisle': 1375, 'kobo': 1376, 'abe': 1377, 'yorke': 1378, 'matson': 1379, 'rekha': 1380, 'tarquin': 1381, 'rebecca': 1382, 'rajendra': 1383, 'lata': 1384, 'suri': 1385, 'lagercrantz': 1386, 'yasser': 1387, 'little': 1388, 'maharani': 1389, 'gayatri': 1390, 'silver': 1391, 'clint': 1392, 'emerson': 1393, 'doerr': 1394, 'spencer': 1395, 'univ': 1396, 'dilip': 1397, \"d'souza\": 1398, 'goddard': 1399, 'hugh': 1400, 'joshi': 1401, 'tanvir': 1402, 'kubica': 1403, 'gurcharan': 1404, 'temsula': 1405, 'ao': 1406, 'subramaniam': 1407, 'narendra': 1408, 'alessandro': 1409, 'vijayan': 1410, 'anton': 1411, 'chekhov': 1412, 'yves': 1413, 'ferri': 1414, 'azzarello': 1415, 'kurzweil': 1416, 'derek': 1417, 'parvati': 1418, 'slash': 1419, 'kocienda': 1420, 'hadley': 1421, 'audrey': 1422, 'truschke': 1423, 'larsen': 1424, 'hogan': 1425, 'heminway': 1426, 'isabelle': 1427, 'ronin': 1428, 'akshay': 1429, 'manwani': 1430, 'rv': 1431, 'sinek': 1432, 'edited': 1433, 'by': 1434, 'weisberger': 1435, 'chaudhri': 1436, 'sanghi': 1437, 'titan': 1438, 'cast': 1439, 'gawande': 1440, 'munroe': 1441, 'langenscheidt': 1442, 'thorpe': 1443, 'fielding': 1444, 'tarun': 1445, 'holiday': 1446, 'puja': 1447, 'seinfeld': 1448, 'brenna': 1449, 'lloyd': 1450, 'harsh': 1451, 'sy': 1452, 'corp': 1453, 'rousseau': 1454, 'ralph': 1455, 'jandy': 1456, 'straczynski': 1457, 'lougheed': 1458, 'fulves': 1459, 'pratap': 1460, 'bhanu': 1461, 'debashish': 1462, 'irengbam': 1463, 'anshul': 1464, 'maheshwari': 1465, 'erica': 1466, 'sanjaya': 1467, 'baru': 1468, 'kumari': 1469, 'bloch': 1470, 'arvind': 1471, 'pooja': 1472, 'hofstadter': 1473, 'friedman': 1474, 'grady': 1475, 'lilly': 1476, 'ta': 1477, 'nehisi': 1478, 'coates': 1479, 'bimal': 1480, 'saha': 1481, 'lane': 1482, 'giridhar': 1483, 'pauline': 1484, 'manfredi': 1485, 'montefiore': 1486, 'luther': 1487, 'nixon': 1488, 'alton': 1489, 'service': 1490, 'ori': 1491, 'hofmekler': 1492, 'antonio': 1493, 'faulks': 1494, 'marcel': 1495, 'uday': 1496, 'abhijit': 1497, 'grover': 1498, 'limited': 1499, 'kishore': 1500, 'preston': 1501, 'tomoko': 1502, 'nakamichi': 1503, 'kant': 1504, 'nicola': 1505, 'scheffler': 1506, 'teresa': 1507, 'lina': 1508, 'diamandis': 1509, 'bradford': 1510, 'zetter': 1511, 'li': 1512, 'shourie': 1513, 'rudrangshu': 1514, 'rennie': 1515, 'lister': 1516, 'woody': 1517, 'samuel': 1518, 'richardson': 1519, 'theodore': 1520, 'sundar': 1521, 'wooden': 1522, 'dwight': 1523, 'easty': 1524, 'nir': 1525, 'eyal': 1526, 'gulzar': 1527, 'nasreen': 1528, 'munni': 1529, 'kabir': 1530, 'amanda': 1531, 'moazed': 1532, 'harish': 1533, 'butterfield': 1534, 'sunita': 1535, 'sheth': 1536, 'carroll': 1537, 'yuval': 1538, 'harari': 1539, 'devidayal': 1540, 'stump': 1541, 'westaby': 1542, 'seles': 1543, 'shivprasad': 1544, 'alina': 1545, 'amitava': 1546, 'kuldeep': 1547, 'miles': 1548, 'dt': 1549, 'services': 1550, 'epictetus': 1551, 'juan': 1552, 'penguin': 1553, 'vikas': 1554, 'ankit': 1555, 'kaye': 1556, 'sophia': 1557, 'amoruso': 1558, 'harold': 1559, 'noble': 1560, 'tui': 1561, 'sutherland': 1562, 'barton': 1563, 'chamberlain': 1564, 'kazu': 1565, 'kibuishi': 1566, 'lynne': 1567, 'pustak': 1568, 'mahal': 1569, 'staff': 1570, 'tana': 1571, 'calvin': 1572, 'ashish': 1573, 'swarup': 1574, 'ayaan': 1575, 'hirsi': 1576, 'sekhri': 1577, 'seema': 1578, 'aimee': 1579, 'forman': 1580, 'maya': 1581, 'haggard': 1582, 'blain': 1583, 'vandana': 1584, 'dickins': 1585, 'crouch': 1586, 'black': 1587, 'arijon': 1588, 'duncan': 1589, 'rene': 1590, 'warren': 1591, 'goldstein': 1592, 'navin': 1593, 'leslie': 1594, 'rajdeep': 1595, 'sardesai': 1596, 'sally': 1597, 'pitman': 1598, 'pratchett': 1599, 'austen': 1600, 'carey': 1601, 'baptiste': 1602, 'ezekiel': 1603, 'hendricks': 1604, 'haig': 1605, 'mcdermott': 1606, 'stanley': 1607, 'i': 1608, 'sheela': 1609, 'reddy': 1610, 'johanna': 1611, 'basford': 1612, 'faiyaz': 1613, 'barr': 1614, 'schneier': 1615, 'seuss': 1616, 'sasson': 1617, 'bronte': 1618, 'crystal': 1619, 'gaurav': 1620, 'mihir': 1621, 'ginsburg': 1622, 'rawat': 1623, 'bancroft': 1624, 'salaria': 1625, \"o'hailey\": 1626, 'chambers': 1627, 'jurek': 1628, 'dixit': 1629, 'toles': 1630, 'debroy': 1631, 'melanie': 1632, 'swamy': 1633, 'dey': 1634, 'arpit': 1635, 'lehane': 1636, 'nitin': 1637, 'lalit': 1638, 'chowdhury': 1639, 'rowan': 1640, 'coleman': 1641, 'carson': 1642, 'jorge': 1643, 'borges': 1644, 'bisht': 1645, 'mardy': 1646, 'grothe': 1647, 'hyndman': 1648, 'alafair': 1649, 'burke': 1650, 'cormen': 1651, 'jake': 1652, 'sheryl': 1653, 'sandberg': 1654, 'saunders': 1655, 'swapan': 1656, 'pinker': 1657, 'wilkinson': 1658, 'rashmi': 1659, 'agundez': 1660, 'rinku': 1661, 'lord': 1662, 'charnwood': 1663, 'green': 1664, 'kakar': 1665, 'tetlock': 1666, 'rich': 1667, 'rees': 1668, 'tully': 1669, 'kakalios': 1670, 'thapar': 1671, 'weiner': 1672, 'sushant': 1673, 'uma': 1674, 'kapila': 1675, 'castle': 1676, 'swami': 1677, 'joshua': 1678, 'flanagan': 1679, 'higdon': 1680, 'sarnath': 1681, 'shuchi': 1682, 'levison': 1683, 'gimenez': 1684, 'association': 1685, 'ernst': 1686, 'snashall': 1687, 'rajmohan': 1688, 'gorey': 1689, 'rajinder': 1690, 'suraj': 1691, 'bishweshwar': 1692, 'nandi': 1693, 'prep': 1694, 'hickman': 1695, 'kausik': 1696, 'shalini': 1697, 'prof': 1698, 'francesca': 1699, 'bunny': 1700, 'rachna': 1701, 'eva': 1702, 'direction': 1703, 'pat': 1704, 'soha': 1705, 'khushnuma': 1706, 'daruwala': 1707, 'kaul': 1708, 'feynman': 1709, 'bonewitz': 1710, 'kuang': 1711, 'waddell': 1712, 'bowler': 1713, 'warner': 1714, 'bros': 1715, 'ellis': 1716, 'shilpa': 1717, 'finn': 1718, 'prabhudesai': 1719, 'hergé': 1720, 'bar': 1721, 'zohar': 1722, 'hobb': 1723, 'piero': 1724, 'kurosawa': 1725, 'heller': 1726, 'shivam': 1727, 'shankar': 1728, 'myrna': 1729, 'govind': 1730, 'ward': 1731, 'mina': 1732, 'petrovic': 1733, 'lala': 1734, 'mindy': 1735, 'kaling': 1736, 'woods': 1737, 'hiromu': 1738, 'arakawa': 1739, 'shivshankar': 1740, 'kent': 1741, 'starlin': 1742, 'ananth': 1743, 'sandhya': 1744, 'nicci': 1745, 'anindya': 1746, 'jeffery': 1747, 'deaver': 1748, 'lotia': 1749, 'fali': 1750, 'nariman': 1751, 'kristoff': 1752, 'mackenzie': 1753, 'milan': 1754, 'levy': 1755, 'x': 1756, 'misra': 1757, 'jamieson': 1758, 'ela': 1759, 'bayross': 1760, 'duckett': 1761, 'austin': 1762, 'robotham': 1763, 'toole': 1764, 'hank': 1765, 'ketcham': 1766, '3dtotal': 1767, 'anon': 1768, 'shikha': 1769, 'ames': 1770, 'barun': 1771, 'chanda': 1772, 'cory': 1773, 'garry': 1774, 'kasparov': 1775, 'conrad': 1776, 'aravind': 1777, 'adiga': 1778, 'gerry': 1779, 'kumon': 1780, 'skinner': 1781, 'laurence': 1782, 'harwood': 1783, 'hermann': 1784, 'hesse': 1785, 'saurav': 1786, 'parker': 1787, 'gray': 1788, 'katie': 1789, 'oscar': 1790, 'wilde': 1791, 'butler': 1792, 'kaushik': 1793, 'margarita': 1794, 'madrigal': 1795, 'gopalan': 1796, 'doherty': 1797, 'nambi': 1798, 'vince': 1799, 'yagya': 1800, 'haridas': 1801, 'renu': 1802, 'brynjolfsson': 1803, \"o'neil\": 1804, 'zlatan': 1805, 'ibrahimovic': 1806, 'dickenson': 1807, 'prentice': 1808, 'mari': 1809, 'blume': 1810, 'takahashi': 1811, 'guillem': 1812, 'balague': 1813, 'khopkar': 1814, 'life': 1815, 'ashokan': 1816, 'wong': 1817, 'kiew': 1818, 'kit': 1819, 'guillain': 1820, 'roland': 1821, 'dry': 1822, 'durant': 1823, 'davy': 1824, 'cielen': 1825, 'arno': 1826, 'meysman': 1827, 'mohamed': 1828, 'murugesan': 1829, 'ramaswamy': 1830, 'neuwirth': 1831, 'shields': 1832, 'aniela': 1833, 'jaffe': 1834, 'clara': 1835, 'macdonald': 1836, 'defouw': 1837, 'palden': 1838, 'gyatso': 1839, 'tenzing': 1840, 'dorst': 1841, 'dict': 1842, 'dutkanicz': 1843, 'ranata': 1844, 'bhatt': 1845, 'eckstein': 1846, 'mansfield': 1847, 'biz': 1848, 'dyer': 1849, 'villiers': 1850, 'happymon': 1851, 'elke': 1852, 'buchholz': 1853, 'susanne': 1854, 'kaeppele': 1855, 'karoline': 1856, 'hille': 1857, 'irina': 1858, 'stotland': 1859, 'gerhard': 1860, 'buhler': 1861, 'seeley': 1862, 'fabok': 1863, 'shooting': 1864, 'targets': 1865, 'namrata': 1866, 'purohit': 1867, 'matthan': 1868, 'logophilia': 1869, 'girish': 1870, 'kuber': 1871, 'honnold': 1872, 'practice': 1873, 'paper': 1874, 'novy': 1875, 'tomkins': 1876, 'leander': 1877, 'kahney': 1878, 'sontag': 1879, 'lindsay': 1880, 'warwick': 1881, 'kerpen': 1882, 'maneka': 1883, 'beckham': 1884, 'negi': 1885, 'haruba': 1886, 'chrismd': 1887, 'lexie': 1888, 'williamson': 1889, 'sherry': 1890, 'petersik': 1891, 'elayne': 1892, 'angel': 1893, 'timsal': 1894, 'masud': 1895, 'heywood': 1896, 'piparaiya': 1897, 'braungart': 1898, 'heilbroner': 1899, 'mahalingam': 1900, 'deodhar': 1901, 'arsheep': 1902, 'bahga': 1903, 'madisetti': 1904, 'sanjeeva': 1905, 'gaston': 1906, 'marjorie': 1907, 'sana': 1908, 'takeda': 1909, 'wendell': 1910, 'schwartz': 1911, 'uslan': 1912, 'mcwhorter': 1913, 'mcewan': 1914, 'iu': 1915, 'averbakh': 1916, 'neat': 1917, 'irshad': 1918, 'thalakala': 1919, 'mandela': 1920, 'urasawa': 1921, 'roman': 1922, 'lucy': 1923, 'alok': 1924, 'mani': 1925, 'walkenbach': 1926, 'payne': 1927, 'holly': 1928, 'aditi': 1929, 'mukund': 1930, 'connor': 1931, 'franta': 1932, 'paramhansa': 1933, 'yogananda': 1934, 'bridgman': 1935, 'renee': 1936, 'greaney': 1937, 'kurbjuweit': 1938, 'nainy': 1939, 'nadella': 1940, 'hyams': 1941, \"o'\": 1942, 'brien': 1943, 'baweja': 1944, 'gl': 1945, 'batra': 1946, 'subburaj': 1947, 'cup': 1948, 'ravikesh': 1949, 'divia': 1950, 'kirby': 1951, 'bhogle': 1952, 'pelé': 1953, 'jhimli': 1954, 'tobin': 1955, 'aatish': 1956, 'taseer': 1957, 'beahm': 1958, 'willis': 1959, 'narasimha': 1960, 'karumanchi': 1961, 'bart': 1962, 'yasso': 1963, 'steinmetz': 1964, 'saiswaroopa': 1965, 'brusatte': 1966, 'hoyland': 1967, 'boccaccio': 1968, 'reza': 1969, 'farazmand': 1970, 'srishti': 1971, 'chaudhary': 1972, 'hutchinson': 1973, 'fulford': 1974, 'gracie': 1975, 'gerstner': 1976, 'f2': 1977, 'freestylers': 1978, 'satyananda': 1979, 'saraswati': 1980, 'adity': 1981, 'jocelyn': 1982, 'dart': 1983, 'coulson': 1984, 'terri': 1985, 'libenson': 1986, 'stiglitz': 1987, 'calum': 1988, 'laird': 1989, 'aparajitha': 1990, 'nagesh': 1991, 'johan': 1992, 'lengen': 1993, 'bradbury': 1994, 'mcguire': 1995, 'amazing': 1996, 'battles': 1997, 'waitzkin': 1998, 'turgenev': 1999, 'deary': 2000, 'grippando': 2001, 'shigeru': 2002, 'mizuki': 2003, 'bhaavna': 2004, 'buttigieg': 2005, 'hanh': 2006, 'ajvide': 2007, 'lindqvist': 2008, 'styron': 2009, 'andreas': 2010, 'antonopoulos': 2011, 'yoshihiro': 2012, 'togashi': 2013, 'briggs': 2014, 'alvin': 2015, 'toffler': 2016, 'janda': 2017, 'lumbard': 2018, 'dugdall': 2019, 'sampras': 2020, 'mayle': 2021, 'atwood': 2022, 'tallent': 2023, 'srinath': 2024, 'kalavathy': 2025, 'beighley': 2026, 'harbus': 2027, 'zumthor': 2028, 'kruglinski': 2029, 'scot': 2030, 'wingo': 2031, 'shepherd': 2032, 'kevyn': 2033, 'aucoin': 2034, 'rajib': 2035, 'jurgens': 2036, 'kalyan': 2037, 'bhattacharyya': 2038, 'yasir': 2039, 'fischer': 2040, 'tonmoy': 2041, 'shostack': 2042, 'lumet': 2043, 'sushmita': 2044, 'jamwal': 2045, 'christa': 2046, 'faust': 2047, 'stuti': 2048, 'attia': 2049, 'hosain': 2050, 'rounak': 2051, 'banik': 2052, 'suhas': 2053, 'palshikar': 2054, 'jonny': 2055, 'bairstow': 2056, 'barney': 2057, 'pilger': 2058, 'sujatha': 2059, 'bernieres': 2060, 'krishnamurthi': 2061, 'haykin': 2062, 'carlson': 2063, 'darian': 2064, 'leader': 2065, 'sukanya': 2066, 'venkatraghavan': 2067, 'livio': 2068, 'seifer': 2069, 'ursula': 2070, 'guin': 2071, 'puzzles': 2072, 'lockwood': 2073, 'flaubert': 2074, 'olson': 2075, 'fogle': 2076, 'karachiwala': 2077, 'zeena': 2078, 'dhalla': 2079, 'annapoorna': 2080, 'hudson': 2081, 'sandesh': 2082, 'sapru': 2083, 'hoge': 2084, 'vander': 2085, 'meer': 2086, 'dalvi': 2087, 'visaria': 2088, 'murasaki': 2089, 'shikibu': 2090, 'ignotofsky': 2091, 'katarina': 2092, 'bivald': 2093, 'stern': 2094, 'kalvitis': 2095, 'sisir': 2096, 'sugata': 2097, 'neihsial': 2098, '2': 2099, 'alfred': 2100, 'aho': 2101, 'parthesh': 2102, 'thakkar': 2103, 'béteille': 2104, 'kishtainy': 2105, 'gadbois': 2106, 'abrams': 2107, 'geoffrey': 2108, 'galt': 2109, 'harpham': 2110, 'barrington': 2111, 'barber': 2112, 'azad': 2113, 'toyama': 2114, 'callender': 2115, 'suzy': 2116, 'cathcart': 2117, 'süskind': 2118, 'vivek': 2119, 'kulkarni': 2120, 'partha': 2121, 'gujarat': 2122, 'co': 2123, 'operative': 2124, 'milk': 2125, 'ltd': 2126, 'dalzell': 2127, 'suk': 2128, 'westover': 2129, 'welch': 2130, 'malone': 2131, 'lorraine': 2132, 'heath': 2133, 'elmar': 2134, 'neveling': 2135, 'cryan': 2136, 'handley': 2137, 'towada': 2138, 'sanchit': 2139, 'estelle': 2140, 'maskame': 2141, 'bhjarti': 2142, 'akshar': 2143, 'boulle': 2144, 'lebon': 2145, 'penrose': 2146, 'bardhan': 2147, 'pranab': 2148, 'kondo': 2149, 'silge': 2150, 'emraan': 2151, 'sandel': 2152, 'caldwell': 2153, 'lomong': 2154, 'abbi': 2155, 'glines': 2156, 'surendranath': 2157, 'aron': 2158, 'ralston': 2159, 'mughal': 2160, 'rasmussen': 2161, 'cavanagh': 2162, 'rajasekaran': 2163, 'vijayalakshmi': 2164, 'turton': 2165, 'trescothick': 2166, 'weedon': 2167, 'grossmith': 2168, 'cal': 2169, 'newport': 2170, 'narkhede': 2171, 'dowling': 2172, 'nalini': 2173, 'brunson': 2174, 'shweta': 2175, 'taneja': 2176, 'arihant': 2177, 'judea': 2178, 'pearl': 2179, 'abhimanyu': 2180, 'sisodia': 2181, 'brandenburg': 2182, 'roetert': 2183, 'higashiyama': 2184, 'dabral': 2185, 'sin': 2186, 'nintendo': 2187, 'bendavid': 2188, 'champakalakshmi': 2189, 'doguhan': 2190, 'uluca': 2191, 'lester': 2192, 'roderick': 2193, 'hunt': 2194, 'ammal': 2195, 'kahn': 2196, 'pandita': 2197, 'kirsty': 2198, 'thathapudi': 2199, 'jharna': 2200, 'jp': 2201, 'mushrif': 2202, 'darwin': 2203, 'hindman': 2204, 'comaneci': 2205, 'nina': 2206, 'blabey': 2207, 'currey': 2208, 'vanessa': 2209, 'mcdowell': 2210, 'alam': 2211, 'brooker': 2212, 'jan': 2213, 'werner': 2214, 'müller': 2215, 'brijesh': 2216, 'rosenberg': 2217, 'auclair': 2218, 'hazel': 2219, 'padmanabhan': 2220, 'uris': 2221, 'jayanti': 2222, 'anubhav': 2223, 'debbie': 2224, 'tung': 2225, 'yule': 2226, 'neels': 2227, 'bhaswati': 2228, 'sylvain': 2229, 'reynard': 2230, 'hultgen': 2231, 'naughton': 2232, 'mullish': 2233, 'sankaran': 2234, 'ujjwal': 2235, 'marti': 2236, 'perarnau': 2237, 'auel': 2238, 'bringhurst': 2239, 'pirsig': 2240, 'dinesh': 2241, 'ratcliffe': 2242, 'mihaly': 2243, 'csikszentmihalyi': 2244, 'funk': 2245, 'france': 2246, 'belleville': 2247, 'miura': 2248, 'ehrenberg': 2249, 'valentino': 2250, 'rossi': 2251, 'jai': 2252, 'gadre': 2253, 'jaron': 2254, 'lanier': 2255, 'sorrenson': 2256, 'metzger': 2257, 'yuxi': 2258, 'hayden': 2259, 'polychroniou': 2260, 'lamprecht': 2261, 'maha': 2262, 'bram': 2263, 'stoker': 2264, 'carabine': 2265, 'buday': 2266, 'dunning': 2267, 'leena': 2268, 'saldanha': 2269, 'lackberg': 2270, 'harkness': 2271, 'fox': 2272, 'chandu': 2273, 'anitha': 2274, 'devasia': 2275, 'ruby': 2276, 'piper': 2277, 'corona': 2278, 'pilgrim': 2279, 'brenda': 2280, 'ueland': 2281, 'ogilvy': 2282, 'grogan': 2283, 'barba': 2284, 'thorsten': 2285, 'schneider': 2286, 'stocke': 2287, 'ranganathan': 2288, 'patty': 2289, 'mccord': 2290, 'meade': 2291, 'falkner': 2292, 'millar': 2293, 'spark': 2294, 'tek': 2295, 'bhattarai': 2296, 'nilanjan': 2297, 'terese': 2298, 'mailhot': 2299, 'chabon': 2300, 'mala': 2301, 'sharapova': 2302, 'mireille': 2303, 'guiliano': 2304, 'outlook': 2305, 'tappenden': 2306, 'easto': 2307, 'setterfield': 2308, 'vishal': 2309, 'goyal': 2310, 'anders': 2311, 'ericsson': 2312, 'govindan': 2313, 'yarrow': 2314, 'tanmay': 2315, 'moning': 2316, 'longman': 2317, 'vasuki': 2318, 'belavadi': 2319, 'druckerman': 2320, 'millwood': 2321, 'hargrave': 2322, 'tallulah': 2323, 'pollan': 2324, 'grandin': 2325, 'bunn': 2326, 'manoj': 2327, 'suarez': 2328, 'sebold': 2329, 'yongey': 2330, 'mingyur': 2331, 'rinpoche': 2332, 'jemisin': 2333, 'case': 2334, 'millspaugh': 2335, 'hultgren': 2336, 'jen': 2337, 'sincero': 2338, 'tiffany': 2339, 'haddish': 2340, 'chhibber': 2341, 'roach': 2342, 'sparky': 2343, 'sweets': 2344, 'virginia': 2345, 'axline': 2346, 'satyanarayana': 2347, 'bryce': 2348, 'marr': 2349, 'coetzee': 2350, 'kissinger': 2351, 'acres': 2352, 'pearse': 2353, 'crane': 2354, 'holger': 2355, 'kersten': 2356, 'maurer': 2357, 'mckinney': 2358, 'lings': 2359, 'hobbs': 2360, 'jeanette': 2361, 'winterson': 2362, 'bergkamp': 2363, 'pisenti': 2364, 'capriolo': 2365, 'sleeper': 2366, 'coe': 2367, 'paterson': 2368, 'bertil': 2369, 'lintner': 2370, 'dhanani': 2371, 'leyson': 2372, 'utkarsh': 2373, 'prajwal': 2374, 'hegde': 2375, 'melinda': 2376, 'gates': 2377, 'sibaji': 2378, 'haze': 2379, 'paddy': 2380, 'upton': 2381, 'kupperberg': 2382, 'mackay': 2383, 'litt': 2384, 'blackburn': 2385, 'bhargava': 2386, 'bostrom': 2387, 'jiawei': 2388, 'caimin': 2389, 'magal': 2390, 'ssbcrack': 2391, 'marryam': 2392, 'reshii': 2393, 'nevil': 2394, 'shute': 2395, 'norway': 2396, 'samir': 2397, 'madhavan': 2398, 'smallwood': 2399, 'imam': 2400, 'hyeonseo': 2401, 'weston': 2402, 'pfitzinger': 2403, 'hero': 2404, 'small': 2405, 'egmont': 2406, 'uk': 2407, 'cornog': 2408, 'kailas': 2409, 'guo': 2410, 'andrés': 2411, 'iniesta': 2412, 'adusumilli': 2413, 'ranulph': 2414, 'fiennes': 2415, 'pressfield': 2416, 'lokapally': 2417, 'brendan': 2418, 'tatiana': 2419, 'rosnay': 2420, 'lovecraft': 2421, 'pavone': 2422, 'carrie': 2423, 'fletcher': 2424, \"dell'amore\": 2425, 'tuchman': 2426, 'reis': 2427, 'mahnke': 2428, 'chamley': 2429, 'marv': 2430, 'wolfman': 2431, 'santopolo': 2432, 'subhadra': 2433, 'kaori': 2434, 'ozaki': 2435, 'frantz': 2436, 'fanon': 2437, 'parr': 2438, 'shridhar': 2439, 'damle': 2440, 'corry': 2441, 'rta': 2442, 'chishti': 2443, 'gamow': 2444, 'gideon': 2445, 'haigh': 2446, 'collin': 2447, 'gibson': 2448, 'tehmina': 2449, 'durrani': 2450, 'mitnick': 2451, 'konnikova': 2452, 'winters': 2453, 'ernesto': 2454, \"'che'\": 2455, 'guevara': 2456, 'esha': 2457, 'skloot': 2458, 'kirill': 2459, 'eremenko': 2460, 'shilbhadra': 2461, 'rini': 2462, 'chakrabarti': 2463, 'soumitra': 2464, 'tanuman': 2465, 'bhadur': 2466, 'klsi': 2467, 'knut': 2468, 'hamsun': 2469, 'pal': 2470, 'snigdha': 2471, 'budhiraja': 2472, 'nuala': 2473, 'ellwood': 2474, 'seirawan': 2475, 'zheng': 2476, 'kavita': 2477, 'qiu': 2478, 'xiaolong': 2479, 'hatke': 2480, 'stine': 2481, 'snedden': 2482, 'my': 2483, 'pony': 2484, 'hellmuth': 2485, 'marcos': 2486, 'prado': 2487, 'schein': 2488, 'krishnaswamy': 2489, 'catrin': 2490, 'josie': 2491, 'chakravarthy': 2492, 'choudhry': 2493, 'jayashree': 2494, 'girija': 2495, 'viraraghavan': 2496, 'couch': 2497, 'shonda': 2498, 'rhimes': 2499, 'tanushree': 2500, 'podder': 2501, 'roam': 2502, 'keir': 2503, 'radnedge': 2504, 'heather': 2505, 'merriam': 2506, 'webster': 2507, 'chicago': 2508, 'ramanujam': 2509, 'raghunathan': 2510, 'fukuyama': 2511, 'severance': 2512, 'ove': 2513, 'knausgaard': 2514, 'hoskote': 2515, 'tyler': 2516, 'pavneet': 2517, 'sartre': 2518, 'mizuma': 2519, 'baldwin': 2520, 'southwell': 2521, 'sheffield': 2522, 'seton': 2523, 'emmerich': 2524, 'macleod': 2525, 'sjoukje': 2526, 'zaal': 2527, 'reich': 2528, 'chad': 2529, 'hadfield': 2530, 'subodh': 2531, 'habib': 2532, 'binodini': 2533, 'somi': 2534, 'sergei': 2535, 'eisenstein': 2536, 'maughan': 2537, 'zuckerman': 2538, 'arundhathi': 2539, 'naveed': 2540, 'darren': 2541, 'levine': 2542, 'sarila': 2543, 'shutapa': 2544, 'rifkin': 2545, 'jarrett': 2546, 'krosoczka': 2547, 'ferrari': 2548, 'kawaii': 2549, 'studio': 2550, 'pastrovicchio': 2551, 'egan': 2552, 'shahnaz': 2553, 'judd': 2554, 'winick': 2555, 'abdullah': 2556, 'yusuf': 2557, 'mcmahon': 2558, 'weinberg': 2559, 'gould': 2560, 'nirmal': 2561, 'soars': 2562, 'dawson': 2563, 'sonal': 2564, 'goldblatt': 2565, 'player': 2566, 'stehlik': 2567, 'babinec': 2568, 'kobe': 2569, 'bryant': 2570, 'forster': 2571, 'dexter': 2572, 'dias': 2573, 'ranade': 2574, 'anu': 2575, 'vaidyanathan': 2576, 'subba': 2577, 'trotsky': 2578, 'mikael': 2579, 'lindnord': 2580, 'makarand': 2581, 'waingankar': 2582, 'murtaza': 2583, 'haider': 2584, 'hubbard': 2585, 'gretchen': 2586, 'rubin': 2587, \"reader's\": 2588, 'digest': 2589, 'macneal': 2590, 'mcallister': 2591, 'shannon': 2592, 'hale': 2593, 'reif': 2594, 'dougal': 2595, 'dixon': 2596, 'inaba': 2597, 'friel': 2598, 'liss': 2599, 'dietz': 2600, 'dunthorne': 2601, 'northouse': 2602, 'ved': 2603, 'nishad': 2604, 'arulkumaran': 2605, 'kumaraswamipillai': 2606, 'ruggieri': 2607, 'ramalingam': 2608, 'syal': 2609, 'yoshitoki': 2610, 'oima': 2611, 'weatherford': 2612, 'mahendra': 2613, 'jakhar': 2614, 'ravish': 2615, 'pyle': 2616, 'degeneres': 2617, 'lamb': 2618, 'ace': 2619, 'solnit': 2620, 'rogak': 2621, 'elijah': 2622, 'brahms': 2623, 'roozan': 2624, 'antoli': 2625, 'boukreev': 2626, 'erma': 2627, 'bombeck': 2628, 'phaedra': 2629, 'baxter': 2630, 'kavadlo': 2631, 'cindy': 2632, 'neuschwander': 2633, 'simblet': 2634, 'ramachandran': 2635, 'zinsser': 2636, 'lucantoni': 2637, 'gallwey': 2638, 'jairam': 2639, 'ramesh': 2640, 'nicole': 2641, 'radziwill': 2642, 'bandari': 2643, 'urmila': 2644, 'pawar': 2645, 'ronnie': 2646, \"o'sullivan\": 2647, 'jatindra': 2648, 'morihei': 2649, 'ueshiba': 2650, 'cukier': 2651, 'manisha': 2652, 'bilton': 2653, 'cara': 2654, 'thomson': 2655, 'larteguy': 2656, 'xan': 2657, 'tucker': 2658, \"men's\": 2659, 'health': 2660, 'ana': 2661, 'calderon': 2662, 'iyengar': 2663, 'tejpal': 2664, 'devi': 2665, 'yesodharan': 2666, 'isaiah': 2667, 'berlin': 2668, 'dawkins': 2669, 'dice': 2670, 'media': 2671, 'gothelf': 2672, 'larman': 2673, 'thrale': 2674, 'eben': 2675, 'georg': 2676, 'hegel': 2677, 'revathi': 2678, 'rosamund': 2679, 'hodge': 2680, 'spillman': 2681, 'carmen': 2682, 'bin': 2683, 'ladin': 2684, 'browder': 2685, 'greenwell': 2686, 'smolan': 2687, 'usain': 2688, 'bolt': 2689, 'timofeevich': 2690, 'bothra': 2691, 'bresman': 2692, 'erdnase': 2693, 'milbourne': 2694, 'harland': 2695, 'safian': 2696, 'kerstin': 2697, 'gier': 2698, 'kamthane': 2699, 'corrigan': 2700, 'vineet': 2701, 'bajpai': 2702, 'gyles': 2703, 'brandreth': 2704, 'frindall': 2705, 'usha': 2706, 'fowler': 2707, 'deepika': 2708, 'mahalakshmi': 2709, 'mcdermid': 2710, 'bracht': 2711, 'deanne': 2712, 'panday': 2713, 'baradwaj': 2714, 'rangan': 2715, 'block': 2716, 'mahmoody': 2717, 'vigna': 2718, 'bobo': 2719, 'chernev': 2720, 'knoll': 2721, 'khilnani': 2722, 'thoreau': 2723, 'vijayvargiya': 2724, 'klapstein': 2725, 'marianne': 2726, 'centner': 2727, 'ricardo': 2728, 'canovas': 2729, 'linares': 2730, 'chaube': 2731, 'mackendrick': 2732, 'snehanshu': 2733, 'elie': 2734, 'wiesel': 2735, 'guinan': 2736, 'jong': 2737, 'ambedkar': 2738, 'bedi': 2739, 'bharanidharan': 2740, 'iles': 2741, 'schlender': 2742, 'cyril': 2743, 'lionel': 2744, 'ransom': 2745, 'riggs': 2746, 'harding': 2747, 'tribe': 2748, 'punter': 2749, 'baroness': 2750, 'orczy': 2751, 'upinder': 2752, 'prosenjit': 2753, 'stout': 2754, 'strycker': 2755, 'percival': 2756, 'kunders': 2757, 'fast': 2758, 'paola': 2759, 'landay': 2760, 'rosalind': 2761, 'fergusson': 2762, 'davidson': 2763, 'sebag': 2764, 'bharadwaj': 2765, 'sajita': 2766, 'melissa': 2767, 'bruder': 2768, 'montroll': 2769, 'bahl': 2770, 'mohit': 2771, 'washington': 2772, 'gopalakrishnan': 2773, 'fil': 2774, 'noorani': 2775, 'harrington': 2776, 'elster': 2777, 'susanta': 2778, 'cain': 2779, 'devanshi': 2780, 'micheal': 2781, 'dinnick': 2782, 'mele': 2783, 'sreejata': 2784, 'macgregor': 2785, 'atkins': 2786, 'escudier': 2787, 'arumilli': 2788, 'orrell': 2789, 'munshi': 2790, 'rita': 2791, 'pearson': 2792, 'try': 2793, 'guys': 2794, 'ehrlich': 2795, 'bezzant': 2796, 'blair': 2797, 'denaud': 2798, 'shoaib': 2799, 'akhtar': 2800, 'stoyan': 2801, 'stefanov': 2802, 'hoff': 2803, 'aziz': 2804, 'ansari': 2805, 'moxham': 2806, 'yoon': 2807, 'rahman': 2808, 'azzam': 2809, 'menezes': 2810, 'talavane': 2811, 'feehan': 2812, 'antony': 2813, 'cummins': 2814, 'qing': 2815, 'nigel': 2816, 'lesmoir': 2817, 'mentzer': 2818, 'yuki': 2819, 'tabata': 2820, 'harry': 2821, 'bauld': 2822, 'malpas': 2823, 'glenn': 2824, 'mcgrath': 2825, 'aimen': 2826, 'cruickshank': 2827, 'mead': 2828, 'dwayne': 2829, 'mcduffie': 2830, 'semeiks': 2831, 'mariam': 2832, 'dimon': 2833, 'cotton': 2834, 'ajit': 2835, 'mookerjee': 2836, 'mei': 2837, 'fong': 2838, 'kira': 2839, 'kann': 2840, 'walvin': 2841, 'isikoff': 2842, 'schoenfeld': 2843, 'maxim': 2844, 'lapan': 2845, 'henri': 2846, 'charrière': 2847, \"o'brian\": 2848, 'aijaz': 2849, 'ashraf': 2850, 'wani': 2851, 'lucinda': 2852, 'riley': 2853, 'chandrakant': 2854, 'kawasaki': 2855, 'abbott': 2856, 'meg': 2857, 'in': 2858, 'conversation': 2859, 'with': 2860, 'ngondi': 2861, 'narain': 2862, 'avirook': 2863, 'maxwell': 2864, 'kieron': 2865, 'greta': 2866, 'thunberg': 2867, 'dalton': 2868, 'trumbo': 2869, 'johnston': 2870, 'sikorski': 2871, 'bhasha': 2872, 'reenu': 2873, 'talwar': 2874, 'sikka': 2875, 'dot': 2876, 'barlowe': 2877, 'cline': 2878, 'wirth': 2879, 'pam': 2880, 'jenoff': 2881, 'rana': 2882, 'crick': 2883, 'bachi': 2884, 'karkaria': 2885, 'wu': 2886, 'shruti': 2887, 'schumacher': 2888, 'sharif': 2889, 'rangnekar': 2890, 'bhakri': 2891, 'finkel': 2892, 'pastine': 2893, 'rickie': 2894, 'khosla': 2895, 'mcraven': 2896, 'balagurusamy': 2897, 'gemmell': 2898, 'young': 2899, 'goren': 2900, 'raja': 2901, 'krishnamurthy': 2902, 'boxall': 2903, 'tbd': 2904, 'brigette': 2905, 'tasha': 2906, 'hyacinth': 2907, 'fatima': 2908, 'bhutto': 2909, 'carlos': 2910, 'santana': 2911, 'unnithan': 2912, 'sahni': 2913, 'sudhanshu': 2914, 'bisen': 2915, 'jocko': 2916, 'willink': 2917, 'swift': 2918, 'mulgan': 2919, 'lakshmi': 2920, 'sharon': 2921, 'kendrick': 2922, 'indick': 2923, 'berkowski': 2924, 'chalkfulloflove': 2925, 'judika': 2926, 'illes': 2927, 'edelman': 2928, 'rosenbloom': 2929, 'benvenuti': 2930, 'kieth': 2931, 'giffen': 2932, 'tan': 2933, 'pinter': 2934, 'lance': 2935, 'fadia': 2936, 'hilbert': 2937, 'saradindu': 2938, 'macmillan': 2939, 'catriona': 2940, 'truss': 2941, 'shekhar': 2942, 'mahamaya': 2943, 'navlakha': 2944, 'sacks': 2945, 'garima': 2946, 'kushwaha': 2947, 'yadvinder': 2948, 'sandhu': 2949, 'jyotiprakash': 2950, 'tamuli': 2951, 'mouchumi': 2952, 'handique': 2953, 'samudrala': 2954, 'slott': 2955, 'carl': 2956, 'zimmer': 2957, 'siddhesh': 2958, 'inamdar': 2959, 'demspey': 2960, 'dawn': 2961, 'thorp': 2962, 'danko': 2963, 'michie': 2964, 'macey': 2965, 'beyeler': 2966, 'allie': 2967, 'da': 2968, 'cotterill': 2969, 'felicity': 2970, \"o'dell\": 2971, 'weinstein': 2972, 'perry': 2973, 'lea': 2974, 'khyrunnisa': 2975, 'rajadhyaksha': 2976, 'bolte': 2977, 'sadhu': 2978, 'harriet': 2979, 'khaled': 2980, 'hosseini': 2981, 'addy': 2982, 'osmani': 2983, 'derren': 2984, 'ghavri': 2985, 'hutt': 2986, 'folsom': 2987, 'angelou': 2988, 'gurney': 2989, 'aurèlien': 2990, 'gèron': 2991, 'colman': 2992, 'rider': 2993, 'saumya': 2994, 'maureen': 2995, 'marzi': 2996, 'hilary': 2997, 'mantel': 2998, 'popova': 2999, 'shiva': 3000, 'roffer': 3001, 'zorba': 3002, 'laloo': 3003, 'samanth': 3004, 'nigar': 3005, 'hashimzade': 3006, 'myles': 3007, 'rik': 3008, 'hoskin': 3009, 'berridge': 3010, 'yashwant': 3011, 'lewin': 3012, 'kepnes': 3013, 'denny': 3014, 'flinn': 3015, 'tellejohn': 3016, 'manaswi': 3017, 'kaminoff': 3018, 'sloan': 3019, 'lilian': 3020, 'rooney': 3021, 'downie': 3022, 'gaskell': 3023, 'covey': 3024, 'kaaren': 3025, 'pixton': 3026, 'poitier': 3027, 'goel': 3028, 'lupton': 3029, 'tanenbaum': 3030, 'ernestine': 3031, 'gilbreth': 3032, 'darrel': 3033, 'bjork': 3034, 'seymour': 3035, 'papert': 3036, 'guber': 3037, 'kalish': 3038, 'fatus': 3039, 'branko': 3040, 'milanovic': 3041, 'sapolsky': 3042, 'nissim': 3043, 'supratim': 3044, 'winn': 3045, 'coralie': 3046, 'bickford': 3047, 'khatri': 3048, 'wynne': 3049, 'colm': 3050, 'tóibín': 3051, 'kumkum': 3052, 'mcdougall': 3053, 'fish': 3054, 'sharath': 3055, 'komarraju': 3056, 'smythe': 3057, 'canfield': 3058, 'deuter': 3059, 'bradbery': 3060, 'joanna': 3061, 'turnbull': 3062, 'fishkin': 3063, 'beecroft': 3064, 'jeeva': 3065, 'jose': 3066, 'romain': 3067, 'puertolas': 3068, 'barnett': 3069, 'mangan': 3070, 'pilone': 3071, 'gokhale': 3072, 'pk': 3073, 'benardot': 3074, 'mukul': 3075, 'kanitkar': 3076, 'anoop': 3077, 'katz': 3078, 'nageswara': 3079, 'design': 3080, 'museum': 3081, 'enterprise': 3082, 'alderman': 3083, 'lavender': 3084, 'blumenthal': 3085, 'ashna': 3086, 'kedia': 3087, 'tibballs': 3088, 'wolmar': 3089, 'bevelin': 3090, 'farr': 3091, 'madhuri': 3092, 'barack': 3093, 'moscovich': 3094, 'annapurna': 3095, 'pewdiepie': 3096, 'shivaprasad': 3097, 'shivani': 3098, 'jacek': 3099, 'zurada': 3100, 'amrita': 3101, 'editorials': 3102, 'eastwood': 3103, 'butcher': 3104, 'longworth': 3105, 'crockford': 3106, 'yuri': 3107, 'diogenes': 3108, 'schaum': 3109, 'nipun': 3110, 'yatin': 3111, 'gilstrap': 3112, 'maharghya': 3113, 'tr': 3114, 'bartlett': 3115, 'tomory': 3116, 'kemp': 3117, 'mel': 3118, 'bay': 3119, 'sonmez': 3120, 'raina': 3121, 'telgemeier': 3122, 'feroze': 3123, \"o'hearn\": 3124, 'niraja': 3125, 'jayal': 3126, 'salim': 3127, 'empson': 3128, 'ebert': 3129, 'perrine': 3130, 'klabnik': 3131, 'deulgaonkar': 3132, 'mick': 3133, 'goodrick': 3134, 'sandford': 3135, 'preeti': 3136, 'shenoy': 3137, 'ogden': 3138, 'linell': 3139, 'giridharadas': 3140, 'kerri': 3141, 'maniscalco': 3142, 'jeffreys': 3143, 'gear': 3144, 'harkirat': 3145, 'holzner': 3146, 'boris': 3147, 'kordemsky': 3148, 'parry': 3149, 'waugh': 3150, 'desh': 3151, 'victore': 3152, 'riyaz': 3153, 'tayyibji': 3154, 'icke': 3155, 'bunyan': 3156, 'stedman': 3157, 'kamal': 3158, 'franklin': 3159, 'bharti': 3160, 'dayal': 3161, 'brushwood': 3162, 'becky': 3163, 'nisevich': 3164, 'bede': 3165, 'wwf': 3166, 'piers': 3167, 'read': 3168, 'barclay': 3169, 'lemire': 3170, 'keller': 3171, 'defoe': 3172, 'sathish': 3173, 'phi': 3174, 'learning': 3175, 'eddy': 3176, 'behrendt': 3177, 'oprah': 3178, 'chirag': 3179, 'bagadia': 3180, 'gribble': 3181, 'bibek': 3182, 'desmond': 3183, 'morris': 3184, 'roberta': 3185, 'viktor': 3186, 'frankl': 3187, 'mcenroe': 3188, 'collie': 3189, 'devlin': 3190, 'pegasus': 3191, 'hewitt': 3192, 'milburne': 3193, 'dodie': 3194, 'hahn': 3195, 'almossawi': 3196, 'sameer': 3197, 'plec': 3198, 'bhavi': 3199, 'carney': 3200, 'anupam': 3201, 'kher': 3202, 'mairi': 3203, 'mackinnon': 3204, 'marcella': 3205, 'grassi': 3206, 'landau': 3207, 'sathe': 3208, 'ranum': 3209, 'kerchever': 3210, 'manu': 3211, 'nicholls': 3212, 'rohini': 3213, 'clayborne': 3214, 'gladir': 3215, 'rex': 3216, 'lindsey': 3217, 'gallavardin': 3218, 'no': 3219, 'moench': 3220, 'hamlyn': 3221, 'euwe': 3222, 'swerling': 3223, 'kushner': 3224, 'kalki': 3225, 'tomasz': 3226, 'nurkiewicz': 3227, 'quentin': 3228, 'docter': 3229, 'satyawadi': 3230, 'minteer': 3231, 'criado': 3232, 'perez': 3233, 'twain': 3234, 'bork': 3235, 'balan': 3236, 'butterworth': 3237, 'najwa': 3238, 'zebian': 3239, 'somasundaram': 3240, 'annemann': 3241, 'navneesh': 3242, 'boardman': 3243, 'yogesh': 3244, 'chandekar': 3245, 'uzma': 3246, 'jalaluddin': 3247, 'baime': 3248, 'natalie': 3249, 'goldberg': 3250, 'mcgreal': 3251, 'ramadorai': 3252, 'sumpter': 3253, 'vai': 3254, 'workman': 3255, 'heinrichs': 3256, 'latif': 3257, 'shackle': 3258, 'milind': 3259, 'mulick': 3260, 'michaels': 3261, 'shonali': 3262, 'sabherwal': 3263, 'durga': 3264, 'farmelo': 3265, 'sreeram': 3266, 'chaulia': 3267, 'bayles': 3268, 'perrin': 3269, 'bohjalian': 3270, 'toye': 3271, 'santanu': 3272, 'pattanayak': 3273, 'bao': 3274, 'ninh': 3275, 'heiner': 3276, 'schenke': 3277, 'kamila': 3278, 'shamsie': 3279, 'burnett': 3280, 'langley': 3281, 'swick': 3282, 'colgan': 3283, 'simmi': 3284, 'seger': 3285, 'koneitzko': 3286, 'gurihiru': 3287, 'abhishek': 3288, 'kernighan': 3289, 'schmid': 3290, 'koch': 3291, 'gladwell': 3292, 'mikhail': 3293, 'tal': 3294, 'kissel': 3295, 'brawn': 3296, 'karthik': 3297, 'govinbarajulu': 3298, 'padhy': 3299, 'pranay': 3300, 'rakeysh': 3301, 'omprakash': 3302, 'kamlesh': 3303, 'rensil': 3304, \"d'silva\": 3305, 'siddharth': 3306, 'dhanvant': 3307, 'shanghvi': 3308, 'stina': 3309, 'wirsén': 3310, 'sangeeta': 3311, 'sudhir': 3312, 'katharina': 3313, 'cowan': 3314, 'bhatnagar': 3315, 'jenna': 3316, 'rainey': 3317, 'schumer': 3318, 'komal': 3319, 'andrews': 3320, 'wingate': 3321, 'eng': 3322, 'mischel': 3323, 'froning': 3324, 'mccandless': 3325, 'parrott': 3326, 'hardt': 3327, 'shobhan': 3328, 'bantwal': 3329, 'shaffer': 3330, 'neville': 3331, 'denver': 3332, 'lindley': 3333, 'wise': 3334, 'bauer': 3335, 'ovais': 3336, 'mehboob': 3337, 'match': 3338, 'caryl': 3339, 'lena': 3340, 'dunham': 3341, 'mignola': 3342, 'saleha': 3343, 'banu': 3344, 'harkishan': 3345, 'dutt': 3346, 'skurka': 3347, 'valmiki': 3348, 'kleppmann': 3349, 'nikhilananda': 3350, 'saadat': 3351, 'hasan': 3352, 'manto': 3353, 'zeihan': 3354, 'basden': 3355, 'herzog': 3356, 'zvi': 3357, 'kohavi': 3358, 'greenfield': 3359, 'neetu': 3360, 'dickinson': 3361, 'tremayne': 3362, 'kanhaiya': 3363, 'isakov': 3364, 'kirk': 3365, 'raschka': 3366, 'thibault': 3367, 'burniat': 3368, 'mathieu': 3369, 'damour': 3370, 'cowsill': 3371, 'wizards': 3372, 'rpg': 3373, 'somiah': 3374, 'meyer': 3375, 'chatfield': 3376, 'dickason': 3377, 'farnsworth': 3378, 'rommel': 3379, 'rodrigues': 3380, 'miler': 3381, 'oatmeal': 3382, 'strength': 3383, 'conditioning': 3384, 'nsca': 3385, 'lescroart': 3386, 'topol': 3387, 'fiske': 3388, 'udayasankar': 3389, 'bonatti': 3390, 'bass': 3391, 'poundstone': 3392, 'joachim': 3393, 'berendt': 3394, 'speake': 3395, 'nandan': 3396, 'nilekani': 3397, 'arnab': 3398, 'ramchandra': 3399, 'flock': 3400, 'bagaria': 3401, 'jeevan': 3402, 'kang': 3403, 'charlene': 3404, 'tarbox': 3405, 'cranston': 3406, 'climo': 3407, 'medha': 3408, 'kudaisya': 3409, 'andaleeb': 3410, 'wajid': 3411, 'susanna': 3412, 'kaysen': 3413, 'deep': 3414, 'condoleezza': 3415, 'rice': 3416, 'joel': 3417, 'dicker': 3418, 'austwick': 3419, 'desani': 3420, 'reetika': 3421, 'khera': 3422, 'teltumbde': 3423, 'yengde': 3424, 'teju': 3425, 'badri': 3426, 'manali': 3427, 'highlights': 3428, 'carlton': 3429, 'charteris': 3430, 'test': 3431, 'iacocca': 3432, 'zhisui': 3433, 'hobrough': 3434, 'johann': 3435, 'clyde': 3436, 'suryakanta': 3437, 'tess': 3438, 'gerritsen': 3439, 'clay': 3440, 'rachael': 3441, 'hill': 3442, 'chbosky': 3443, 'weintraub': 3444, 'hughes': 3445, 'rajat': 3446, 'keshav': 3447, 'aneel': 3448, 'knuth': 3449, 'nazia': 3450, 'erum': 3451, 'kelsey': 3452, 'oseid': 3453, 'naz': 3454, 'tejaswini': 3455, 'pagadala': 3456, 'tamara': 3457, 'ireland': 3458, 'rowland': 3459, 'gillham': 3460, 'florence': 3461, 'sakade': 3462, 'speed': 3463, 'gamma': 3464, 'muni': 3465, 'pasricha': 3466, 'dorothee': 3467, 'wenner': 3468, 'olga': 3469, 'lecaye': 3470, 'bhowmik': 3471, 'cavallo': 3472, 'bunt': 3473, 'shraddha': 3474, 'mahurkar': 3475, 'falkoff': 3476, 'thaneeya': 3477, 'mcardle': 3478, 'ibbotson': 3479, 'keegan': 3480, 'rosi': 3481, 'mcnab': 3482, 'wooten': 3483, 'shivang': 3484, 'tracy': 3485, 'japikse': 3486, 'prachi': 3487, 'voltaire': 3488, 'brandt': 3489, 'shalvis': 3490, 'meher': 3491, 'mcarthur': 3492, 'rakesh': 3493, 'truman': 3494, 'capote': 3495, 'suprio': 3496, 'hazleton': 3497, 'myers': 3498, 'pink': 3499, 'subrata': 3500, 'mitra': 3501, 'liebig': 3502, 'brené': 3503, 'coles': 3504, 'bronnie': 3505, 'aitken': 3506, 'issac': 3507, 'alexa': 3508, 'chung': 3509, 'camus': 3510, 'gilberg': 3511, 'adelman': 3512, 'lillian': 3513, 'herlands': 3514, 'hornstein': 3515, 'percy': 3516, 'hilal': 3517, 'chika': 3518, 'miyata': 3519, 'olivier': 3520, 'föllmi': 3521, 'bhardwaj': 3522, 'mohd': 3523, 'arif': 3524, 'siddique': 3525, 'kwan': 3526, 'elkeles': 3527, 'whitmore': 3528, 'tarek': 3529, 'fatah': 3530, 'wendy': 3531, 'doniger': 3532, 'shouvik': 3533, 'trollope': 3534, 'lawrey': 3535, 'bhandari': 3536, 'kundra': 3537, 'coutinho': 3538, 'yunus': 3539, 'hamid': 3540, 'baig': 3541, 'bierut': 3542, 'usman': 3543, 'malachy': 3544, 'tasker': 3545, 'stokoe': 3546, 'kouhei': 3547, 'adharanand': 3548, 'winchester': 3549, 'lobsang': 3550, 'rampa': 3551, 'gosling': 3552, 'hemant': 3553, 'riichiro': 3554, 'inagaki': 3555, 'haddon': 3556, 'riddell': 3557, 'devendra': 3558, 'philpot': 3559, 'horrocks': 3560, 'timberg': 3561, 'phelps': 3562, 'roshan': 3563, 'tolani': 3564, 'pervez': 3565, 'mistry': 3566, 'feeney': 3567, 'christophe': 3568, 'chabouté': 3569, 'tudor': 3570, 'bompa': 3571, 'cohn': 3572, 'maitreyee': 3573, 'lear': 3574, 'celeste': 3575, 'shinichi': 3576, 'welling': 3577, 'lammle': 3578, 'julius': 3579, 'naresh': 3580, 'jayn': 3581, 'witt': 3582, 'sansom': 3583, 'thi': 3584, 'bui': 3585, 'kul': 3586, 'want': 3587, 'chitra': 3588, 'divakaruni': 3589, 'yossi': 3590, 'ghinsberg': 3591, 'yuvraj': 3592, 'maloy': 3593, 'gaur': 3594, 'rochester': 3595, 'aruna': 3596, 'chakravarti': 3597, 'pradip': 3598, 'hodgkinson': 3599, 'dabholkar': 3600, 'fotedar': 3601, 'weinersmith': 3602, 'brindle': 3603, 'hava': 3604, 'vinita': 3605, 'salvi': 3606, 'leiserson': 3607, 'rivest': 3608, 'stein': 3609, 'british': 3610, 'library': 3611, 'imran': 3612, 'nyari': 3613, 'nain': 3614, 'hansaji': 3615, 'yogendra': 3616, 'junichiro': 3617, 'tanizaki': 3618, 'chitrangada': 3619, 'jomny': 3620, 'sun': 3621, 'mccarter': 3622, 'kuan': 3623, 'yew': 3624, 'mccoy': 3625, 'noad': 3626, 'nirad': 3627, 'chaudhuri': 3628, 'esquirel': 3629, 'venables': 3630, 'damodar': 3631, 'cece': 3632, 'telep': 3633, 'alva': 3634, 'nikola': 3635, 'tesla': 3636, 'blandford': 3637, 'proust': 3638, 'samhita': 3639, 'arni': 3640, 'rhett': 3641, 'mclaughlin': 3642, 'bharat': 3643, 'scaachi': 3644, 'koul': 3645, 'wollstonecraft': 3646, 'simran': 3647, 'plath': 3648, 'janette': 3649, 'sadik': 3650, 'solomonow': 3651, 'henderson': 3652, 'jared': 3653, 'tendler': 3654, 'anju': 3655, 'nierenberg': 3656, 'andrei': 3657, 'besedin': 3658, 'gilbert': 3659, 'strang': 3660, 'schaller': 3661, 'goodman': 3662, 'lib': 3663, 'jo': 3664, 'nesbo': 3665, 'earl': 3666, 'awkward': 3667, 'yeti': 3668, 'rutherford': 3669, 'moran': 3670, 'armando': 3671, 'maradona': 3672, 'ravindra': 3673, 'pardhasaradhi': 3674, 'satyrnarayana': 3675, 'kari': 3676, 'hotakainen': 3677, 'rajeev': 3678, 'reginald': 3679, 'kattan': 3680, 'ibarra': 3681, 'hersey': 3682, 'cartwright': 3683, 'maggie': 3684, 'stiefvater': 3685, 'cappon': 3686, 'unnamati': 3687, 'syama': 3688, 'levitt': 3689, 'gibbons': 3690, 'sergio': 3691, 'cariello': 3692, 'barlog': 3693, 'goodall': 3694, 'tomris': 3695, 'tangaz': 3696, 'sashikala': 3697, 'shashank': 3698, 'shyamlal': 3699, 'vageria': 3700, 'webb': 3701, 'vasily': 3702, 'veena': 3703, 'audre': 3704, 'lorde': 3705, 'niladri': 3706, 'ramanujan': 3707, 'cross': 3708, 'koerner': 3709, 'shrikant': 3710, 'prasoon': 3711, 'julietta': 3712, 'arup': 3713, 'richa': 3714, 'jacqueline': 3715, 'frost': 3716, 'manahar': 3717, 'pupul': 3718, 'jayakar': 3719, 'robbins': 3720, 'shirley': 3721, 'fitikides': 3722, 'pénélope': 3723, 'bagieu': 3724, 'schmuller': 3725, 'hett': 3726, 'wister': 3727, 'willa': 3728, 'cather': 3729, 'zane': 3730, 'grey': 3731, 'brand': 3732, 'buss': 3733, 'jayme': 3734, 'adelson': 3735, 'philpott': 3736, 'javier': 3737, 'fernandez': 3738, 'gonzalez': 3739, 'eckhart': 3740, 'tolle': 3741, 'kirkpatrick': 3742, 'asher': 3743, 'calaprice': 3744, 'shamya': 3745, 'soji': 3746, 'shimada': 3747, 'shika': 3748, 'redfield': 3749, 'jamison': 3750, 'mountaineers': 3751, 'matha': 3752, 'mahadev': 3753, 'desai': 3754, 'bindra': 3755, 'brijnath': 3756, 'kundera': 3757, 'lorin': 3758, 'hochstein': 3759, 'boo': 3760, 'gavaskar': 3761, 'hardeep': 3762, 'hillary': 3763, 'rodham': 3764, 'clinton': 3765, 'brock': 3766, 'lesnar': 3767, 'macy': 3768, 'metzl': 3769, 'md': 3770, 'kowalchik': 3771, 'brinda': 3772, \"o'shea\": 3773, 'grimley': 3774, 'mimi': 3775, 'bargfrede': 3776, 'bloom': 3777, 'rainer': 3778, 'rilke': 3779, 'u': 3780, 'salwi': 3781, 'takeshi': 3782, 'obata': 3783, 'mccartney': 3784, 'kathleen': 3785, 'haywood': 3786, 'somnath': 3787, 'laman': 3788, 'goetz': 3789, 'jnuta': 3790, 'joygopal': 3791, 'meena': 3792, 'kandasamy': 3793, 'falk': 3794, 'ramkrishna': 3795, 'bhandrakar': 3796, 'bhandarkar': 3797, 'irby': 3798, 'maurois': 3799, 'choma': 3800, 'nachmanovitch': 3801, 'dynamo': 3802, 'nityananda': 3803, 'somashekara': 3804, 'guru': 3805, 'manjunatha': 3806, 'mcmeekin': 3807, 'mcmurtry': 3808, 'mclean': 3809, 'mcmillan': 3810, 'johar': 3811, 'henning': 3812, 'mankell': 3813, 'klinger': 3814, 'tanumita': 3815, 'murch': 3816, 'tarrant': 3817, 'adult': 3818, 'coloring': 3819, 'artists': 3820, 'marinelli': 3821, 'hariharan': 3822, 'roshen': 3823, 'dalal': 3824, 'kalidasa': 3825, 'lt': 3826, 'gen': 3827, 'harbakhsh': 3828, 'felix': 3829, 'vaynerchuk': 3830, 'clemens': 3831, 'worroll': 3832, 'houghton': 3833, 'afremow': 3834, 'armentrout': 3835, 'pimpler': 3836, 'ayto': 3837, 'mamet': 3838, 'hirohiko': 3839, 'araki': 3840, 'hughart': 3841, 'delhi': 3842, 'ethem': 3843, 'alpaydin': 3844, 'damien': 3845, 'hearne': 3846, 'ambi': 3847, 'parameswaran': 3848, 'kalpana': 3849, 'swaminathan': 3850, 'yana': 3851, 'toboso': 3852, 'chang': 3853, 'yash': 3854, 'birla': 3855, 'tristan': 3856, 'gooley': 3857, 'redmanidea': 3858, 'kennedy': 3859, 'ushinor': 3860, 'charu': 3861, 'granville': 3862, 'damian': 3863, 'echo': 3864, 'bodine': 3865, 'amie': 3866, 'elspeth': 3867, 'beard': 3868, 'bechdel': 3869, 'heartin': 3870, 'kanikathottu': 3871, 'pushp': 3872, 'vishwas': 3873, 'mudagal': 3874, 'nimish': 3875, 'pandit': 3876, 'satyakam': 3877, 'vidyalankar': 3878, 'dix': 3879, 'fay': 3880, 'z': 3881, 'danielewski': 3882, 'tilak': 3883, 'devasher': 3884, 'trump': 3885, 'aman': 3886, 'hingorani': 3887, 'preet': 3888, 'bharara': 3889, 'ira': 3890, 'mukhoty': 3891, 'quiller': 3892, 'hedges': 3893, 'horrix': 3894, 'elfriede': 3895, 'jelinek': 3896, 'rupert': 3897, 'snell': 3898, 'malerman': 3899, 'zorawar': 3900, 'daulet': 3901, 'santosh': 3902, 'blank': 3903, 'stockman': 3904, 'roxna': 3905, 'watt': 3906, 'puffin': 3907, 'nautiyal': 3908, 'nadiya': 3909, 'carrick': 3910, 'gerard': 3911, 'way': 3912, 'tetsuko': 3913, 'kuroyanagi': 3914, 'mumford': 3915, 'parag': 3916, 'puneet': 3917, 'raasch': 3918, 'american': 3919, 'heritage': 3920, 'fumio': 3921, 'sasaki': 3922, 'syen': 3923, 'trenton': 3924, 'stewart': 3925, 'reese': 3926, 'dobson': 3927, 'bikramaditya': 3928, 'dhameja': 3929, 'priyansu': 3930, 'sekhar': 3931, 'panda': 3932, 'ashlock': 3933, 'rachith': 3934, 'akash': 3935, 'chandresh': 3936, 'motwani': 3937, 'noriko': 3938, 'masukawa': 3939, 'ham': 3940, 'pellant': 3941, 'anuradha': 3942, 'bhavsar': 3943, 'shirish': 3944, 'sahasrabuddhe': 3945, 'pr': 3946, 'lupo': 3947, 'jagdish': 3948, 'liv': 3949, 'constantine': 3950, 'tarangini': 3951, 'sriraman': 3952, 'kannampilly': 3953, 'beatrix': 3954, 'potter': 3955, 'lemay': 3956, 'rafe': 3957, 'colburn': 3958, 'kyrnin': 3959, 'avinash': 3960, 'tiwary': 3961, 'naftalin': 3962, 'byatt': 3963, 'bharati': 3964, 'sundari': 3965, 'venkatraman': 3966, 'roll': 3967, 'naoko': 3968, 'takeuchi': 3969, 'bentley': 3970, 'conway': 3971, 'rod': 3972, 'gilmour': 3973, 'haff': 3974, 'triplett': 3975, 'jonnes': 3976, 'neelakantan': 3977, 'hailey': 3978, 'seneca': 3979, 'rusbridger': 3980, 'sharad': 3981, 'icwa': 3982, 'amita': 3983, 'singhvi': 3984, 'bronstein': 3985, 'houts': 3986, 'isaque': 3987, 'bagwan': 3988, 'dictionary': 3989, 'contreras': 3990, 'koelpin': 3991, 'erickson': 3992, 'delgado': 3993, 'sigman': 3994, 'civardi': 3995, 'subhas': 3996, 'nidhi': 3997, 'chanani': 3998, 'houston': 3999, 'bates': 4000, 'wickwire': 4001, 'kirkman': 4002, 'sebastián': 4003, 'fest': 4004, 'budi': 4005, 'kurniawan': 4006, 'tavleen': 4007, 'ellington': 4008, 'darden': 4009, 'pulizzi': 4010, 'cathia': 4011, 'jenainati': 4012, 'donoghue': 4013, 'ricky': 4014, 'ponting': 4015, 'cleese': 4016, 'lundin': 4017, 'zia': 4018, 'mody': 4019, 'velicovich': 4020, 'moorjani': 4021, 'satendra': 4022, 'सतेंद्र': 4023, 'कुमार': 4024, 'meik': 4025, 'wiking': 4026, 'epstein': 4027, 'rajput': 4028, 'abby': 4029, 'jimenez': 4030, 'badrinath': 4031, 'weingarten': 4032, 'balakrishna': 4033, 'kamath': 4034, 'bynghall': 4035, 'balfour': 4036, 'kotler': 4037, 'auster': 4038, 'saravanan': 4039, 'senthil': 4040, 'jeevananthan': 4041, 'nandita': 4042, 'haksar': 4043, 'valerio': 4044, 'massimo': 4045, 'matloff': 4046, 'longo': 4047, 'stenning': 4048, 'bowater': 4049, 'assa': 4050, 'doron': 4051, 'botham': 4052, 'bourdain': 4053, 'disha': 4054, 'kajoli': 4055, 'vujicic': 4056, 'cardozo': 4057, 'laughlin': 4058, 'davies': 4059, 'swaraa': 4060, 'lamott': 4061, 'morpurgo': 4062, 'aniruddha': 4063, 'dhamorikar': 4064, 'chintan': 4065, 'schiff': 4066, 'ruchi': 4067, 'kokcha': 4068, 'oaks': 4069, 'banana': 4070, 'yoshimoto': 4071, 'vogler': 4072, 'smit': 4073, 'baldick': 4074, 'kitty': 4075, 'kelley': 4076, 'waid': 4077, 'staples': 4078, 'pedro': 4079, 'domingos': 4080, 'basharat': 4081, 'peer': 4082, 'marsh': 4083, 'sahay': 4084, 'easterine': 4085, 'kire': 4086, 'ake': 4087, 'viberg': 4088, 'genis': 4089, 'carreras': 4090, 'marino': 4091, 'halberstam': 4092, 'erc': 4093, 'davidar': 4094, 'schwaber': 4095, 'bruno': 4096, 'munari': 4097, 'dolly': 4098, 'alderton': 4099, 'aurobindo': 4100, 'millman': 4101, 'ruy': 4102, 'castro': 4103, 'freddie': 4104, 'vardhanabhuti': 4105, 'sieckmann': 4106, 'soma': 4107, 'molly': 4108, 'suber': 4109, 'jez': 4110, 'humble': 4111, 'harvey': 4112, 'penick': 4113, 'zygmunt': 4114, 'miloszewski': 4115, 'parashar': 4116, 'vaillant': 4117, 'haware': 4118, 'petzold': 4119, 'wendon': 4120, 'martinez': 4121, 'nicklen': 4122, 'jerilee': 4123, 'goodfellow': 4124, 'dalai': 4125, 'lama': 4126, 'kanetakr': 4127, 'shrirang': 4128, 'korde': 4129, 'asok': 4130, 'nadhani': 4131, 'turow': 4132, 'kathy': 4133, 'sierra': 4134, 'oliva': 4135, 'cressida': 4136, 'cowell': 4137, 'langfield': 4138, 'duddell': 4139, 'amitabh': 4140, 'manjrekar': 4141, 'shireen': 4142, 'novak': 4143, 'djokovic': 4144, 'dicharry': 4145, 'maupassant': 4146, 'neeraj': 4147, 'vidhanshu': 4148, 'everett': 4149, 'carolyn': 4150, 'keene': 4151, 'rupi': 4152, 'soraya': 4153, 'chemaly': 4154, 'moupia': 4155, 'jeph': 4156, 'loeb': 4157, 'olivia': 4158, 'laing': 4159, 'powell': 4160, 'itl': 4161, 'esl': 4162, 'shamsul': 4163, 'islam': 4164, 'diamond': 4165, 'wilco': 4166, 'jodie': 4167, 'daber': 4168, 'benchley': 4169, 'pichon': 4170, 'reshma': 4171, 'saujani': 4172, 'sethi': 4173, 'melina': 4174, 'gerosa': 4175, 'bellows': 4176, 'mcquiston': 4177, 'crispin': 4178, 'duke': 4179, 'blauvelt': 4180, 'bbc': 4181, 'pentland': 4182, 'anamika': 4183, 'neeti': 4184, 'vinit': 4185, 'delacy': 4186, 'foster': 4187, 'provost': 4188, 'gross': 4189, 'rodger': 4190, 'schildt': 4191, 'ashok': 4192, 'soota': 4193, 'aciman': 4194, 'none': 4195, 'kling': 4196, 'lustbader': 4197, 'samanta': 4198, 'debasis': 4199, 'wolfe': 4200, 'sisson': 4201, 'eduardo': 4202, 'sacheri': 4203, 'karnazes': 4204, 'amish': 4205, 'daphne': 4206, 'gulland': 4207, 'geeta': 4208, 'akilandeswari': 4209, 'sutton': 4210, 'barto': 4211, 'willenbrink': 4212, 'aparna': 4213, 'takehiko': 4214, 'inoue': 4215, 'thushan': 4216, 'ganegedara': 4217, 'venugopal': 4218, 'hazeley': 4219, 'juneja': 4220, 'bo': 4221, 'burnham': 4222, 'marcia': 4223, 'mohith': 4224, 'shrivastava': 4225, 'council': 4226, 'for': 4227, 'promotion': 4228, 'urdu': 4229, 'manhattan': 4230, 'packard': 4231, 'tathagata': 4232, 'siddhartha': 4233, 'mcnamara': 4234, 'pressman': 4235, 'zappia': 4236, 'sakyong': 4237, 'mipham': 4238, 'brahma': 4239, 'chellaney': 4240, 'montaigne': 4241, 'krabbé': 4242, 'mcguff': 4243, 'ramaiah': 4244, \"'lofty'\": 4245, 'tessa': 4246, 'dare': 4247, 'waterfield': 4248, 'stadter': 4249, 'plutarch': 4250, 'norwood': 4251, 'ruskovich': 4252, 'aristotle': 4253, 'akiba': 4254, 'chandramohan': 4255, 'puppala': 4256, 'naylor': 4257, 'klaus': 4258, 'stacey': 4259, 'halls': 4260, 'hardev': 4261, 'bahri': 4262, 'louise': 4263, 'penny': 4264, 'madhav': 4265, 'saina': 4266, 'nehwal': 4267, 'chand': 4268, 'kandpal': 4269, 'nigma': 4270, 'talib': 4271, 'tolkien': 4272, 'crews': 4273, 'kasie': 4274, 'francois': 4275, 'chollet': 4276, 'authors': 4277, 'kovach': 4278, 'monsarrat': 4279, 'krech': 4280, 'sakshama': 4281, 'dhariwal': 4282, 'dev': 4283, 'haqqani': 4284, 'soumya': 4285, 'behera': 4286, 'neena': 4287, 'murr': 4288, 'amby': 4289, 'burfoot': 4290, 'alavala': 4291, 'chennakesava': 4292, 'goodrich': 4293, 'manjushree': 4294, 'thapa': 4295, 'impact': 4296, 'index': 4297, 'jacques': 4298, 'daria': 4299, 'nagle': 4300, 'martini': 4301, 'fraction': 4302, 'angie': 4303, 'hilton': 4304, 'walton': 4305, 'matchett': 4306, 'striano': 4307, 'mulligan': 4308, 'quincy': 4309, 'denise': 4310, 'spellberg': 4311, 'smita': 4312, 'meltzer': 4313, 'krakauer': 4314, 'mansell': 4315, 'albee': 4316, 'tibshirani': 4317, 'azeem': 4318, 'ibrahim': 4319, 'arita': 4320, 'espn': 4321, 'nupur': 4322, 'kingshuk': 4323, 'krthivasan': 4324, 'chopin': 4325, 'greenbaum': 4326, 'michaelides': 4327, 'daulat': 4328, 'shaktawat': 4329, 'tekriwal': 4330, 'kabacoff': 4331, 'fumiko': 4332, 'chiba': 4333, 'rajaraman': 4334, 'adabala': 4335, 'ingo': 4336, 'arndt': 4337, 'clapham': 4338, 'candace': 4339, 'pert': 4340, 'rie': 4341, 'stylist': 4342, 'kroc': 4343, 'valve': 4344, 'ma': 4345, 'vasapannarava': 4346, 'neelimkumar': 4347, 'khaire': 4348, 'asim': 4349, 'lyons': 4350, 'zarrilli': 4351, 'phillip': 4352, 'spragg': 4353, 'adler': 4354, 'hocking': 4355, 'xiangcai': 4356, 'xu': 4357, 'sharankumar': 4358, 'limbale': 4359, 'bhoomkar': 4360, 'lajos': 4361, 'egri': 4362, 'ramanath': 4363, 'sahai': 4364, 'francoise': 4365, 'barbira': 4366, 'freedman': 4367, 'brady': 4368, 'holding': 4369, 'kautilya': 4370, 'dafydd': 4371, 'stuttard': 4372, 'buckley': 4373, 'elliott': 4374, 'janice': 4375, 'sriram': 4376, 'chellapilla': 4377, 'håkan': 4378, 'nesser': 4379, 'mn': 4380, 'hijioka': 4381, 'uttam': 4382, 'desiderius': 4383, 'erasmus': 4384, 'grimmett': 4385, 'hem': 4386, 'baral': 4387, 'saxena': 4388, 'downs': 4389, 'kort': 4390, 'eggers': 4391, 'northrup': 4392, 'mas': 4393, \"oyama's\": 4394, 'hein': 4395, 'kiessling': 4396, 'tove': 4397, 'jansson': 4398, 'bernays': 4399, 'shapiro': 4400, 'nanditha': 4401, 'meghna': 4402, 'pant': 4403, 'grolemund': 4404, 'ankush': 4405, 'saikia': 4406, 'davidowitz': 4407, 'janis': 4408, 'fisher': 4409, 'ennis': 4410, 'dhanavel': 4411, 'tyson': 4412, 'shreya': 4413, 'dhanwanthary': 4414, 'polson': 4415, 'elin': 4416, 'hilderbrand': 4417, 'vijayakarthikeyan': 4418, 'kimball': 4419, 'kai': 4420, 'fu': 4421, 'dorland': 4422, 'yusei': 4423, 'matsui': 4424, 'educational': 4425, 'testing': 4426, 'kimon': 4427, 'nicolaides': 4428, 'mahalekshmi': 4429, 'yoram': 4430, 'bauman': 4431, 'mm': 4432, 'vetticad': 4433, 'arianna': 4434, 'huffington': 4435, 'lubanovic': 4436, 'conaway': 4437, 'christin': 4438, 'meadows': 4439, 'nivedita': 4440, 'xun': 4441, 'rawlings': 4442, 'annette': 4443, 'nassim': 4444, 'taleb': 4445, 'drnaso': 4446, 'greive': 4447, \"gold's\": 4448, 'gym': 4449, 'kanae': 4450, 'minato': 4451, 'althoff': 4452, 'gin': 4453, 'kamimura': 4454, 'tajiri': 4455, 'hideki': 4456, 'sonoda': 4457, 'dollez': 4458, 'kashyap': 4459, 'madhur': 4460, 'jaffrey': 4461, 'jimmy': 4462, 'robertson': 4463, 'angela': 4464, 'sydney': 4465, 'padua': 4466, 'demille': 4467, 'moeen': 4468, 'carnell': 4469, 'radice': 4470, 'jens': 4471, 'muller': 4472, 'remington': 4473, 'yvon': 4474, 'chouinard': 4475, 'hegarty': 4476, 'toshio': 4477, 'ban': 4478, 'gibbs': 4479, 'manish': 4480, 'chanchal': 4481, 'vivaksh': 4482, 'defalco': 4483, 'dwyer': 4484, 'tc': 4485, 'schelling': 4486, 'chiusano': 4487, 'minnie': 4488, 'vaid': 4489, 'philon': 4490, 'piramal': 4491, 'gita': 4492, 'ruchita': 4493, 'abnett': 4494, 'rehman': 4495, 'berkmann': 4496, 'ashwini': 4497, 'wadler': 4498, 'leonardo': 4499, 'faccio': 4500, 'sridhar': 4501, 'hislop': 4502, 'noakes': 4503, 'dupare': 4504, 'stearns': 4505, 'bercaw': 4506, 'yadugiri': 4507, 'neufert': 4508, 'diwakar': 4509, 'jaydip': 4510, 'andrie': 4511, 'vries': 4512, 'joris': 4513, 'meys': 4514, 'pavich': 4515, 'sathaye': 4516, 'malory': 4517, 'mcgee': 4518, 'aroon': 4519, 'mau': 4520, 'kun': 4521, 'yim': 4522, 'spence': 4523, 'kleon': 4524, 'bracken': 4525, 'masse': 4526, 'vaishnav': 4527, 'susskind': 4528, 'garvey': 4529, 'norling': 4530, 'gurpreet': 4531, 'avijit': 4532, 'gutowski': 4533, 'walsh': 4534, 'alender': 4535, 'gulu': 4536, 'murad': 4537, 'rios': 4538, 'sid': 4539, 'dell': 4540, 'pillsbury': 4541, 'iris': 4542, 'edith': 4543, 'wharton': 4544, 'giles': 4545, 'tillotson': 4546, 'andrea': 4547, 'portes': 4548, 'jeanne': 4549, 'boyarsky': 4550, 'horschig': 4551, 'avay': 4552, 'burton': 4553, 'tarbary': 4554, 'amitav': 4555, 'nugent': 4556, 'silman': 4557, 'fabbri': 4558, 'dane': 4559, 'huckelbridge': 4560, 'verstegen': 4561, 'iuliana': 4562, 'cosmina': 4563, 'harrop': 4564, 'schaefer': 4565, 'clarence': 4566, 'ho': 4567, 'nimmu': 4568, 'kyne': 4569, 'mohandas': 4570, 'divya': 4571, 'reeve': 4572, 'kalin': 4573, 'cel': 4574, 'welsh': 4575, 'bhupendra': 4576, 'ahluwalia': 4577, 'lily': 4578, 'leila': 4579, 'dashiell': 4580, 'hammett': 4581, 'corbusier': 4582, 'rosa': 4583, 'cannon': 4584, 'mautner': 4585, 'khatija': 4586, 'akbar': 4587, 'modern': 4588, 'gennick': 4589, 'sister': 4590, 'jesme': 4591, 'viki': 4592, 'gervais': 4593, 'giorgia': 4594, 'lupi': 4595, 'posavec': 4596, 'stefanie': 4597, 'kathryn': 4598, 'barnard': 4599, 'keiichi': 4600, 'arawi': 4601, 'foer': 4602, 'bolton': 4603, 'marguerite': 4604, 'duras': 4605, 'auroville': 4606, 'consulting': 4607, 'marks': 4608, 'ajahn': 4609, 'brahm': 4610}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_df['New_Author'] = s\n",
        "au_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nBY2DVYNJ2zx",
        "outputId": "4c23cb3a-ff78-4ee8-9034-16ccb04f840f"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Title           Author   Price  \\\n",
              "0         The Prisoner's Gold (The Hunters 3)   Chris Kuzneski  220.00   \n",
              "1          Guru Dutt: A Tragedy in Three Acts     Arun Khopkar  202.93   \n",
              "2                Leviathan (Penguin Classics)    Thomas Hobbes  299.00   \n",
              "3          A Pocket Full of Rye (Miss Marple)  Agatha Christie  180.00   \n",
              "4  LIFE 70 Years of Extraordinary Photography  Editors of Life  965.62   \n",
              "\n",
              "   New_Ratings  New_Reviews Year_of_publish  Format_(French),Paperback2010  \\\n",
              "0            8          4.0            2016                              0   \n",
              "1           14          3.9            2012                              0   \n",
              "2            6          4.8            1982                              0   \n",
              "3           13          4.1            2017                              0   \n",
              "4            1          5.0            2006                              0   \n",
              "\n",
              "   Format_(German),Paperback2014  Format_(Kannada),Paperback2014  \\\n",
              "0                              0                               0   \n",
              "1                              0                               0   \n",
              "2                              0                               0   \n",
              "3                              0                               0   \n",
              "4                              0                               0   \n",
              "\n",
              "   Format_Board book  ...  BookCategory_Comics & Mangas  \\\n",
              "0                  0  ...                             0   \n",
              "1                  0  ...                             0   \n",
              "2                  0  ...                             0   \n",
              "3                  0  ...                             0   \n",
              "4                  0  ...                             0   \n",
              "\n",
              "   BookCategory_Computing, Internet & Digital Media  \\\n",
              "0                                                 0   \n",
              "1                                                 0   \n",
              "2                                                 0   \n",
              "3                                                 0   \n",
              "4                                                 0   \n",
              "\n",
              "   BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "0                                       0                    0   \n",
              "1                                       0                    0   \n",
              "2                                       0                    1   \n",
              "3                                       1                    0   \n",
              "4                                       0                    0   \n",
              "\n",
              "   BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "0                                             0                      0   \n",
              "1                                             0                      0   \n",
              "2                                             0                      0   \n",
              "3                                             0                      0   \n",
              "4                                             0                      0   \n",
              "\n",
              "   BookCategory_Romance  BookCategory_Sports  \\\n",
              "0                     0                    0   \n",
              "1                     0                    0   \n",
              "2                     0                    0   \n",
              "3                     0                    0   \n",
              "4                     0                    0   \n",
              "\n",
              "                                  Synopsis_sequences        New_Author  \n",
              "0  [1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...         [59, 556]  \n",
              "1  [4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...       [248, 1814]  \n",
              "2  [400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...        [60, 1061]  \n",
              "3  [4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...          [12, 13]  \n",
              "4  [8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...  [557, 428, 1815]  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f088ea4f-6183-40e4-9dd2-ec818ee09ba0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Comics &amp; Mangas</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "      <th>Synopsis_sequences</th>\n",
              "      <th>New_Author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>Chris Kuzneski</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...</td>\n",
              "      <td>[59, 556]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>Arun Khopkar</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...</td>\n",
              "      <td>[248, 1814]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>Thomas Hobbes</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...</td>\n",
              "      <td>[60, 1061]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...</td>\n",
              "      <td>[12, 13]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>Editors of Life</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...</td>\n",
              "      <td>[557, 428, 1815]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f088ea4f-6183-40e4-9dd2-ec818ee09ba0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f088ea4f-6183-40e4-9dd2-ec818ee09ba0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f088ea4f-6183-40e4-9dd2-ec818ee09ba0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2c9e1b9-ace2-4207-ac02-4438c012c4f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2c9e1b9-ace2-4207-ac02-4438c012c4f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2c9e1b9-ace2-4207-ac02-4438c012c4f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "au_df = au_df.drop(['Author'],axis=1)"
      ],
      "metadata": {
        "id": "QEjpfxVNK7Ek"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "au_df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1q6gXpHVLUKS",
        "outputId": "f849fe17-6ca3-4e9f-ff41-85b165c2c1db"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  Title   Price  New_Ratings  \\\n",
              "5694  Who Ordered This Truckload of Dung?: Inspiring...  1009.0            9   \n",
              "5695              PostCapitalism: A Guide to Our Future   781.0            2   \n",
              "5696                             The Great Zoo Of China   449.0           28   \n",
              "5697                                            Engleby   108.0            1   \n",
              "5698  Only Dull People Are Brilliant at Breakfast (P...    99.0            7   \n",
              "\n",
              "      New_Reviews Year_of_publish  Format_(French),Paperback2010  \\\n",
              "5694          4.9            2005                              0   \n",
              "5695          4.1            2016                              0   \n",
              "5696          4.1            2016                              0   \n",
              "5697          1.0            2008                              0   \n",
              "5698          4.5            2016                              0   \n",
              "\n",
              "      Format_(German),Paperback2014  Format_(Kannada),Paperback2014  \\\n",
              "5694                              0                               0   \n",
              "5695                              0                               0   \n",
              "5696                              0                               0   \n",
              "5697                              0                               0   \n",
              "5698                              0                               0   \n",
              "\n",
              "      Format_Board book  Format_Cards  ...  BookCategory_Comics & Mangas  \\\n",
              "5694                  0             0  ...                             0   \n",
              "5695                  0             0  ...                             0   \n",
              "5696                  0             0  ...                             0   \n",
              "5697                  0             0  ...                             0   \n",
              "5698                  0             0  ...                             0   \n",
              "\n",
              "      BookCategory_Computing, Internet & Digital Media  \\\n",
              "5694                                                 0   \n",
              "5695                                                 0   \n",
              "5696                                                 0   \n",
              "5697                                                 0   \n",
              "5698                                                 0   \n",
              "\n",
              "      BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "5694                                       0                    1   \n",
              "5695                                       0                    0   \n",
              "5696                                       1                    0   \n",
              "5697                                       1                    0   \n",
              "5698                                       0                    1   \n",
              "\n",
              "      BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "5694                                             0                      0   \n",
              "5695                                             0                      1   \n",
              "5696                                             0                      0   \n",
              "5697                                             0                      0   \n",
              "5698                                             0                      0   \n",
              "\n",
              "      BookCategory_Romance  BookCategory_Sports  \\\n",
              "5694                     0                    0   \n",
              "5695                     0                    0   \n",
              "5696                     0                    0   \n",
              "5697                     0                    0   \n",
              "5698                     0                    0   \n",
              "\n",
              "                                     Synopsis_sequences    New_Author  \n",
              "5694  [42, 81, 5, 9, 14, 1596, 2, 440, 4843, 231, 3,...  [4609, 4610]  \n",
              "5695  [532, 41, 216, 16, 40, 74, 988, 2, 404, 5, 34,...    [30, 1279]  \n",
              "5696  [1, 1300, 563, 23, 51, 1837, 4, 207, 8, 2208, ...     [74, 144]  \n",
              "5697  [3278, 23, 4, 207, 14, 7, 1, 52, 3, 3278, 4, 4...   [973, 1494]  \n",
              "5698  [6344, 143, 34, 5, 2792, 80, 92, 5, 34, 12, 68...  [1790, 1791]  \n",
              "\n",
              "[5 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-929556a6-b4dd-4514-9a11-31708c6985a4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Comics &amp; Mangas</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "      <th>Synopsis_sequences</th>\n",
              "      <th>New_Author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>Who Ordered This Truckload of Dung?: Inspiring...</td>\n",
              "      <td>1009.0</td>\n",
              "      <td>9</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[42, 81, 5, 9, 14, 1596, 2, 440, 4843, 231, 3,...</td>\n",
              "      <td>[4609, 4610]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>PostCapitalism: A Guide to Our Future</td>\n",
              "      <td>781.0</td>\n",
              "      <td>2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[532, 41, 216, 16, 40, 74, 988, 2, 404, 5, 34,...</td>\n",
              "      <td>[30, 1279]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>The Great Zoo Of China</td>\n",
              "      <td>449.0</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 1300, 563, 23, 51, 1837, 4, 207, 8, 2208, ...</td>\n",
              "      <td>[74, 144]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>Engleby</td>\n",
              "      <td>108.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[3278, 23, 4, 207, 14, 7, 1, 52, 3, 3278, 4, 4...</td>\n",
              "      <td>[973, 1494]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>Only Dull People Are Brilliant at Breakfast (P...</td>\n",
              "      <td>99.0</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[6344, 143, 34, 5, 2792, 80, 92, 5, 34, 12, 68...</td>\n",
              "      <td>[1790, 1791]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-929556a6-b4dd-4514-9a11-31708c6985a4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-929556a6-b4dd-4514-9a11-31708c6985a4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-929556a6-b4dd-4514-9a11-31708c6985a4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5dbb9f28-7c77-4bef-bcea-2e661e0d7717\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5dbb9f28-7c77-4bef-bcea-2e661e0d7717')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5dbb9f28-7c77-4bef-bcea-2e661e0d7717 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Title**"
      ],
      "metadata": {
        "id": "RI1How9CLa1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tl_df = au_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "uZWiqZqiLWV7"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s,w = embedding(tl_df,'Title')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xB1XuIELZDl",
        "outputId": "25dddb8e-77a9-451a-a35d-00c69e4105d8"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7512 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVWhKh7DLfCq",
        "outputId": "947190bf-e0b6-4d4d-ce8f-c1bdc568b005"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'of': 2, 'and': 3, 'a': 4, 'to': 5, 'in': 6, 'for': 7, 'book': 8, '1': 9, 'with': 10, 'guide': 11, 'edition': 12, 'from': 13, 'english': 14, 'how': 15, 'classics': 16, 'series': 17, '2': 18, 'on': 19, 'an': 20, 'my': 21, 'vol': 22, 'story': 23, '3': 24, 'world': 25, 'life': 26, 'you': 27, 'india': 28, 'penguin': 29, 'novel': 30, 'one': 31, 'man': 32, 'new': 33, 'complete': 34, 'your': 35, 'i': 36, 'love': 37, 'art': 38, 'dictionary': 39, 'graphic': 40, 'is': 41, 'adventures': 42, 'oxford': 43, 'indian': 44, 'modern': 45, 'it': 46, 'by': 47, 'stories': 48, '4': 49, 'at': 50, 'all': 51, 'history': 52, 'me': 53, 'data': 54, 'learning': 55, 'grammar': 56, 'asterix': 57, 'no': 58, 'secret': 59, 'volume': 60, 'that': 61, 'big': 62, 'great': 63, 'course': 64, 'cd': 65, 'books': 66, 'vintage': 67, '5': 68, 'what': 69, 'novels': 70, 'step': 71, 'design': 72, 'practice': 73, 'autobiography': 74, 'first': 75, 'about': 76, 'who': 77, 'easy': 78, 'war': 79, 'cambridge': 80, 'girl': 81, 'way': 82, 'other': 83, 'programming': 84, 'trilogy': 85, 'death': 86, 'science': 87, 'language': 88, 'words': 89, '–': 90, 'game': 91, 'learn': 92, 'fire': 93, 'tintin': 94, 'library': 95, 'time': 96, '2018': 97, 'black': 98, 'level': 99, 'last': 100, 'c': 101, 'grade': 102, 'through': 103, 'mind': 104, '10': 105, 'little': 106, 'reference': 107, '6': 108, 'exam': 109, 'day': 110, 'journey': 111, 'be': 112, 'best': 113, 'lost': 114, 'up': 115, 'piano': 116, 'not': 117, 'our': 118, 'made': 119, 'magic': 120, 'business': 121, 'true': 122, 'power': 123, 'key': 124, 'yourself': 125, 'short': 126, 'night': 127, 'calvin': 128, 'hobbes': 129, '7': 130, 'red': 131, 'most': 132, 'secrets': 133, 'comics': 134, 'will': 135, 'things': 136, 'international': 137, 'theory': 138, 'we': 139, 'biography': 140, 'know': 141, 'music': 142, 'classic': 143, 'trinity': 144, 'jack': 145, 'inside': 146, 'introduction': 147, 'python': 148, 'drawing': 149, 'collection': 150, 'read': 151, 'omnibus': 152, 'questions': 153, 'training': 154, 'college': 155, 'dark': 156, 'cricket': 157, 'women': 158, 'colouring': 159, 'java': 160, 'essential': 161, 'computer': 162, 'album': 163, 'digital': 164, 'cat': 165, '2019': 166, 'house': 167, 'ultimate': 168, 'are': 169, '8': 170, 'set': 171, 'greatest': 172, 'ielts': 173, 'times': 174, 'out': 175, 'everything': 176, 'ice': 177, 'dragon': 178, 'test': 179, 'why': 180, 'writing': 181, 'good': 182, 'creative': 183, 'years': 184, 'more': 185, 'using': 186, 'making': 187, 'batman': 188, 'days': 189, \"india's\": 190, 'like': 191, 'collins': 192, 'answers': 193, 'as': 194, 'guitar': 195, 'poirot': 196, \"world's\": 197, 'film': 198, 'official': 199, 'illustrated': 200, 'london': 201, 'machine': 202, 'tales': 203, 'visual': 204, 'men': 205, 'study': 206, 'pack': 207, 'ideas': 208, 'high': 209, 'never': 210, 'three': 211, 'dover': 212, 'bestseller': 213, 'french': 214, 'body': 215, 'part': 216, 'do': 217, 'harry': 218, 'into': 219, 'truth': 220, 'practical': 221, 'games': 222, 'campfire': 223, 'techniques': 224, 'self': 225, 'end': 226, 'must': 227, 's': 228, 'blue': 229, 'this': 230, 'people': 231, 'revised': 232, 'song': 233, 'success': 234, 'thriller': 235, 'century': 236, 'r': 237, 'us': 238, 'quick': 239, 'photography': 240, 'lessons': 241, 'year': 242, 'work': 243, 'readers': 244, 'human': 245, 'stilton': 246, 'can': 247, 'second': 248, 'teach': 249, 'reads': 250, 'pieces': 251, 'ladybird': 252, 'blood': 253, 'x': 254, 'city': 255, 'rom': 256, 'basic': 257, 'dog': 258, 'tell': 259, 'political': 260, 'or': 261, 'adventure': 262, 'skills': 263, 'archie': 264, 'works': 265, 'james': 266, '11': 267, 'anniversary': 268, 'sherlock': 269, 'beyond': 270, 'dirk': 271, 'pitt': 272, 'murder': 273, 'private': 274, 'structures': 275, 'south': 276, 'lives': 277, 'beautiful': 278, 'just': 279, 'am': 280, 'architecture': 281, 'chess': 282, 'write': 283, 'right': 284, 'interview': 285, 'building': 286, 'go': 287, 'jeeves': 288, 'approach': 289, 'yoga': 290, 'king': 291, 'economy': 292, '30': 293, \"don't\": 294, 'mastering': 295, 'without': 296, '50': 297, 'alex': 298, 'over': 299, 'principles': 300, 'better': 301, 'rock': 302, 'mystery': 303, 'anatomy': 304, 'simple': 305, 'national': 306, 'performance': 307, 'development': 308, 'his': 309, 'when': 310, 'advanced': 311, 'play': 312, 'hindi': 313, 'fat': 314, 'dk': 315, '100': 316, 'brain': 317, 'workbook': 318, 'phonics': 319, 'age': 320, 'essentials': 321, 'their': 322, '15': 323, 'audio': 324, 'word': 325, 'cross': 326, 'pocket': 327, 'home': 328, 'dead': 329, 'handbook': 330, 'manga': 331, '2020': 332, 'kids': 333, 'systems': 334, 'club': 335, 'real': 336, 'applications': 337, 'perfect': 338, 'heart': 339, 'vocabulary': 340, 'tale': 341, 'if': 342, 'after': 343, 'five': 344, 'holmes': 345, 'build': 346, 'includes': 347, 'ball': 348, 'running': 349, 'behind': 350, 'saga': 351, 'heroes': 352, 'introducing': 353, 'chronicles': 354, 'german': 355, 'box': 356, 'files': 357, 'politics': 358, 'creed': 359, 'future': 360, 'family': 361, 'solutions': 362, 'wodehouse': 363, 'robert': 364, 'pop': 365, 'own': 366, 'tinkle': 367, 'earth': 368, 'living': 369, 'web': 370, 'get': 371, 'shadow': 372, 'sea': 373, 'dream': 374, 'search': 375, 'shopaholic': 376, 'speaking': 377, 'composition': 378, 'bond': 379, 'every': 380, 'kingdom': 381, 'superman': 382, 'naruto': 383, 'need': 384, 'global': 385, 'six': 386, 'p': 387, 'make': 388, 'now': 389, 'only': 390, 'letters': 391, 'memoir': 392, 'piece': 393, 'have': 394, 'essays': 395, 'understanding': 396, 'star': 397, 'school': 398, 'third': 399, 'software': 400, 'tiger': 401, 'hero': 402, 'introductions': 403, 'four': 404, 'survival': 405, 'general': 406, 'potter': 407, 'reacher': 408, 'run': 409, 'vols': 410, 'editions': 411, 'planet': 412, 'deep': 413, 'exercises': 414, 'revolution': 415, 'two': 416, \"assassin's\": 417, \"everyman's\": 418, 'note': 419, 'being': 420, 'letter': 421, 'case': 422, 'american': 423, 'side': 424, 'color': 425, 'young': 426, 'tests': 427, 'patterns': 428, 'gods': 429, 'lies': 430, 'prime': 431, 'seven': 432, 'algorithms': 433, 'fall': 434, 'democracy': 435, 'deluxe': 436, 'origami': 437, 'let': 438, 'method': 439, 'see': 440, 'sunday': 441, '12': 442, 'field': 443, 'hidden': 444, 'author': 445, 'marvel': 446, 'class': 447, 'geronimo': 448, 'crime': 449, 'intelligence': 450, 'miss': 451, 'super': 452, 'tools': 453, 'updated': 454, 'dreams': 455, 'rising': 456, 'review': 457, 'thesaurus': 458, 'creator': 459, 'football': 460, 'g': 461, 'sun': 462, 'fifty': 463, 'definitive': 464, 'code': 465, 'speak': 466, 'vs': 467, 'top': 468, '14': 469, 'here': 470, 'white': 471, 'security': 472, 'wars': 473, 'woman': 474, 'india’s': 475, 'lord': 476, 'engineering': 477, 'forever': 478, 'sky': 479, 'her': 480, 'thrones': 481, 'number': 482, '2nd': 483, 'bourne': 484, 'japanese': 485, 'win': 486, 'analysis': 487, 'draw': 488, \"you've\": 489, 'leadership': 490, 'beginner': 491, 'spy': 492, 'gold': 493, 'companion': 494, 'technology': 495, 'light': 496, 'old': 497, \"hbr's\": 498, 'personal': 499, 'half': 500, '2016': 501, 'untold': 502, 'shades': 503, 'court': 504, 'master': 505, 'change': 506, 'freedom': 507, 'around': 508, 'inspiration': 509, 'fear': 510, 'diary': 511, 'golden': 512, 'force': 513, 'knight': 514, 'fun': 515, 'wisdom': 516, 'strange': 517, 'bride': 518, 'garfield': 519, 'paper': 520, 'geographic': 521, 'photographs': 522, 'loved': 523, 'empire': 524, 'fundamentals': 525, 'cartoon': 526, 'law': 527, 'elements': 528, 'public': 529, 'manual': 530, 'strategies': 531, 'double': 532, 'she': 533, 'eye': 534, 'intermediate': 535, 'statistics': 536, 'spring': 537, 'javascript': 538, 'extraordinary': 539, 'iron': 540, 'asian': 541, 'dc': 542, 'philosophy': 543, 'sap': 544, 'harvard': 545, 'head': 546, '52': 547, 'mountain': 548, 'technical': 549, 'road': 550, 'boys': 551, 'coloring': 552, 'v': 553, 'becoming': 554, 'bad': 555, 'hands': 556, 'culture': 557, 'wonder': 558, 'memory': 559, 'sword': 560, 'online': 561, '9': 562, 'winning': 563, 'lesson': 564, 'hills': 565, 'legends': 566, 'race': 567, 'higher': 568, 'quartet': 569, 'dance': 570, 'fiction': 571, 'its': 572, '20': 573, 'information': 574, 'before': 575, 'sql': 576, 'ready': 577, 'spider': 578, 'start': 579, 'stars': 580, 'd': 581, 'strength': 582, 'spoken': 583, 'small': 584, 'creating': 585, 'detective': 586, 'very': 587, \"student's\": 588, 'press': 589, 'style': 590, 'tips': 591, 'project': 592, 'they': 593, 'where': 594, 'stage': 595, 'digest': 596, 'core': 597, 'john': 598, 'kings': 599, 'ghost': 600, \"beginner's\": 601, 'rise': 602, 'any': 603, 'action': 604, 'microsoft': 605, 'affair': 606, 'papers': 607, 'nation': 608, 'tie': 609, 'steps': 610, 'internet': 611, 'kill': 612, 'stranger': 613, 'reading': 614, 'diaries': 615, 'amazon': 616, 'adult': 617, 'wild': 618, 'born': 619, 'steve': 620, 'thoughts': 621, \"you'll\": 622, 'job': 623, 'friends': 624, 'invisible': 625, 'broken': 626, 'memoirs': 627, 'thea': 628, 'professional': 629, 'something': 630, 'absolute': 631, 'thinking': 632, \"i'm\": 633, 'moon': 634, 'changed': 635, 'universe': 636, 'sports': 637, 'foreign': 638, 'policy': 639, 'fourth': 640, 'between': 641, 'princess': 642, '13': 643, 'sport': 644, 'woods': 645, 'back': 646, 'identity': 647, 'solving': 648, 'career': 649, 'league': 650, 'ages': 651, 'island': 652, 'igcse': 653, 'wooster': 654, 'cards': 655, 'beginners': 656, 'china': 657, 'pokemon': 658, 'portrait': 659, 'confessions': 660, 'academia': 661, 'f': 662, 'matter': 663, 'marple': 664, 'free': 665, 'health': 666, 'classical': 667, 'keyboard': 668, 'selected': 669, '0': 670, 'sheets': 671, 'certified': 672, 'attack': 673, 'bible': 674, 'langdon': 675, 'away': 676, 'masters': 677, '22': 678, 'adults': 679, 'wedding': 680, 'party': 681, 'daughter': 682, 'thrilling': 683, 'contemporary': 684, 'quest': 685, 'zen': 686, 'sigma': 687, 'se': 688, \"alfred's\": 689, 'captain': 690, 'underpants': 691, 'punch': 692, 'bruce': 693, 'creativity': 694, 'ember': 695, 'improvement': 696, 'nine': 697, 'winner': 698, 'orphan': 699, 'country': 700, 'projects': 701, 'silence': 702, 'boy': 703, 'numa': 704, '21st': 705, 'literature': 706, 'genius': 707, 'girls': 708, 'god': 709, 'avengers': 710, 'powerful': 711, 'ever': 712, 'system': 713, 'finding': 714, 'struggle': 715, 'legacy': 716, 'economics': 717, 'bestselling': 718, 'physics': 719, 'kiss': 720, 'justice': 721, 'examination': 722, 'problem': 723, 'demon': 724, 'cormoran': 725, 'strike': 726, 'lonely': 727, '18': 728, 'singh': 729, 'guides': 730, 'want': 731, 'dr': 732, '007': 733, 'mr': 734, 'was': 735, 'next': 736, 'concepts': 737, 'google': 738, 'screenwriting': 739, 'help': 740, 'psychology': 741, 'mysteries': 742, 'writers': 743, 'bilingual': 744, '21': 745, 'common': 746, 'jungle': 747, 'think': 748, 'ancient': 749, 'mini': 750, 'birds': 751, 'america': 752, 'water': 753, 'alive': 754, 'hit': 755, 'prince': 756, 'smart': 757, 'treasure': 758, 'room': 759, 'plays': 760, 'calligraphy': 761, 'independence': 762, 'under': 763, 'still': 764, 'urdu': 765, 'blake': 766, 'mortimer': 767, 'communication': 768, 'featuring': 769, 'romance': 770, 'excel': 771, 'billion': 772, 'brand': 773, 'ryan': 774, 'everyone': 775, 'against': 776, 'doing': 777, 'find': 778, 'there': 779, 'emperor': 780, 'dawn': 781, 'prize': 782, 'original': 783, 'special': 784, 'comic': 785, 'movie': 786, 'insights': 787, 'minister': 788, 'another': 789, 'use': 790, 'lincoln': 791, 'silent': 792, 'fight': 793, 'gandhi': 794, 'minutes': 795, 'than': 796, 'were': 797, 'childhood': 798, 'rebus': 799, 'going': 800, 'started': 801, 'chamber': 802, 'fitness': 803, 'characters': 804, 'dangerous': 805, 'live': 806, 'too': 807, 'west': 808, 'die': 809, 'comedy': 810, 'so': 811, 'stop': 812, 'social': 813, 'animals': 814, 'medicine': 815, 'laugh': 816, 'analytics': 817, 'amma': 818, 'fantastic': 819, 'impossible': 820, 'he': 821, 'nate': 822, 'getting': 823, 'ronaldo': 824, '19': 825, 'thing': 826, 'happened': 827, 'methods': 828, 'doctor': 829, 'artificial': 830, 'plans': 831, 'kit': 832, 'name': 833, 'script': 834, 'effective': 835, 'education': 836, 'full': 837, 'changing': 838, 'messi': 839, 'luca': 840, 'caioli': 841, 'dummies': 842, 'memories': 843, 'hannibal': 844, 'translation': 845, 'emergency': 846, 'told': 847, 'spanish': 848, 'many': 849, 'expert': 850, 'academic': 851, 'foundations': 852, 'brothers': 853, 'knowledge': 854, 'richard': 855, 'pick': 856, 'long': 857, 'picture': 858, 'talk': 859, 'soul': 860, 'east': 861, 'york': 862, 'myths': 863, 'throne': 864, 'jobs': 865, 'tricks': 866, 'lee': 867, 'daily': 868, 'hours': 869, 'everest': 870, 'ashes': 871, 'summer': 872, 'sisters': 873, 'child': 874, 'asked': 875, 'lorien': 876, 'legacies': 877, 'gre': 878, 'understand': 879, 'different': 880, 'model': 881, 'choice': 882, 'student': 883, 'mountains': 884, 'application': 885, 'cracking': 886, 'team': 887, 'tokyo': 888, 'puzzles': 889, 'stone': 890, 'always': 891, 'angels': 892, 'final': 893, 'gates': 894, 'manifesto': 895, 'shakespeare': 896, 'loves': 897, 'everybody': 898, 'thousand': 899, 'minds': 900, 'myth': 901, 'endurance': 902, 'brief': 903, 'oracle': 904, 'killer': 905, 'control': 906, 'eyes': 907, 'krishna': 908, 'amar': 909, 'chitra': 910, 'katha': 911, 'support': 912, 'everyday': 913, 'storm': 914, 'legendary': 915, 'bone': 916, 'create': 917, 'examinations': 918, 'ii': 919, 'happiness': 920, 'coding': 921, 'cycle': 922, 'dare': 923, 'hunt': 924, 'quantum': 925, 'snow': 926, 'street': 927, 'been': 928, 'preparation': 929, 'incredible': 930, 'speed': 931, 'pokémon': 932, 'lie': 933, 'cookbook': 934, 'lettering': 935, 'competitive': 936, 'colour': 937, 'tattoo': 938, 'playing': 939, 'tm': 940, 'manning': 941, 'shot': 942, 'sense': 943, 'zone': 944, 'page': 945, 'miniature': 946, 'grey': 947, 'workbooks': 948, 'muscle': 949, 'electronic': 950, 'z': 951, 'hell': 952, 'zoo': 953, 'media': 954, 'warrior': 955, 'scarecrow': 956, 'html': 957, 'himalayas': 958, 'revolutionary': 959, 'recipes': 960, 'e': 961, 'sanskrit': 962, 'sacred': 963, 'blockchain': 964, 'kung': 965, 'martial': 966, 'really': 967, 'reflections': 968, 'conspiracy': 969, \"learner's\": 970, 'ocean': 971, 'abrsm': 972, 'mindfulness': 973, 'line': 974, 'relations': 975, 'calm': 976, 'growth': 977, 'well': 978, 'alone': 979, '60': 980, 'aws': 981, 'exams': 982, '220': 983, 'darker': 984, '125': 985, 'travel': 986, 'awakening': 987, '200': 988, 'lady': 989, 'natural': 990, 'judy': 991, 'stay': 992, '3rd': 993, 'bantam': 994, 'process': 995, 'towards': 996, 'sin': 997, 'improve': 998, 'happy': 999, 'once': 1000, 'tom': 1001, 'bosch': 1002, 'goddess': 1003, 'kingsbridge': 1004, 'glass': 1005, 'collected': 1006, 'beauty': 1007, 'computing': 1008, 'river': 1009, 'bones': 1010, 'conqueror': 1011, 'likely': 1012, 'quiz': 1013, 'anti': 1014, \"america's\": 1015, '2007': 1016, 'hunter': 1017, 'structure': 1018, 'landscapes': 1019, 'academy': 1020, 'mrs': 1021, 'successful': 1022, 'expanded': 1023, 'left': 1024, 'ghoul': 1025, 'facts': 1026, 'tamil': 1027, 'strangers': 1028, 'crash': 1029, 'workout': 1030, \"sarah's\": 1031, '25': 1032, 'card': 1033, 'technique': 1034, 'point': 1035, 'bose': 1036, 'inspirational': 1037, 'gone': 1038, '101': 1039, 'cartoons': 1040, 'marketing': 1041, 'down': 1042, 'studies': 1043, 'masterpieces': 1044, 'president': 1045, 'bridge': 1046, 'computers': 1047, 'atlantis': 1048, 'royal': 1049, 'famous': 1050, 'hanuman': 1051, 'cloud': 1052, 'notes': 1053, 'mumbai': 1054, 'billionaire': 1055, 'primary': 1056, 'instruction': 1057, 'children': 1058, 'hope': 1059, 'infinity': 1060, 'kid': 1061, 'character': 1062, '1000': 1063, 'twenty': 1064, 'fast': 1065, 'become': 1066, 'midnight': 1067, 'problems': 1068, 'peak': 1069, 'multiple': 1070, 'films': 1071, 'ten': 1072, 'score': 1073, 'raj': 1074, 'walking': 1075, \"i've\": 1076, 'nothing': 1077, '10th': 1078, 'faber': 1079, '2017': 1080, 'khan': 1081, 'screenplay': 1082, 'usage': 1083, 'crisis': 1084, 'junior': 1085, 'extreme': 1086, 'state': 1087, 'parts': 1088, 'tarzan': 1089, 'russ': 1090, 'newspaper': 1091, 'strips': 1092, 'feluda': 1093, 'verbs': 1094, 'army': 1095, 'hal': 1096, 'facebook': 1097, 'sidney': 1098, 'writings': 1099, 'sticker': 1100, 'initial': 1101, 'strategy': 1102, 'meet': 1103, 'daughters': 1104, 'matters': 1105, '2022': 1106, 'leader': 1107, 'object': 1108, 'dvd': 1109, 'tv': 1110, 'revenge': 1111, 'world’s': 1112, 'jolly': 1113, 'css': 1114, 'islam': 1115, 'paradise': 1116, 'fargo': 1117, 'cinematography': 1118, 'quotient': 1119, 'epic': 1120, 'look': 1121, \"thompson's\": 1122, 'inspiring': 1123, 'soldier': 1124, 'wife': 1125, 'painting': 1126, 'tragedy': 1127, '70': 1128, 'battle': 1129, 'defense': 1130, 'arts': 1131, 'nations': 1132, 'buddha': 1133, 'bear': 1134, 'called': 1135, 'bird': 1136, 'tin': 1137, 'drum': 1138, 'shooting': 1139, 'target': 1140, 'built': 1141, 'limits': 1142, 'associate': 1143, 'tim': 1144, 'administration': 1145, 'coursebook': 1146, 'remember': 1147, 'evil': 1148, 'certification': 1149, 'hindu': 1150, 'david': 1151, 'ways': 1152, 'runners': 1153, 'laws': 1154, 'economic': 1155, 'valley': 1156, 'mandala': 1157, 'ccna': 1158, 'deadly': 1159, 'serial': 1160, 'hat': 1161, 'uncle': 1162, 'historical': 1163, 'sustainable': 1164, 'father': 1165, 'emotional': 1166, 'progress': 1167, 'brides': 1168, 'faith': 1169, 'popular': 1170, 'learners': 1171, '02': 1172, 'enemy': 1173, 'gate': 1174, \"lee's\": 1175, '1e': 1176, \"artist's\": 1177, 'touch': 1178, 'objective': 1179, 'lines': 1180, 'dragons': 1181, 'pro': 1182, 'supreme': 1183, '90': 1184, 'believe': 1185, 'trump': 1186, 'wins': 1187, 'simply': 1188, 'hearts': 1189, 'green': 1190, 'mistress': 1191, 'instructions': 1192, '105': 1193, 'open': 1194, 'edge': 1195, 'advice': 1196, 'whole': 1197, 'chaos': 1198, 'developing': 1199, 'everywhere': 1200, 'sacketts': 1201, 'cry': 1202, 'yes': 1203, 'cities': 1204, 'crossing': 1205, 'spirit': 1206, 'group': 1207, 'inventors': 1208, 'created': 1209, 'synonyms': 1210, 'antonyms': 1211, 'tower': 1212, 'crows': 1213, 'desert': 1214, \"it's\": 1215, '01': 1216, 'cinema': 1217, 'designing': 1218, \"marvel's\": 1219, 'booker': 1220, 'jackson': 1221, 'teacher': 1222, 'reissue': 1223, 'save': 1224, 'brilliant': 1225, 'scientists': 1226, 'clothbound': 1227, 'nehru': 1228, 'scribbles': 1229, 'wheel': 1230, 'kane': 1231, 'baby': 1232, 'literary': 1233, 'face': 1234, 'computation': 1235, 'hate': 1236, 'thief': 1237, 'advertising': 1238, 'battles': 1239, 'olympic': 1240, 'kashmir': 1241, 'missing': 1242, 'text': 1243, 'place': 1244, 'oca': 1245, 'networks': 1246, 'tomorrow': 1247, 'mother': 1248, 'shadows': 1249, 'lifetime': 1250, 'smoke': 1251, 'tear': 1252, 'riddles': 1253, 'son': 1254, 'copy': 1255, 'boxed': 1256, 'animal': 1257, 'madness': 1258, 'crown': 1259, 'wings': 1260, 'presents': 1261, 'network': 1262, 'wordsworth': 1263, 'devil': 1264, 'funny': 1265, 'designs': 1266, 'watercolor': 1267, 'encyclopedia': 1268, 'land': 1269, 'makers': 1270, 'burning': 1271, 'far': 1272, 'animation': 1273, \"roget's\": 1274, 'revelations': 1275, 'bbc': 1276, 'iconic': 1277, 'generation': 1278, 'healing': 1279, 'working': 1280, 'strong': 1281, 'western': 1282, 'errors': 1283, 'became': 1284, 'dollar': 1285, 'superheroes': 1286, 'bryson': 1287, 'cyber': 1288, 'curious': 1289, 'builder': 1290, 'cds': 1291, 'african': 1292, 'artist': 1293, 'prep': 1294, 'pencils': 1295, 'calling': 1296, 'manuscript': 1297, 'odd': 1298, 'someone': 1299, 'takes': 1300, 'unexpected': 1301, 'rich': 1302, 'selection': 1303, 'chemistry': 1304, 'lose': 1305, 'stand': 1306, 'chinese': 1307, 'cup': 1308, 'bell': 1309, '17': 1310, 'modesty': 1311, 'blaise': 1312, 'math': 1313, 'substance': 1314, 'phrasebook': 1315, 'mental': 1316, 'glory': 1317, 'lean': 1318, 'oriented': 1319, 'heaven': 1320, 'handwriting': 1321, 'cube': 1322, 'maps': 1323, 'faster': 1324, 'mad': 1325, 'ring': 1326, 'rosie': 1327, 'trouble': 1328, 'modi': 1329, 'caste': 1330, 'united': 1331, 'collectible': 1332, 'php': 1333, 'romantic': 1334, 'yours': 1335, 'zero': 1336, 'acting': 1337, 'adrian': 1338, 'mole': 1339, 'looking': 1340, 'haven': 1341, 'concise': 1342, '2ed': 1343, 'train': 1344, 'critical': 1345, 'bleach': 1346, 'textbook': 1347, 'present': 1348, 'designers': 1349, 'witcher': 1350, 'sat': 1351, 'flowers': 1352, 'himalaya': 1353, 'michael': 1354, 'cars': 1355, 'avatar': 1356, 'airbender': 1357, 'stepping': 1358, 'sex': 1359, 'promise': 1360, 'hall': 1361, 'mit': 1362, 'watch': 1363, 'effect': 1364, 'stress': 1365, 'gifts': 1366, 'total': 1367, 'beasts': 1368, 'match': 1369, 'ramayana': 1370, 'murders': 1371, 'materials': 1372, 'mcqs': 1373, 'minute': 1374, 'pakistan': 1375, '4th': 1376, 'meditation': 1377, 'fu': 1378, 'wwe': 1379, 'robie': 1380, 'decisions': 1381, 'jones': 1382, 'poetry': 1383, 'vba': 1384, 'dictionaries': 1385, 'de': 1386, 'drones': 1387, 'guns': 1388, 'portable': 1389, 'wealth': 1390, 'logic': 1391, 'wall': 1392, 'soft': 1393, 'architect': 1394, 'coming': 1395, 'pearson': 1396, 'william': 1397, 'inferno': 1398, 'eleven': 1399, 'flash': 1400, 'crazy': 1401, 'encounters': 1402, 'silicon': 1403, 'himalayan': 1404, 'park': 1405, 'switching': 1406, 'dozen': 1407, 'hackers': 1408, 'engineers': 1409, 'married': 1410, 'karamazov': 1411, 'nutshell': 1412, 'walk': 1413, 'reason': 1414, 'automation': 1415, 'sounds': 1416, 'talking': 1417, 'outlander': 1418, 'urban': 1419, \"clancy's\": 1420, 'folktales': 1421, 'value': 1422, 'brush': 1423, 'paintings': 1424, 'non': 1425, 'paperback': 1426, 'season': 1427, 'north': 1428, 'fashion': 1429, 'hour': 1430, 'brighter': 1431, 'conversation': 1432, 'previous': 1433, 'since': 1434, 'conquer': 1435, 'climbing': 1436, 'highest': 1437, 'idioms': 1438, 'playbook': 1439, 'view': 1440, 'elephants': 1441, 'dust': 1442, 'less': 1443, 'annuals': 1444, 'comprehensive': 1445, 'texts': 1446, 'explained': 1447, 'saved': 1448, 'functional': 1449, 'honour': 1450, 'yorkshire': 1451, 'vet': 1452, 'herriot': 1453, 'included': 1454, 'voices': 1455, 'partition': 1456, 'them': 1457, 'lion': 1458, 'thrawn': 1459, 'tennis': 1460, 'eagles': 1461, 'tricolore': 1462, 'innovators': 1463, 'geniuses': 1464, 'abraham': 1465, \"there's\": 1466, 'servant': 1467, 'hand': 1468, \"collector's\": 1469, 'corner': 1470, 'jane': 1471, 'teaches': 1472, 'u': 1473, 'killing': 1474, 'joke': 1475, 'exploring': 1476, '25th': 1477, 'percy': 1478, 'einstein': 1479, 'mistakes': 1480, 'wizard': 1481, 'tesla': 1482, 'giant': 1483, 'digests': 1484, 'indira': 1485, 'eleventh': 1486, 'annihilation': 1487, 'mortal': 1488, 'rider': 1489, 'dot': 1490, 'chandra': 1491, '1947': 1492, 'jokes': 1493, 'picador': 1494, 'islamic': 1495, 'does': 1496, 'reader': 1497, 'sonic': 1498, 'almost': 1499, 'content': 1500, 'shantaram': 1501, 'tree': 1502, 'perspective': 1503, 'mining': 1504, 'until': 1505, 'students': 1506, 'indians': 1507, 'neural': 1508, 'award': 1509, 'jason': 1510, 'wrong': 1511, 'sensation': 1512, 'take': 1513, 'makes': 1514, 'champion': 1515, \"can't\": 1516, 'keep': 1517, 'legend': 1518, 'production': 1519, 'amazing': 1520, 'spelling': 1521, 'has': 1522, 'tides': 1523, 'scholastic': 1524, 'rapidex': 1525, 'telugu': 1526, 'places': 1527, 'odyssey': 1528, 'activities': 1529, 'rhyme': 1530, 'pupil': 1531, 'much': 1532, 'come': 1533, 'british': 1534, 'boost': 1535, 'split': 1536, 'swords': 1537, 'spell': 1538, 'disaster': 1539, 'flying': 1540, 'wilt': 1541, 'marriage': 1542, 'reality': 1543, 'artists': 1544, 'say': 1545, 'publishing': 1546, 'sell': 1547, 'revision': 1548, 'cut': 1549, 'dying': 1550, 'programmer': 1551, 'goes': 1552, '1984': 1553, 'unstoppable': 1554, 'hardbound': 1555, 'delux': 1556, 'tradition': 1557, 'topsy': 1558, 'conquest': 1559, 'please': 1560, 'supremacy': 1561, 'joona': 1562, 'linna': 1563, 'mammoth': 1564, 'dick': 1565, 'front': 1566, 'checkpoint': 1567, 'holy': 1568, 'm': 1569, 'based': 1570, 'co': 1571, 'geography': 1572, 'spectacular': 1573, 'lake': 1574, 'ted': 1575, 'got': 1576, 'boruto': 1577, 'question': 1578, 'net': 1579, 'celebrating': 1580, 'female': 1581, 'hotel': 1582, '16': 1583, 'management': 1584, 'baghdad': 1585, 'then': 1586, 'rusty': 1587, 'months': 1588, 'lawyer': 1589, 'major': 1590, 'motion': 1591, 'thomas': 1592, 'twisted': 1593, 'fish': 1594, 'conversations': 1595, 'infinite': 1596, 'society': 1597, 'inspector': 1598, 'poker': 1599, 'pros': 1600, 'player': 1601, 'tournament': 1602, 'patient': 1603, 'humble': 1604, 'obelix': 1605, 'strain': 1606, 'leaders': 1607, 'turn': 1608, 'order': 1609, '1969': 1610, '2015': 1611, 'sachin': 1612, 'birth': 1613, 'loss': 1614, 'depth': 1615, 'leonard': 1616, 'ends': 1617, 'wise': 1618, 'energy': 1619, 'grace': 1620, 'arjuna': 1621, 'hinduism': 1622, 'but': 1623, \"sheldon's\": 1624, 'castle': 1625, 'figure': 1626, 'laughter': 1627, 'movement': 1628, 'rocks': 1629, 'week': 1630, 'flies': 1631, 'premium': 1632, 'lead': 1633, '24': 1634, 'eat': 1635, \"billionaire's\": 1636, 'roy': 1637, 'lang': 1638, 'leav': 1639, 'meditations': 1640, 'journalism': 1641, 'transform': 1642, 'di': 1643, 'lane': 1644, 'egypt': 1645, 'lightning': 1646, 'table': 1647, 'screen': 1648, 'assassin': 1649, 'hill': 1650, 'machines': 1651, 'healthy': 1652, 'statistical': 1653, 'chase': 1654, 'government': 1655, 'peter': 1656, '150': 1657, 'hodder': 1658, 'move': 1659, 'break': 1660, 'gem': 1661, 'manager': 1662, 'maximum': 1663, 'volumes': 1664, 'wonders': 1665, 'forest': 1666, 'sequel': 1667, 'blink': 1668, 'therapy': 1669, 'compact': 1670, 'archive': 1671, 'casino': 1672, 'royale': 1673, 'election': 1674, 'plan': 1675, 'widow': 1676, 'guy': 1677, 'wit': 1678, 'mass': 1679, 'scientific': 1680, 'comprehension': 1681, 'branding': 1682, 'bed': 1683, 'cold': 1684, 'thrift': 1685, 'treehouse': 1686, 'tensorflow': 1687, 'demons': 1688, 'twelve': 1689, '03': 1690, 'da': 1691, 'vinci': 1692, 'architects': 1693, 'masterworks': 1694, 'journal': 1695, 'designer': 1696, 'image': 1697, 'authentic': 1698, 'carry': 1699, 'meaning': 1700, 'wilde': 1701, 'b': 1702, 'vivekananda': 1703, 'off': 1704, 'europe': 1705, 'logo': 1706, 'ai': 1707, 'disney': 1708, 'diet': 1709, 'phantom': 1710, 'abel': 1711, 'unknown': 1712, 'rabbit': 1713, \"man's\": 1714, 'shapes': 1715, 'knightfall': 1716, 'bharat': 1717, 'boat': 1718, 'fearless': 1719, 'grindelwald': 1720, 'villains': 1721, 'giants': 1722, \"master's\": 1723, 'delhi': 1724, 'saint': 1725, 'standard': 1726, 'olympiad': 1727, 'examples': 1728, 'dreamer': 1729, 'poppy': 1730, 'nature': 1731, 'formula': 1732, 'discovering': 1733, 'berlin': 1734, 'fullmetal': 1735, 'achieve': 1736, 'stronger': 1737, 'easiest': 1738, 'triumph': 1739, 'heat': 1740, 'mensa': 1741, 'weeks': 1742, 'guru': 1743, 'acts': 1744, 'rye': 1745, 'injury': 1746, 'lifestyle': 1747, 'vedanta': 1748, 'accursed': 1749, 'bitcoin': 1750, 'tuttle': 1751, \"anil's\": 1752, 'origin': 1753, 'daylight': 1754, 'travelling': 1755, 'archer': 1756, 'favorite': 1757, 'arrangements': 1758, 'forsaken': 1759, 'mark': 1760, 'moments': 1761, 'lovers': 1762, 'brighten': 1763, 'lotus': 1764, 'office': 1765, 'italian': 1766, 'miraculous': 1767, 'noir': 1768, 'drone': 1769, 'asia': 1770, 'rifles': 1771, 'relaxation': 1772, 'reluctant': 1773, 'upper': 1774, 'alphabet': 1775, 'cover': 1776, 'barefoot': 1777, 'round': 1778, 'names': 1779, 'universal': 1780, 'packed': 1781, '48': 1782, 'greene': 1783, 'pyramid': 1784, 'storyteller': 1785, 'routing': 1786, 'notorious': 1787, 'mysql': 1788, 'gray': 1789, 'practices': 1790, 'techie': 1791, 'turned': 1792, 'mighty': 1793, 'marry': 1794, 'early': 1795, 'wind': 1796, \"children's\": 1797, 'kidnapped': 1798, 'garden': 1799, '2013': 1800, 'heritage': 1801, 'innovative': 1802, 'shoot': 1803, 'eighty': 1804, 'highland': 1805, 'upon': 1806, 'confession': 1807, 'rediscover': 1808, 'sickle': 1809, 'atank': 1810, 'samjhauta': 1811, 'ke': 1812, 'maxwell': 1813, 'gift': 1814, 'pillars': 1815, 'awesome': 1816, 'distance': 1817, 'spiritual': 1818, 'gripping': 1819, 'mallory': 1820, 'spies': 1821, '3000': 1822, 'phrases': 1823, 'awareness': 1824, 'signet': 1825, \"photographer's\": 1826, 'whispers': 1827, 'judges': 1828, 'globalization': 1829, 'era': 1830, 'forces': 1831, 'dancing': 1832, 'reckoners': 1833, 'paris': 1834, 'lego': 1835, 'noble': 1836, 'printed': 1837, 'challenge': 1838, 'peace': 1839, 'venice': 1840, 'stunning': 1841, 'mission': 1842, 'mouseford': 1843, 'camera': 1844, 'transformed': 1845, 'drama': 1846, 'discipline': 1847, 'cissp': 1848, 'isc': 1849, 'players': 1850, 'scandalous': 1851, 'fighting': 1852, '1939': 1853, 'research': 1854, 'summit': 1855, 'geeks': 1856, '65': 1857, 'clouds': 1858, 'column': 1859, 'numbers': 1860, 'population': 1861, 'shaped': 1862, 'wanted': 1863, 'hawk': 1864, 'feast': 1865, 'roses': 1866, 'threat': 1867, 'movies': 1868, 'ultron': 1869, 'itself': 1870, 'jury': 1871, '1987': 1872, 'vikramaditya': 1873, 'veergatha': 1874, 'vengeance': 1875, 'monsters': 1876, 'month': 1877, 'savage': 1878, 'scales': 1879, 'arpeggios': 1880, 'dogs': 1881, 'emerald': 1882, 'shape': 1883, 'prey': 1884, 'southern': 1885, 'clare': 1886, 'mackintosh': 1887, 'jean': 1888, 'persepolis': 1889, 'pantheon': 1890, 'fallen': 1891, '30th': 1892, 'subhas': 1893, 'terms': 1894, \"mother's\": 1895, 'larousse': 1896, 'shook': 1897, 'lightbringer': 1898, 'ad': 1899, 'alfred': 1900, 'give': 1901, 'ma': 1902, 'amulet': 1903, 'ultimatum': 1904, 'duke': 1905, 'seasons': 1906, 'hedgehog': 1907, 'archives': 1908, 'fail': 1909, 'kind': 1910, 'call': 1911, 'sleeping': 1912, 'processing': 1913, 'curse': 1914, 'kept': 1915, 'billionaires': 1916, 'crowd': 1917, 'crimes': 1918, 'risk': 1919, 'hard': 1920, 'insight': 1921, 'queen': 1922, 'clifton': 1923, 'marcus': 1924, 'kafka': 1925, 'scale': 1926, 'reign': 1927, 'anchor': 1928, 'friend': 1929, 'predator': 1930, 'spin': 1931, \"screenwriter's\": 1932, 'collectors': 1933, 'temple': 1934, 'angular': 1935, 'deliver': 1936, 'thieves': 1937, 'gentleman': 1938, 'rescue': 1939, 'tagore': 1940, 'activity': 1941, 'gymnast': 1942, 'darkseid': 1943, 'guys': 1944, 'easily': 1945, 'elon': 1946, 'musk': 1947, 'jim': 1948, 'uzumaki': 1949, 'illustrations': 1950, 'literacy': 1951, 'knew': 1952, 'mills': 1953, 'boon': 1954, 'exclusive': 1955, 'blindsighted': 1956, 'grant': 1957, 'county': 1958, 'turning': 1959, 'difference': 1960, 'pep': 1961, 'feynman': 1962, 'desire': 1963, 'minecraft': 1964, 'worst': 1965, 'flow': 1966, 'deal': 1967, 'winter': 1968, 'wallflowers': 1969, 'ben': 1970, 'concurrency': 1971, 'developers': 1972, 'prisoners': 1973, 'bay': 1974, 'guardians': 1975, 'souls': 1976, 'serious': 1977, 'corbett': 1978, 'ho': 1979, 'plus': 1980, 'air': 1981, 'tears': 1982, 'dry': 1983, 'countdown': 1984, 'trekking': 1985, 'cartooning': 1986, 'actor': 1987, 'raman': 1988, 'pollyanna': 1989, 'video': 1990, 'fifteen': 1991, 'august': 1992, 'indispensable': 1993, 'karate': 1994, 'mayhem': 1995, 'bach': 1996, 'financial': 1997, 'pictures': 1998, 'frontiers': 1999, 'collector': 2000, 'laxman': 2001, 'operation': 2002, 'jinnah': 2003, 'transformation': 2004, 'sandman': 2005, 'nightingale': 2006, 'among': 2007, 'k2': 2008, 'steel': 2009, 'belong': 2010, 'results': 2011, 'moby': 2012, 'ex': 2013, 'laurie': 2014, 'baker': 2015, 'puffin': 2016, 'touching': 2017, \"father's\": 2018, 'muhammad': 2019, 'toefl': 2020, 'gabriel': 2021, 'allon': 2022, 'stan': 2023, 'hulk': 2024, 'hive': 2025, 'forget': 2026, '69': 2027, 'capital': 2028, 'generations': 2029, 'beginning': 2030, 'sister': 2031, 'circus': 2032, \"let's\": 2033, 'remastered': 2034, 'escape': 2035, 'directing': 2036, 'actors': 2037, 'television': 2038, 'food': 2039, 'virtuoso': 2040, 'w': 2041, 'million': 2042, 'hugo': 2043, 'starring': 2044, 'shibumi': 2045, 'alfreds': 2046, \"cuckoo's\": 2047, 'profit': 2048, 'awkward': 2049, 'yeti': 2050, 'sidemen': 2051, 'waiting': 2052, 'maid': 2053, 'sama': 2054, 'proof': 2055, 'hurt': 2056, 'linux': 2057, 'lethal': 2058, 'military': 2059, 'learned': 2060, 'growing': 2061, 'deception': 2062, 'confidence': 2063, 'confident': 2064, 'courses': 2065, 'tactics': 2066, 'pony': 2067, 'tells': 2068, 'medical': 2069, 'recent': 2070, 'remembers': 2071, 'diploma': 2072, 'crossfire': 2073, 'trail': 2074, 'weight': 2075, 'folk': 2076, 'andromeda': 2077, 'crooked': 2078, 'express': 2079, 'underworld': 2080, 'drop': 2081, 'anthology': 2082, 'goodbye': 2083, 'played': 2084, '06': 2085, '08': 2086, 'poems': 2087, 'phrasal': 2088, 'neymar': 2089, 'director': 2090, 'satyajit': 2091, 'azure': 2092, 'these': 2093, '250': 2094, 'unity': 2095, 'arabic': 2096, \"writer's\": 2097, 'mostly': 2098, \"you're\": 2099, 'exercise': 2100, 'grades': 2101, 'laid': 2102, 'olympics': 2103, '36': 2104, 'golf': 2105, 'begins': 2106, \"animator's\": 2107, 'formulas': 2108, 'animators': 2109, 'arise': 2110, 'resurgent': 2111, 'thought': 2112, 'russian': 2113, 'arthur': 2114, 'intimate': 2115, 'mughal': 2116, 'sands': 2117, 'christian': 2118, 'large': 2119, 'basics': 2120, 'ed': 2121, 'ff': 2122, 'voice': 2123, 'listen': 2124, 'robin': 2125, 'hood': 2126, 'poet': 2127, 'christopher': 2128, 'lasts': 2129, 'swimming': 2130, 'coaching': 2131, 'rose': 2132, 'extended': 2133, 'stuff': 2134, 'rights': 2135, 'money': 2136, 'friendship': 2137, 'neverwhere': 2138, 'stoic': 2139, 'seneca': 2140, 'products': 2141, 'witness': 2142, 'lectures': 2143, 'threatening': 2144, 'courtneys': 2145, 'cruel': 2146, 'secrecy': 2147, 'within': 2148, 'eaters': 2149, 'vocals': 2150, '2000': 2151, '2e': 2152, 'wisden': 2153, 'microservices': 2154, 'madhubani': 2155, 'coin': 2156, 'none': 2157, 'hundred': 2158, 'stroke': 2159, '07': 2160, 'accidental': 2161, 'happily': 2162, 'mandarin': 2163, 'lucky': 2164, 'challenges': 2165, 'reckless': 2166, 'tibet': 2167, 'given': 2168, 'puzzle': 2169, 'aw': 2170, 'espionage': 2171, 'planning': 2172, 'jerusalem': 2173, 'j': 2174, 'lighting': 2175, 'states': 2176, '23': 2177, 'server': 2178, 'fly': 2179, 'weapon': 2180, 'pattern': 2181, 'mein': 2182, 'kampf': 2183, 'riders': 2184, 'mysterious': 2185, 'intensity': 2186, 'flight': 2187, 'muslim': 2188, 'feminism': 2189, 'past': 2190, 'both': 2191, 'connections': 2192, 'moonwalk': 2193, 'donald': 2194, 'max': 2195, 'perennial': 2196, 'studio': 2197, 'forming': 2198, \"cricket's\": 2199, \"chieftain's\": 2200, 'shield': 2201, 'programs': 2202, 'silver': 2203, 'jquery': 2204, '26': 2205, 'discover': 2206, 'leather': 2207, 'bound': 2208, 'macmillan': 2209, 'elves': 2210, 'graphix': 2211, 'leaves': 2212, 'timeless': 2213, 'hot': 2214, 'explosive': 2215, 'strip': 2216, 'leonardo': 2217, 'magician': 2218, 'nice': 2219, 'tigers': 2220, 'iot': 2221, 'infrastructure': 2222, \"developer's\": 2223, 'chief': 2224, 'directors': 2225, 'baroque': 2226, 'sociology': 2227, 'blowing': 2228, 'broke': 2229, 'amos': 2230, 'decker': 2231, 'xi': 2232, 'found': 2233, 'highly': 2234, 'taliban': 2235, 'return': 2236, 'type': 2237, 'inequality': 2238, 'solo': 2239, 'diamond': 2240, 'pearl': 2241, 'platinum': 2242, 'fox': 2243, 'solution': 2244, 'ultra': 2245, 'seen': 2246, 'chicken': 2247, 'sita': 2248, 'trent': 2249, 'poor': 2250, 'poverty': 2251, 'idea': 2252, '1971': 2253, 'sparrow': 2254, 'mahabharata': 2255, '27': 2256, 'cricketer': 2257, 'experience': 2258, 'interviews': 2259, 'almanack': 2260, 'personality': 2261, 'tinker': 2262, 'tailor': 2263, 'maya': 2264, 'while': 2265, 'rome': 2266, 'folding': 2267, 'form': 2268, 'act': 2269, 'mummy': 2270, 'environmental': 2271, 'manhattan': 2272, 'oath': 2273, 'middle': 2274, 'rejuvenate': 2275, 'boyfriend': 2276, 'thereby': 2277, 'hangs': 2278, 'cats': 2279, 'busy': 2280, 'band': 2281, 'writer': 2282, 'leveraging': 2283, 'news': 2284, 'maus': 2285, 'brown': 2286, 'suit': 2287, 'sai': 2288, 'teachings': 2289, 'happens': 2290, 'couple': 2291, 'door': 2292, '04': 2293, 'micekings': 2294, 'some': 2295, 'prediction': 2296, 'destruction': 2297, 'queens': 2298, 'translated': 2299, 'galaxy': 2300, 'beast': 2301, 'forgotten': 2302, 'invincible': 2303, 'drought': 2304, 'langenscheidt': 2305, 'rebel': 2306, 'theories': 2307, 'tarot': 2308, 'coffee': 2309, 'sam': 2310, 'minerals': 2311, 'fame': 2312, 'anna': 2313, 'caraval': 2314, 'joker': 2315, 'guernsey': 2316, 'potato': 2317, 'peel': 2318, 'pie': 2319, 'rejacketed': 2320, 'fifth': 2321, 'fate': 2322, 'boxset': 2323, 'catch': 2324, 'roger': 2325, 'scenes': 2326, 'guildhall': 2327, 'unleashed': 2328, 'vizbig': 2329, 'mughals': 2330, 'guinness': 2331, 'records': 2332, 'alchemist': 2333, 'choices': 2334, 'assassin’s': 2335, 'interior': 2336, 'tata': 2337, 'eyewitness': 2338, 'car': 2339, 'kannada': 2340, 'magnificent': 2341, 'window': 2342, 'collaboration': 2343, 'nationalism': 2344, 'magical': 2345, 'runner': 2346, 'halloween': 2347, '61': 2348, 'afghan': 2349, 'napoleon': 2350, 'remarkable': 2351, 'keeper': 2352, 'peaceful': 2353, 'active': 2354, 'idle': 2355, 'salesforce': 2356, 'managers': 2357, 'cristiano': 2358, 'dutt': 2359, 'leviathan': 2360, 'effortless': 2361, 'programmes': 2362, 'nourishing': 2363, 'sivananda': 2364, 'centre': 2365, 'pan': 2366, 'shaolin': 2367, \"itachi's\": 2368, 'arena': 2369, 'fico': 2370, 'informal': 2371, 'lew': 2372, 'uncharted': 2373, \"thief's\": 2374, 'ask': 2375, 'facing': 2376, 'frozen': 2377, 'crilley': 2378, 'akiko': 2379, 'syllabus': 2380, 'timeline': 2381, 'lived': 2382, 'besharam': 2383, 'sunshine': 2384, 'footnotes': 2385, 'ove': 2386, 'affirming': 2387, 'sybex': 2388, 'politician': 2389, 'brave': 2390, 'revisited': 2391, 'malice': 2392, \"doll's\": 2393, 'ladybug': 2394, 'eternal': 2395, 'rafa': 2396, 'delusion': 2397, 'lazy': 2398, 'illusions': 2399, 'driven': 2400, 'tatas': 2401, 'notebook': 2402, 'pages': 2403, 'native': 2404, 'seventy': 2405, 'comptia': 2406, '901': 2407, '902': 2408, 'enhance': 2409, 'influence': 2410, 'perception': 2411, 'increase': 2412, 'appeal': 2413, 'mortality': 2414, 'piercing': 2415, 'safe': 2416, 'rings': 2417, 'spirited': 2418, 'drums': 2419, 'cradle': 2420, 'worldly': 2421, 'thinkers': 2422, 'exotic': 2423, 'lands': 2424, 'startup': 2425, 'refreshing': 2426, 'price': 2427, 'market': 2428, 'killers': 2429, 'reverse': 2430, 'microservice': 2431, 'aligning': 2432, 'moong': 2433, 'microchips': 2434, 'farmer': 2435, 'poirot’s': 2436, 'cases': 2437, 'monk': 2438, 'angel': 2439, 'tezuka': 2440, 'bossypants': 2441, 'hunting': 2442, 'baaz': 2443, 'foundation': 2444, \"everyone's\": 2445, 'written': 2446, 'spielberg': 2447, 'humour': 2448, 'contemporaries': 2449, 'anything': 2450, 'transforming': 2451, 'highlander': 2452, 'dork': 2453, 'progressive': 2454, 'pharaoh': 2455, 'cave': 2456, 'congo': 2457, 'gaul': 2458, 'goths': 2459, 'seventh': 2460, 'plague': 2461, 'migration': 2462, 'mourinho': 2463, 'kingdoms': 2464, 'prequel': 2465, 'pele': 2466, 'narrows': 2467, 'sixth': 2468, \"rackham's\": 2469, 'plants': 2470, 'zombies': 2471, '39': 2472, 'twice': 2473, 'invitation': 2474, 'rules': 2475, 'micro': 2476, 'communications': 2477, 'path': 2478, 'symposium': 2479, 'shortcuts': 2480, 'peaks': 2481, 'fatal': 2482, 'drawn': 2483, 'pale': 2484, 'chance': 2485, 'ibm': 2486, 'milk': 2487, 'kp': 2488, 'sikkim': 2489, 'merger': 2490, 'pulitzer': 2491, \"barron's\": 2492, '1980': 2493, 'discontents': 2494, 'warren': 2495, 'matarese': 2496, 'circle': 2497, 'inner': 2498, 'optimal': 2499, 'england': 2500, 'deaths': 2501, 'react': 2502, 'bright': 2503, 'catcher': 2504, 'colors': 2505, '49': 2506, 'quality': 2507, 'icnd1': 2508, 'inspire': 2509, 'imagination': 2510, \"pharaoh's\": 2511, 'honest': 2512, 'creatives': 2513, 'narasimha': 2514, 'rao': 2515, 'nurture': 2516, \"child's\": 2517, 'murty': 2518, 'wonderful': 2519, \"gulzar's\": 2520, 'commentary': 2521, \"champion's\": 2522, 'wolf': 2523, \"handmaid's\": 2524, 'tomb': 2525, 'partisans': 2526, 'darling': 2527, 'reaching': 2528, 'mina': 2529, 'hacking': 2530, 'rivals': 2531, 'greatness': 2532, 'faces': 2533, 'tide': 2534, 'sympathizer': 2535, 'rebirth': 2536, 'im': 2537, 'it’s': 2538, 'un': 2539, 'runaway': 2540, 'leo': 2541, 'deadpool': 2542, 'lama': 2543, 'recommendation': 2544, 'condor': 2545, 'crew': 2546, 'mist': 2547, 'indra': 2548, 'care': 2549, 'biology': 2550, 'slaughterhouse': 2551, 'loathing': 2552, 'las': 2553, 'vegas': 2554, 'colossal': 2555, 'narratives': 2556, 'northeast': 2557, 'crystal': 2558, 'nikola': 2559, 'burman': 2560, 'earthsea': 2561, 'tombs': 2562, 'frontier': 2563, 'tropic': 2564, 'wordsearch': 2565, 'themed': 2566, 'renaissance': 2567, 'unicorn': 2568, 'castafiore': 2569, 'shallows': 2570, 'outline': 2571, 'dentist': 2572, '58': 2573, 'pilgrim': 2574, 'curtain': 2575, 'leopard': 2576, 'lara': 2577, 'instruments': 2578, 'returns': 2579, 'chi': 2580, 'skyward': 2581, '1942': 2582, 'monkey': 2583, 'addison': 2584, 'wesley': 2585, 'tabby': 2586, 'mctat': 2587, 'reckoning': 2588, 'institutions': 2589, 'glossary': 2590, 'portraits': 2591, 'cc': 2592, '1857': 2593, 'naked': 2594, 'those': 2595, 'explore': 2596, 'afterlife': 2597, 'perfume': 2598, 'murderer': 2599, 'meridian': 2600, 'analytical': 2601, 'cosmological': 2602, 'beatles': 2603, 'yesterdays': 2604, 'verse': 2605, 'adams': 2606, 'alph': 2607, 'alice': 2608, 'wonderland': 2609, 'elephant': 2610, 'sins': 2611, 'breaking': 2612, 'om': 2613, 'geology': 2614, 'sciences': 2615, 'ridiculously': 2616, 'ignored': 2617, 'doodling': 2618, 'meetings': 2619, 'inspired': 2620, 'did': 2621, 'titan': 2622, 'greek': 2623, 'heirs': 2624, 'ramona': 2625, 'achieving': 2626, 'tidying': 2627, 'tidy': 2628, 'mythology': 2629, 'divide': 2630, 'command': 2631, 'liberal': 2632, 'individual': 2633, 'thirteen': 2634, \"isn't\": 2635, 'trial': 2636, 'jeet': 2637, 'les': 2638, 'miserables': 2639, 'evolutionary': 2640, 'evelyn': 2641, 'psus': 2642, 'es': 2643, 'nobody': 2644, '71': 2645, 'tongue': 2646, 'tribe': 2647, 'blade': 2648, 'icon': 2649, 'analyst': 2650, 'apply': 2651, 'select': 2652, 'funnybones': 2653, 'y': 2654, 'greed': 2655, 'lust': 2656, 'victory': 2657, 'gym': 2658, 'gryffindor': 2659, 'humans': 2660, 'enterprise': 2661, 'dan': 2662, 'phil': 2663, 'maybe': 2664, 'authorized': 2665, 'rahman': 2666, 'republic': 2667, 'bastards': 2668, 'krav': 2669, 'maga': 2670, 'terrorism': 2671, 'rituals': 2672, 'convey': 2673, '42481': 2674, 'mirror': 2675, 'defence': 2676, 'programmers': 2677, 'preschool': 2678, 'britain': 2679, 'fantastically': 2680, 'again': 2681, 'habits': 2682, 'teenage': 2683, 'vision': 2684, 'autumn': 2685, \"gabriel's\": 2686, 'rapture': 2687, 'negotiating': 2688, 'depended': 2689, 'construction': 2690, 'caricature': 2691, 'gear': 2692, 'legal': 2693, 'confidential': 2694, 'munich': 2695, '20th': 2696, 'motorcycle': 2697, 'jewel': 2698, 'royals': 2699, 'ravinder': 2700, 'igcse®': 2701, 'rupa': 2702, 'count': 2703, 'monte': 2704, 'cristo': 2705, 'agent': 2706, 'tiger’s': 2707, 'tried': 2708, 'seriously': 2709, 'taoist': 2710, 'diagnosis': 2711, 'example': 2712, 'dracula': 2713, 'streaming': 2714, 'apache': 2715, 'rex': 2716, 'dinosaur': 2717, 'webcomics': 2718, 'martian': 2719, 'astonishing': 2720, 'should': 2721, 'met': 2722, 'abap': 2723, 'dynamic': 2724, '5e': 2725, 'sometimes': 2726, 'rss': 2727, 'furious': 2728, 'clay': 2729, 'bonus': 2730, 'amelia': 2731, 'bedelia': 2732, 'changer': 2733, 'worrying': 2734, 'border': 2735, '7th': 2736, 'bloomsbury': 2737, 'fix': 2738, 'tenali': 2739, 'vanishing': 2740, 'grows': 2741, 'hello': 2742, 'frames': 2743, 'intelligences': 2744, 'longman': 2745, 'farm': 2746, 'giant’s': 2747, 'bread': 2748, 'bébé': 2749, 'parenting': 2750, 'keys': 2751, 'favourite': 2752, \"eagle's\": 2753, 'along': 2754, 'including': 2755, 'flower': 2756, 'remedies': 2757, 'explorer': 2758, 'marc': 2759, 'suarez': 2760, 'lovely': 2761, 'testament': 2762, 'entrepreneur': 2763, 'inheritance': 2764, 'quotations': 2765, 'zynpagua': 2766, \"serpent's\": 2767, 'thug': 2768, 'baltimore': 2769, 'formulae': 2770, 'definitions': 2771, 'ford': 2772, 'company': 2773, 'phoenix': 2774, '45': 2775, 'quiet': 2776, 'woosters': 2777, 'publicly': 2778, 'shamed': 2779, 'solved': 2780, 'upsc': 2781, 'entrance': 2782, \"sherpa's\": 2783, 'jouney': 2784, 'premier': 2785, 'richest': 2786, 'talent': 2787, 'grown': 2788, 'ibt': 2789, 'whale': 2790, 'fry': 2791, 'gilded': 2792, 'absurd': 2793, 'warehouse': 2794, 'query': 2795, 'egyptian': 2796, 'tableau': 2797, 'tutorials': 2798, 'juror': 2799, 'clockwork': 2800, \"asia's\": 2801, 'myself': 2802, 'sharks': 2803, 'gustav': 2804, 'sonata': 2805, 'because': 2806, 'wooden': 2807, 'possible': 2808, 'currency': 2809, '37': 2810, \"what's\": 2811, 'falling': 2812, 'arrow': 2813, 'dharmendra': 2814, 'george': 2815, 'terrors': 2816, 'christmas': 2817, 'quickly': 2818, 'coach': 2819, 'humanity': 2820, 'delusions': 2821, 'thanks': 2822, 'gulzar': 2823, 'morgan': 2824, 'kaufmann': 2825, 'mentality': 2826, 'frankenstein': 2827, 'crack': 2828, 'services': 2829, 'journeys': 2830, 'lecture': 2831, 'ivory': 2832, 'performances': 2833, 'bhagwaan': 2834, 'pakwaan': 2835, 'hanon': 2836, 'pianist': 2837, 'tripwire': 2838, 'daytripper': 2839, 'fought': 2840, 'elite': 2841, 'published': 2842, 'random': 2843, 'resistance': 2844, 'virat': 2845, 'massive': 2846, 'following': 2847, 'guilty': 2848, 'scott': 2849, 'share': 2850, 'photos': 2851, 'twenties': 2852, 'vampire': 2853, 'feelings': 2854, 'consciousness': 2855, \"ludlum's\": 2856, 'covert': 2857, 'pyjama': 2858, \"millennial's\": 2859, 'freelance': 2860, 'surprise': 2861, 'gut': 2862, 'instincts': 2863, 'hyperbole': 2864, 'unfortunate': 2865, 'situations': 2866, 'flawed': 2867, 'coping': 2868, 'mechanisms': 2869, 'bala': 2870, \"ambassador's\": 2871, \"idiot's\": 2872, 'melodies': 2873, 'loud': 2874, 'textiles': 2875, 'sniper': 2876, 'selenium': 2877, 'webdriver': 2878, 'empty': 2879, '1s': 2880, 'cool': 2881, 'definitely': 2882, 'feudal': 2883, 'exactly': 2884, 'vish': 2885, 'puri': 2886, 'feature': 2887, 'explanation': 2888, 'communist': 2889, 'playscript': 2890, 'asking': 2891, 'baccalaureate': 2892, 'december': 2893, 'nelson': 2894, 'mandela': 2895, 'adobe': 2896, '2010': 2897, 'person': 2898, 'cannot': 2899, 'alternative': 2900, 'napkin': 2901, 'russia': 2902, 'chicago': 2903, 'owls': 2904, '1967': 2905, 'wicket': 2906, 'frank': 2907, \"miller's\": 2908, 'whom': 2909, 'tolls': 2910, 'exit': 2911, 'tendulkar': 2912, 'prisoner': 2913, 'vibes': 2914, 'administrator': 2915, 'terror': 2916, 'unbound': 2917, 'industrial': 2918, 'speeches': 2919, 'bowden': 2920, 'hilo': 2921, 'saving': 2922, \"qur'an\": 2923, 'sitcom': 2924, 'furiously': 2925, 'plectrum': 2926, 'headway': 2927, 'elt': 2928, 'scroll': 2929, 'cleopatra': 2930, 'giving': 2931, 'bullets': 2932, 'revealed': 2933, 'anywhere': 2934, 'vishnu': 2935, \"hunter's\": 2936, 'crossed': 2937, 'jahangir': 2938, 'slash': 2939, \"apple's\": 2940, 'during': 2941, \"schaum's\": 2942, 'lucifer': 2943, 'cotton': 2944, 'malone': 2945, 'till': 2946, 'vendetta': 2947, 'occasions': 2948, \"reader's\": 2949, 'addictive': 2950, 'aurangzeb': 2951, 'print': 2952, 'monster': 2953, 't': 2954, 'jp': 2955, 'introductory': 2956, 'area': 2957, 'solve': 2958, 'clever': 2959, 'flood': 2960, 'maths': 2961, 'j2ee': 2962, 'surgeon': 2963, 'complicated': 2964, 'o': 2965, 'doors': 2966, 'malayalam': 2967, 'westmoreland': 2968, 'dynasty': 2969, 'swastika': 2970, 'displaced': 2971, 'refugee': 2972, 'chasing': 2973, 'airframe': 2974, 'odessa': 2975, 'copper': 2976, 'sahir': 2977, 'ludhianvi': 2978, \"people's\": 2979, 'hart': 2980, 'chapter': 2981, 'misery': 2982, 'adulthood': 2983, 'olympus': 2984, 'merchant': 2985, 'ratna': 2986, 'sagar': 2987, 'sunburned': 2988, 'altitude': 2989, 'ties': 2990, 'charms': 2991, '112': 2992, 'realization': 2993, 'vigyan': 2994, 'bhairava': 2995, 'tantra': 2996, 'cauldron': 2997, 'program': 2998, 'gaining': 2999, \"king's\": 3000, 'further': 3001, 'parents': 3002, 'nonfiction': 3003, 'quotes': 3004, 'folks': 3005, 'probe': 3006, 'guest': 3007, 'gitanjali': 3008, 'teams': 3009, 'cosimo': 3010, 'easier': 3011, 'cellist': 3012, 'sarajevo': 3013, 'endless': 3014, 'defining': 3015, 'cancer': 3016, 'gave': 3017, '6e': 3018, 'betrayal': 3019, 'close': 3020, 'put': 3021, 'serve': 3022, \"men's\": 3023, 'seas': 3024, 'investigates': 3025, 'founder': 3026, 'religion': 3027, '366': 3028, 'perseverance': 3029, 'translations': 3030, 'epictetus': 3031, 'aurelius': 3032, 'decade': 3033, 'gladiator': 3034, 'uml': 3035, 'sein': 3036, 'penmanship': 3037, 'aesthetics': 3038, 'kenneth': 3039, 'anderson': 3040, 'craze': 3041, 'cryptography': 3042, 'qi': 3043, 'thank': 3044, 'thread': 3045, 'treatise': 3046, 'sundarbans': 3047, 'greetings': 3048, 'somewhere': 3049, 'across': 3050, 'organization': 3051, 'tony': 3052, 'jordan': 3053, 'che': 3054, 'guevara': 3055, 'bodybuilding': 3056, 'springer': 3057, 'craft': 3058, 'unmaking': 3059, 'manmohan': 3060, 'mercedes': 3061, 'harmonica': 3062, 'senior': 3063, 'cage': 3064, 'evolution': 3065, 'suspect': 3066, 'bawse': 3067, 'conquering': 3068, 'together': 3069, 'cks': 3070, 'unending': 3071, 'former': 3072, \"chief's\": 3073, 'landscape': 3074, 'surprising': 3075, 'blind': 3076, 'cornered': 3077, 'means': 3078, 'immigrants': 3079, 'hopeless': 3080, 'fellow': 3081, 'papercraft': 3082, 'longclaw': 3083, 'photographic': 3084, 'parliament': 3085, 'verbal': 3086, 'advantage': 3087, 'prank': 3088, 'hollywood': 3089, 'hoax': 3090, 'encore': 3091, 'nouvelle': 3092, 'reliable': 3093, 'imperfect': 3094, 'byomkesh': 3095, 'bakshi': 3096, 'objects': 3097, 'carpet': 3098, 'microprocessors': 3099, 'tao': 3100, 'slow': 3101, 'turns': 3102, \"rubik's\": 3103, 'exile': 3104, 'st': 3105, \"ladies'\": 3106, 'agency': 3107, 'algebra': 3108, 'abundance': 3109, 'exponential': 3110, 'vaastu': 3111, 'prosperity': 3112, 'stuxnet': 3113, 'launch': 3114, 'false': 3115, 'daniel': 3116, 'sexuality': 3117, 'pictorial': 3118, 'hbo': 3119, 'arnold': 3120, 'storytelling': 3121, 'reinforcement': 3122, 'rl': 3123, 'harper': 3124, 'governance': 3125, 'lizard': 3126, \"actor's\": 3127, 'stanislavsky': 3128, 'practiced': 3129, 'hooked': 3130, 'habit': 3131, 'monopolies': 3132, 'dominate': 3133, 'date': 3134, 'gun': 3135, 'workflows': 3136, 'scams': 3137, 'corporate': 3138, 'operating': 3139, 'hands–on': 3140, 'legionary': 3141, 'fantasy': 3142, 'grip': 3143, 'fool': 3144, 'networking': 3145, 'eternity': 3146, 'manufacturing': 3147, 'consent': 3148, 'normal': 3149, 'paragraphs': 3150, 'pretty': 3151, 'ray’s': 3152, 'heroines': 3153, 'contract': 3154, 'novice': 3155, 'ark': 3156, 'holiday': 3157, 'concerns': 3158, 'policies': 3159, 'salute': 3160, 'joy': 3161, 'mages': 3162, 'untranslatable': 3163, 'covers': 3164, 'avatari': 3165, 'enchiridion': 3166, 'patel': 3167, 'travels': 3168, 'collective': 3169, 'tanjore': 3170, 'consequences': 3171, 'portfolio': 3172, 'storey': 3173, 'screenwriters': 3174, 'conflict': 3175, 'desktop': 3176, 'communicative': 3177, 'app': 3178, 'entrepreneurs': 3179, 'ghosts': 3180, 'girlboss': 3181, 'pataudi': 3182, 'nawab': 3183, 'seal': 3184, '05': 3185, '31': 3186, 'kissinger': 3187, 'jail': 3188, 'demystifying': 3189, \"'2016\": 3190, 'powers': 3191, 'potential': 3192, 'friendly': 3193, 'marathi': 3194, 'beat': 3195, '1958': 3196, 'boink': 3197, '74': 3198, 'scanner': 3199, 'darkly': 3200, 'processes': 3201, 'civil': 3202, 'fabric': 3203, 'reaper': 3204, \"archie's\": 3205, 'capsule': 3206, 'tensor': 3207, 'intelligent': 3208, 'heir': 3209, '40': 3210, '42': 3211, \"solomon's\": 3212, 'mines': 3213, 'cinematographers': 3214, 'introvert': 3215, 'musketeers': 3216, 'bring': 3217, 'bodies': 3218, 'ganesha': 3219, 'criminal': 3220, 'milestones': 3221, 'retold': 3222, 'bought': 3223, 'harlequin': 3224, 'duchess': 3225, 'freed': 3226, 'karna': 3227, 'impact': 3228, 'alibaba': 3229, 'titles': 3230, 'wishes': 3231, 'rainbow': 3232, 'netflix': 3233, 'ignorance': 3234, 'navarone': 3235, 'fatherland': 3236, 'violin': 3237, 'keras': 3238, 'hufflepuff': 3239, 'hitman': 3240, 'anders': 3241, 'footballer': 3242, 'sutra': 3243, 'pitman': 3244, 'shorthand': 3245, 'malala': 3246, 'stood': 3247, 'took': 3248, 'matlab': 3249, 'grotesque': 3250, 'susan': 3251, '5th': 3252, 'doodle': 3253, 'destiny': 3254, 'council': 3255, 'neil': 3256, 'gaiman': 3257, 'reasons': 3258, 'compendium': 3259, 'hercules': 3260, 'arguments': 3261, \"doesn't\": 3262, 'classroom': 3263, 'advance': 3264, 'stickers': 3265, 'shine': 3266, 'arch': 3267, 'nutrition': 3268, 'innocent': 3269, 'aegon': 3270, 'iii': 3271, 'mine': 3272, 'paradoxical': 3273, 'burgers': 3274, 'incorrect': 3275, 'multicolour': 3276, 'railways': 3277, 'few': 3278, 'neither': 3279, 'nor': 3280, 'goliath': 3281, 'collect': 3282, 'identities': 3283, 'k': 3284, 'aural': 3285, 'berkley': 3286, 'schaum': 3287, 'rethinking': 3288, 'fairy': 3289, 'diwali': 3290, 'elementary': 3291, 'illusion': 3292, 'languages': 3293, 'developer': 3294, 'sitters': 3295, 'argument': 3296, 'seuss': 3297, 'rural': 3298, 'database': 3299, '1z0': 3300, 'charles': 3301, 'reich': 3302, 'nazis': 3303, 'sculpted': 3304, 'tenant': 3305, 'wildfell': 3306, 'brontë': 3307, 'remaking': 3308, 'sets': 3309, 'waves': 3310, 'center': 3311, \"scorer's\": 3312, 'shoe': 3313, 'rig': 3314, 'rigging': 3315, 'scam': 3316, 'scoring': 3317, 'angry': 3318, \"runner's\": 3319, 'difficult': 3320, 'avon': 3321, 'latest': 3322, 'screw': 3323, 'gopeshvara': 3324, 'vrishnis': 3325, 'igcse™': 3326, 'suspense': 3327, 'madhouse': 3328, 'climate': 3329, 'denial': 3330, 'destroying': 3331, 'driving': 3332, \"o's\": 3333, 'exploration': 3334, 'conditioning': 3335, 'mystic': 3336, 'crafts': 3337, 'uplifting': 3338, 'itv': 3339, 'zoe': 3340, 'martin': 3341, 'jr': 3342, 'floral': 3343, 'metaphors': 3344, \"history's\": 3345, 'fonts': 3346, 'swordfish': 3347, 'amateur': 3348, 'him': 3349, 'relationship': 3350, 'rage': 3351, '3ed': 3352, 'grow': 3353, 'unique': 3354, 'superstar': 3355, 'bloodline': 3356, 'flag': 3357, 'shining': 3358, 'encounter': 3359, 'weirdos': 3360, 'le': 3361, 'tenth': 3362, 'mata': 3363, 'beliefs': 3364, 'product': 3365, 'jump': 3366, 'tender': 3367, 'performing': 3368, 'argue': 3369, 'october': 3370, 'floating': 3371, 'strangest': 3372, 'paul': 3373, 'laughs': 3374, 'infernal': 3375, 'devices': 3376, \"women's\": 3377, 'shirdi': 3378, 'baba': 3379, 'idiot': 3380, 'wiley': 3381, 'chocolate': 3382, 'shop': 3383, 'omega': 3384, 'ayako': 3385, 'destroyed': 3386, 'germany': 3387, 'teachers': 3388, 'avenger': 3389, \"didn't\": 3390, 'superforecasting': 3391, 'mitrokhin': 3392, 'kgb': 3393, 'destinations': 3394, 'sas': 3395, \"devil's\": 3396, 'annual': 3397, 'criminals': 3398, 'colleges': 3399, 'actress': 3400, 'puller': 3401, \"hiker's\": 3402, 'automata': 3403, 'marathon': 3404, 'bihar': 3405, 'ref': 3406, 'grave': 3407, '1918': 3408, 'rulebook': 3409, 'map': 3410, 'fixer': 3411, 'lecter': 3412, '13th': 3413, 'genocide': 3414, 'sound': 3415, 'injuries': 3416, 'sheldon’s': 3417, '17th': 3418, 'thirty': 3419, 'amphigorey': 3420, 'nigella': 3421, 'dissent': 3422, 'meets': 3423, 'ambedkar': 3424, 'dipa': 3425, 'karmakar': 3426, 'compete': 3427, 'monday': 3428, 'pain': 3429, 'leading': 3430, 'manchester': 3431, 'causes': 3432, 'childrens': 3433, 'bengali': 3434, 'ccc': 3435, 'tour': 3436, 'motorcycles': 3437, 'banquet': 3438, 'dose': 3439, 'oil': 3440, 'saddam': 3441, 'ahead': 3442, 'narendra': 3443, 'playlist': 3444, 'direction': 3445, 'illuminated': 3446, 'regeneration': 3447, 'perils': 3448, 'moderately': 3449, '·': 3450, 'cups': 3451, 'woes': 3452, 'throes': 3453, 'ramayan': 3454, 'discworld': 3455, 'nights': 3456, 'account': 3457, 'smarter': 3458, 'wyndham': 3459, 'arthashastra': 3460, 'regrets': 3461, 'pl': 3462, 'dame': 3463, 'millennium': 3464, 'smile': 3465, 'blandings': 3466, '80': 3467, 'wizarding': 3468, 'fbi': 3469, 'attorney': 3470, 'weapons': 3471, 'precis': 3472, 'cloth': 3473, 'rama': 3474, 'reeds': 3475, 'magpie': 3476, 'grim': 3477, '28': 3478, 'chris': 3479, 'standing': 3480, 'barrons': 3481, 'perfection': 3482, 'succeed': 3483, 'sanchin': 3484, 'afraid': 3485, 'nick': 3486, 'farseer': 3487, 'interpretation': 3488, 'suzuki': 3489, 'could': 3490, 'hyperledger': 3491, 'parties': 3492, 'runs': 3493, 'falls': 3494, 'federer': 3495, \"cricketers'\": 3496, 'filmmaking': 3497, 'banks': 3498, 'pregnancy': 3499, \"doctor's\": 3500, 'finish': 3501, 'hardware': 3502, 'interface': 3503, 'dickens': 3504, 'partners': 3505, 'tommy': 3506, 'tuppence': 3507, 'torn': 3508, 'aincrad': 3509, 'praise': 3510, 'aliebn': 3511, \"hitler's\": 3512, 'el': 3513, 'choke': 3514, 'courage': 3515, 'inventions': 3516, 'dhoni': 3517, 'unravelling': 3518, 'jar': 3519, 'hanging': 3520, 'medieval': 3521, 'sultanat': 3522, 'ms': 3523, 'macros': 3524, 'hole': 3525, 'diego': 3526, '1974': 3527, 'tiny': 3528, '14th': 3529, 'accurate': 3530, 'kurt': 3531, 'ultramarathon': 3532, 'losing': 3533, 'nostradamus': 3534, 'spells': 3535, 'bowling': 3536, 'improving': 3537, 'cultural': 3538, 'thrillers': 3539, 'stretching': 3540, '09': 3541, 'colloquial': 3542, 'illuminae': 3543, 'experiments': 3544, 'waste': 3545, 'leaner': 3546, 'configuration': 3547, 'siege': 3548, 'malcolm': 3549, \"today's\": 3550, 'herding': 3551, 'needs': 3552, 'improvisation': 3553, 'demystified': 3554, 'ee': 3555, 'vicious': 3556, 'knightquest': 3557, 'shame': 3558, 'roller': 3559, 'francis': 3560, 'thrive': 3561, 'lux': 3562, 'bizarre': 3563, 'challenging': 3564, 'horizon': 3565, 'quidditch': 3566, 'romanov': 3567, 'ransom': 3568, 'signs': 3569, 'constitution': 3570, 'lone': 3571, 'serverless': 3572, 'dennis': 3573, 'menace': 3574, 'comicbooks': 3575, 'alexander': 3576, '4ed': 3577, 'knot': 3578, 'rule': 3579, 'gathering': 3580, 'shade': 3581, 'hibernate': 3582, 'mathematics': 3583, 'suck': 3584, \"i'll\": 3585, 'designed': 3586, 'monastery': 3587, 'came': 3588, 'darkness': 3589, 'tibetan': 3590, 'youthful': 3591, 'spear': 3592, 'pursuit': 3593, 'ferguson': 3594, 'vanya': 3595, 'don’t': 3596, 'scandal': 3597, 'theodore': 3598, 'boone': 3599, 'sentences': 3600, 'kumon': 3601, 'struts': 3602, 'acoustic': 3603, 'rgt': 3604, 'inc': 3605, 'बदलता': 3606, 'siddhartha': 3607, 'swami': 3608, 'helm': 3609, \"couple's\": 3610, 'farewell': 3611, 'odysseus': 3612, 'sudoku': 3613, 'troll': 3614, 'bjp’s': 3615, 'ian': 3616, 'jonathon': 3617, 'payne': 3618, 'kitchen': 3619, 'swim': 3620, 'scaling': 3621, 'shogun': 3622, 'components': 3623, 'devops': 3624, 'adaptive': 3625, 'becomes': 3626, 'ambedkar–gandhi': 3627, 'debate': 3628, 'fugitive': 3629, 'lore': 3630, 'facility': 3631, 'dear': 3632, 'feminist': 3633, 'homicidal': 3634, 'psycho': 3635, 'clash': 3636, 'fen': 3637, '72': 3638, 'intrepid': 3639, 'trade': 3640, 'shiva': 3641, 'taking': 3642, 'chanakya': 3643, 'wears': 3644, 'prada': 3645, 'kalam': 3646, 'survive': 3647, 'anticipated': 3648, 'survived': 3649, 'isro': 3650, 'ias': 3651, 'pelican': 3652, 'rhetoric': 3653, 'low': 3654, 'redefining': 3655, 'rabindranath': 3656, 'technologies': 3657, 'wellington': 3658, 'dutch': 3659, 'universities': 3660, 'paradigms': 3661, 'discourses': 3662, 'kalarippayattu': 3663, 'zlatan': 3664, 'rip': 3665, 'kirby': 3666, '1970': 3667, '1973': 3668, 'yet': 3669, 'bymariandrew': 3670, 'industry': 3671, 'experts': 3672, 'punisher': 3673, 'said': 3674, 'granta': 3675, 'valerian': 3676, 'fudge': 3677, 'collections': 3678, 'rest': 3679, 'immigration': 3680, 'dell': 3681, 'squat': 3682, 'barnes': 3683, 'walt': 3684, 'scrooge': 3685, 'duck': 3686, 'don': 3687, 'rosa': 3688, 'mla': 3689, \"prisoner's\": 3690, 'hunters': 3691, 'chirunning': 3692, 'nile': 3693, 'karmayogi': 3694, 'sreedharan': 3695, 'oppressive': 3696, 'liberating': 3697, 'cryptocurrencies': 3698, 'tai': 3699, 'enlightenment': 3700, 'acreenshots': 3701, 'guided': 3702, 'handholding': 3703, 'stealth': 3704, 'fixed': 3705, 'cash': 3706, 'corruption': 3707, 'wrestlemania': 3708, 'investigator': 3709, 'arkana': 3710, 'doomsday': 3711, 'bloke': 3712, 'directions': 3713, 'daisy': 3714, 'grylls': 3715, 'beethoven': 3716, 'byculla': 3717, 'bangkok': 3718, 'longest': 3719, 'handful': 3720, 'bookstores': 3721, 'buyers': 3722, 'booksellers': 3723, 'vajpayee': 3724, 'paradox': 3725, 'seductive': 3726, 'pickwick': 3727, 'zona': 3728, 'spots': 3729, 'ab': 3730, 'villiers': 3731, 'ceasefire': 3732, 'violations': 3733, 'india–pakistan': 3734, 'escalation': 3735, 'dynamics': 3736, 'calypso': 3737, 'targets': 3738, 'silhouette': 3739, 'bullseye': 3740, 'firearms': 3741, 'pistols': 3742, 'airsoft': 3743, 'bb': 3744, 'pellet': 3745, \"girl's\": 3746, 'fit': 3747, 'messiah': 3748, 'privacy': 3749, 'unlocking': 3750, 'honnold': 3751, 'lowercase': 3752, 'per': 3753, 'durable': 3754, 'matte': 3755, 'boots': 3756, 'sign': 3757, 'cook': 3758, 'hr': 3759, 'personnel': 3760, 'recruitment': 3761, 'b1': 3762, 'standalone': 3763, 'panda': 3764, 'snake': 3765, 'ninth': 3766, 'innovation': 3767, 'temptations': 3768, 'khaiye': 3769, 'aur': 3770, 'vajan': 3771, 'ghataiye': 3772, 'beckham': 3773, 'quintessential': 3774, 'quintuplets': 3775, 'thrills': 3776, 'molehills': 3777, 'usability': 3778, 'sanjay': 3779, 'wrote': 3780, 'palgrave': 3781, 'aisle': 3782, 'damned': 3783, 'explanatory': 3784, 'pahari': 3785, 'painters': 3786, 'northern': 3787, 'philosophers': 3788, 'wacky': 3789, 'iima': 3790, 'paying': 3791, 'monstress': 3792, 'cert': 3793, 'dynamite': 3794, 'openings': 3795, 'opening': 3796, 'schools': 3797, 'babel': 3798, 'diarrhe': 3799, 'middlegames': 3800, 'cadogan': 3801, 'pluto': 3802, 'urasawa': 3803, 'communicate': 3804, 'effectively': 3805, 'robotic': 3806, '6c': 3807, 'badami': 3808, 'aihole': 3809, 'pattadakal': 3810, 'jaico': 3811, 'deccan': 3812, 'guidebook': 3813, 'soulmates': 3814, 'reunited': 3815, \"heart's\": 3816, 'insider': 3817, 'memorize': 3818, 'enlighten': 3819, 'custodian': 3820, 'yogi': 3821, 'surrender': 3822, \"bridgman's\": 3823, 'allegiance': 3824, 'africa': 3825, 'refresh': 3826, 'microsoft’s': 3827, 'imagine': 3828, 'prepositions': 3829, 'soldier’s': 3830, 'kargil': 3831, 'atlas': 3832, 'shrugged': 3833, 'moab': 3834, 'washpot': 3835, 'rrb': 3836, 'ntpc': 3837, 'categories': 3838, 'preliminary': 3839, 'korean': 3840, 'graphics': 3841, 'interiors': 3842, 'firm': 3843, 'jose': 3844, 'technic': 3845, 'jeetnay': 3846, 'raaste': 3847, 'whirlwind': 3848, 'brawl': 3849, 'lakshmi': 3850, 'lawnmageddon': 3851, \"lahaul's\": 3852, 'enduring': 3853, 'initiation': 3854, '2017–2020': 3855, '1991': 3856, 'buffet': 3857, 'features': 3858, 'runner’s': 3859, 'environment': 3860, 'striking': 3861, 'multimedia': 3862, 'jana': 3863, 'natya': 3864, 'manch': 3865, 'draupadi': 3866, 'dinosaurs': 3867, \"irvine's\": 3868, 'ascent': 3869, 'decameron': 3870, 'poorly': 3871, 'curfew': 3872, 'endure': 3873, 'curiously': 3874, 'elastic': 3875, '307': 3876, 'assignments': 3877, \"friday's\": 3878, 'says': 3879, 'can’t': 3880, 'stopped': 3881, 'drinking': 3882, 'seabiscuit': 3883, 'racehorse': 3884, 'f2': 3885, 'surya': 3886, 'namaskar': 3887, 'chandragupta': 3888, 'charlie': 3889, 'elevator': 3890, 'dahl': 3891, 'workflow': 3892, \"ripley's\": 3893, 'emmie': 3894, 'dares': 3895, 'commando': 3896, 'tap': 3897, 'buffett': 3898, 'practically': 3899, '1966': 3900, 'prescription': 3901, 'vyavaharik': 3902, 'shabdkosh': 3903, 'fahrenheit': 3904, '451': 3905, 'steelheart': 3906, 'suits': 3907, 'oblivion': 3908, 'odds': 3909, 'fathers': 3910, 'sons': 3911, 'horrible': 3912, 'histories': 3913, 'cane': 3914, 'abe': 3915, 'onward': 3916, 'redux': 3917, 'shortest': 3918, \"mayor's\": 3919, 'ccent': 3920, 'merit': 3921, \"sophie's\": 3922, 'astonish': 3923, 'playful': 3924, 'powershift': 3925, 'violence': 3926, \"annie's\": 3927, 'bottle': 3928, 'differently': 3929, 'succeeding': 3930, 'lights': 3931, 'simran': 3932, 'brotherhood': 3933, 'sudha': 3934, '7ed': 3935, 'limit': 3936, 'hold': 3937, '\\x92em': 3938, 'emphasis': 3939, 'tough': 3940, 'sequels': 3941, 'sackett': 3942, 'angoor': 3943, 'quran': 3944, 'cate': 3945, 'austin': 3946, 'provence': 3947, 'disappearance': 3948, 'sally': 3949, 'sequeira': 3950, 'forbidden': 3951, 'patriots': 3952, 'mcginty’s': 3953, '1945': 3954, 'operations': 3955, 'britannia': 3956, 'tantri': 3957, 'mantri': 3958, 'wicked': 3959, 'wiles': 3960, 'spider’s': 3961, '5ed': 3962, 'cujo': 3963, 'ina': 3964, 'mynah': 3965, 'mo': 3966, \"scarecrows'\": 3967, 'unrevealed': 3968, 'hack': 3969, 'cracked': 3970, 'pathways': 3971, 'coluring': 3972, \"raven's\": 3973, 'sad': 3974, 'picaros': 3975, 'pursued': 3976, 'sauveterre': 3977, 'siblings': 3978, 'click': 3979, 'lois': 3980, 'clark': 3981, 'ashenden': 3982, 'trivia': 3983, 'anecdotes': 3984, 'partner': 3985, 'deceiver': 3986, 'yeh': 3987, 'dinoñ': 3988, 'ki': 3989, 'baat': 3990, 'hai': 3991, 'bobby': 3992, 'fischer': 3993, 'modeling': 3994, 'bracelet': 3995, 'accomplished': 3996, 'tolstoy': 3997, 'tootsie': 3998, 'sunlight': 3999, 'personalized': 4000, 'engines': 4001, 'clear': 4002, 'bro': 4003, 'skeleton': 4004, 'rulers': 4005, \"corelli's\": 4006, 'mandolin': 4007, 'rumpelstiltskin': 4008, 'marissa': 4009, 'mayer': 4010, 'yahoo': 4011, 'lacan': 4012, 'blunders': 4013, 'darwin': 4014, 'calculus': 4015, 'balls': 4016, 'spotlight': 4017, 'atuan': 4018, 'farthest': 4019, 'shore': 4020, 'tehanu': 4021, 'codex': 4022, 'calcutta': 4023, 'madame': 4024, 'bovary': 4025, 'chords': 4026, 'cadences': 4027, 'sculpt': 4028, 'pilates': 4029, 'brains': 4030, 'embroideries': 4031, 'nooyi': 4032, 'mouthful': 4033, 'passengers': 4034, '1c': 4035, 'beckoning': 4036, 'isle': 4037, 'reach': 4038, 'blunder': 4039, 'raiser': 4040, 'sino': 4041, '1962': 4042, 'fanatics': 4043, 'mushy': 4044, 'lump': 4045, 'tall': 4046, 'genji': 4047, 'abridged': 4048, 'recommend': 4049, 'spark': 4050, 'orphans': 4051, 'elixir': 4052, 'ruin': 4053, 'virginity': 4054, 'blanc': 4055, '1920': 4056, 'pr': 4057, 'paperbacks': 4058, '1950–1989': 4059, 'geek': 4060, 'heresy': 4061, 'heidegger': 4062, 'hippo': 4063, 'pearly': 4064, 'wandering': 4065, 'blinding': 4066, 'knife': 4067, 'stark': 4068, 'raving': 4069, 'giddy': 4070, 'ads': 4071, 'treasured': 4072, 'bazaar': 4073, \"amul's\": 4074, 'amul': 4075, 'teaching': 4076, 'bk': 4077, 'abpl': 4078, 'rupi': 4079, 'kaur': 4080, 'iambic': 4081, 'pentameter': 4082, 'blank': 4083, 'brionne': 4084, 'educated': 4085, 'hallowe’en': 4086, 'mask': 4087, 'enchantress': 4088, 'moveable': 4089, 'samarkand': 4090, 'bartimaeus': 4091, 'sequence': 4092, 'montana': 4093, 'sleep': 4094, \"morgenstern's\": 4095, 'fluent': 4096, 'miranda': 4097, 'castaway': 4098, 'codes': 4099, 'ciphers': 4100, 'jurgen': 4101, 'klopp': 4102, 'lips': 4103, 'writes': 4104, 'ajmer': 4105, 'sharif': 4106, 'sufism': 4107, 'aap': 4108, 'insider’s': 4109, 'controversial': 4110, 'creatively': 4111, 'apples': 4112, 'events': 4113, 'mention': 4114, 'exposure': 4115, 'paninian': 4116, \"titan's\": 4117, 'fleas': 4118, 'querkles': 4119, 'pest': 4120, 'saints': 4121, 'significance': 4122, 'englishspeaking': 4123, 'kwai': 4124, \"emperor's\": 4125, 'concerning': 4126, 'landmark': 4127, 'mithya': 4128, 'decoding': 4129, 'department': 4130, 'sensitive': 4131, 'varg': 4132, 'push': 4133, \"climber's\": 4134, 'friday': 4135, '127': 4136, 'achievement': 4137, 'imax': 4138, \"prince's\": 4139, 'vow': 4140, 'untouched': 4141, \"programmer's\": 4142, \"he's\": 4143, 'haar': 4144, 'mightier': 4145, 'fuzzy': 4146, 'synthesis': 4147, 'hardcastle': 4148, 'costa': 4149, 'daredevil': 4150, 'pair': 4151, 'trescothick': 4152, 'retrospect': 4153, 'prospect': 4154, 'barca': 4155, 'ruskin': 4156, 'selections': 4157, 'spent': 4158, 'minimalism': 4159, 'washi': 4160, '96': 4161, 'stream': 4162, 'himself': 4163, 'slave': 4164, 'psy': 4165, 'changeling': 4166, 'underground': 4167, 'message': 4168, 'defender': 4169, 'dharma': 4170, 'ravana': 4171, 'roar': 4172, 'requirements': 4173, 'roles': 4174, 'osho': 4175, 'dimension': 4176, 'doubleday': 4177, 'cardinal': 4178, 'kremlin': 4179, \"kakashi's\": 4180, 'request': 4181, 'addiction': 4182, 'vices': 4183, 'whiskey': 4184, 'shovel': 4185, '89': 4186, 'dials': 4187, 'zelda': 4188, 'artifacts': 4189, 'horror': 4190, 'bastard': 4191, 'apps': 4192, 'novella': 4193, 'samaithu': 4194, 'paar': 4195, 'renowned': 4196, 'clots': 4197, 'killed': 4198, 'karkare': 4199, 'bookshop': 4200, 'dongri': 4201, 'dubai': 4202, 'decades': 4203, 'mafia': 4204, 'shaw': 4205, 'katie': 4206, 'ipl': 4207, 'polio': 4208, 'eradication': 4209, 'ceo': 4210, 'spacex': 4211, 'shaping': 4212, \"socrate's\": 4213, 'piccadilly': 4214, 'motorbike': 4215, 'remains': 4216, '1989': 4217, 'populism': 4218, 'dreaming': 4219, '732': 4220, 'bugs': 4221, 'transcendent': 4222, 'thierry': 4223, 'henry': 4224, 'kalpana': 4225, 'chawla': 4226, 'mila': 4227, 'infatuation': 4228, 'ayurveda': 4229, 'tucker': 4230, 'wayne': 4231, 'dragonfly': 4232, 'amber': 4233, 'voyager': 4234, 'bombshell': 4235, 'czech': 4236, 'routledge': 4237, 'grammars': 4238, \"'c'\": 4239, 'cousins': 4240, 'o’dwyer': 4241, \"guardiola's\": 4242, 'bayern': 4243, 'clan': 4244, \"earth's\": 4245, 'typographic': 4246, 'maintenance': 4247, '40th': 4248, 'playboy': 4249, \"cordina's\": 4250, 'cordina': 4251, '1996': 4252, \"everest's\": 4253, \"survivor's\": 4254, 'uncover': 4255, 'discoveries': 4256, 'masterminds': 4257, 'firefight': 4258, 'gaza': 4259, 'sketch': 4260, \"newlywed's\": 4261, 'troubleshooting': 4262, \"owner's\": 4263, 'celtic': 4264, 'berserk': 4265, 'disclosure': 4266, 'labor': 4267, 'had': 4268, 'jaane': 4269, 'bhi': 4270, 'yaaron': 4271, 'since1983': 4272, 'marrow': 4273, 'nei': 4274, 'rejuvenating': 4275, 'dissenting': 4276, 'virtual': 4277, 'mcgraw': 4278, \"hill's\": 4279, 'medium': 4280, 'attachments': 4281, 'optimism': 4282, 'despair': 4283, 'fundamental': 4284, 'endings': 4285, 'endgame': 4286, 'encyclopaedia': 4287, 'mohenjodaro': 4288, 'mapr': 4289, 'streams': 4290, 'symbol': 4291, 'rennaissance': 4292, 'decline': 4293, 'radha': 4294, 'sighs': 4295, 'patrik': 4296, 'hedstrom': 4297, 'erica': 4298, 'falck': 4299, 'rhythm': 4300, 'indulekha': 4301, 'empress': 4302, 'nur': 4303, 'jahan': 4304, 'feminists': 4305, 'prelude': 4306, 'ogilvy': 4307, 'marley': 4308, '1189': 4309, '1868': 4310, 'hana': 4311, 'picks': 4312, 'en': 4313, 'plein': 4314, 'jimmy': 4315, 'corrigan': 4316, 'smartest': 4317, 'responsibility': 4318, 'moonfleet': 4319, 'cityscapes': 4320, 'locations': 4321, 'patternmaking': 4322, 'exchange': 4323, 'painful': 4324, 'brings': 4325, 'icons': 4326, 'berries': 4327, 'immediately': 4328, 'kavalier': 4329, 'blankets': 4330, 'brazil': 4331, 'camping': 4332, \"bodybuilder's\": 4333, 'ironman': 4334, 'echo': 4335, 'holidays': 4336, '1500': 4337, 'enhancement': 4338, 'rocket': 4339, 'thirteenth': 4340, 'prepares': 4341, 'deserts': 4342, 'mirrors': 4343, 'seek': 4344, 'expertise': 4345, 'babaji': 4346, 'siddah': 4347, 'kriya': 4348, 'cultures': 4349, 'swift': 4350, 'iced': 4351, 'fever': 4352, '3a': 4353, 'papyrus': 4354, 'bringing': 4355, 'discovers': 4356, 'moneyball': 4357, 'unfair': 4358, 'labour': 4359, 'pets': 4360, 'mouth': 4361, \"won't\": 4362, 'ink': 4363, 'strucutre': 4364, 'judo': 4365, 'jojutsu': 4366, 'boxing': 4367, 'cha': 4368, 'iconography': 4369, 'iconometry': 4370, 'homecoming': 4371, 'monument': 4372, 'psychedelics': 4373, 'accounting': 4374, 'autism': 4375, 'kills': 4376, '8b': 4377, 'stock': 4378, 'abrar': 4379, \"alvi's\": 4380, 'marquez': 4381, 'ludlum': 4382, 'luis': 4383, 'gerrard': 4384, 'idol': 4385, 'brida': 4386, 'wired': 4387, \"monk's\": 4388, 'bardos': 4389, 'helen': 4390, 'badass': 4391, 'motivation': 4392, 'vibe': 4393, 'ideology': 4394, 'pigeons': 4395, 'supercar': 4396, 'elevation': 4397, 'dibs': 4398, 'baliah': 4399, 'talented': 4400, 'ripley': 4401, 'pcm': 4402, 'alan': 4403, 'mulally': 4404, 'motor': 4405, 'companies': 4406, 'used': 4407, 'foe': 4408, 'beating': 4409, 'majestic': 4410, 'dhirubhai': 4411, 'ambani': 4412, 'secondary': 4413, 'checkpoints': 4414, 'jesus': 4415, 'wrangling': 4416, 'pandas': 4417, 'numpy': 4418, 'ipython': 4419, 'outside': 4420, 'disruptive': 4421, \"here's\": 4422, 'earliest': 4423, 'sources': 4424, 'ghostman': 4425, 'queenie': 4426, 'hennessy': 4427, 'sent': 4428, 'harold': 4429, 'defector': 4430, 'dribbling': 4431, 'minis': 4432, 'stillness': 4433, 'explaining': 4434, 'hadoop': 4435, 'athabasca': 4436, 'seventeen': 4437, 'block': 4438, 'spectaculars': 4439, 'orange': 4440, 'madding': 4441, 'billy': 4442, 'milligan': 4443, 'volatile': 4444, 'baldacci': 4445, 'camel': 4446, 'erased': 4447, \"schindler's\": 4448, 'list': 4449, 'talks': 4450, \"poseidon's\": 4451, 'moment': 4452, 'lift': 4453, 'empowering': 4454, 'changes': 4455, 'carlin': 4456, 'orgy': 4457, 'vyasa': 4458, 'grid': 4459, 'xl': 4460, 'xs': 4461, \"guru's\": 4462, 'bank': 4463, 'ugc': 4464, 'slet': 4465, 'crowds': 4466, 'obama': 4467, 'hopey': 4468, 'changey': 4469, 'prizes': 4470, 'switzerland': 4471, 'swallowed': 4472, 'grokking': 4473, 'imperium': 4474, 'cicero': 4475, '300': 4476, 'winners': 4477, 'superintelligence': 4478, 'paths': 4479, 'dangers': 4480, 'flips': 4481, 'seo': 4482, 'traffic': 4483, 'dumb': 4484, 'mob': 4485, 'buy': 4486, 'ou': 4487, 'tsmart': 4488, 'selling': 4489, 'glance': 4490, 'combined': 4491, 'ebooks': 4492, 'flavour': 4493, 'spice': 4494, 'supermarket': 4495, 'cartridges': 4496, 'experiences': 4497, 'opinions': 4498, 'landfall': 4499, \"smallwood's\": 4500, 'tutor': 4501, 'korea': 4502, 'memorable': 4503, 'scarlatti': 4504, 'marathoning': 4505, 'horimiya': 4506, 'masterwork': 4507, 'otherworldy': 4508, 'minifigure': 4509, 'iniesta': 4510, 'millennials': 4511, 'previously': 4512, 'feather': 4513, 'overcome': 4514, 'raksha': 4515, 'bandhan': 4516, 'kohli': 4517, 'butterfly': 4518, 'followers': 4519, 'truly': 4520, 'madly': 4521, 'riverdale': 4522, 'mickey': 4523, 'haller': 4524, 'kirstin': 4525, 'cameras': 4526, 'edit': 4527, 'wish': 4528, 'wimpy': 4529, 'undead': 4530, 'expats': 4531, 'catching': 4532, 'bakes': 4533, 'ares': 4534, 'decision': 4535, 'dior': 4536, 'memoire': 4537, 'current': 4538, 'middlemarch': 4539, 'blackest': 4540, 'staff': 4541, 'earths': 4542, 'vimal': 4543, 'kaise': 4544, 'bani': 4545, 'saffron': 4546, 'material': 4547, 'istanbul': 4548, 'sangeetha': 4549, 'paadam': 4550, 'wretched': 4551, 'summons': 4552, 'suitcase': 4553, 'longer': 4554, 'compositions': 4555, 'tweet': 4556, \"'the\": 4557, \"story'\": 4558, 'louise': 4559, 'pentland': 4560, 'handcrafted': 4561, 'kali': 4562, 'ethical': 4563, \"hacker's\": 4564, 'gravity': 4565, 'explains': 4566, 'kerry': 4567, \"packer's\": 4568, 'warlight': 4569, 'neuromancer': 4570, 'sprawl': 4571, 'hostage': 4572, 'controlling': 4573, 'element': 4574, 'con': 4575, 'copperplate': 4576, 'typography': 4577, 'bolivian': 4578, 'commandos': 4579, 'immortal': 4580, 'henrietta': 4581, 'lacks': 4582, 'supercharge': 4583, 'hunger': 4584, 'canons': 4585, 'undergraduate': 4586, 'various': 4587, 'accident': 4588, \"lenin's\": 4589, 'heroine': 4590, 'chen': 4591, 'heard': 4592, 'radio': 4593, 'zita': 4594, 'spacegirl': 4595, 'das': 4596, 'kapital': 4597, 'unwritten': 4598, 'today': 4599, 'reveals': 4600, 'harperresource': 4601, 'cursed': 4602, 'officl': 4603, 'advances': 4604, 'inquiry': 4605, 'gentle': 4606, 'instead': 4607, 'telling': 4608, 'kodaikanal': 4609, 'poison': 4610, 'cs5': 4611, 'hegemony': 4612, 'access': 4613, \"delavier's\": 4614, 'size': 4615, 'meanings': 4616, 'breathe': 4617, 'fell': 4618, 'potion': 4619, 'unfolding': 4620, 'fifa': 4621, 'merriam': 4622, 'webster': 4623, 'dress': 4624, 'corduroy': 4625, 'denim': 4626, \"storyteller's\": 4627, 'speakers': 4628, 'passion': 4629, '17e': 4630, \"indian's\": 4631, 'origins': 4632, 'prehuman': 4633, 'orient': 4634, 'myron': 4635, 'bolitar': 4636, 'cricketing': 4637, 'camb': 4638, 'lalla': 4639, 'lal': 4640, 'ded': 4641, 'vinegar': 4642, 'hogarth': 4643, 'incessant': 4644, 'crescent': 4645, \"giovanni's\": 4646, 'would': 4647, 'marx': 4648, \"barcelona's\": 4649, 'strikeforce': 4650, 'mix': 4651, 'tape': 4652, 'ray': 4653, 'ignore': 4654, 'az': 4655, '103': 4656, 'isis': 4657, 'fingerstyle': 4658, 'songbooks': 4659, \"astronaut's\": 4660, 'charandas': 4661, 'chor': 4662, 'playwrights': 4663, \"maharaja's\": 4664, 'household': 4665, \"daughter's\": 4666, 'harvest': 4667, 'hbj': 4668, 'buried': 4669, 'sherpa': 4670, 'climbers': 4671, 'k2′s': 4672, 'deadliest': 4673, 'sadhguru': 4674, 'goldfinger': 4675, 'combative': 4676, 'edgware': 4677, 'dies': 4678, 'didi': 4679, 'mamata': 4680, 'banerjee': 4681, 'lateral': 4682, 'hey': 4683, 'kiddo': 4684, \"'one\": 4685, \"will'\": 4686, 'waugh': 4687, 'wide': 4688, 'feeding': 4689, 'frenzy': 4690, '46': 4691, 'deluding': 4692, 'khasak': 4693, 'daredreamers': 4694, 'gooseberries': 4695, 'mansions': 4696, 'rosy': 4697, 'relative': 4698, 'myselfie': 4699, 'calmer': 4700, 'arkham': 4701, 'asylum': 4702, 'normans': 4703, 'willows': 4704, 'sweden': 4705, 'spss': 4706, 'mamba': 4707, 'cyanide': 4708, 'happines': 4709, 'finger': 4710, \"historian's\": 4711, 'watches': 4712, 'aspects': 4713, 'types': 4714, 'dusty': 4715, 'shindig': 4716, 'yuvi': 4717, 'sundown': 4718, 'hogwarts': 4719, 'outlines': 4720, \"bishop's\": 4721, 'pawn': 4722, 'roll': 4723, '600': 4724, 'gags': 4725, 'unti': 4726, 'doll': 4727, 'factory': 4728, 'psychological': 4729, 'bakemonogatari': 4730, 'corfu': 4731, 'spivet': 4732, 'roars': 4733, 'geologist': 4734, 'collecting': 4735, 'identifying': 4736, 'mega': 4737, 'mazes': 4738, 'sandstorm': 4739, 'aandhi': 4740, 'triathlon': 4741, 'hostile': 4742, 'takeover': 4743, 'halo': 4744, 'coraline': 4745, 'submarine': 4746, \"hogan's\": 4747, 'tables': 4748, 'salt': 4749, 'protocol': 4750, 'unbelievable': 4751, 'warned': 4752, 'linguistics': 4753, 'semantics': 4754, 'genghis': 4755, 'virgin': 4756, 'classxi': 4757, 'xii': 4758, 'bold': 4759, 'conspirator': 4760, 'polar': 4761, 'shift': 4762, 'kidding': 4763, 'swimmer': 4764, 'eadweard': 4765, 'muybridge': 4766, 'technological': 4767, 'bliss': 4768, 'impatient': 4769, 'optimist': 4770, 'bill': 4771, 'funeral': 4772, 'singles': 4773, 'paraphrase': 4774, 'lullabies': 4775, 'promises': 4776, 'above': 4777, 'mountaineer': 4778, 'bowl': 4779, 'cherries': 4780, 'pits': 4781, 'bind': 4782, 'gag': 4783, 'pepper': 4784, 'chanel': 4785, 'calisthenics': 4786, 'sir': 4787, 'cumference': 4788, 'tens': 4789, 'aventures': 4790, 'farsi': 4791, 'persian': 4792, 'eminent': 4793, 'untamed': 4794, 'wel': 4795, 'effects': 4796, 'may': 4797, 'vary': 4798, 'explainer': 4799, 'uncertain': 4800, 'contradictions': 4801, 'intertwined': 4802, 'n': 4803, 'haksar': 4804, 'applied': 4805, 'motherwit': 4806, 'dragonball': 4807, 'shambhala': 4808, 'healed': 4809, 'warriors': 4810, 'hatching': 4811, 'twitter': 4812, \"'impossible\": 4813, \"down'\": 4814, 'fawley': 4815, 'soothsayer': 4816, 'americanah': 4817, 'centurions': 4818, 'beer': 4819, 'paints': 4820, 'inks': 4821, 'markers': 4822, 'glitter': 4823, 'landline': 4824, 'mastered': 4825, 'rajendra': 4826, 'chola': 4827, 'alchemy': 4828, 'parker': 4829, 'pyne': 4830, 'intolerant': 4831, 'japan': 4832, 'proper': 4833, 'mankind': 4834, '2008': 4835, 'devolved': 4836, 'ux': 4837, 'agile': 4838, 'applying': 4839, 'iterative': 4840, 'webley': 4841, '1925': 4842, '2005': 4843, 'prosecution': 4844, \"neurosurgeon's\": 4845, 'snapshots': 4846, 'mba': 4847, 'hurting': 4848, 'men’s': 4849, 'feeds': 4850, 'jakes': 4851, 'cooking': 4852, 'saudi': 4853, 'arabia': 4854, 'speedsolving': 4855, 'follow': 4856, 'file': 4857, 'notice': 4858, 'belongs': 4859, \"we're\": 4860, '339': 4861, 'jaw': 4862, 'hung': 4863, 'gar': 4864, 'awaken': 4865, 'unleash': 4866, 'manipulation': 4867, 'peep': 4868, 'secure': 4869, 'storyboarding': 4870, 'scad': 4871, 'translate': 4872, 'insults': 4873, 'ruby': 4874, 'harappa': 4875, '1977': 4876, 'later': 4877, 'sum': 4878, 'kisses': 4879, 'smythe': 4880, 'smith': 4881, 'prem': 4882, 'purana': 4883, 'mythological': 4884, 'daybreakers': 4885, 'coral': 4886, 'reef': 4887, 'standardized': 4888, 'varnikaa': 4889, 'mermaids': 4890, 'singing': 4891, 'carol': 4892, 'enquiry': 4893, 'chrysanthemum': 4894, 'pools': 4895, 'trading': 4896, 'looming': 4897, 'stressed': 4898, 'mani': 4899, 'ratnam': 4900, 'mp3': 4901, 'liar': 4902, '116': 4903, 'sleights': 4904, '236': 4905, 'instructive': 4906, '62': 4907, 'luckiest': 4908, 'sicilian': 4909, 'alignment': 4910, 'strategic': 4911, 'tune': 4912, 'dhruv': 4913, 'designer′s': 4914, 'illustrator': 4915, 'cast': 4916, 'excuse': 4917, 'opposite': 4918, 'attract': 4919, 'ouch': 4920, 'airgunning': 4921, 'divorced': 4922, 'environmentalism': 4923, \"murphy's\": 4924, 'counsel': 4925, 'jaitley': 4926, 'screwtape': 4927, 'lewis': 4928, 'signature': 4929, 'timepass': 4930, 'protima': 4931, 'bedi': 4932, 'mystics': 4933, 'devil’s': 4934, 'punchbowl': 4935, 'penn': 4936, 'upstart': 4937, 'visionary': 4938, 'mm': 4939, 'invoice': 4940, 'verification': 4941, 'kannur': 4942, 'bloodiest': 4943, 'boundary': 4944, 'godel': 4945, 'escher': 4946, 'skibbereen': 4947, 'devotion': 4948, 'wiki': 4949, 'leaks': 4950, 'julian': 4951, \"assange's\": 4952, 'sh': 4953, 'adi': 4954, 'parva': 4955, 'churning': 4956, 'microeconomics': 4957, 'ants': 4958, 'scarlet': 4959, 'pimpernel': 4960, 'aziloth': 4961, 'priorities': 4962, 'chosen': 4963, 'vector': 4964, 'saris': 4965, 'hiragana': 4966, 'katakana': 4967, 'didn’t': 4968, 'evans': 4969, \"mind's\": 4970, \"liar's\": 4971, 'durban': 4972, 'altered': 4973, 'carbon': 4974, 'gollancz': 4975, 'feathers': 4976, 'reveal': 4977, 'ruby’s': 4978, 'worry': 4979, 'paharganj': 4980, 'hospitals': 4981, 'facilities': 4982, 'chikankari': 4983, 'lucknawi': 4984, 'defending': 4985, 'jacob': 4986, 'kernel': 4987, \"she's\": 4988, 'station': 4989, 'zebra': 4990, 'wednesdays': 4991, 'horizons': 4992, 'html5': 4993, 'bonsai': 4994, 'constitutional': 4995, 'godfather': 4996, 'preparedness': 4997, 'chiwalking': 4998, 'lifelong': 4999, 'locke': 5000, 'lamora': 5001, 'storage': 5002, 'postman': 5003, 'indemnity': 5004, 'mildred': 5005, 'pierce': 5006, 'lenin': 5007, 'jurassic': 5008, 'minimum': 5009, 'physical': 5010, 'tardis': 5011, 'django': 5012, 'scratch': 5013, 'bard': 5014, 'birdsong': 5015, 'mechanical': 5016, 'mayor': 5017, 'casterbridge': 5018, 'mice': 5019, 'relentless': 5020, 'brian': 5021, 'weiss': 5022, 'equality': 5023, 'cking': 5024, 'instant': 5025, 'airgun': 5026, 'equipment': 5027, 'preston': 5028, 'blair': 5029, 'animate': 5030, 'paint': 5031, 'novellas': 5032, 'kalaripayat': 5033, 'microcontrollers': 5034, '8085': 5035, '8086': 5036, '8051': 5037, '8096': 5038, 'controversially': 5039, 'homeport': 5040, 'pooh': 5041, 'hedge': 5042, 'also': 5043, 'unix': 5044, 'shell': 5045, 'westaway': 5046, 'fernao': 5047, 'lopes': 5048, 'helena': 5049, 'giraffe': 5050, 'goa': 5051, 'subtle': 5052, 'energies': 5053, 'placement': 5054, 'fraudster': 5055, 'ninja': 5056, 'shinrin': 5057, 'yoku': 5058, 'bathing': 5059, 'fractals': 5060, 'sycamore': 5061, 'row': 5062, 'worshipping': 5063, 'mike': 5064, 'mentzer': 5065, '714': 5066, 'sydney': 5067, 'clover': 5068, 'parallel': 5069, 'essay': 5070, 'acceptance': 5071, 'iceberg': 5072, 'confessed': 5073, 'glenn': 5074, 'mcgrath': 5075, \"mi6's\": 5076, 'al': 5077, 'qaeda': 5078, 'assurance': 5079, 'burqa': 5080, 'prominent': 5081, 'psychiatrist': 5082, 'longform': 5083, 'pamela': 5084, 'ludlum’s': 5085, 'cassandra': 5086, 'revisi': 5087, 'function': 5088, '5000': 5089, 'motifs': 5090, 'texas': 5091, 'ranger': 5092, 'dothraki': 5093, 'conversational': 5094, 'parenthood': 5095, 'willing': 5096, 'pinkalicious': 5097, 'fair': 5098, 'bastar': 5099, \"arnold's\": 5100, 'sugar': 5101, 'corrupted': 5102, 'slavery': 5103, 'obesity': 5104, 'roulette': 5105, \"putin's\": 5106, '59': 5107, \"years'\": 5108, '1919': 5109, 'brodie': 5110, 'q': 5111, 'iteration': 5112, 'gradients': 5113, 'trpo': 5114, 'alphago': 5115, 'papillon': 5116, 'hercule': 5117, 'parsifal': 5118, 'mosaic': 5119, 'queer': 5120, 'jiya': 5121, 'jale': 5122, 'songs': 5123, 'inappropriate': 5124, 'gross': 5125, 'embarrassing': 5126, \"baby's\": 5127, 'assets': 5128, 'aged': 5129, 'commemorating': 5130, 'institution': 5131, 'conflicts': 5132, 'interest': 5133, 'bloodlands': 5134, 'aarushi': 5135, \"alice's\": 5136, 'altar': 5137, 'marrying': 5138, 'homo': 5139, 'deus': 5140, 'enlightened': 5141, 'johnny': 5142, 'skull': 5143, 'personalities': 5144, 'nr': 5145, 'narayana': 5146, 'murthy': 5147, 'cheats': 5148, 'frauds': 5149, 'fraud': 5150, 'fragile': 5151, 'surgeon’s': 5152, 'malware': 5153, 'dissecting': 5154, 'malicious': 5155, 'unseen': 5156, 'scavengers': 5157, 'sehmat': 5158, 'enchanted': 5159, 'ashoka': 5160, \"kommandant's\": 5161, 'pudding': 5162, 'englishwale': 5163, 'com': 5164, 'hypothesis': 5165, 'rai': 5166, 'bahadur': 5167, 'mohan': 5168, 'oberoi': 5169, 'attention': 5170, 'merchants': 5171, 'mattered': 5172, 'straight': 5173, 'hermit': 5174, 'shivaji': 5175, 'vile': 5176, 'repertoire': 5177, 'troy': 5178, 'bow': 5179, 'shack': 5180, 'jun': 5181, 'bidding': 5182, 'fireside': 5183, \"noah's\": 5184, 'hindustani': 5185, 'transition': 5186, 'rachels': 5187, 'shifting': 5188, 'trends': 5189, 'otherwise': 5190, 'arduino': 5191, 'workshop': 5192, 'possessing': 5193, 'panchayati': 5194, 'youtube': 5195, 'gamer': 5196, 'popularmmos': 5197, 'css3': 5198, 'xml': 5199, 'xhtml': 5200, 'ajax': 5201, 'robotics': 5202, 'runaways': 5203, 'unwanted': 5204, 'tornado': 5205, 'sieges': 5206, 'mid': 5207, 'trumper': 5208, 'vallabhbhai': 5209, 'ownership': 5210, 'navy': 5211, 'seals': 5212, \"gulliver's\": 5213, 'madras': 5214, 'pregnant': 5215, 'kavakos': 5216, 'riveting': 5217, '104': 5218, 'comdex': 5219, '9c': 5220, 'enjoying': 5221, 'numerical': 5222, 'computations': 5223, 'ecosystem': 5224, 'structured': 5225, 'pavilions': 5226, 'spirits': 5227, 'fairies': 5228, 'genies': 5229, 'goddesses': 5230, 'programmability': 5231, 'engineer': 5232, '696': 5233, 'internals': 5234, 'birthday': 5235, 'dororo': 5236, 'counts': 5237, 'unblock': 5238, 'jules': 5239, 'verne': 5240, 'menagerie': 5241, 'myster': 5242, 'pte': 5243, 'testbuilder': 5244, 'testbuilders': 5245, 'screenplays': 5246, 'ourselves': 5247, 'eats': 5248, 'shoots': 5249, 'commutiny': 5250, 'sparking': 5251, 'youth': 5252, 'biographies': 5253, 'almanac': 5254, 'infographics': 5255, 'bhagat': 5256, 'orpheus': 5257, 'descent': 5258, 'assamese': 5259, 'perversions': 5260, 'heredity': 5261, 'championship': 5262, 'punching': 5263, 'aggressive': 5264, 'android': 5265, 'markets': 5266, 'dealer': 5267, 'sphere': 5268, 'lhasa': 5269, 'scientist': 5270, 'opencv': 5271, 'psychopath': 5272, 'romanian': 5273, 'guesstimation': 5274, 'cocktail': 5275, 'dublin': 5276, 'squad': 5277, 'marginal': 5278, 'gains': 5279, 'sadhu': 5280, 'judas': 5281, 'architecting': 5282, 'implementing': 5283, 'sensors': 5284, 'volcano': 5285, 'surviving': 5286, \"surgeon's\": 5287, 'sinbad': 5288, 'misadventures': 5289, 'butterfingers': 5290, 'malabar': 5291, \"filmmaker's\": 5292, \"scientist's\": 5293, 're': 5294, 'plasticity': 5295, 'metal': 5296, 'splendid': 5297, 'suns': 5298, 'breed': 5299, 'panic': 5300, 'infidel': 5301, 'finishing': 5302, 'jawaharlal': 5303, 'ministers': 5304, '1963': 5305, 'commandment': 5306, 'trace': 5307, 'nepali': 5308, 'went': 5309, 'punctuation': 5310, 'fathom': 5311, 'caged': 5312, 'sings': 5313, 'vmc': 5314, 'baked': 5315, 'realist': 5316, 'painter': 5317, 'gurney': 5318, 'scikit': 5319, '41': 5320, 'bridget': 5321, 'singleton': 5322, 'sikh': 5323, 'riots': 5324, 'drooling': 5325, 'scoop': 5326, 'herrings': 5327, 'doodles': 5328, 'extrovert': 5329, 'figuring': 5330, 'privatization': 5331, 'pollution': 5332, 'hammurabi': 5333, 'sterling': 5334, 'shakespeare’s': 5335, 'sonnets': 5336, 'twist': 5337, 'meghalaya': 5338, 'rivers': 5339, 'fishy': 5340, 'entrepreneurial': 5341, 'debunking': 5342, 'sector': 5343, 'chevalier': 5344, 'overconsumption': 5345, 'communities': 5346, 'unmade': 5347, 'automate': 5348, 'boring': 5349, '10s': 5350, 'bang': 5351, 'holes': 5352, 'strings': 5353, 'brass': 5354, 'chatbots': 5355, 'speech': 5356, 'recognition': 5357, \"fowler's\": 5358, 'fortress': 5359, 'democracy’s': 5360, 'sourdough': 5361, 'enormous': 5362, 'turnip': 5363, 'brick': 5364, 'shortlisted': 5365, 'socrates': 5366, 'philosopher': 5367, 'vienna': 5368, 'kama': 5369, 'gothic': 5370, 'indestructibles': 5371, 'rumble': 5372, 'measure': 5373, 'unadulterated': 5374, '7b': 5375, 'editors': 5376, 'briefs': 5377, 'watsons': 5378, 'sanditon': 5379, '2c': 5380, 'cheaper': 5381, 'babur': 5382, 'cholera': 5383, 'munch': 5384, 'krüger': 5385, 'pigs': 5386, 'mindstorms': 5387, 'pretzels': 5388, 'grownups': 5389, 'rain': 5390, 'adultery': 5391, 'testosterone': 5392, 'predicament': 5393, 'thieme': 5394, 'charter': 5395, 'statute': 5396, 'voting': 5397, 'procedure': 5398, 'nearly': 5399, 'scandinavian': 5400, 'utopia': 5401, 'kolkata': 5402, 'police': 5403, \"midsummer's\": 5404, 'equation': 5405, 'plug': 5406, 'drug': 5407, 'anonymous': 5408, 'trials': 5409, 'faf': 5410, 'scottish': 5411, 'brooklyn': 5412, 'ranga': 5413, 'roopa': 5414, 'images': 5415, 'bedroom': 5416, 'courtroom': 5417, 'agra': 5418, 'comebacks': 5419, 'soup': 5420, 'couples': 5421, '55': 5422, 'specific': 5423, '2023': 5424, 'planner': 5425, 'calendar': 5426, 'graveyard': 5427, 'confessor': 5428, 'obliged': 5429, 'visit': 5430, 'fakir': 5431, 'trapped': 5432, 'ikea': 5433, 'wardrobe': 5434, 'graded': 5435, 'visitor': 5436, 'visualizing': 5437, 'concept': 5438, 'wiese': 5439, 'productions': 5440, 'integrated': 5441, 'versions': 5442, 'upto': 5443, 'museum': 5444, 'targaryen': 5445, 'scribed': 5446, 'archmaester': 5447, 'gyldayn': 5448, 'heartbeat': 5449, \"bob's\": 5450, 'burger': 5451, 'dirty': 5452, 'sick': 5453, 'rated': 5454, 'politically': 5455, 'wren': 5456, \"martin's\": 5457, 'regular': 5458, 'encouters': 5459, \"tintin's\": 5460, 'nighthawk': 5461, 'containers': 5462, 'openstack': 5463, 'audacity': 5464, 'reclaiming': 5465, 'tomie': 5466, 'blasts': 5467, 'handy': 5468, '904': 5469, 'vikram': 5470, 'sarabhai': 5471, 'armada': 5472, 'ty': 5473, 'crucible': 5474, 'cezanne': 5475, 'provençal': 5476, 'cybersecurity': 5477, 'space': 5478, 'traditional': 5479, 'costs': 5480, 'caribbean': 5481, 'dna': 5482, 'apple': 5483, 'tech': 5484, 'fine': 5485, 'hammer': 5486, 'egil': 5487, 'nix': 5488, 'sorting': 5489, 'mel': 5490, \"bay's\": 5491, 'ace': 5492, 'kristys': 5493, 'realizing': 5494, 'villages': 5495, '047': 5496, 'verdict': 5497, 'led': 5498, 'torrents': 5499, 'honor': 5500, 'passing': 5501, 'icnd2': 5502, '44': 5503, '1933': 5504, 'won': 5505, 'julia': 5506, 'donaldson': 5507, 'lydia': 5508, 'monks': 5509, 'abs': 5510, 'superhuman': 5511, 'rust': 5512, 'bronte': 5513, 'paddington': 5514, 'advancing': 5515, 'guitarist': 5516, 'imran': 5517, 'celebrity': 5518, 'ogden': 5519, 'nash': 5520, 'eloquence': 5521, 'phrase': 5522, 'stalking': 5523, 'ripper': 5524, 'punjab': 5525, \"shakespeare's\": 5526, 'inimitable': 5527, 'intervention': 5528, 'sri': 5529, 'lanka': 5530, 'ipkf': 5531, 'boxes': 5532, 'hacked': 5533, 'moscow': 5534, '359': 5535, 'mathematical': 5536, 'recreations': 5537, 'recreational': 5538, 'comfort': 5539, 'feck': 5540, 'perfuction': 5541, 'amd': 5542, 'ahmedabad': 5543, 'quimby': 5544, \"pilgrim's\": 5545, 'moody': 5546, 'oceans': 5547, 'joint': 5548, 'bottom': 5549, \"richard's\": 5550, 'buckle': 5551, 'shall': 5552, 'drinks': 5553, 'wayfarers': 5554, 'discuss': 5555, 'mud': 5556, 'sweat': 5557, 'shivering': 5558, 'appalachian': 5559, 'andes': 5560, 'survivors': 5561, 'eyre': 5562, 'wuthering': 5563, 'heights': 5564, 'villette': 5565, 'descender': 5566, 'invention': 5567, 'fortunes': 5568, 'misfortunes': 5569, 'moll': 5570, 'flanders': 5571, 'oregon': 5572, 'veteran': 5573, 'sheet': 5574, 'halahala': 5575, 'anita': 5576, 'gets': 5577, 'bail': 5578, 'courts': 5579, 'microsoft®': 5580, 'he’s': 5581, 'excuses': 5582, 'epilogue': 5583, 'knife’s': 5584, 'cardiac': 5585, 'knotty': 5586, 'affairs': 5587, 'habibi': 5588, \"rutley's\": 5589, 'mineralogy': 5590, 'peoplewatching': 5591, 'desmond': 5592, 'morris': 5593, 'pivot': 5594, 'bi': 5595, \"hart's\": 5596, 'quarterly': 5597, 'bertram’s': 5598, 'globetrotter': 5599, 'colours': 5600, 'sizes': 5601, 'claimed': 5602, 'convenience': 5603, 'obsessions': 5604, 'bondage': 5605, 'deterrence': 5606, 'tsavo': 5607, 'originals': 5608, 'madagascar': 5609, 'doesn’t': 5610, 'freezing': 5611, 'renew': 5612, 'maha': 5613, 'manavas': 5614, 'nineteen': 5615, 'butter': 5616, 'moving': 5617, 'cinetech': 5618, 'thus': 5619, 'spoke': 5620, 'zarathustra': 5621, 'fighter': 5622, 'pilot': 5623, 'mp': 5624, 'anil': 5625, 'kumar': 5626, 'ali': 5627, 'travancore': 5628, 'gautama': 5629, 'explored': 5630, 'collectables': 5631, 'classes': 5632, 'luther': 5633, 'fictions': 5634, 'meandering': 5635, 'metaphorical': 5636, 'seventies': 5637, 'americana': 5638, 'psychism': 5639, 'homoeopathy': 5640, 'buildings': 5641, 'ponniyin': 5642, 'selvan': 5643, 'bharata': 5644, 'natyam': 5645, 'reactive': 5646, 'rxjava': 5647, 'asynchronous': 5648, 'event': 5649, 'warli': 5650, 'huckleberry': 5651, 'finn': 5652, 'apartment': 5653, 'unlocked': 5654, 'viable': 5655, 'platter': 5656, 'wrestling': 5657, 'frost': 5658, 'starlight': 5659, 'thorns': 5660, 'vinod': 5661, 'jungfrau': 5662, 'ayesha': 5663, 'ferrari': 5664, 'mans': 5665, 'feet': 5666, 'module': 5667, 'maximize': 5668, 'achievements': 5669, 'propaganda': 5670, 'freeing': 5671, 'owner': 5672, 'scrum': 5673, 'scrumn': 5674, 'tcs': 5675, 'outnumbered': 5676, 'fake': 5677, 'filter': 5678, 'bubbles': 5679, 'pagan': 5680, '9b': 5681, 'lulu': 5682, 'stevenson': 5683, 'flea': 5684, \"vai's\": 5685, 'luck': 5686, '83': 5687, 'neurobic': 5688, 'prevent': 5689, \"human's\": 5690, 'persuasion': 5691, 'risalo': 5692, 'khushwant': 5693, \"singh's\": 5694, 'watercolour': 5695, 'logs': 5696, 'rano': 5697, 'phulo': 5698, 'splinter': 5699, 'cell': 5700, 'checkmate': 5701, 'fab': 5702, 'curzon': 5703, 'dirac': 5704, 'explorers': 5705, 'ict': 5706, 'belly': 5707, 'ivan': 5708, 'ilyich': 5709, 'kalahari': 5710, 'typing': 5711, 'doctrine': 5712, 'captaincy': 5713, 'leagues': 5714, 'roswell': 5715, 'shipton': 5716, 'tilman': 5717, 'dic': 5718, 'attendant': 5719, 'subhash': 5720, 'sorrow': 5721, '4a': 5722, 'stormy': 5723, 'able': 5724, 'palace': 5725, 'treason': 5726, 'jennifer': 5727, 'lawrence': 5728, 'loveliest': 5729, 'unpredictable': 5730, 'claws': 5731, 'bug': 5732, 'rewriting': 5733, 'consultant': 5734, 'linda': 5735, 'seger': 5736, 'swiped': 5737, 'saw': 5738, 'mikhail': 5739, 'tal': 5740, 'stella': 5741, 'adler': 5742, 'applause': 5743, 'competition': 5744, 'people’s': 5745, 'workplace': 5746, 'genesis': 5747, 'basketball': 5748, 'nba': 5749, 'according': 5750, 'floor': 5751, 'unreal': 5752, 'elections': 5753, \"spider's\": 5754, 'indica': 5755, 'subcontinent': 5756, 'rang': 5757, 'basanti': 5758, 'squirrel': 5759, 'seized': 5760, 'fault': 5761, 'cutthroat': 5762, 'isaac': 5763, 'metaphor': 5764, 'lik': 5765, 'compilation': 5766, 'analogies': 5767, 'similes': 5768, 'zakir': 5769, 'hussain': 5770, 'lower': 5771, 'heal': 5772, 'llewellyn': 5773, 'assessment': 5774, 'diplomat': 5775, 'marshmallow': 5776, \"lover's\": 5777, 'tea': 5778, 'visualized': 5779, 'craving': 5780, 'base': 5781, 'vedic': 5782, 'buddhist': 5783, 'monasteries': 5784, 'ghats': 5785, 'rovers': 5786, \"1980's\": 5787, 'outsider': 5788, 'dowry': 5789, 'electronics': 5790, 'eight': 5791, 'meena': 5792, 'kumari': 5793, '32': 5794, 'circuit': 5795, 'advocate': 5796, 'rev': 5797, 'kapilavastu': 5798, 'she’s': 5799, '“learned”': 5800, 'cure': 5801, 'hellboy': 5802, 'seed': 5803, 'zenda': 5804, 'princely': 5805, 'accidentally': 5806, 'stanley': 5807, 'kubrick': 5808, 'bibliotheca': 5809, 'universalis': 5810, '1b': 5811, 'intensive': 5812, 'scalable': 5813, 'maintainable': 5814, 'prodigal': 5815, 'cinematographer': 5816, 'soumendu': 5817, 'yew': 5818, 'superpower': 5819, 'annapurna': 5820, '8000': 5821, 'metre': 5822, 'finite': 5823, 'viesturs': 5824, '000': 5825, 'meter': 5826, \"higdon's\": 5827, 'survivor': 5828, 'competitions': 5829, 'plinth': 5830, 'paramount': 5831, 'button': 5832, 'corridor': 5833, 'sean': 5834, 'dillon': 5835, 'twins': 5836, 'tihar': 5837, '764': 5838, 'administering': 5839, 'electrical': 5840, 'lockwood': 5841, '1914': 5842, 'sharp': 5843, 'atlantic': 5844, 'limited': 5845, 'amy': 5846, 'vallée': 5847, 'desires': 5848, 'dungeons': 5849, 'dungeon': 5850, 'roleplaying': 5851, \"clockmaker's\": 5852, 'buster': 5853, 'kasab': 5854, 'ronin': 5855, 'decide': 5856, 'connect': 5857, 'sale': 5858, 'plotting': 5859, 'oatmeal': 5860, 'mexican': 5861, 'lambs': 5862, 'systematic': 5863, '371': 5864, 'harmonized': 5865, 'chorales': 5866, 'chorale': 5867, 'figured': 5868, 'bass': 5869, \"hitchhiker's\": 5870, 'hitchhikers': 5871, 'dismas': 5872, 'hardy': 5873, 'fiske': 5874, 'telegram': 5875, 'nixon': 5876, 'momente': 5877, 'münchen': 5878, \"gruffalo's\": 5879, 'enough': 5880, 'trick': 5881, 'insanely': 5882, 'devious': 5883, 'interviewing': 5884, 'nada': 5885, 'brahma': 5886, 'proverbs': 5887, 'imagining': 5888, 'sultan': 5889, 'ascension': 5890, 'sujit': 5891, 'mukherjee': 5892, 'demythsifying': 5893, 'chakra': 5894, 'bma': 5895, 'lobster': 5896, 'pretend': 5897, 'birla': 5898, 'hypothetical': 5899, 'interrupted': 5900, 'quebert': 5901, 'revise': 5902, 'hali': 5903, 'arithmetic': 5904, 'ssc': 5905, 'railway': 5906, 'aadhaar': 5907, 'brother': 5908, 'radical': 5909, 'known': 5910, 'kanshiram': 5911, 'dalits': 5912, 'marigold': 5913, 'discovery': 5914, 'highlights': 5915, 'pictures®': 5916, 'picasso': 5917, '29': 5918, 'flashcards': 5919, 'kaplan': 5920, 'iacocca': 5921, 'chairman': 5922, 'mao': 5923, \"mao's\": 5924, 'physician': 5925, 'anime': 5926, 'captivated': 5927, 'uncovering': 5928, 'depression': 5929, 'ghanta': 5930, 'topping': 5931, 'rizzoli': 5932, 'isles': 5933, 'engine': 5934, 'optimization': 5935, 'all–in–one': 5936, \"marcello's\": 5937, 'marcello': 5938, 'perspectives': 5939, 'perks': 5940, 'wallflower': 5941, 'liverpool': 5942, \"players'\": 5943, 'edn': 5944, 'undomestic': 5945, 'correctly': 5946, 'mothering': 5947, 'miracle': 5948, 'underdog': 5949, 'glocal': 5950, 'chandrababu': 5951, 'naidu': 5952, 'annelies': 5953, 'contains': 5954, 'soviets': 5955, 'bennett': 5956, 'eastward': 5957, 'engagement': 5958, 'zoology': 5959, 'debriefing': 5960, 'interrogation': 5961, 'hussein': 5962, 'nadia': 5963, 'zidane': 5964, 'tess': 5965, \"d'urbervilles\": 5966, '2025': 5967, 'welfare': 5968, 'marching': 5969, 'analysing': 5970, 'modi’s': 5971, 'midterm': 5972, 'baaja': 5973, 'dial': 5974, 'unsaid': 5975, '53': 5976, 'divination': 5977, 'gameplay': 5978, 'srsly': 5979, 'hamlet': 5980, 'omg': 5981, 'capture': 5982, 'dirt': 5983, 'solitude': 5984, 'stripes': 5985, 'sholay': 5986, 'situation': 5987, 'tonight': 5988, 'tome': 5989, 'wasteland': 5990, 'stole': 5991, 'candide': 5992, 'founders': 5993, 'larry': 5994, 'sergey': 5995, 'brin': 5996, 'heartbreaker': 5997, 'possibilities': 5998, 'worlds': 5999, \"directors'\": 6000, 'didda': 6001, 'gonefishing': 6002, 'prophet': 6003, 'shia': 6004, 'sunni': 6005, 'uses': 6006, 'effort': 6007, 'windmills': 6008, 'drive': 6009, 'motivates': 6010, 'kautilya’s': 6011, 'intellectual': 6012, 'roots': 6013, 'finds': 6014, 'brene': 6015, 'typefaces': 6016, 'inverting': 6017, 'dearly': 6018, 'departing': 6019, 'nanda': 6020, 'devi': 6021, 'authoritative': 6022, '4c': 6023, 'grand': 6024, 'sophy': 6025, 'camino': 6026, 'rainmaker': 6027, 'instructor': 6028, 'impression': 6029, 'aaron': 6030, 'falk': 6031, \"democracy's\": 6032, 'dilbert': 6033, 'principle': 6034, 'pseudocode': 6035, 'ram': 6036, 'jethmalani': 6037, 'fresh': 6038, 'droppings': 6039, 'diagon': 6040, 'alley': 6041, 'scrapbook': 6042, 'jk': 6043, 'rowlings': 6044, 'suddenly': 6045, 'siyasi': 6046, 'muslims': 6047, 'islams': 6048, 'integration': 6049, 'sourcebook': 6050, 'homage': 6051, 'fourteenth': 6052, 'deranged': 6053, 'mutant': 6054, 'goons': 6055, 'girlfriend': 6056, 'asians': 6057, 'fully': 6058, 'jew': 6059, 'unveiling': 6060, 'fuel': 6061, 'semitism': 6062, 'jewelry': 6063, 'jewelled': 6064, 'embellished': 6065, 'gai': 6066, 'jin': 6067, 'banker': 6068, 'lending': 6069, 'maryam': 6070, 'cheaters': 6071, 'bangla': 6072, 'finale': 6073, 'bjp': 6074, 'forests': 6075, 'explain': 6076, 'rekha': 6077, 'glamorous': 6078, 'naval': 6079, 'marine': 6080, 'kenyans': 6081, 'sangeeta': 6082, 'padam': 6083, 'varnam': 6084, 'precision': 6085, 'alike': 6086, '56': 6087, 'gemstones': 6088, 'fossils': 6089, 'ulysses': 6090, 'annotated': 6091, 'incident': 6092, 'disha': 6093, 'nocturnes': 6094, 'nightfall': 6095, 'rather': 6096, 'superb': 6097, \"riddell's\": 6098, 'rahul': 6099, 'dravid': 6100, 'finished': 6101, 'obsession': 6102, '1a': 6103, 'ib': 6104, 'kailash': 6105, 'nonesuch': 6106, 'foucault': 6107, 'marwaris': 6108, 'jagat': 6109, 'seth': 6110, 'birlas': 6111, 'substitution': 6112, 'physiology': 6113, 'kata': 6114, 'bench': 6115, 'defying': 6116, 'missions': 6117, 'israeli': 6118, 'periodization': 6119, \"norah's\": 6120, 'brethren': 6121, 'managing': 6122, 'hungryalists': 6123, 'poets': 6124, 'sparked': 6125, 'manu': 6126, 'tense': 6127, '9–1': 6128, 'certificate': 6129, 'lamentation': 6130, 'shardlake': 6131, 'victorian': 6132, 'oleander': 6133, 'harrowing': 6134, 'yuvraj': 6135, 'tripple': 6136, 'laurel': 6137, 'wreath': 6138, 'decentralized': 6139, 'composer': 6140, 'hellbent': 6141, 'vote': 6142, 'broker': 6143, 'clocks': 6144, 'jorasanko': 6145, 'athletic': 6146, 'jumping': 6147, 'jackal': 6148, 'chambers': 6149, 'crossword': 6150, 'setters': 6151, 'fedegraphica': 6152, 'shri': 6153, 'satcharita': 6154, 'demonetization': 6155, 'chinar': 6156, 'soonish': 6157, '8c': 6158, 'flora': 6159, 'swimsuit': 6160, 'sauptik': 6161, 'matches': 6162, 'utter': 6163, 'moms': 6164, 'exhibition': 6165, 'printing': 6166, 'hardcover': 6167, 'served': 6168, 'searchers': 6169, 'margaret': 6170, 'hilda': 6171, 'roberts': 6172, 'lilian': 6173, 'townsend': 6174, 'essence': 6175, 'grundrisse': 6176, 'done': 6177, 'degree': 6178, 'womens': 6179, 'incurable': 6180, 'um': 6181, 'closet': 6182, 'alcoholic': 6183, 'neck': 6184, 'ur': 6185, 'writitng': 6186, 'firs': 6187, 'singapore': 6188, 'boom': 6189, 'safely': 6190, 'endangered': 6191, 'frederick': 6192, 'noad': 6193, 'abiotic': 6194, 'stones': 6195, 'logos': 6196, 'slender': 6197, 'escaping': 6198, 'supermarketwala': 6199, 'consumer': 6200, 'deafo': 6201, 'recon': 6202, 'commitment': 6203, 'swiss': 6204, 'robinson': 6205, 'berger': 6206, 'sodom': 6207, 'gomorrah': 6208, 'unreasonable': 6209, 'fellows': 6210, \"sita's\": 6211, 'estate': 6212, 'rhett': 6213, \"link's\": 6214, 'mythicality': 6215, 'curiosity': 6216, 'tomfoolery': 6217, 'enigma': 6218, 'mahendra': 6219, 'vocab': 6220, 'we’ll': 6221, 'newsroom': 6222, 'shepherd': 6223, 'vindication': 6224, \"caesar's\": 6225, 'rivalry': 6226, '50th': 6227, 'cigars': 6228, 'streetfight': 6229, 'styled': 6230, 'arranging': 6231, 'rooms': 6232, 'tabletops': 6233, 'bookshelves': 6234, 'unlawful': 6235, 'olive': 6236, 'greens': 6237, 'observing': 6238, '1206': 6239, '1526': 6240, 'fools': 6241, \"gamer's\": 6242, 'linear': 6243, 'doab': 6244, 'dil': 6245, 'phelps': 6246, 'redefine': 6247, 'flowing': 6248, 'precursive': 6249, 'bat': 6250, 'ugly': 6251, 'beauchamp': 6252, 'agatha': 6253, 'christie': 6254, 'moghul': 6255, 'ruler': 6256, 'administrative': 6257, 'kimi': 6258, 'raikkonen': 6259, 'trojan': 6260, 'whose': 6261, 'famously': 6262, 'kathak': 6263, 'vegetables': 6264, '1979': 6265, 'extra': 6266, 'excellent': 6267, 'stencil': 6268, 'vmware': 6269, 'vsphere': 6270, 'datacenter': 6271, 'sinner': 6272, 'shiver': 6273, 'associated': 6274, 'dahlia': 6275, 'bodybuilder': 6276, 'gauntlet': 6277, 'laughing': 6278, '1932–1956': 6279, 'watchmen': 6280, 'fitz': 6281, 'novelization': 6282, 'l': 6283, 'rainbows': 6284, 'quarto': 6285, 'penguine': 6286, 'torchbearers': 6287, 'trailblazers': 6288, 'quiver': 6289, 'arrows': 6290, 'rti': 6291, 'investigation': 6292, 'savaging': 6293, 'civilized': 6294, 'verrier': 6295, 'elwin': 6296, 'tribals': 6297, 'slower': 6298, 'sheep': 6299, 'russians': 6300, 'lilies': 6301, 'feb': 6302, 'dismantle': 6303, 'rhymes': 6304, 'whimsy': 6305, 'abol': 6306, 'tabol': 6307, 'investigative': 6308, 'satire': 6309, 'sweetie': 6310, \"poet's\": 6311, 'combinations': 6312, 'heavier': 6313, 'cobain': 6314, \"koerner's\": 6315, 'ultrarunning': 6316, '50k': 6317, 'miles': 6318, 'points': 6319, 'challanges': 6320, 'thankless': 6321, 'beartown': 6322, 'binding': 6323, 'kamisama': 6324, 'teeth': 6325, 'expect': 6326, 'expecting': 6327, 'labours': 6328, 'brainless': 6329, '4e': 6330, 'haunting': 6331, 'brazen': 6332, 'ladies': 6333, 'rocked': 6334, 'kicked': 6335, \"hornet's\": 6336, 'nest': 6337, 'westerns': 6338, 'belgium': 6339, \"foucault's\": 6340, 'pendulum': 6341, 'monolingual': 6342, 'frederica': 6343, 'villians': 6344, 'wrist': 6345, 'zuckerberg': 6346, 'fastest': 6347, 'albert': 6348, 'morphin': 6349, 'rangers': 6350, 'dagger': 6351, 'fades': 6352, 'frieda': 6353, 'klein': 6354, 'disturb': 6355, 'ramsay': 6356, 'gemina': 6357, 'zodiac': 6358, 'pushkin': 6359, 'vertigo': 6360, 'unquiet': 6361, 'moods': 6362, \"ottokar's\": 6363, 'sceptre': 6364, 'mountaineering': 6365, 'unified': 6366, 'obsessive': 6367, 'numero': 6368, 'pather': 6369, 'panchali': 6370, 'sketchbook': 6371, 'bigger': 6372, 'argumentative': 6373, 'twilight': 6374, 'liberalism': 6375, 'babylon': 6376, 'ansible': 6377, 'automating': 6378, 'deployment': 6379, 'forevers': 6380, 'undercity': 6381, 'sunil': 6382, 'gavaskar': 6383, 'taj': 6384, 'march': 6385, 'perilous': 6386, 'interventions': 6387, 'iv': 6388, 'monsoon': 6389, 'clutch': 6390, 'determination': 6391, 'domination': 6392, 'mindset': 6393, \"metzl's\": 6394, 'staying': 6395, 'trespassing': 6396, 'specification': 6397, 'copyright': 6398, 'mistake': 6399, 'closing': 6400, 'failed': 6401, 'impoverished': 6402, 'balancing': 6403, 'astronomy': 6404, 'constitutions': 6405, 'sloth': 6406, 'archery': 6407, 'keeping': 6408, 'parliamentarian': 6409, 'revealing': 6410, 'recovering': 6411, 'mister': 6412, 'sahara': 6413, 'jnu': 6414, 'minister’s': 6415, 'horrifying': 6416, 'economix': 6417, 'mandrake': 6418, 'dailies': 6419, 'cobra': 6420, 'labyrinths': 6421, 'anne': 6422, 'lister': 6423, 'horse': 6424, 'bronze–age': 6425, 'eurasian': 6426, 'steppes': 6427, 'graduate': 6428, 'dynamo': 6429, 'amaze': 6430, 'viscount': 6431, 'bridgerton': 6432, 'mahaviri': 6433, 'chalisa': 6434, 'deewar': 6435, 'foothpath': 6436, 'ottoman': 6437, \"germany's\": 6438, 'bid': 6439, '1898': 6440, 'hickory': 6441, 'dickory': 6442, 'dock': 6443, 'streets': 6444, 'laredo': 6445, 'lonesome': 6446, 'dove': 6447, 'unsuitable': 6448, 'riga': 6449, 'wallander': 6450, 'duplicate': 6451, 'unfulfilled': 6452, 'relieving': 6453, 'mandalas': 6454, \"sicilian's\": 6455, 'rewire': 6456, 'malavikagnimitram': 6457, 'dancer': 6458, 'duty': 6459, 'voynich': 6460, 'opposites': 6461, 'woodland': 6462, 'adventurous': 6463, 'rites': 6464, 'mean': 6465, 'spacemice': 6466, 'sled': 6467, 'athletes': 6468, \"grafity's\": 6469, 'fairytale': 6470, 'opposition': 6471, 'spatial': 6472, 'arcgis': 6473, 'js': 6474, \"jojo's\": 6475, 'li': 6476, 'ox': 6477, 'stands': 6478, 'cryptocurrency': 6479, 'vine': 6480, 'khullam': 6481, 'khulla': 6482, 'rishi': 6483, 'kapoor': 6484, 'uncensored': 6485, 'crow': 6486, 'dixon': 6487, 'lyrics': 6488, 'hounded': 6489, 'druid': 6490, 'nawabs': 6491, 'nudes': 6492, 'noodles': 6493, 'junji': 6494, \"ito's\": 6495, 'yon': 6496, 'mu': 6497, 'sentence': 6498, 'sardar': 6499, 'stephen': 6500, 'hawking': 6501, 'websites': 6502, 'butler': 6503, 'comes': 6504, 'swans': 6505, 'prayer': 6506, \"walker's\": 6507, 'outdoor': 6508, 'clues': 6509, 'stalker': 6510, 'invent': 6511, 'cult': 6512, 'clout': 6513, 'downfall': 6514, 'asaram': 6515, 'bapu': 6516, 'wirebound': 6517, 'focus': 6518, 'democratic': 6519, 'slanguage': 6520, 'engaging': 6521, \"psychic's\": 6522, 'intuition': 6523, 'aurora': 6524, 'divided': 6525, 'necessary': 6526, 'tragicomic': 6527, 'nata': 6528, 'vedas': 6529, 'overshare': 6530, 'armed': 6531, 'courting': 6532, 'abyss': 6533, 'knots': 6534, 'crosses': 6535, 'prosecutor’s': 6536, 'punishment': 6537, 'empresses': 6538, 'begums': 6539, 'daitya': 6540, 'spectacle': 6541, '256': 6542, 'sketching': 6543, 'snooker': 6544, 'shots': 6545, 'tail': 6546, 'diplomacy': 6547, 'gingerbread': 6548, 'litigators': 6549, \"footballer's\": 6550, 'soccer': 6551, 'iq': 6552, 'doesnt': 6553, 'evolving': 6554, 'subramanian': 6555, 'swamy': 6556, 'emoji': 6557, 'amir': 6558, 'carrick': 6559, 'whiteout': 6560, 'umbrella': 6561, 'dallas': 6562, 'fi': 6563, 'totto': 6564, 'chan': 6565, 'trucks': 6566, 'dragsters': 6567, 'dune': 6568, 'buggies': 6569, 'choppers': 6570, 'mindful': 6571, 'athlete': 6572, 'kaoboys': 6573, 'zeus': 6574, 'grants': 6575, 'stupid': 6576, 'bullshit': 6577, 'mobile': 6578, 'vikings': 6579, 'sexiest': 6580, 'sunset': 6581, 'central': 6582, 'keepers': 6583, 'pointers': 6584, '8a': 6585, 'sunny': 6586, 'sharer': 6587, 'sleigh': 6588, 'bells': 6589, 'ariba': 6590, 'functionality': 6591, 'implementation': 6592, 'drag': 6593, 'hell…': 6594, 'shutter': 6595, 'handbooks': 6596, \"'tomorrow'\": 6597, 'gives': 6598, \"'today'\": 6599, 'stave': 6600, 'freindship': 6601, 'parrish': 6602, 'identification': 6603, 'documents': 6604, 'kerala': 6605, 'often': 6606, 'thor': 6607, 'mangog': 6608, 'maugham': 6609, 'palaces': 6610, 'rifle': 6611, 'talon': 6612, 'chantry': 6613, 'terrifying': 6614, 'tippy': 6615, 'tinkletrousers': 6616, 'kohinoor': 6617, 'infamous': 6618, 'largest': 6619, 'lambdas': 6620, 'possession': 6621, 'desirable': 6622, 'malhotra': 6623, 'rejecting': 6624, 'fittest': 6625, 'cavemice': 6626, 'paws': 6627, 'tragical': 6628, 'comical': 6629, 'sailor': 6630, 'pearls': 6631, 'gangtok': 6632, 'assassinations': 6633, 'reptile': 6634, 'ivanov': 6635, 'seagull': 6636, 'cherry': 6637, 'orchard': 6638, 'maa': 6639, 'crushing': 6640, 'corridors': 6641, '555': 6642, \"squash's\": 6643, \"sport's\": 6644, 'unbeaten': 6645, 'empires': 6646, 'edison': 6647, 'westinghouse': 6648, 'electrify': 6649, 'alaska': 6650, 'vanara': 6651, 'baali': 6652, 'sugreeva': 6653, 'tara': 6654, 'debut': 6655, 'which': 6656, 'send': 6657, 'rollercoaster': 6658, 'instinct': 6659, 'creates': 6660, 'l210': 6661, 'institute': 6662, \"babylon's\": 6663, 'wartime': 6664, 'zurich': 6665, '1953': 6666, 'literally': 6667, 'collocation': 6668, 'splunk': 6669, 'demystify': 6670, 'datasets': 6671, 'reports': 6672, 'sharing': 6673, 'unfinished': 6674, 'pashmina': 6675, 'nypd': 6676, 'loving': 6677, 'rafael': 6678, 'nadal': 6679, 'careers': 6680, 'durbar': 6681, 'apocalypse': 6682, 'jigs': 6683, 'fixtures': 6684, 'audiences': 6685, 'radically': 6686, 'businesses': 6687, 'shikari': 6688, \"shambu's\": 6689, 'escapades': 6690, 'anyway…': 6691, 'quotation': 6692, 'morale': 6693, 'judgements': 6694, 'twinkle': 6695, \"soldier's\": 6696, 'enemies': 6697, 'near': 6698, '2a': 6699, 'badalte': 6700, 'gaon': 6701, 'badalta': 6702, 'dehat': 6703, 'गाँव': 6704, 'देहात': 6705, 'nayi': 6706, 'samajikta': 6707, 'ka': 6708, 'uday': 6709, 'नयी': 6710, 'सामाजिकता': 6711, 'का': 6712, 'उदय': 6713, 'hygge': 6714, 'range': 6715, 'generalists': 6716, 'specialized': 6717, \"girlfriend's\": 6718, 'hilarious': 6719, 'heartbreaking': 6720, 'misinformed': 6721, 'servicenow': 6722, 'swears': 6723, 'fiddler': 6724, 'subway': 6725, 'violinist': 6726, 'handouts': 6727, 'foremost': 6728, 'velvet': 6729, 'gloves': 6730, 'morning': 6731, 'heretic': 6732, 'reformation': 6733, 'pixar': 6734, 'slytherin': 6735, 'bhutan': 6736, 'eastern': 6737, 'indigo': 6738, 'mummies': 6739, 'jeans': 6740, 'ganga': 6741, 'constant': 6742, 'arms': 6743, 'examined': 6744, 'karan': 6745, 'reprint': 6746, 'interfacing': 6747, \"smith's\": 6748, 'umpiring': 6749, 'flavours': 6750, \"tycoon's\": 6751, 'outrageous': 6752, 'proposal': 6753, 'absolutely': 6754, 'nasty': 6755, 'inception': 6756, 'andy': 6757, 'murray': 6758, 'wimbledon': 6759, 'garbage': 6760, 'website': 6761, 'develop': 6762, 'commerce': 6763, 'store': 6764, 'botham': 6765, 'ackroyd': 6766, 'relic': 6767, \"sister's\": 6768, \"insider's\": 6769, 'topic': 6770, 'gk': 6771, \"destiny's\": 6772, 'snyder': 6773, 'greg': 6774, 'capullo': 6775, 'judicial': 6776, 'immersion': 6777, 'beneath': 6778, 'mia': 6779, 'injustice': 6780, '54': 6781, 'phew': 6782, 'x•y': 6783, 'service': 6784, 'continent': 6785, 'insects': 6786, 'spiders': 6787, 'kanha': 6788, 'reserve': 6789, 'optimizing': 6790, 'boot': 6791, 'karmyoddha': 6792, 'crashes': 6793, 'satya': 6794, 'kahun': 6795, 'toh': 6796, 'obsessed': 6797, 'breakheart': 6798, 'pass': 6799, 'nowhere': 6800, 'wave': 6801, 'nikki': 6802, 'mythic': 6803, 'zombie': 6804, 'protection': 6805, 'sneaker': 6806, 'founded': 6807, 'adidas': 6808, 'puma': 6809, 'feud': 6810, 'ear': 6811, 'exceed': 6812, 'keeps': 6813, \"wasn't\": 6814, 'hers': 6815, 'oprah': 6816, 'fascism': 6817, 'algorithm': 6818, 'readers’': 6819, 'curfewed': 6820, 'harm': 6821, 'naga': 6822, 'village': 6823, 'remembered': 6824, 'september': 6825, 'expressing': 6826, 'swedish': 6827, 'philographics': 6828, '10a': 6829, 'amateurs': 6830, 'medal': 6831, 'crush': 6832, 'rankin': 6833, \"leonard's\": 6834, 'void': 6835, 'licken': 6836, 'luggage': 6837, '1q84': 6838, 'spot': 6839, 'mahatma': 6840, 'pitch': 6841, 'deploy': 6842, 'responsive': 6843, 'garrincha': 6844, \"brazil's\": 6845, 'footballing': 6846, 'summits': 6847, 'heroism': 6848, 'plot': 6849, 'establishment': 6850, 'benjamin': 6851, 'franklin': 6852, 'frcr': 6853, 'specialty': 6854, 'finesse': 6855, '120': 6856, 'yards': 6857, 'sudeep': 6858, 'nagarkar': 6859, 'dilip': 6860, 'shanghvi': 6861, 'decode': 6862, 'accelerate': 6863, 'organizations': 6864, 'harvey': 6865, \"penick's\": 6866, 'grain': 6867, 'polish': 6868, 'prosecutor': 6869, 'szacki': 6870, 'departures': 6871, 'nano': 6872, 'housing': 6873, 'dv': 6874, 'undefined': 6875, 'monkeys': 6876, 'links': 6877, 'dalai': 6878, 'experements': 6879, 'pandemic': 6880, 'tally': 6881, 'erp': 6882, 'reversible': 6883, 'retelling': 6884, 'creatures': 6885, '808': 6886, 'unimaginable': 6887, 'sacrifice': 6888, 'mogul': 6889, 'homeopathic': 6890, 'therapeutics': 6891, \"rowling's\": 6892, 'gallery': 6893, 'curiosities': 6894, 'brushing': 6895, \"cartoonist's\": 6896, 'sharpe': 6897, 'sharpe’s': 6898, 'seringapatam': 6899, '1799': 6900, 'transformative': 6901, 'destination': 6902, 'phillip': 6903, 'marlowe': 6904, 'gamestorming': 6905, 'rulebreakers': 6906, 'changemakers': 6907, 'shelter': 6908, 'styles': 6909, 'rewired': 6910, 'reinvent': 6911, 'stability': 6912, 'bel': 6913, 'ami': 6914, 'diffusion': 6915, 'innovations': 6916, 'nancy': 6917, 'drew': 6918, 'clue': 6919, 'ornament': 6920, 'hector': 6921, 'honey': 6922, 'ks': 6923, \"queen's\": 6924, 'rani': 6925, 'jhansee': 6926, 'mutiny': 6927, '500': 6928, 'mixing': 6929, 'acrylic': 6930, 'precise': 6931, 'lifes': 6932, 'devadatta': 6933, 'amazonia': 6934, 'compiler': 6935, 'reborn': 6936, \"golwalkar's\": 6937, 'nationhood': 6938, 'defined': 6939, 'critique': 6940, 'negotiate': 6941, 'tops': 6942, 'wilco': 6943, 'riding': 6944, 'begin': 6945, 'oscar': 6946, 'jaws': 6947, 'ijeawele': 6948, 'suggestions': 6949, 'bodily': 6950, 'robots': 6951, 'unemployment': 6952, 'shambu': 6953, 'phonetics': 6954, 'gervase': 6955, 'bets': 6956, 'dad': 6957, \"son's\": 6958, 'panchatantra': 6959, 'newton': 6960, 'spread': 6961, 'nouns': 6962, 'khali': 6963, 'foolproof': 6964, 'miracles': 6965, 'rooster': 6966, 'bar': 6967, 'ivanhoe': 6968, 'wreck': 6969, 'raid': 6970, \"israel's\": 6971, 'campaign': 6972, 'denied': 6973, 'bomb': 6974, 'osborne': 6975, 'entrepreneurship': 6976, 'simplified': 6977, 'ipo': 6978, 'kumaon': 6979, 'uncollected': 6980, 'index': 6981, 'pull': 6982, \"dragon's\": 6983, 'tooth': 6984, 'cli': 6985, 'colt': 6986, 'mysooru': 6987, 'chimera': 6988, 'quite': 6989, 'hawthorne': 6990, 'daring': 6991, 'claw': 6992, 'binary': 6993, 'primal': 6994, 'chronic': 6995, 'cardio': 6996, 'carbohydrate': 6997, 'dependency': 6998, 'blackmailed': 6999, \"oliver's\": 7000, 'birmingham': 7001, 'leaf': 7002, 'vagabond': 7003, \"python's\": 7004, 'autocad': 7005, 'ladybirds': 7006, 'ups': 7007, 'egghead': 7008, 'fowl': 7009, 'pharaohs': 7010, 'fred': 7011, 'springtime': 7012, 'dx': 7013, 'beauties': 7014, 'persuaders': 7015, 'factivity': 7016, 'syama': 7017, 'prasad': 7018, 'mookerjee': 7019, 'plex': 7020, 'thinks': 7021, 'renegade': 7022, 'riddell': 7023, 'gene': 7024, \"practitioner's\": 7025, 'eliza': 7026, 'prophecy': 7027, 'frame': 7028, 'battleground': 7029, 'anthropocene': 7030, 'everymans': 7031, 'stops': 7032, 'pgecet': 7033, 'lyrebird': 7034, 'secretly': 7035, 'misadventure': 7036, 'afzal': 7037, 'idaho': 7038, 'spiritually': 7039, 'bluetooth': 7040, \"that's\": 7041, 'mesopotamia': 7042, 'supplementary': 7043, 'gotta': 7044, 'hbr': 7045, 'mews': 7046, 'familiars': 7047, 'rajpal': 7048, \"cross's\": 7049, 'gamache': 7050, 'omens': 7051, 'prophecies': 7052, 'agnes': 7053, 'nutter': 7054, 'witch': 7055, 'saina': 7056, 'nehwal': 7057, 'issues': 7058, 'ageing': 7059, 'glowing': 7060, 'skin': 7061, 'hobbit': 7062, 'fill': 7063, 'probable': 7064, 'knightsend': 7065, 'rabindrachitravali': 7066, 'whistler': 7067, \"cook's\": 7068, 'ingredients': 7069, 'naikan': 7070, 'gratitude': 7071, 'reflection': 7072, 'romancing': 7073, 'misunderstanding': 7074, 'bullet': 7075, 'affinity': 7076, 'tooling': 7077, 'yellow': 7078, 'resource': 7079, 'bkxii': 7080, 'cad': 7081, 'cam': 7082, 'kathmandu': 7083, 'elegy': 7084, 'discourse': 7085, 'labyrinth': 7086, 'flexibility': 7087, 'rehabilitation': 7088, 'abdul': 7089, 'switch': 7090, 'biological': 7091, 'powerhouse': 7092, 'harder': 7093, 'voyage': 7094, 'prose': 7095, 'madriani': 7096, 'recall': 7097, 'hawkeye': 7098, 'walton': 7099, 'enriching': 7100, 'ks1': 7101, 'pashu': 7102, \"trainer's\": 7103, 'trash': 7104, 'transfer': 7105, 'mitch': 7106, 'rapp': 7107, 'noon': 7108, 'happyness': 7109, 'postmortem': 7110, \"jefferson's\": 7111, 'eiger': 7112, 'ventures': 7113, 'beachcomber': 7114, 'feel': 7115, \"who's\": 7116, 'virginia': 7117, 'woolf': 7118, 'baptism': 7119, 'inference': 7120, 'rohingyas': 7121, 'myanmar’s': 7122, 'abduction': 7123, 'sealed': 7124, '2011': 7125, 'mehboob': 7126, 'kingfizzer': 7127, 'vijay': 7128, 'mallya': 7129, 'zog': 7130, 'doctors': 7131, 'formal': 7132, 'kate': 7133, 'chopin': 7134, 'ranthambhore': 7135, 'husbands': 7136, 'calculation': 7137, 'kakeibo': 7138, 'bloods': 7139, 'muse': 7140, 'nightmares': 7141, 'vsi': 7142, 'highway': 7143, 'rat': 7144, 'molecules': 7145, 'emotion': 7146, 'learner’s': 7147, 'multilingual': 7148, 'failure': 7149, 'peacock': 7150, 'emporium': 7151, 'grinding': 7152, \"mcdonald's\": 7153, 'dota': 7154, 'bhagwan': 7155, 'rajneesh': 7156, 'snakes': 7157, 'naxal': 7158, 'ocejwcd': 7159, 'oce': 7160, 'component': 7161, 'nike': 7162, 'kari': 7163, 'sixty': 7164, \"schirmer's\": 7165, '1071': 7166, 'practises': 7167, 'fan': 7168, 'mercy': 7169, 'atlee': 7170, 'pine': 7171, 'trylle': 7172, 'torch': 7173, 'qigong': 7174, 'treating': 7175, 'ailments': 7176, 'outcaste': 7177, 'dramatic': 7178, 'basis': 7179, 'motives': 7180, 'breathless': 7181, 'tb12': 7182, 'sustained': 7183, 'devlok': 7184, 'devdutt': 7185, 'pattanaik': 7186, 'holding': 7187, 'beirut': 7188, 'hacker′s': 7189, 'exploiting': 7190, 'flaws': 7191, '1922': 7192, 'dyes': 7193, \"president's\": 7194, 'colony': 7195, 'ripley’s': 7196, 'landon': 7197, 'implosion': 7198, 'tryst': 7199, 'thangka': 7200, 'useful': 7201, 'van': 7202, 'veeteren': 7203, 'giratina': 7204, 'yesterday': 7205, 'copperfield': 7206, 'karenina': 7207, 'folly': 7208, 'nepal': 7209, 'karnatik': 7210, '11a': 7211, '483': 7212, 'enterprises': 7213, 'teaming': 7214, \"society's\": 7215, 'toughest': 7216, 'northrups': 7217, 'dslr': 7218, 'mas': 7219, \"oyamas's\": 7220, 'gentlemen': 7221, 'isi': 7222, 'moomin': 7223, 'crystallizing': 7224, 'opinion': 7225, 'unbroken': 7226, 'moral': 7227, 'purpose': 7228, 'meru': 7229, 'publish': 7230, 'functions': 7231, 'simulations': 7232, 'tabata': 7233, 'interval': 7234, 'mail': 7235, '100th': 7236, 'undisputed': 7237, 'fade': 7238, 'aiq': 7239, 'harness': 7240, 'hiding': 7241, 'deliciously': 7242, 'suspenseful': 7243, 'etl': 7244, 'toolkit': 7245, 'extracting': 7246, 'cleaning': 7247, 'conforming': 7248, 'delivering': 7249, 'superpowers': 7250, \"dorland's\": 7251, '29e': 7252, 'assassination': 7253, 'shikhandi': 7254, 'ánd': 7255, '‘queer’': 7256, 'goat': 7257, 'surely': 7258, 'joking': 7259, 'discrete': 7260, '130': 7261, 'magazine': 7262, 'reporting': 7263, 'record': 7264, 'macroeconomics': 7265, 'critic': 7266, 'metric': 7267, 'packages': 7268, 'mossad': 7269, 'laureline': 7270, 'platform': 7271, 'harnessing': 7272, 'learned®': 7273, 'seeing': 7274, 'bollywood': 7275, 'burglary': 7276, 'fluently': 7277, 'stimulating': 7278, 'anyone': 7279, 'factor': 7280, 'malay': 7281, 'drafting': 7282, 'rx': 7283, 'procrustes': 7284, 'philosophical': 7285, 'aphorisms': 7286, 'ideal': 7287, 'guts': 7288, 'battlefield': 7289, 'sabrina': 7290, 'transformer': 7291, 'golds': 7292, 'taught': 7293, 'professionally': 7294, 'hoopa': 7295, 'hangman': 7296, 'ragdoll': 7297, 'alter': 7298, 'ego': 7299, 'livre': 7300, \"l'eleve\": 7301, 'b2': 7302, 'aptitude': 7303, 'mango': 7304, 'trees': 7305, 'maradona': 7306, 'lovelace': 7307, 'babbage': 7308, 'moeen': 7309, 'sportspersons': 7310, 'biggest': 7311, 'enabled': 7312, 'commercial': 7313, 'dhtml': 7314, 'modernism': 7315, 'surfing': 7316, 'businessman': 7317, 'unusual': 7318, 'hegarty': 7319, 'wabi': 7320, 'sabi': 7321, 'impermanence': 7322, 'whisperer': 7323, 'loyalty': 7324, 'herd': 7325, 'osamu': 7326, 'palestine': 7327, 'pronunciation': 7328, 'regained': 7329, 'carnage': 7330, '102': 7331, 'holi': 7332, 'spook’s': 7333, 'starblade': 7334, 'preface': 7335, 'wizards': 7336, 'wand': 7337, 'scala': 7338, 'isro’s': 7339, 'mars': 7340, 'ibrahimovic': 7341, 'import': 7342, 'visualize': 7343, 'gullbarga': 7344, 'bidar': 7345, 'bijapur': 7346, 'kune': 7347, 'maharajas': 7348, 'continues': 7349, 'painted': 7350, 'jamsetji': 7351, 'mothers': 7352, 'manipur': 7353, 'fatherhood': 7354, 'affirmative': 7355, 'generics': 7356, 'seconds': 7357, 'wires': 7358, 'ashok': 7359, 'betty': 7360, 'veronica': 7361, 'shattered': 7362, 'sputnik': 7363, 'sweetheart': 7364, 'architects′': 7365, 'soccernomics': 7366, 'spain': 7367, 'france': 7368, 'finally': 7369, 'prosody': 7370, 'morte': 7371, \"d'arthur\": 7372, 'brendon': 7373, 'mccullum': 7374, 'declared': 7375, 'masterful': 7376, 'selp': 7377, 'helf': 7378, 'show': 7379, 'passenger': 7380, 'forgot': 7381, 'api': 7382, 'theater': 7383, 'sorry': 7384, 'unleashing': 7385, 'professions': 7386, 'harmless': 7387, 'hitch': 7388, 'photographers': 7389, 'everyman': 7390, 'peril': 7391, 'bhojpuri': 7392, 'vice': 7393, 'behavior': 7394, 'civilization': 7395, 'eva': 7396, 'peppa': 7397, 'pig': 7398, 'sleepover': 7399, \"governor's\": 7400, 'bait': 7401, 'batsman': 7402, 'captivity': 7403, 'mahim': 7404, 'ukulele': 7405, 'la': 7406, 'liga': 7407, 'direct': 7408, 'revolutionized': 7409, 'adversary': 7410, 'innocence': 7411, 'mayada': 7412, 'iraq': 7413, 'darshan': 7414, 'monuments': 7415, 'misfit': 7416, 'ocp': 7417, '1z0–808': 7418, '1z0–809': 7419, 'article': 7420, \"'how\": 7421, \"leaders'\": 7422, 'watkins': 7423, 'trails': 7424, 'travelled': 7425, 'himachal': 7426, 'arabian': 7427, 'leatherbound': 7428, 'jatakas': 7429, \"madrigal's\": 7430, 'proven': 7431, 'prithviraj': 7432, 'chauhan': 7433, 'jesse': 7434, 'luke': 7435, 'cambodia': 7436, 'lying': 7437, 'wait': 7438, \"amateur's\": 7439, 'misconceptions': 7440, 'mastery': 7441, 'quin': 7442, \"rome's\": 7443, 'flame': 7444, 'vespasian': 7445, 'fierce': 7446, 'champawat': 7447, 'conservationist': 7448, \"bihar's\": 7449, 'caught': 7450, 'holds': 7451, 'barred': 7452, 'framework': 7453, 'getter': 7454, 'appointment': 7455, 'foods': 7456, \"anarchists'\": 7457, 'stanhope': 7458, 'dome': 7459, '1999': 7460, '2001': 7461, 'ravenclaw': 7462, 'pong': 7463, 'touched': 7464, 'unfiltered': 7465, 'balance': 7466, 'increases': 7467, 'threatens': 7468, 'maltese': 7469, 'falcon': 7470, 'pi': 7471, 'duryodhana…': 7472, \"disney's\": 7473, 'cisa': 7474, 'auditor': 7475, 'rains': 7476, 'programmed': 7477, 'madhubala': 7478, 'solver': 7479, 'recognize': 7480, 'identify': 7481, 'define': 7482, 'ppapers': 7483, 'amen': 7484, 'balloon': 7485, 'storms': 7486, 'nichijou': 7487, 'moonwalking': 7488, 'remembering': 7489, 'mcqueen': 7490, 'lover': 7491, 'banyan': 7492, 'habitats': 7493, 'tropics': 7494, 'familiar': 7495, 'important': 7496, 'uncommon': 7497, 'thoughtful': 7498, 'investor': 7499, 'abstract': 7500, 'hunchback': 7501, 'notre': 7502, 'ordered': 7503, 'truckload': 7504, 'dung': 7505, 'welcoming': 7506, \"life's\": 7507, 'difficulties': 7508, 'postcapitalism': 7509, 'engleby': 7510, 'dull': 7511, 'breakfast': 7512}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tl_df['New_Title'] = s\n",
        "tl_df.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lbZ25z0RTW3Z",
        "outputId": "8a01ded4-20cf-4f43-8a3a-2f8cbb3a6d9d"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Title   Price  New_Ratings  \\\n",
              "0                The Prisoner's Gold (The Hunters 3)  220.00            8   \n",
              "1                 Guru Dutt: A Tragedy in Three Acts  202.93           14   \n",
              "2                       Leviathan (Penguin Classics)  299.00            6   \n",
              "3                 A Pocket Full of Rye (Miss Marple)  180.00           13   \n",
              "4         LIFE 70 Years of Extraordinary Photography  965.62            1   \n",
              "5  ChiRunning: A Revolutionary Approach to Effort...  900.00            8   \n",
              "\n",
              "   New_Reviews Year_of_publish  Format_(French),Paperback2010  \\\n",
              "0          4.0            2016                              0   \n",
              "1          3.9            2012                              0   \n",
              "2          4.8            1982                              0   \n",
              "3          4.1            2017                              0   \n",
              "4          5.0            2006                              0   \n",
              "5          4.5            2009                              0   \n",
              "\n",
              "   Format_(German),Paperback2014  Format_(Kannada),Paperback2014  \\\n",
              "0                              0                               0   \n",
              "1                              0                               0   \n",
              "2                              0                               0   \n",
              "3                              0                               0   \n",
              "4                              0                               0   \n",
              "5                              0                               0   \n",
              "\n",
              "   Format_Board book  Format_Cards  ...  \\\n",
              "0                  0             0  ...   \n",
              "1                  0             0  ...   \n",
              "2                  0             0  ...   \n",
              "3                  0             0  ...   \n",
              "4                  0             0  ...   \n",
              "5                  0             0  ...   \n",
              "\n",
              "   BookCategory_Computing, Internet & Digital Media  \\\n",
              "0                                                 0   \n",
              "1                                                 0   \n",
              "2                                                 0   \n",
              "3                                                 0   \n",
              "4                                                 0   \n",
              "5                                                 0   \n",
              "\n",
              "   BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "0                                       0                    0   \n",
              "1                                       0                    0   \n",
              "2                                       0                    1   \n",
              "3                                       1                    0   \n",
              "4                                       0                    0   \n",
              "5                                       0                    0   \n",
              "\n",
              "   BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "0                                             0                      0   \n",
              "1                                             0                      0   \n",
              "2                                             0                      0   \n",
              "3                                             0                      0   \n",
              "4                                             0                      0   \n",
              "5                                             0                      0   \n",
              "\n",
              "   BookCategory_Romance  BookCategory_Sports  \\\n",
              "0                     0                    0   \n",
              "1                     0                    0   \n",
              "2                     0                    0   \n",
              "3                     0                    0   \n",
              "4                     0                    0   \n",
              "5                     0                    1   \n",
              "\n",
              "                                  Synopsis_sequences        New_Author  \\\n",
              "0  [1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...         [59, 556]   \n",
              "1  [4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...       [248, 1814]   \n",
              "2  [400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...        [60, 1061]   \n",
              "3  [4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...          [12, 13]   \n",
              "4  [8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...  [557, 428, 1815]   \n",
              "5  [1, 645, 120, 3, 1, 141, 4, 1976, 953, 15, 289...      [1062, 1063]   \n",
              "\n",
              "                                      New_Title  \n",
              "0                   [1, 3690, 493, 1, 3691, 24]  \n",
              "1           [1743, 2359, 4, 1127, 6, 211, 1744]  \n",
              "2                                [2360, 29, 16]  \n",
              "3              [4, 327, 837, 2, 1745, 451, 664]  \n",
              "4                  [26, 1128, 184, 2, 539, 240]  \n",
              "5  [3692, 4, 959, 289, 5, 2361, 1746, 665, 349]  \n",
              "\n",
              "[6 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b66c7804-2629-440f-a372-4858459c3cdf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "      <th>Synopsis_sequences</th>\n",
              "      <th>New_Author</th>\n",
              "      <th>New_Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Prisoner's Gold (The Hunters 3)</td>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 5008, 896, 6, 33, 581, 382, 116, 15, 1, 36...</td>\n",
              "      <td>[59, 556]</td>\n",
              "      <td>[1, 3690, 493, 1, 3691, 24]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Guru Dutt: A Tragedy in Three Acts</td>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 6296, 1011, 3, 4, 1950, 1099, 8, 1275, 159...</td>\n",
              "      <td>[248, 1814]</td>\n",
              "      <td>[1743, 2359, 4, 1127, 6, 211, 1744]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leviathan (Penguin Classics)</td>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[400, 1, 55, 262, 353, 281, 4, 525, 145, 5, 37...</td>\n",
              "      <td>[60, 1061]</td>\n",
              "      <td>[2360, 29, 16]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A Pocket Full of Rye (Miss Marple)</td>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[4, 4369, 3, 7, 330, 6, 1, 1857, 3, 4, 1162, 8...</td>\n",
              "      <td>[12, 13]</td>\n",
              "      <td>[4, 327, 837, 2, 1745, 451, 664]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIFE 70 Years of Extraordinary Photography</td>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[8, 672, 613, 36, 23, 51, 804, 1, 37, 9, 56, 5...</td>\n",
              "      <td>[557, 428, 1815]</td>\n",
              "      <td>[26, 1128, 184, 2, 539, 240]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ChiRunning: A Revolutionary Approach to Effort...</td>\n",
              "      <td>900.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 645, 120, 3, 1, 141, 4, 1976, 953, 15, 289...</td>\n",
              "      <td>[1062, 1063]</td>\n",
              "      <td>[3692, 4, 959, 289, 5, 2361, 1746, 665, 349]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows × 49 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b66c7804-2629-440f-a372-4858459c3cdf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b66c7804-2629-440f-a372-4858459c3cdf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b66c7804-2629-440f-a372-4858459c3cdf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fb0d6922-a3bc-4d96-8dce-c7d97c9fc767\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb0d6922-a3bc-4d96-8dce-c7d97c9fc767')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fb0d6922-a3bc-4d96-8dce-c7d97c9fc767 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tl_df = tl_df.drop(['Title'],axis=1)\n",
        "tl_df.tail(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VWbKrD1VTcjH",
        "outputId": "5656401c-e4c2-461e-eea1-c189ad4bcf65"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
              "5696  449.0           28          4.1            2016   \n",
              "5697  108.0            1          1.0            2008   \n",
              "5698   99.0            7          4.5            2016   \n",
              "\n",
              "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
              "5696                              0                              0   \n",
              "5697                              0                              0   \n",
              "5698                              0                              0   \n",
              "\n",
              "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
              "5696                               0                  0             0   \n",
              "5697                               0                  0             0   \n",
              "5698                               0                  0             0   \n",
              "\n",
              "      Format_Flexibound  ...  \\\n",
              "5696                  0  ...   \n",
              "5697                  0  ...   \n",
              "5698                  0  ...   \n",
              "\n",
              "      BookCategory_Computing, Internet & Digital Media  \\\n",
              "5696                                                 0   \n",
              "5697                                                 0   \n",
              "5698                                                 0   \n",
              "\n",
              "      BookCategory_Crime, Thriller & Mystery  BookCategory_Humour  \\\n",
              "5696                                       1                    0   \n",
              "5697                                       1                    0   \n",
              "5698                                       0                    1   \n",
              "\n",
              "      BookCategory_Language, Linguistics & Writing  BookCategory_Politics  \\\n",
              "5696                                             0                      0   \n",
              "5697                                             0                      0   \n",
              "5698                                             0                      0   \n",
              "\n",
              "      BookCategory_Romance  BookCategory_Sports  \\\n",
              "5696                     0                    0   \n",
              "5697                     0                    0   \n",
              "5698                     0                    0   \n",
              "\n",
              "                                     Synopsis_sequences    New_Author  \\\n",
              "5696  [1, 1300, 563, 23, 51, 1837, 4, 207, 8, 2208, ...     [74, 144]   \n",
              "5697  [3278, 23, 4, 207, 14, 7, 1, 52, 3, 3278, 4, 4...   [973, 1494]   \n",
              "5698  [6344, 143, 34, 5, 2792, 80, 92, 5, 34, 12, 68...  [1790, 1791]   \n",
              "\n",
              "                                              New_Title  \n",
              "5696                               [1, 63, 953, 2, 657]  \n",
              "5697                                             [7510]  \n",
              "5698  [390, 7511, 231, 169, 1225, 50, 7512, 29, 106,...  \n",
              "\n",
              "[3 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-735e173a-5340-452e-b42c-3eb8f5bd3e78\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>Format_Flexibound</th>\n",
              "      <th>...</th>\n",
              "      <th>BookCategory_Computing, Internet &amp; Digital Media</th>\n",
              "      <th>BookCategory_Crime, Thriller &amp; Mystery</th>\n",
              "      <th>BookCategory_Humour</th>\n",
              "      <th>BookCategory_Language, Linguistics &amp; Writing</th>\n",
              "      <th>BookCategory_Politics</th>\n",
              "      <th>BookCategory_Romance</th>\n",
              "      <th>BookCategory_Sports</th>\n",
              "      <th>Synopsis_sequences</th>\n",
              "      <th>New_Author</th>\n",
              "      <th>New_Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>449.0</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[1, 1300, 563, 23, 51, 1837, 4, 207, 8, 2208, ...</td>\n",
              "      <td>[74, 144]</td>\n",
              "      <td>[1, 63, 953, 2, 657]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>108.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[3278, 23, 4, 207, 14, 7, 1, 52, 3, 3278, 4, 4...</td>\n",
              "      <td>[973, 1494]</td>\n",
              "      <td>[7510]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>99.0</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[6344, 143, 34, 5, 2792, 80, 92, 5, 34, 12, 68...</td>\n",
              "      <td>[1790, 1791]</td>\n",
              "      <td>[390, 7511, 231, 169, 1225, 50, 7512, 29, 106,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-735e173a-5340-452e-b42c-3eb8f5bd3e78')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-735e173a-5340-452e-b42c-3eb8f5bd3e78 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-735e173a-5340-452e-b42c-3eb8f5bd3e78');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c872424-375e-4b81-8812-129fce7e562a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c872424-375e-4b81-8812-129fce7e562a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c872424-375e-4b81-8812-129fce7e562a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversion"
      ],
      "metadata": {
        "id": "migy9ENSq_DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "con_df = tl_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "ev6rHnwGxlRv"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(con_df,column):\n",
        "  max_length = max(len(v) for v in con_df[column])\n",
        "  new_columns = [f'{column}{i + 1}' for i in range(max_length)]\n",
        "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
        "  print(con_df)"
      ],
      "metadata": {
        "id": "nk3o8-3pvADh"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversion(con_df,'New_Title')\n",
        "con_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbD4yfGfynB_",
        "outputId": "41b4187d-19e7-4655-f01c-6ead516dc896"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
            "0      220.00            8          4.0            2016   \n",
            "1      202.93           14          3.9            2012   \n",
            "2      299.00            6          4.8            1982   \n",
            "3      180.00           13          4.1            2017   \n",
            "4      965.62            1          5.0            2006   \n",
            "...       ...          ...          ...             ...   \n",
            "5694  1009.00            9          4.9            2005   \n",
            "5695   781.00            2          4.1            2016   \n",
            "5696   449.00           28          4.1            2016   \n",
            "5697   108.00            1          1.0            2008   \n",
            "5698    99.00            7          4.5            2016   \n",
            "\n",
            "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
            "0                                 0                              0   \n",
            "1                                 0                              0   \n",
            "2                                 0                              0   \n",
            "3                                 0                              0   \n",
            "4                                 0                              0   \n",
            "...                             ...                            ...   \n",
            "5694                              0                              0   \n",
            "5695                              0                              0   \n",
            "5696                              0                              0   \n",
            "5697                              0                              0   \n",
            "5698                              0                              0   \n",
            "\n",
            "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
            "0                                  0                  0             0   \n",
            "1                                  0                  0             0   \n",
            "2                                  0                  0             0   \n",
            "3                                  0                  0             0   \n",
            "4                                  0                  0             0   \n",
            "...                              ...                ...           ...   \n",
            "5694                               0                  0             0   \n",
            "5695                               0                  0             0   \n",
            "5696                               0                  0             0   \n",
            "5697                               0                  0             0   \n",
            "5698                               0                  0             0   \n",
            "\n",
            "      Format_Flexibound  ...  New_Title23  New_Title24  New_Title25  \\\n",
            "0                     0  ...            0            0            0   \n",
            "1                     0  ...            0            0            0   \n",
            "2                     0  ...            0            0            0   \n",
            "3                     0  ...            0            0            0   \n",
            "4                     0  ...            0            0            0   \n",
            "...                 ...  ...          ...          ...          ...   \n",
            "5694                  0  ...            0            0            0   \n",
            "5695                  0  ...            0            0            0   \n",
            "5696                  0  ...            0            0            0   \n",
            "5697                  0  ...            0            0            0   \n",
            "5698                  0  ...            0            0            0   \n",
            "\n",
            "      New_Title26  New_Title27  New_Title28  New_Title29  New_Title30  \\\n",
            "0               0            0            0            0            0   \n",
            "1               0            0            0            0            0   \n",
            "2               0            0            0            0            0   \n",
            "3               0            0            0            0            0   \n",
            "4               0            0            0            0            0   \n",
            "...           ...          ...          ...          ...          ...   \n",
            "5694            0            0            0            0            0   \n",
            "5695            0            0            0            0            0   \n",
            "5696            0            0            0            0            0   \n",
            "5697            0            0            0            0            0   \n",
            "5698            0            0            0            0            0   \n",
            "\n",
            "      New_Title31  New_Title32  \n",
            "0               0            0  \n",
            "1               0            0  \n",
            "2               0            0  \n",
            "3               0            0  \n",
            "4               0            0  \n",
            "...           ...          ...  \n",
            "5694            0            0  \n",
            "5695            0            0  \n",
            "5696            0            0  \n",
            "5697            0            0  \n",
            "5698            0            0  \n",
            "\n",
            "[5699 rows x 80 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
              "0      220.00            8          4.0            2016   \n",
              "1      202.93           14          3.9            2012   \n",
              "2      299.00            6          4.8            1982   \n",
              "3      180.00           13          4.1            2017   \n",
              "4      965.62            1          5.0            2006   \n",
              "...       ...          ...          ...             ...   \n",
              "5694  1009.00            9          4.9            2005   \n",
              "5695   781.00            2          4.1            2016   \n",
              "5696   449.00           28          4.1            2016   \n",
              "5697   108.00            1          1.0            2008   \n",
              "5698    99.00            7          4.5            2016   \n",
              "\n",
              "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
              "0                                 0                              0   \n",
              "1                                 0                              0   \n",
              "2                                 0                              0   \n",
              "3                                 0                              0   \n",
              "4                                 0                              0   \n",
              "...                             ...                            ...   \n",
              "5694                              0                              0   \n",
              "5695                              0                              0   \n",
              "5696                              0                              0   \n",
              "5697                              0                              0   \n",
              "5698                              0                              0   \n",
              "\n",
              "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
              "0                                  0                  0             0   \n",
              "1                                  0                  0             0   \n",
              "2                                  0                  0             0   \n",
              "3                                  0                  0             0   \n",
              "4                                  0                  0             0   \n",
              "...                              ...                ...           ...   \n",
              "5694                               0                  0             0   \n",
              "5695                               0                  0             0   \n",
              "5696                               0                  0             0   \n",
              "5697                               0                  0             0   \n",
              "5698                               0                  0             0   \n",
              "\n",
              "      Format_Flexibound  ...  New_Title23  New_Title24  New_Title25  \\\n",
              "0                     0  ...            0            0            0   \n",
              "1                     0  ...            0            0            0   \n",
              "2                     0  ...            0            0            0   \n",
              "3                     0  ...            0            0            0   \n",
              "4                     0  ...            0            0            0   \n",
              "...                 ...  ...          ...          ...          ...   \n",
              "5694                  0  ...            0            0            0   \n",
              "5695                  0  ...            0            0            0   \n",
              "5696                  0  ...            0            0            0   \n",
              "5697                  0  ...            0            0            0   \n",
              "5698                  0  ...            0            0            0   \n",
              "\n",
              "      New_Title26  New_Title27  New_Title28  New_Title29  New_Title30  \\\n",
              "0               0            0            0            0            0   \n",
              "1               0            0            0            0            0   \n",
              "2               0            0            0            0            0   \n",
              "3               0            0            0            0            0   \n",
              "4               0            0            0            0            0   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "5694            0            0            0            0            0   \n",
              "5695            0            0            0            0            0   \n",
              "5696            0            0            0            0            0   \n",
              "5697            0            0            0            0            0   \n",
              "5698            0            0            0            0            0   \n",
              "\n",
              "      New_Title31  New_Title32  \n",
              "0               0            0  \n",
              "1               0            0  \n",
              "2               0            0  \n",
              "3               0            0  \n",
              "4               0            0  \n",
              "...           ...          ...  \n",
              "5694            0            0  \n",
              "5695            0            0  \n",
              "5696            0            0  \n",
              "5697            0            0  \n",
              "5698            0            0  \n",
              "\n",
              "[5699 rows x 80 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bb6b1b0-2138-47dd-a79b-8a9c030b8215\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>Format_Flexibound</th>\n",
              "      <th>...</th>\n",
              "      <th>New_Title23</th>\n",
              "      <th>New_Title24</th>\n",
              "      <th>New_Title25</th>\n",
              "      <th>New_Title26</th>\n",
              "      <th>New_Title27</th>\n",
              "      <th>New_Title28</th>\n",
              "      <th>New_Title29</th>\n",
              "      <th>New_Title30</th>\n",
              "      <th>New_Title31</th>\n",
              "      <th>New_Title32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>1009.00</td>\n",
              "      <td>9</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>781.00</td>\n",
              "      <td>2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>449.00</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>108.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>99.00</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5699 rows × 80 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bb6b1b0-2138-47dd-a79b-8a9c030b8215')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bb6b1b0-2138-47dd-a79b-8a9c030b8215 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bb6b1b0-2138-47dd-a79b-8a9c030b8215');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0ecaf49-1da1-41ee-8b51-b879872093c0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0ecaf49-1da1-41ee-8b51-b879872093c0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0ecaf49-1da1-41ee-8b51-b879872093c0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversion(con_df,'New_Author')\n",
        "con_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2xab47aVyzrs",
        "outputId": "d3d081b6-ca93-4ad0-9f3a-513c73b92657"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
            "0      220.00            8          4.0            2016   \n",
            "1      202.93           14          3.9            2012   \n",
            "2      299.00            6          4.8            1982   \n",
            "3      180.00           13          4.1            2017   \n",
            "4      965.62            1          5.0            2006   \n",
            "...       ...          ...          ...             ...   \n",
            "5694  1009.00            9          4.9            2005   \n",
            "5695   781.00            2          4.1            2016   \n",
            "5696   449.00           28          4.1            2016   \n",
            "5697   108.00            1          1.0            2008   \n",
            "5698    99.00            7          4.5            2016   \n",
            "\n",
            "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
            "0                                 0                              0   \n",
            "1                                 0                              0   \n",
            "2                                 0                              0   \n",
            "3                                 0                              0   \n",
            "4                                 0                              0   \n",
            "...                             ...                            ...   \n",
            "5694                              0                              0   \n",
            "5695                              0                              0   \n",
            "5696                              0                              0   \n",
            "5697                              0                              0   \n",
            "5698                              0                              0   \n",
            "\n",
            "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
            "0                                  0                  0             0   \n",
            "1                                  0                  0             0   \n",
            "2                                  0                  0             0   \n",
            "3                                  0                  0             0   \n",
            "4                                  0                  0             0   \n",
            "...                              ...                ...           ...   \n",
            "5694                               0                  0             0   \n",
            "5695                               0                  0             0   \n",
            "5696                               0                  0             0   \n",
            "5697                               0                  0             0   \n",
            "5698                               0                  0             0   \n",
            "\n",
            "      Format_Flexibound  ...  New_Author8  New_Author9  New_Author10  \\\n",
            "0                     0  ...            0            0             0   \n",
            "1                     0  ...            0            0             0   \n",
            "2                     0  ...            0            0             0   \n",
            "3                     0  ...            0            0             0   \n",
            "4                     0  ...            0            0             0   \n",
            "...                 ...  ...          ...          ...           ...   \n",
            "5694                  0  ...            0            0             0   \n",
            "5695                  0  ...            0            0             0   \n",
            "5696                  0  ...            0            0             0   \n",
            "5697                  0  ...            0            0             0   \n",
            "5698                  0  ...            0            0             0   \n",
            "\n",
            "      New_Author11  New_Author12  New_Author13  New_Author14  New_Author15  \\\n",
            "0                0             0             0             0             0   \n",
            "1                0             0             0             0             0   \n",
            "2                0             0             0             0             0   \n",
            "3                0             0             0             0             0   \n",
            "4                0             0             0             0             0   \n",
            "...            ...           ...           ...           ...           ...   \n",
            "5694             0             0             0             0             0   \n",
            "5695             0             0             0             0             0   \n",
            "5696             0             0             0             0             0   \n",
            "5697             0             0             0             0             0   \n",
            "5698             0             0             0             0             0   \n",
            "\n",
            "      New_Author16  New_Author17  \n",
            "0                0             0  \n",
            "1                0             0  \n",
            "2                0             0  \n",
            "3                0             0  \n",
            "4                0             0  \n",
            "...            ...           ...  \n",
            "5694             0             0  \n",
            "5695             0             0  \n",
            "5696             0             0  \n",
            "5697             0             0  \n",
            "5698             0             0  \n",
            "\n",
            "[5699 rows x 97 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
              "0      220.00            8          4.0            2016   \n",
              "1      202.93           14          3.9            2012   \n",
              "2      299.00            6          4.8            1982   \n",
              "3      180.00           13          4.1            2017   \n",
              "4      965.62            1          5.0            2006   \n",
              "...       ...          ...          ...             ...   \n",
              "5694  1009.00            9          4.9            2005   \n",
              "5695   781.00            2          4.1            2016   \n",
              "5696   449.00           28          4.1            2016   \n",
              "5697   108.00            1          1.0            2008   \n",
              "5698    99.00            7          4.5            2016   \n",
              "\n",
              "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
              "0                                 0                              0   \n",
              "1                                 0                              0   \n",
              "2                                 0                              0   \n",
              "3                                 0                              0   \n",
              "4                                 0                              0   \n",
              "...                             ...                            ...   \n",
              "5694                              0                              0   \n",
              "5695                              0                              0   \n",
              "5696                              0                              0   \n",
              "5697                              0                              0   \n",
              "5698                              0                              0   \n",
              "\n",
              "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
              "0                                  0                  0             0   \n",
              "1                                  0                  0             0   \n",
              "2                                  0                  0             0   \n",
              "3                                  0                  0             0   \n",
              "4                                  0                  0             0   \n",
              "...                              ...                ...           ...   \n",
              "5694                               0                  0             0   \n",
              "5695                               0                  0             0   \n",
              "5696                               0                  0             0   \n",
              "5697                               0                  0             0   \n",
              "5698                               0                  0             0   \n",
              "\n",
              "      Format_Flexibound  ...  New_Author8  New_Author9  New_Author10  \\\n",
              "0                     0  ...            0            0             0   \n",
              "1                     0  ...            0            0             0   \n",
              "2                     0  ...            0            0             0   \n",
              "3                     0  ...            0            0             0   \n",
              "4                     0  ...            0            0             0   \n",
              "...                 ...  ...          ...          ...           ...   \n",
              "5694                  0  ...            0            0             0   \n",
              "5695                  0  ...            0            0             0   \n",
              "5696                  0  ...            0            0             0   \n",
              "5697                  0  ...            0            0             0   \n",
              "5698                  0  ...            0            0             0   \n",
              "\n",
              "      New_Author11  New_Author12  New_Author13  New_Author14  New_Author15  \\\n",
              "0                0             0             0             0             0   \n",
              "1                0             0             0             0             0   \n",
              "2                0             0             0             0             0   \n",
              "3                0             0             0             0             0   \n",
              "4                0             0             0             0             0   \n",
              "...            ...           ...           ...           ...           ...   \n",
              "5694             0             0             0             0             0   \n",
              "5695             0             0             0             0             0   \n",
              "5696             0             0             0             0             0   \n",
              "5697             0             0             0             0             0   \n",
              "5698             0             0             0             0             0   \n",
              "\n",
              "      New_Author16  New_Author17  \n",
              "0                0             0  \n",
              "1                0             0  \n",
              "2                0             0  \n",
              "3                0             0  \n",
              "4                0             0  \n",
              "...            ...           ...  \n",
              "5694             0             0  \n",
              "5695             0             0  \n",
              "5696             0             0  \n",
              "5697             0             0  \n",
              "5698             0             0  \n",
              "\n",
              "[5699 rows x 97 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-15b71374-38b3-4d24-a3c9-5e603f4b5a0c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>Format_Flexibound</th>\n",
              "      <th>...</th>\n",
              "      <th>New_Author8</th>\n",
              "      <th>New_Author9</th>\n",
              "      <th>New_Author10</th>\n",
              "      <th>New_Author11</th>\n",
              "      <th>New_Author12</th>\n",
              "      <th>New_Author13</th>\n",
              "      <th>New_Author14</th>\n",
              "      <th>New_Author15</th>\n",
              "      <th>New_Author16</th>\n",
              "      <th>New_Author17</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>1009.00</td>\n",
              "      <td>9</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>781.00</td>\n",
              "      <td>2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>449.00</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>108.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>99.00</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5699 rows × 97 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15b71374-38b3-4d24-a3c9-5e603f4b5a0c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15b71374-38b3-4d24-a3c9-5e603f4b5a0c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15b71374-38b3-4d24-a3c9-5e603f4b5a0c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed77ecaa-1c43-4475-ac86-30e5d5e5a26a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed77ecaa-1c43-4475-ac86-30e5d5e5a26a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed77ecaa-1c43-4475-ac86-30e5d5e5a26a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversion(con_df,'Synopsis_sequences')\n",
        "con_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HFVj7LjCvp16",
        "outputId": "c7418e07-b32e-462e-fbc2-8fa17b3ded12"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
            "0      220.00            8          4.0            2016   \n",
            "1      202.93           14          3.9            2012   \n",
            "2      299.00            6          4.8            1982   \n",
            "3      180.00           13          4.1            2017   \n",
            "4      965.62            1          5.0            2006   \n",
            "...       ...          ...          ...             ...   \n",
            "5694  1009.00            9          4.9            2005   \n",
            "5695   781.00            2          4.1            2016   \n",
            "5696   449.00           28          4.1            2016   \n",
            "5697   108.00            1          1.0            2008   \n",
            "5698    99.00            7          4.5            2016   \n",
            "\n",
            "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
            "0                                 0                              0   \n",
            "1                                 0                              0   \n",
            "2                                 0                              0   \n",
            "3                                 0                              0   \n",
            "4                                 0                              0   \n",
            "...                             ...                            ...   \n",
            "5694                              0                              0   \n",
            "5695                              0                              0   \n",
            "5696                              0                              0   \n",
            "5697                              0                              0   \n",
            "5698                              0                              0   \n",
            "\n",
            "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
            "0                                  0                  0             0   \n",
            "1                                  0                  0             0   \n",
            "2                                  0                  0             0   \n",
            "3                                  0                  0             0   \n",
            "4                                  0                  0             0   \n",
            "...                              ...                ...           ...   \n",
            "5694                               0                  0             0   \n",
            "5695                               0                  0             0   \n",
            "5696                               0                  0             0   \n",
            "5697                               0                  0             0   \n",
            "5698                               0                  0             0   \n",
            "\n",
            "      Format_Flexibound  ...  Synopsis_sequences2044  Synopsis_sequences2045  \\\n",
            "0                     0  ...                       0                       0   \n",
            "1                     0  ...                       0                       0   \n",
            "2                     0  ...                       0                       0   \n",
            "3                     0  ...                       0                       0   \n",
            "4                     0  ...                       0                       0   \n",
            "...                 ...  ...                     ...                     ...   \n",
            "5694                  0  ...                       0                       0   \n",
            "5695                  0  ...                       0                       0   \n",
            "5696                  0  ...                       0                       0   \n",
            "5697                  0  ...                       0                       0   \n",
            "5698                  0  ...                       0                       0   \n",
            "\n",
            "      Synopsis_sequences2046  Synopsis_sequences2047  Synopsis_sequences2048  \\\n",
            "0                          0                       0                       0   \n",
            "1                          0                       0                       0   \n",
            "2                          0                       0                       0   \n",
            "3                          0                       0                       0   \n",
            "4                          0                       0                       0   \n",
            "...                      ...                     ...                     ...   \n",
            "5694                       0                       0                       0   \n",
            "5695                       0                       0                       0   \n",
            "5696                       0                       0                       0   \n",
            "5697                       0                       0                       0   \n",
            "5698                       0                       0                       0   \n",
            "\n",
            "      Synopsis_sequences2049  Synopsis_sequences2050  Synopsis_sequences2051  \\\n",
            "0                          0                       0                       0   \n",
            "1                          0                       0                       0   \n",
            "2                          0                       0                       0   \n",
            "3                          0                       0                       0   \n",
            "4                          0                       0                       0   \n",
            "...                      ...                     ...                     ...   \n",
            "5694                       0                       0                       0   \n",
            "5695                       0                       0                       0   \n",
            "5696                       0                       0                       0   \n",
            "5697                       0                       0                       0   \n",
            "5698                       0                       0                       0   \n",
            "\n",
            "      Synopsis_sequences2052  Synopsis_sequences2053  \n",
            "0                          0                       0  \n",
            "1                          0                       0  \n",
            "2                          0                       0  \n",
            "3                          0                       0  \n",
            "4                          0                       0  \n",
            "...                      ...                     ...  \n",
            "5694                       0                       0  \n",
            "5695                       0                       0  \n",
            "5696                       0                       0  \n",
            "5697                       0                       0  \n",
            "5698                       0                       0  \n",
            "\n",
            "[5699 rows x 2150 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n",
            "<ipython-input-177-8b32da16d18d>:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  con_df[new_columns] = con_df[column].apply(lambda x: pd.Series(x + [0] * (max_length - len(x))))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Price  New_Ratings  New_Reviews Year_of_publish  \\\n",
              "0      220.00            8          4.0            2016   \n",
              "1      202.93           14          3.9            2012   \n",
              "2      299.00            6          4.8            1982   \n",
              "3      180.00           13          4.1            2017   \n",
              "4      965.62            1          5.0            2006   \n",
              "...       ...          ...          ...             ...   \n",
              "5694  1009.00            9          4.9            2005   \n",
              "5695   781.00            2          4.1            2016   \n",
              "5696   449.00           28          4.1            2016   \n",
              "5697   108.00            1          1.0            2008   \n",
              "5698    99.00            7          4.5            2016   \n",
              "\n",
              "      Format_(French),Paperback2010  Format_(German),Paperback2014  \\\n",
              "0                                 0                              0   \n",
              "1                                 0                              0   \n",
              "2                                 0                              0   \n",
              "3                                 0                              0   \n",
              "4                                 0                              0   \n",
              "...                             ...                            ...   \n",
              "5694                              0                              0   \n",
              "5695                              0                              0   \n",
              "5696                              0                              0   \n",
              "5697                              0                              0   \n",
              "5698                              0                              0   \n",
              "\n",
              "      Format_(Kannada),Paperback2014  Format_Board book  Format_Cards  \\\n",
              "0                                  0                  0             0   \n",
              "1                                  0                  0             0   \n",
              "2                                  0                  0             0   \n",
              "3                                  0                  0             0   \n",
              "4                                  0                  0             0   \n",
              "...                              ...                ...           ...   \n",
              "5694                               0                  0             0   \n",
              "5695                               0                  0             0   \n",
              "5696                               0                  0             0   \n",
              "5697                               0                  0             0   \n",
              "5698                               0                  0             0   \n",
              "\n",
              "      Format_Flexibound  ...  Synopsis_sequences2044  Synopsis_sequences2045  \\\n",
              "0                     0  ...                       0                       0   \n",
              "1                     0  ...                       0                       0   \n",
              "2                     0  ...                       0                       0   \n",
              "3                     0  ...                       0                       0   \n",
              "4                     0  ...                       0                       0   \n",
              "...                 ...  ...                     ...                     ...   \n",
              "5694                  0  ...                       0                       0   \n",
              "5695                  0  ...                       0                       0   \n",
              "5696                  0  ...                       0                       0   \n",
              "5697                  0  ...                       0                       0   \n",
              "5698                  0  ...                       0                       0   \n",
              "\n",
              "      Synopsis_sequences2046  Synopsis_sequences2047  Synopsis_sequences2048  \\\n",
              "0                          0                       0                       0   \n",
              "1                          0                       0                       0   \n",
              "2                          0                       0                       0   \n",
              "3                          0                       0                       0   \n",
              "4                          0                       0                       0   \n",
              "...                      ...                     ...                     ...   \n",
              "5694                       0                       0                       0   \n",
              "5695                       0                       0                       0   \n",
              "5696                       0                       0                       0   \n",
              "5697                       0                       0                       0   \n",
              "5698                       0                       0                       0   \n",
              "\n",
              "      Synopsis_sequences2049  Synopsis_sequences2050  Synopsis_sequences2051  \\\n",
              "0                          0                       0                       0   \n",
              "1                          0                       0                       0   \n",
              "2                          0                       0                       0   \n",
              "3                          0                       0                       0   \n",
              "4                          0                       0                       0   \n",
              "...                      ...                     ...                     ...   \n",
              "5694                       0                       0                       0   \n",
              "5695                       0                       0                       0   \n",
              "5696                       0                       0                       0   \n",
              "5697                       0                       0                       0   \n",
              "5698                       0                       0                       0   \n",
              "\n",
              "      Synopsis_sequences2052  Synopsis_sequences2053  \n",
              "0                          0                       0  \n",
              "1                          0                       0  \n",
              "2                          0                       0  \n",
              "3                          0                       0  \n",
              "4                          0                       0  \n",
              "...                      ...                     ...  \n",
              "5694                       0                       0  \n",
              "5695                       0                       0  \n",
              "5696                       0                       0  \n",
              "5697                       0                       0  \n",
              "5698                       0                       0  \n",
              "\n",
              "[5699 rows x 2150 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5790735-5b03-48f6-a741-2463daef074c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "      <th>New_Ratings</th>\n",
              "      <th>New_Reviews</th>\n",
              "      <th>Year_of_publish</th>\n",
              "      <th>Format_(French),Paperback2010</th>\n",
              "      <th>Format_(German),Paperback2014</th>\n",
              "      <th>Format_(Kannada),Paperback2014</th>\n",
              "      <th>Format_Board book</th>\n",
              "      <th>Format_Cards</th>\n",
              "      <th>Format_Flexibound</th>\n",
              "      <th>...</th>\n",
              "      <th>Synopsis_sequences2044</th>\n",
              "      <th>Synopsis_sequences2045</th>\n",
              "      <th>Synopsis_sequences2046</th>\n",
              "      <th>Synopsis_sequences2047</th>\n",
              "      <th>Synopsis_sequences2048</th>\n",
              "      <th>Synopsis_sequences2049</th>\n",
              "      <th>Synopsis_sequences2050</th>\n",
              "      <th>Synopsis_sequences2051</th>\n",
              "      <th>Synopsis_sequences2052</th>\n",
              "      <th>Synopsis_sequences2053</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>220.00</td>\n",
              "      <td>8</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>202.93</td>\n",
              "      <td>14</td>\n",
              "      <td>3.9</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>299.00</td>\n",
              "      <td>6</td>\n",
              "      <td>4.8</td>\n",
              "      <td>1982</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>180.00</td>\n",
              "      <td>13</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2017</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>965.62</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2006</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5694</th>\n",
              "      <td>1009.00</td>\n",
              "      <td>9</td>\n",
              "      <td>4.9</td>\n",
              "      <td>2005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5695</th>\n",
              "      <td>781.00</td>\n",
              "      <td>2</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5696</th>\n",
              "      <td>449.00</td>\n",
              "      <td>28</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5697</th>\n",
              "      <td>108.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2008</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5698</th>\n",
              "      <td>99.00</td>\n",
              "      <td>7</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5699 rows × 2150 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5790735-5b03-48f6-a741-2463daef074c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5790735-5b03-48f6-a741-2463daef074c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5790735-5b03-48f6-a741-2463daef074c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8393b2a8-4734-47ab-8a62-3dfffa2631e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8393b2a8-4734-47ab-8a62-3dfffa2631e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8393b2a8-4734-47ab-8a62-3dfffa2631e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "pJNenQ2XWkLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_df = con_df.copy(deep=True)"
      ],
      "metadata": {
        "id": "nv8NJwJvFZJe"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_df=model_df.drop(['New_Author','New_Title','Synopsis_sequences'],axis=1)"
      ],
      "metadata": {
        "id": "XqtfVSYAF2DI"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming 'target_column' is your target variable\n",
        "target_column = 'Price'\n",
        "\n",
        "# Assuming all columns except the target column are features\n",
        "feature_columns = [col for col in model_df.columns if col != target_column]\n",
        "\n",
        "# Convert the target column to numeric (if not already)\n",
        "#model_df[target_column] = pd.to_numeric(model_df[target_column], errors='coerce')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(model_df[feature_columns], model_df[target_column], test_size=0.2, random_state=10)\n",
        "\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model (for regression, you might use mean squared error)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFjabqfDFvNh",
        "outputId": "7309f6ae-9f3a-4312-c4da-e0764bf11945"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 299924.39\n"
          ]
        }
      ]
    }
  ]
}